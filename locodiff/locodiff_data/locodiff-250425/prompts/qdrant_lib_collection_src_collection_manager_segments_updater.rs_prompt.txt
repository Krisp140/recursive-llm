# Instructions

You are being benchmarked. You will see the output of a git log command, and from that must infer the current state of a file. Think carefully, as you must output the exact state of the file to earn full marks.

**Important:** Your goal is to reproduce the file's content *exactly* as it exists at the final commit, even if the code appears broken, buggy, or contains obvious errors. Do **not** try to "fix" the code. Attempting to correct issues will result in a poor score, as this benchmark evaluates your ability to reproduce the precise state of the file based on its history.

# Required Response Format

Wrap the content of the file in triple backticks (```). Any text outside the final closing backticks will be ignored. End your response after outputting the closing backticks.

# Example Response

```python
#!/usr/bin/env python
print('Hello, world!')
```

# File History

> git log -p --cc --topo-order --reverse -- lib/collection/src/collection_manager/segments_updater.rs

commit 446d0c29f70f1154025e644b154adbd270007290
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Sun Aug 15 23:26:01 2021 +0200

    Deadlock fix (#91)
    
    * refactor: segment managers -> collection managers
    
    * fix segments holder deadlock
    
    * apply cargo fmt
    
    * fix cargo clippy
    
    * replace sequential segment locking with multiple try_lock attempts to prevent deadlocks

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
new file mode 100644
index 000000000..e5b67b4de
--- /dev/null
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -0,0 +1,115 @@
+use crate::collection_manager::holders::segment_holder::SegmentHolder;
+use crate::operations::types::{CollectionError, CollectionResult};
+use segment::types::{
+    PayloadInterface, PayloadKeyType, PayloadKeyTypeRef, PointIdType, SeqNumberType,
+};
+use std::collections::{HashMap, HashSet};
+
+/// A collection of functions for updating points and payloads stored in segments
+pub struct SegmentsUpdater {}
+
+impl SegmentsUpdater {
+    fn check_unprocessed_points(
+        points: &[PointIdType],
+        processed: &HashSet<PointIdType>,
+    ) -> CollectionResult<usize> {
+        let missed_point = points.iter().cloned().find(|p| !processed.contains(p));
+        match missed_point {
+            None => Ok(processed.len()),
+            Some(missed_point) => Err(CollectionError::NotFound {
+                missed_point_id: missed_point,
+            }),
+        }
+    }
+
+    /// Tries to delete points from all segments, returns number of actually deleted points
+    pub fn delete_points(
+        segments: &SegmentHolder,
+        op_num: SeqNumberType,
+        ids: &[PointIdType],
+    ) -> CollectionResult<usize> {
+        let res = segments.apply_points(op_num, ids, |id, write_segment| {
+            write_segment.delete_point(op_num, id)
+        })?;
+        Ok(res)
+    }
+
+    pub fn set_payload(
+        segments: &SegmentHolder,
+        op_num: SeqNumberType,
+        payload: &HashMap<PayloadKeyType, PayloadInterface>,
+        points: &[PointIdType],
+    ) -> CollectionResult<usize> {
+        let mut updated_points: HashSet<PointIdType> = Default::default();
+
+        let res = segments.apply_points_to_appendable(op_num, points, |id, write_segment| {
+            updated_points.insert(id);
+            let mut res = true;
+            for (key, payload) in payload {
+                res = write_segment.set_payload(op_num, id, key, payload.into())? && res;
+            }
+            Ok(res)
+        })?;
+
+        SegmentsUpdater::check_unprocessed_points(points, &updated_points)?;
+        Ok(res)
+    }
+
+    pub fn delete_payload(
+        segments: &SegmentHolder,
+        op_num: SeqNumberType,
+        points: &[PointIdType],
+        keys: &[PayloadKeyType],
+    ) -> CollectionResult<usize> {
+        let mut updated_points: HashSet<PointIdType> = Default::default();
+
+        let res = segments.apply_points_to_appendable(op_num, points, |id, write_segment| {
+            updated_points.insert(id);
+            let mut res = true;
+            for key in keys {
+                res = write_segment.delete_payload(op_num, id, key)? && res;
+            }
+            Ok(res)
+        })?;
+
+        SegmentsUpdater::check_unprocessed_points(points, &updated_points)?;
+        Ok(res)
+    }
+
+    pub fn clear_payload(
+        segments: &SegmentHolder,
+        op_num: SeqNumberType,
+        points: &[PointIdType],
+    ) -> CollectionResult<usize> {
+        let mut updated_points: HashSet<PointIdType> = Default::default();
+        let res = segments.apply_points_to_appendable(op_num, points, |id, write_segment| {
+            updated_points.insert(id);
+            write_segment.clear_payload(op_num, id)
+        })?;
+
+        SegmentsUpdater::check_unprocessed_points(points, &updated_points)?;
+        Ok(res)
+    }
+
+    pub fn create_field_index(
+        segments: &SegmentHolder,
+        op_num: SeqNumberType,
+        field_name: PayloadKeyTypeRef,
+    ) -> CollectionResult<usize> {
+        let res = segments.apply_segments(op_num, |write_segment| {
+            write_segment.create_field_index(op_num, field_name)
+        })?;
+        Ok(res)
+    }
+
+    pub fn delete_field_index(
+        segments: &SegmentHolder,
+        op_num: SeqNumberType,
+        field_name: PayloadKeyTypeRef,
+    ) -> CollectionResult<usize> {
+        let res = segments.apply_segments(op_num, |write_segment| {
+            write_segment.delete_field_index(op_num, field_name)
+        })?;
+        Ok(res)
+    }
+}

commit 53ed637137eadff9ff9352d202868572e02bffc6
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Wed Aug 18 00:37:19 2021 +0200

    skip moved points (double) processing for segments updater in apply_points_to_appendable

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index e5b67b4de..82d89e076 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -28,7 +28,7 @@ impl SegmentsUpdater {
         op_num: SeqNumberType,
         ids: &[PointIdType],
     ) -> CollectionResult<usize> {
-        let res = segments.apply_points(op_num, ids, |id, write_segment| {
+        let res = segments.apply_points(op_num, ids, |id, _idx, write_segment| {
             write_segment.delete_point(op_num, id)
         })?;
         Ok(res)

commit 2cbe1d4f6b86ae6fc8b77da5f9c68ae4444d09e6
Author: Alexander Galibey <48586936+galibey@users.noreply.github.com>
Date:   Sun Aug 22 23:11:00 2021 +0300

    Decouple searcher and updater from collection (#93)

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 82d89e076..cc2fe2365 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -1,115 +1,266 @@
-use crate::collection_manager::holders::segment_holder::SegmentHolder;
-use crate::operations::types::{CollectionError, CollectionResult};
+use std::collections::{HashMap, HashSet};
+
+use parking_lot::RwLock;
+
 use segment::types::{
     PayloadInterface, PayloadKeyType, PayloadKeyTypeRef, PointIdType, SeqNumberType,
 };
-use std::collections::{HashMap, HashSet};
+
+use crate::collection_manager::holders::segment_holder::SegmentHolder;
+use crate::operations::payload_ops::PayloadOps;
+use crate::operations::point_ops::{PointInsertOperations, PointOperations};
+use crate::operations::types::{CollectionError, CollectionResult, VectorType};
+use crate::operations::FieldIndexOperations;
 
 /// A collection of functions for updating points and payloads stored in segments
-pub struct SegmentsUpdater {}
-
-impl SegmentsUpdater {
-    fn check_unprocessed_points(
-        points: &[PointIdType],
-        processed: &HashSet<PointIdType>,
-    ) -> CollectionResult<usize> {
-        let missed_point = points.iter().cloned().find(|p| !processed.contains(p));
-        match missed_point {
-            None => Ok(processed.len()),
-            Some(missed_point) => Err(CollectionError::NotFound {
-                missed_point_id: missed_point,
-            }),
-        }
+
+pub(crate) fn check_unprocessed_points(
+    points: &[PointIdType],
+    processed: &HashSet<PointIdType>,
+) -> CollectionResult<usize> {
+    let missed_point = points.iter().cloned().find(|p| !processed.contains(p));
+    match missed_point {
+        None => Ok(processed.len()),
+        Some(missed_point) => Err(CollectionError::NotFound {
+            missed_point_id: missed_point,
+        }),
     }
+}
+
+/// Tries to delete points from all segments, returns number of actually deleted points
+pub(crate) fn delete_points(
+    segments: &SegmentHolder,
+    op_num: SeqNumberType,
+    ids: &[PointIdType],
+) -> CollectionResult<usize> {
+    let res = segments.apply_points(op_num, ids, |id, _idx, write_segment| {
+        write_segment.delete_point(op_num, id)
+    })?;
+    Ok(res)
+}
 
-    /// Tries to delete points from all segments, returns number of actually deleted points
-    pub fn delete_points(
-        segments: &SegmentHolder,
-        op_num: SeqNumberType,
-        ids: &[PointIdType],
-    ) -> CollectionResult<usize> {
-        let res = segments.apply_points(op_num, ids, |id, _idx, write_segment| {
-            write_segment.delete_point(op_num, id)
-        })?;
+pub(crate) fn set_payload(
+    segments: &SegmentHolder,
+    op_num: SeqNumberType,
+    payload: &HashMap<PayloadKeyType, PayloadInterface>,
+    points: &[PointIdType],
+) -> CollectionResult<usize> {
+    let mut updated_points: HashSet<PointIdType> = Default::default();
+
+    let res = segments.apply_points_to_appendable(op_num, points, |id, write_segment| {
+        updated_points.insert(id);
+        let mut res = true;
+        for (key, payload) in payload {
+            res = write_segment.set_payload(op_num, id, key, payload.into())? && res;
+        }
         Ok(res)
-    }
+    })?;
 
-    pub fn set_payload(
-        segments: &SegmentHolder,
-        op_num: SeqNumberType,
-        payload: &HashMap<PayloadKeyType, PayloadInterface>,
-        points: &[PointIdType],
-    ) -> CollectionResult<usize> {
-        let mut updated_points: HashSet<PointIdType> = Default::default();
+    check_unprocessed_points(points, &updated_points)?;
+    Ok(res)
+}
 
-        let res = segments.apply_points_to_appendable(op_num, points, |id, write_segment| {
-            updated_points.insert(id);
-            let mut res = true;
-            for (key, payload) in payload {
-                res = write_segment.set_payload(op_num, id, key, payload.into())? && res;
-            }
-            Ok(res)
-        })?;
+pub(crate) fn delete_payload(
+    segments: &SegmentHolder,
+    op_num: SeqNumberType,
+    points: &[PointIdType],
+    keys: &[PayloadKeyType],
+) -> CollectionResult<usize> {
+    let mut updated_points: HashSet<PointIdType> = Default::default();
 
-        SegmentsUpdater::check_unprocessed_points(points, &updated_points)?;
+    let res = segments.apply_points_to_appendable(op_num, points, |id, write_segment| {
+        updated_points.insert(id);
+        let mut res = true;
+        for key in keys {
+            res = write_segment.delete_payload(op_num, id, key)? && res;
+        }
         Ok(res)
+    })?;
+
+    check_unprocessed_points(points, &updated_points)?;
+    Ok(res)
+}
+
+pub(crate) fn clear_payload(
+    segments: &SegmentHolder,
+    op_num: SeqNumberType,
+    points: &[PointIdType],
+) -> CollectionResult<usize> {
+    let mut updated_points: HashSet<PointIdType> = Default::default();
+    let res = segments.apply_points_to_appendable(op_num, points, |id, write_segment| {
+        updated_points.insert(id);
+        write_segment.clear_payload(op_num, id)
+    })?;
+
+    check_unprocessed_points(points, &updated_points)?;
+    Ok(res)
+}
+
+pub(crate) fn create_field_index(
+    segments: &SegmentHolder,
+    op_num: SeqNumberType,
+    field_name: PayloadKeyTypeRef,
+) -> CollectionResult<usize> {
+    let res = segments.apply_segments(op_num, |write_segment| {
+        write_segment.create_field_index(op_num, field_name)
+    })?;
+    Ok(res)
+}
+
+pub(crate) fn delete_field_index(
+    segments: &SegmentHolder,
+    op_num: SeqNumberType,
+    field_name: PayloadKeyTypeRef,
+) -> CollectionResult<usize> {
+    let res = segments.apply_segments(op_num, |write_segment| {
+        write_segment.delete_field_index(op_num, field_name)
+    })?;
+    Ok(res)
+}
+
+/// Checks point id in each segment, update point if found.
+/// All not found points are inserted into random segment.
+/// Returns: number of updated points.
+pub(crate) fn upsert_points(
+    segments: &RwLock<SegmentHolder>,
+    op_num: SeqNumberType,
+    ids: &[PointIdType],
+    vectors: &[VectorType],
+    payloads: &Option<Vec<Option<HashMap<PayloadKeyType, PayloadInterface>>>>,
+) -> CollectionResult<usize> {
+    if ids.len() != vectors.len() {
+        return Err(CollectionError::BadInput {
+            description: format!(
+                "Amount of ids ({}) and vectors ({}) does not match",
+                ids.len(),
+                vectors.len()
+            ),
+        });
     }
 
-    pub fn delete_payload(
-        segments: &SegmentHolder,
-        op_num: SeqNumberType,
-        points: &[PointIdType],
-        keys: &[PayloadKeyType],
-    ) -> CollectionResult<usize> {
-        let mut updated_points: HashSet<PointIdType> = Default::default();
+    match payloads {
+        None => {}
+        Some(payload_vector) => {
+            if payload_vector.len() != ids.len() {
+                return Err(CollectionError::BadInput {
+                    description: format!(
+                        "Amount of ids ({}) and payloads ({}) does not match",
+                        ids.len(),
+                        payload_vector.len()
+                    ),
+                });
+            }
+        }
+    }
+
+    let mut updated_points: HashSet<PointIdType> = Default::default();
+    let points_map: HashMap<PointIdType, &VectorType> = ids.iter().cloned().zip(vectors).collect();
 
-        let res = segments.apply_points_to_appendable(op_num, points, |id, write_segment| {
+    let segments = segments.read();
+
+    // Get points, which presence in segments with higher version
+    segments.read_points(ids, |id, segment| {
+        if segment.version() > op_num {
             updated_points.insert(id);
-            let mut res = true;
-            for key in keys {
-                res = write_segment.delete_payload(op_num, id, key)? && res;
-            }
-            Ok(res)
-        })?;
+        }
+        Ok(true)
+    })?;
 
-        SegmentsUpdater::check_unprocessed_points(points, &updated_points)?;
-        Ok(res)
+    // Update points in writable segments
+    let res = segments.apply_points_to_appendable(op_num, ids, |id, write_segment| {
+        updated_points.insert(id);
+        write_segment.upsert_point(op_num, id, points_map[&id])
+    })?;
+
+    // Insert new points, which was not updated.
+    let new_point_ids = ids.iter().cloned().filter(|x| !updated_points.contains(x));
+
+    {
+        let default_write_segment =
+            segments
+                .random_appendable_segment()
+                .ok_or(CollectionError::ServiceError {
+                    error: "No segments exists, expected at least one".to_string(),
+                })?;
+
+        let segment_arc = default_write_segment.get();
+        let mut write_segment = segment_arc.write();
+        for point_id in new_point_ids {
+            write_segment.upsert_point(op_num, point_id, points_map[&point_id])?;
+        }
     }
 
-    pub fn clear_payload(
-        segments: &SegmentHolder,
-        op_num: SeqNumberType,
-        points: &[PointIdType],
-    ) -> CollectionResult<usize> {
-        let mut updated_points: HashSet<PointIdType> = Default::default();
-        let res = segments.apply_points_to_appendable(op_num, points, |id, write_segment| {
-            updated_points.insert(id);
-            write_segment.clear_payload(op_num, id)
-        })?;
+    if let Some(payload_vector) = payloads {
+        for (point_id, payload) in ids.iter().zip(payload_vector.iter()) {
+            if payload.is_some() {
+                set_payload(&segments, op_num, payload.as_ref().unwrap(), &[*point_id])?;
+            }
+        }
+    }
 
-        SegmentsUpdater::check_unprocessed_points(points, &updated_points)?;
-        Ok(res)
+    Ok(res)
+}
+
+pub(crate) fn process_point_operation(
+    segments: &RwLock<SegmentHolder>,
+    op_num: SeqNumberType,
+    point_operation: PointOperations,
+) -> CollectionResult<usize> {
+    match point_operation {
+        PointOperations::DeletePoints { ids, .. } => delete_points(&segments.read(), op_num, &ids),
+        PointOperations::UpsertPoints(operation) => {
+            let (ids, vectors, payloads) = match operation {
+                PointInsertOperations::BatchPoints {
+                    ids,
+                    vectors,
+                    payloads,
+                    ..
+                } => (ids, vectors, payloads),
+                PointInsertOperations::PointsList(points) => {
+                    let mut ids = vec![];
+                    let mut vectors = vec![];
+                    let mut payloads = vec![];
+                    for point in points {
+                        ids.push(point.id);
+                        vectors.push(point.vector);
+                        payloads.push(point.payload)
+                    }
+                    (ids, vectors, Some(payloads))
+                }
+            };
+            let res = upsert_points(segments, op_num, &ids, &vectors, &payloads)?;
+            Ok(res)
+        }
     }
+}
 
-    pub fn create_field_index(
-        segments: &SegmentHolder,
-        op_num: SeqNumberType,
-        field_name: PayloadKeyTypeRef,
-    ) -> CollectionResult<usize> {
-        let res = segments.apply_segments(op_num, |write_segment| {
-            write_segment.create_field_index(op_num, field_name)
-        })?;
-        Ok(res)
+pub(crate) fn process_payload_operation(
+    segments: &RwLock<SegmentHolder>,
+    op_num: SeqNumberType,
+    payload_operation: &PayloadOps,
+) -> CollectionResult<usize> {
+    match payload_operation {
+        PayloadOps::SetPayload {
+            payload, points, ..
+        } => set_payload(&segments.read(), op_num, payload, points),
+        PayloadOps::DeletePayload { keys, points, .. } => {
+            delete_payload(&segments.read(), op_num, points, keys)
+        }
+        PayloadOps::ClearPayload { points, .. } => clear_payload(&segments.read(), op_num, points),
     }
+}
 
-    pub fn delete_field_index(
-        segments: &SegmentHolder,
-        op_num: SeqNumberType,
-        field_name: PayloadKeyTypeRef,
-    ) -> CollectionResult<usize> {
-        let res = segments.apply_segments(op_num, |write_segment| {
-            write_segment.delete_field_index(op_num, field_name)
-        })?;
-        Ok(res)
+pub(crate) fn process_field_index_operation(
+    segments: &RwLock<SegmentHolder>,
+    op_num: SeqNumberType,
+    field_index_operation: &FieldIndexOperations,
+) -> CollectionResult<usize> {
+    match field_index_operation {
+        FieldIndexOperations::CreateIndex(field_name) => {
+            create_field_index(&segments.read(), op_num, field_name)
+        }
+        FieldIndexOperations::DeleteIndex(field_name) => {
+            delete_field_index(&segments.read(), op_num, field_name)
+        }
     }
 }

commit bf3d8c25753188b4ca5e69a13c7f26e3c383f05b
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Sun Oct 24 18:10:39 2021 +0200

    data consistency fixes and updates (#112)
    
    * update segment version after completed update only
    
    * more stable updates: check pre-existing points on update, fail recovery, WAL proper ack. check_unprocessed_points WIP
    
    * switch to async channel
    
    * perform update operations in a separate thread (#111)
    
    * perform update operations in a separate thread
    
    * ordered sending update signal
    
    * locate a segment merging versioning bug
    
    * rename id_mapper -> id_tracker
    
    * per-record versioning
    
    * clippy fixes
    
    * cargo fmt
    
    * rm limit of open files
    
    * fail recovery test
    
    * cargo fmt
    
    * wait for worker stops befor dropping the runtime

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index cc2fe2365..c59ab3106 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -1,9 +1,10 @@
 use std::collections::{HashMap, HashSet};
 
-use parking_lot::RwLock;
+use parking_lot::{RwLock, RwLockWriteGuard};
 
 use segment::types::{
     PayloadInterface, PayloadKeyType, PayloadKeyTypeRef, PointIdType, SeqNumberType,
+    VectorElementType,
 };
 
 use crate::collection_manager::holders::segment_holder::SegmentHolder;
@@ -11,6 +12,8 @@ use crate::operations::payload_ops::PayloadOps;
 use crate::operations::point_ops::{PointInsertOperations, PointOperations};
 use crate::operations::types::{CollectionError, CollectionResult, VectorType};
 use crate::operations::FieldIndexOperations;
+use itertools::Itertools;
+use segment::entry::entry_point::{OperationResult, SegmentEntry};
 
 /// A collection of functions for updating points and payloads stored in segments
 
@@ -18,7 +21,15 @@ pub(crate) fn check_unprocessed_points(
     points: &[PointIdType],
     processed: &HashSet<PointIdType>,
 ) -> CollectionResult<usize> {
-    let missed_point = points.iter().cloned().find(|p| !processed.contains(p));
+    let unprocessed_points = points
+        .iter()
+        .cloned()
+        .filter(|p| !processed.contains(p))
+        .collect_vec();
+    let missed_point = unprocessed_points.iter().cloned().next();
+
+    // ToDo: check pre-existing points
+
     match missed_point {
         None => Ok(processed.len()),
         Some(missed_point) => Err(CollectionError::NotFound {
@@ -33,7 +44,7 @@ pub(crate) fn delete_points(
     op_num: SeqNumberType,
     ids: &[PointIdType],
 ) -> CollectionResult<usize> {
-    let res = segments.apply_points(op_num, ids, |id, _idx, write_segment| {
+    let res = segments.apply_points(ids, |id, _idx, write_segment| {
         write_segment.delete_point(op_num, id)
     })?;
     Ok(res)
@@ -45,19 +56,17 @@ pub(crate) fn set_payload(
     payload: &HashMap<PayloadKeyType, PayloadInterface>,
     points: &[PointIdType],
 ) -> CollectionResult<usize> {
-    let mut updated_points: HashSet<PointIdType> = Default::default();
-
-    let res = segments.apply_points_to_appendable(op_num, points, |id, write_segment| {
-        updated_points.insert(id);
-        let mut res = true;
-        for (key, payload) in payload {
-            res = write_segment.set_payload(op_num, id, key, payload.into())? && res;
-        }
-        Ok(res)
-    })?;
+    let updated_points =
+        segments.apply_points_to_appendable(op_num, points, |id, write_segment| {
+            let mut res = true;
+            for (key, payload) in payload {
+                res = write_segment.set_payload(op_num, id, key, payload.into())? && res;
+            }
+            Ok(res)
+        })?;
 
     check_unprocessed_points(points, &updated_points)?;
-    Ok(res)
+    Ok(updated_points.len())
 }
 
 pub(crate) fn delete_payload(
@@ -66,19 +75,17 @@ pub(crate) fn delete_payload(
     points: &[PointIdType],
     keys: &[PayloadKeyType],
 ) -> CollectionResult<usize> {
-    let mut updated_points: HashSet<PointIdType> = Default::default();
-
-    let res = segments.apply_points_to_appendable(op_num, points, |id, write_segment| {
-        updated_points.insert(id);
-        let mut res = true;
-        for key in keys {
-            res = write_segment.delete_payload(op_num, id, key)? && res;
-        }
-        Ok(res)
-    })?;
+    let updated_points =
+        segments.apply_points_to_appendable(op_num, points, |id, write_segment| {
+            let mut res = true;
+            for key in keys {
+                res = write_segment.delete_payload(op_num, id, key)? && res;
+            }
+            Ok(res)
+        })?;
 
     check_unprocessed_points(points, &updated_points)?;
-    Ok(res)
+    Ok(updated_points.len())
 }
 
 pub(crate) fn clear_payload(
@@ -86,14 +93,13 @@ pub(crate) fn clear_payload(
     op_num: SeqNumberType,
     points: &[PointIdType],
 ) -> CollectionResult<usize> {
-    let mut updated_points: HashSet<PointIdType> = Default::default();
-    let res = segments.apply_points_to_appendable(op_num, points, |id, write_segment| {
-        updated_points.insert(id);
-        write_segment.clear_payload(op_num, id)
-    })?;
+    let updated_points =
+        segments.apply_points_to_appendable(op_num, points, |id, write_segment| {
+            write_segment.clear_payload(op_num, id)
+        })?;
 
     check_unprocessed_points(points, &updated_points)?;
-    Ok(res)
+    Ok(updated_points.len())
 }
 
 pub(crate) fn create_field_index(
@@ -101,9 +107,8 @@ pub(crate) fn create_field_index(
     op_num: SeqNumberType,
     field_name: PayloadKeyTypeRef,
 ) -> CollectionResult<usize> {
-    let res = segments.apply_segments(op_num, |write_segment| {
-        write_segment.create_field_index(op_num, field_name)
-    })?;
+    let res = segments
+        .apply_segments(|write_segment| write_segment.create_field_index(op_num, field_name))?;
     Ok(res)
 }
 
@@ -112,9 +117,24 @@ pub(crate) fn delete_field_index(
     op_num: SeqNumberType,
     field_name: PayloadKeyTypeRef,
 ) -> CollectionResult<usize> {
-    let res = segments.apply_segments(op_num, |write_segment| {
-        write_segment.delete_field_index(op_num, field_name)
-    })?;
+    let res = segments
+        .apply_segments(|write_segment| write_segment.delete_field_index(op_num, field_name))?;
+    Ok(res)
+}
+
+fn upsert_with_payload(
+    segment: &mut RwLockWriteGuard<dyn SegmentEntry>,
+    op_num: SeqNumberType,
+    point_id: PointIdType,
+    vector: &[VectorElementType],
+    payload: Option<&HashMap<PayloadKeyType, PayloadInterface>>,
+) -> OperationResult<bool> {
+    let mut res = segment.upsert_point(op_num, point_id, vector)?;
+    if let Some(full_payload) = payload {
+        for (key, payload_value) in full_payload {
+            res &= segment.set_payload(op_num, point_id, key, payload_value.into())?;
+        }
+    }
     Ok(res)
 }
 
@@ -153,27 +173,39 @@ pub(crate) fn upsert_points(
         }
     }
 
-    let mut updated_points: HashSet<PointIdType> = Default::default();
-    let points_map: HashMap<PointIdType, &VectorType> = ids.iter().cloned().zip(vectors).collect();
+    let vectors_map: HashMap<PointIdType, &VectorType> = ids.iter().cloned().zip(vectors).collect();
+    let payloads_map: HashMap<PointIdType, &HashMap<PayloadKeyType, PayloadInterface>> =
+        match payloads {
+            None => Default::default(),
+            Some(payloads_vector) => ids
+                .iter()
+                .clone()
+                .zip(payloads_vector)
+                .filter_map(|(id, payload)| {
+                    payload.as_ref().map(|payload_values| (*id, payload_values))
+                })
+                .collect(),
+        };
 
     let segments = segments.read();
-
-    // Get points, which presence in segments with higher version
-    segments.read_points(ids, |id, segment| {
-        if segment.version() > op_num {
-            updated_points.insert(id);
-        }
-        Ok(true)
-    })?;
-
     // Update points in writable segments
-    let res = segments.apply_points_to_appendable(op_num, ids, |id, write_segment| {
-        updated_points.insert(id);
-        write_segment.upsert_point(op_num, id, points_map[&id])
-    })?;
+    let updated_points =
+        segments.apply_points_to_appendable(op_num, ids, |id, write_segment| {
+            upsert_with_payload(
+                write_segment,
+                op_num,
+                id,
+                vectors_map[&id],
+                payloads_map.get(&id).cloned(),
+            )
+        })?;
 
-    // Insert new points, which was not updated.
-    let new_point_ids = ids.iter().cloned().filter(|x| !updated_points.contains(x));
+    let mut res = updated_points.len();
+    // Insert new points, which was not updated or existed
+    let new_point_ids = ids
+        .iter()
+        .cloned()
+        .filter(|x| !(updated_points.contains(x)));
 
     {
         let default_write_segment =
@@ -186,17 +218,15 @@ pub(crate) fn upsert_points(
         let segment_arc = default_write_segment.get();
         let mut write_segment = segment_arc.write();
         for point_id in new_point_ids {
-            write_segment.upsert_point(op_num, point_id, points_map[&point_id])?;
-        }
-    }
-
-    if let Some(payload_vector) = payloads {
-        for (point_id, payload) in ids.iter().zip(payload_vector.iter()) {
-            if payload.is_some() {
-                set_payload(&segments, op_num, payload.as_ref().unwrap(), &[*point_id])?;
-            }
+            res += upsert_with_payload(
+                &mut write_segment,
+                op_num,
+                point_id,
+                vectors_map[&point_id],
+                payloads_map.get(&point_id).cloned(),
+            )? as usize;
         }
-    }
+    };
 
     Ok(res)
 }

commit 97cb5091bcd9327ebfd38b213079e361aeaaed62
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Mon Jan 24 07:54:47 2022 +0100

    Split Points API #208 (#221)
    
    Split Points API #208

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index c59ab3106..783614aff 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -270,11 +270,11 @@ pub(crate) fn process_payload_operation(
     payload_operation: &PayloadOps,
 ) -> CollectionResult<usize> {
     match payload_operation {
-        PayloadOps::SetPayload {
-            payload, points, ..
-        } => set_payload(&segments.read(), op_num, payload, points),
-        PayloadOps::DeletePayload { keys, points, .. } => {
-            delete_payload(&segments.read(), op_num, points, keys)
+        PayloadOps::SetPayload(sp) => {
+            set_payload(&segments.read(), op_num, &sp.payload, &sp.points)
+        }
+        PayloadOps::DeletePayload(dp) => {
+            delete_payload(&segments.read(), op_num, &dp.points, &dp.keys)
         }
         PayloadOps::ClearPayload { points, .. } => clear_payload(&segments.read(), op_num, points),
     }

commit d51a70fa931bc70443a369d08b3c55bceadfd015
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Mon Jan 24 17:33:57 2022 +0100

    add openapi validation during generation #208 (#248)
    
    * add openapi validation during generation #208
    
    * fix: POST -> PUT in point update api implementation and docs #208
    
    * fix: openapi structure exposure
    
    * fix: api usage in stress test

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 783614aff..12e0640a3 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -9,7 +9,9 @@ use segment::types::{
 
 use crate::collection_manager::holders::segment_holder::SegmentHolder;
 use crate::operations::payload_ops::PayloadOps;
-use crate::operations::point_ops::{PointInsertOperations, PointOperations};
+use crate::operations::point_ops::{
+    BatchInsertOperation, BatchPoints, PointInsertOperations, PointOperations, PointsList,
+};
 use crate::operations::types::{CollectionError, CollectionResult, VectorType};
 use crate::operations::FieldIndexOperations;
 use itertools::Itertools;
@@ -240,13 +242,16 @@ pub(crate) fn process_point_operation(
         PointOperations::DeletePoints { ids, .. } => delete_points(&segments.read(), op_num, &ids),
         PointOperations::UpsertPoints(operation) => {
             let (ids, vectors, payloads) = match operation {
-                PointInsertOperations::BatchPoints {
-                    ids,
-                    vectors,
-                    payloads,
-                    ..
-                } => (ids, vectors, payloads),
-                PointInsertOperations::PointsList(points) => {
+                PointInsertOperations::BatchPoints(BatchInsertOperation {
+                    batch:
+                        BatchPoints {
+                            ids,
+                            vectors,
+                            payloads,
+                            ..
+                        },
+                }) => (ids, vectors, payloads),
+                PointInsertOperations::PointsList(PointsList { points }) => {
                     let mut ids = vec![];
                     let mut vectors = vec![];
                     let mut payloads = vec![];

commit 9440143af0a4e56162829a0dfa6c31705483bfe8
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Mon Jan 24 19:32:14 2022 +0100

    rename point update api structure #208 (#251)

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 12e0640a3..74ca9e20f 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -10,7 +10,7 @@ use segment::types::{
 use crate::collection_manager::holders::segment_holder::SegmentHolder;
 use crate::operations::payload_ops::PayloadOps;
 use crate::operations::point_ops::{
-    BatchInsertOperation, BatchPoints, PointInsertOperations, PointOperations, PointsList,
+    Batch, PointInsertOperations, PointOperations, PointsBatch, PointsList,
 };
 use crate::operations::types::{CollectionError, CollectionResult, VectorType};
 use crate::operations::FieldIndexOperations;
@@ -242,9 +242,9 @@ pub(crate) fn process_point_operation(
         PointOperations::DeletePoints { ids, .. } => delete_points(&segments.read(), op_num, &ids),
         PointOperations::UpsertPoints(operation) => {
             let (ids, vectors, payloads) = match operation {
-                PointInsertOperations::BatchPoints(BatchInsertOperation {
+                PointInsertOperations::PointsBatch(PointsBatch {
                     batch:
-                        BatchPoints {
+                        Batch {
                             ids,
                             vectors,
                             payloads,

commit 559e7a80556d46a471e46de5b34a54ee5342d132
Author: Tim Eggert <tim@elbart.com>
Date:   Tue Jan 25 16:22:18 2022 +0100

    Delete Points By Filter API #39 (#250)
    
    * Delete Points By Filter API #39
    
    * make delete_by_filter part of existing delete query + fix merge issues #39
    
    * apply fmt
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 74ca9e20f..28ee0bd4d 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -3,7 +3,7 @@ use std::collections::{HashMap, HashSet};
 use parking_lot::{RwLock, RwLockWriteGuard};
 
 use segment::types::{
-    PayloadInterface, PayloadKeyType, PayloadKeyTypeRef, PointIdType, SeqNumberType,
+    Filter, PayloadInterface, PayloadKeyType, PayloadKeyTypeRef, PointIdType, SeqNumberType,
     VectorElementType,
 };
 
@@ -266,6 +266,9 @@ pub(crate) fn process_point_operation(
             let res = upsert_points(segments, op_num, &ids, &vectors, &payloads)?;
             Ok(res)
         }
+        PointOperations::DeletePointsByFilter(filter) => {
+            delete_points_by_filter(&segments.read(), op_num, &filter)
+        }
     }
 }
 
@@ -299,3 +302,17 @@ pub(crate) fn process_field_index_operation(
         }
     }
 }
+
+/// Deletes points from all segments matching the given filter
+pub(crate) fn delete_points_by_filter(
+    segments: &SegmentHolder,
+    op_num: SeqNumberType,
+    filter: &Filter,
+) -> CollectionResult<usize> {
+    let mut deleted = 0;
+    segments.apply_segments(|s| {
+        deleted += s.delete_filtered(op_num, filter)?;
+        Ok(true)
+    })?;
+    Ok(deleted)
+}

commit 79fefa2f6725f38fc8e2310f03735e64f835eea8
Author: Gabriel Velo <gabriel.velo@gmail.com>
Date:   Thu Feb 3 06:12:57 2022 -0300

    remove payload using filters (#269) (#278)

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 28ee0bd4d..8af308d85 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -104,6 +104,29 @@ pub(crate) fn clear_payload(
     Ok(updated_points.len())
 }
 
+/// Clear Payloads from all segments matching the given filter
+pub(crate) fn clear_payload_by_filter(
+    segments: &SegmentHolder,
+    op_num: SeqNumberType,
+    filter: &Filter,
+) -> CollectionResult<usize> {
+    let mut points_to_clear: Vec<PointIdType> = Vec::new();
+
+    segments.apply_segments(|s| {
+        let points = s.read_filtered(None, usize::MAX, Some(filter));
+        points_to_clear.extend_from_slice(points.as_slice());
+        Ok(true)
+    })?;
+
+    let updated_points = segments.apply_points_to_appendable(
+        op_num,
+        points_to_clear.as_slice(),
+        |id, write_segment| write_segment.clear_payload(op_num, id),
+    )?;
+
+    Ok(updated_points.len())
+}
+
 pub(crate) fn create_field_index(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
@@ -285,6 +308,9 @@ pub(crate) fn process_payload_operation(
             delete_payload(&segments.read(), op_num, &dp.points, &dp.keys)
         }
         PayloadOps::ClearPayload { points, .. } => clear_payload(&segments.read(), op_num, points),
+        PayloadOps::ClearPayloadByFilter(filter) => {
+            clear_payload_by_filter(&segments.read(), op_num, filter)
+        }
     }
 }
 

commit b008e3dede9defcd3f060211f93f7c8e3be7cbf5
Author: Egor Ivkov <e.o.ivkov@gmail.com>
Date:   Mon Feb 28 11:38:43 2022 +0300

    Select shard for operation (#340)

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 8af308d85..17790c777 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -173,31 +173,6 @@ pub(crate) fn upsert_points(
     vectors: &[VectorType],
     payloads: &Option<Vec<Option<HashMap<PayloadKeyType, PayloadInterface>>>>,
 ) -> CollectionResult<usize> {
-    if ids.len() != vectors.len() {
-        return Err(CollectionError::BadInput {
-            description: format!(
-                "Amount of ids ({}) and vectors ({}) does not match",
-                ids.len(),
-                vectors.len()
-            ),
-        });
-    }
-
-    match payloads {
-        None => {}
-        Some(payload_vector) => {
-            if payload_vector.len() != ids.len() {
-                return Err(CollectionError::BadInput {
-                    description: format!(
-                        "Amount of ids ({}) and payloads ({}) does not match",
-                        ids.len(),
-                        payload_vector.len()
-                    ),
-                });
-            }
-        }
-    }
-
     let vectors_map: HashMap<PointIdType, &VectorType> = ids.iter().cloned().zip(vectors).collect();
     let payloads_map: HashMap<PointIdType, &HashMap<PayloadKeyType, PayloadInterface>> =
         match payloads {

commit f69a7b740fb57da8ed887f36afb173a3f3846c66
Author: Gabriel Velo <gabriel.velo@gmail.com>
Date:   Mon Mar 21 07:09:10 2022 -0300

    json as payload (#306)
    
    add json as payload
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 17790c777..c4213dfb6 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -3,8 +3,8 @@ use std::collections::{HashMap, HashSet};
 use parking_lot::{RwLock, RwLockWriteGuard};
 
 use segment::types::{
-    Filter, PayloadInterface, PayloadKeyType, PayloadKeyTypeRef, PointIdType, SeqNumberType,
-    VectorElementType,
+    Filter, Payload, PayloadKeyType, PayloadKeyTypeRef, PayloadSchemaType, PointIdType,
+    SeqNumberType, VectorElementType,
 };
 
 use crate::collection_manager::holders::segment_holder::SegmentHolder;
@@ -55,16 +55,13 @@ pub(crate) fn delete_points(
 pub(crate) fn set_payload(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
-    payload: &HashMap<PayloadKeyType, PayloadInterface>,
+    payload: &Payload,
     points: &[PointIdType],
 ) -> CollectionResult<usize> {
     let updated_points =
         segments.apply_points_to_appendable(op_num, points, |id, write_segment| {
-            let mut res = true;
-            for (key, payload) in payload {
-                res = write_segment.set_payload(op_num, id, key, payload.into())? && res;
-            }
-            Ok(res)
+            write_segment.set_payload(op_num, id, payload)?;
+            Ok(true)
         })?;
 
     check_unprocessed_points(points, &updated_points)?;
@@ -131,9 +128,11 @@ pub(crate) fn create_field_index(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
     field_name: PayloadKeyTypeRef,
+    field_type: &Option<PayloadSchemaType>,
 ) -> CollectionResult<usize> {
-    let res = segments
-        .apply_segments(|write_segment| write_segment.create_field_index(op_num, field_name))?;
+    let res = segments.apply_segments(|write_segment| {
+        write_segment.create_field_index(op_num, field_name, field_type)
+    })?;
     Ok(res)
 }
 
@@ -152,13 +151,11 @@ fn upsert_with_payload(
     op_num: SeqNumberType,
     point_id: PointIdType,
     vector: &[VectorElementType],
-    payload: Option<&HashMap<PayloadKeyType, PayloadInterface>>,
+    payload: Option<&Payload>,
 ) -> OperationResult<bool> {
     let mut res = segment.upsert_point(op_num, point_id, vector)?;
     if let Some(full_payload) = payload {
-        for (key, payload_value) in full_payload {
-            res &= segment.set_payload(op_num, point_id, key, payload_value.into())?;
-        }
+        res &= segment.set_payload(op_num, point_id, full_payload)?;
     }
     Ok(res)
 }
@@ -171,21 +168,20 @@ pub(crate) fn upsert_points(
     op_num: SeqNumberType,
     ids: &[PointIdType],
     vectors: &[VectorType],
-    payloads: &Option<Vec<Option<HashMap<PayloadKeyType, PayloadInterface>>>>,
+    payloads: &Option<Vec<Option<Payload>>>,
 ) -> CollectionResult<usize> {
     let vectors_map: HashMap<PointIdType, &VectorType> = ids.iter().cloned().zip(vectors).collect();
-    let payloads_map: HashMap<PointIdType, &HashMap<PayloadKeyType, PayloadInterface>> =
-        match payloads {
-            None => Default::default(),
-            Some(payloads_vector) => ids
-                .iter()
-                .clone()
-                .zip(payloads_vector)
-                .filter_map(|(id, payload)| {
-                    payload.as_ref().map(|payload_values| (*id, payload_values))
-                })
-                .collect(),
-        };
+    let payloads_map: HashMap<PointIdType, &Payload> = match payloads {
+        None => Default::default(),
+        Some(payloads_vector) => ids
+            .iter()
+            .clone()
+            .zip(payloads_vector)
+            .filter_map(|(id, payload)| {
+                payload.as_ref().map(|payload_values| (*id, payload_values))
+            })
+            .collect(),
+    };
 
     let segments = segments.read();
     // Update points in writable segments
@@ -273,17 +269,20 @@ pub(crate) fn process_point_operation(
 pub(crate) fn process_payload_operation(
     segments: &RwLock<SegmentHolder>,
     op_num: SeqNumberType,
-    payload_operation: &PayloadOps,
+    payload_operation: PayloadOps,
 ) -> CollectionResult<usize> {
     match payload_operation {
         PayloadOps::SetPayload(sp) => {
-            set_payload(&segments.read(), op_num, &sp.payload, &sp.points)
+            let payload: Payload = sp.payload;
+            set_payload(&segments.read(), op_num, &payload, &sp.points)
         }
         PayloadOps::DeletePayload(dp) => {
             delete_payload(&segments.read(), op_num, &dp.points, &dp.keys)
         }
-        PayloadOps::ClearPayload { points, .. } => clear_payload(&segments.read(), op_num, points),
-        PayloadOps::ClearPayloadByFilter(filter) => {
+        PayloadOps::ClearPayload { ref points, .. } => {
+            clear_payload(&segments.read(), op_num, points)
+        }
+        PayloadOps::ClearPayloadByFilter(ref filter) => {
             clear_payload_by_filter(&segments.read(), op_num, filter)
         }
     }
@@ -295,9 +294,12 @@ pub(crate) fn process_field_index_operation(
     field_index_operation: &FieldIndexOperations,
 ) -> CollectionResult<usize> {
     match field_index_operation {
-        FieldIndexOperations::CreateIndex(field_name) => {
-            create_field_index(&segments.read(), op_num, field_name)
-        }
+        FieldIndexOperations::CreateIndex(index_data) => create_field_index(
+            &segments.read(),
+            op_num,
+            &index_data.field_name,
+            &index_data.field_type,
+        ),
         FieldIndexOperations::DeleteIndex(field_name) => {
             delete_field_index(&segments.read(), op_num, field_name)
         }

commit 036c186e0dfffae10b1319c7621c20752f6e39e3
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Mon May 23 13:28:17 2022 +0200

    Better error reporting in enums (#587)
    
    * remove conflicting deprecated apis
    
    * implement custom JsonSchema for the PointInsertOperations
    
    * fmt
    
    * clippy
    
    * revert unnesessare changes

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index c4213dfb6..c570abf79 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -9,9 +9,7 @@ use segment::types::{
 
 use crate::collection_manager::holders::segment_holder::SegmentHolder;
 use crate::operations::payload_ops::PayloadOps;
-use crate::operations::point_ops::{
-    Batch, PointInsertOperations, PointOperations, PointsBatch, PointsList,
-};
+use crate::operations::point_ops::{Batch, PointInsertOperations, PointOperations};
 use crate::operations::types::{CollectionError, CollectionResult, VectorType};
 use crate::operations::FieldIndexOperations;
 use itertools::Itertools;
@@ -236,16 +234,13 @@ pub(crate) fn process_point_operation(
         PointOperations::DeletePoints { ids, .. } => delete_points(&segments.read(), op_num, &ids),
         PointOperations::UpsertPoints(operation) => {
             let (ids, vectors, payloads) = match operation {
-                PointInsertOperations::PointsBatch(PointsBatch {
-                    batch:
-                        Batch {
-                            ids,
-                            vectors,
-                            payloads,
-                            ..
-                        },
+                PointInsertOperations::PointsBatch(Batch {
+                    ids,
+                    vectors,
+                    payloads,
+                    ..
                 }) => (ids, vectors, payloads),
-                PointInsertOperations::PointsList(PointsList { points }) => {
+                PointInsertOperations::PointsList(points) => {
                     let mut ids = vec![];
                     let mut vectors = vec![];
                     let mut payloads = vec![];

commit fac87018c45b1bc7bc957dbe254ba26349464426
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Fri Jul 1 13:21:57 2022 +0200

    Snapshoting API (#764)
    
    * wip: rest api for snapshots
    
    * fmt
    
    * fix tests
    
    * fmt
    
    * wip: collection snapshoting and recovery
    
    * fmt
    
    * remote shard snapshots
    
    * remote shard test
    
    * fmt
    
    * extend proxy snapshot test + fix double read lock
    
    * fmt + clippy
    
    * openapi schema
    
    * Update openapi/openapi-snapshots.ytt.yaml
    
    Co-authored-by: Arnaud Gourlay <arnaud.gourlay@gmail.com>
    
    * Update openapi/openapi-snapshots.ytt.yaml
    
    Co-authored-by: Arnaud Gourlay <arnaud.gourlay@gmail.com>
    
    * Update src/main.rs
    
    Co-authored-by: Arnaud Gourlay <arnaud.gourlay@gmail.com>
    
    * Update src/main.rs
    
    Co-authored-by: Arnaud Gourlay <arnaud.gourlay@gmail.com>
    
    * reduce writes on snapshots location
    
    Co-authored-by: Arnaud Gourlay <arnaud.gourlay@gmail.com>

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index c570abf79..c484e38e7 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -32,7 +32,7 @@ pub(crate) fn check_unprocessed_points(
 
     match missed_point {
         None => Ok(processed.len()),
-        Some(missed_point) => Err(CollectionError::NotFound {
+        Some(missed_point) => Err(CollectionError::PointNotFound {
             missed_point_id: missed_point,
         }),
     }

commit 026bd040b001f1c66e16fc911322f1f182d1cf0f
Author: Egor Ivkov <e.o.ivkov@gmail.com>
Date:   Fri Jul 15 15:42:25 2022 +0300

    Add import formatting rules (#820)
    
    * Add import formatting rules
    
    * Review fix: update rusty hook

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index c484e38e7..bad6dbced 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -1,7 +1,8 @@
 use std::collections::{HashMap, HashSet};
 
+use itertools::Itertools;
 use parking_lot::{RwLock, RwLockWriteGuard};
-
+use segment::entry::entry_point::{OperationResult, SegmentEntry};
 use segment::types::{
     Filter, Payload, PayloadKeyType, PayloadKeyTypeRef, PayloadSchemaType, PointIdType,
     SeqNumberType, VectorElementType,
@@ -12,8 +13,6 @@ use crate::operations::payload_ops::PayloadOps;
 use crate::operations::point_ops::{Batch, PointInsertOperations, PointOperations};
 use crate::operations::types::{CollectionError, CollectionResult, VectorType};
 use crate::operations::FieldIndexOperations;
-use itertools::Itertools;
-use segment::entry::entry_point::{OperationResult, SegmentEntry};
 
 /// A collection of functions for updating points and payloads stored in segments
 

commit be38254ee8f29902f66fe4bda13be13cf6bc3cef
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Thu Sep 1 12:36:28 2022 +0200

    small refactoring (#746)
    
    * small refactoring
    
    * fix tests

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index bad6dbced..124fda4c6 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -150,7 +150,7 @@ fn upsert_with_payload(
     vector: &[VectorElementType],
     payload: Option<&Payload>,
 ) -> OperationResult<bool> {
-    let mut res = segment.upsert_point(op_num, point_id, vector)?;
+    let mut res = segment.upsert_vector(op_num, point_id, vector)?;
     if let Some(full_payload) = payload {
         res &= segment.set_payload(op_num, point_id, full_payload)?;
     }

commit b9eee55a9fb6d53572622f62756a80e62484009e
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Thu Sep 1 12:50:12 2022 +0200

    Full text search (#963)
    
    * allow additional params for payload field index
    
    * fmt
    
    * wip: full text index building
    
    * fmt
    
    * text search request
    
    * text search request
    
    * full text index persitance and loading
    
    * fmt
    
    * enable fts index in mapping
    
    * clippy
    
    * fix tests + add integration test
    
    * review fixes: extend payload index test
    
    * revert incedental change

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 124fda4c6..88fde5857 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -4,7 +4,7 @@ use itertools::Itertools;
 use parking_lot::{RwLock, RwLockWriteGuard};
 use segment::entry::entry_point::{OperationResult, SegmentEntry};
 use segment::types::{
-    Filter, Payload, PayloadKeyType, PayloadKeyTypeRef, PayloadSchemaType, PointIdType,
+    Filter, Payload, PayloadFieldSchema, PayloadKeyType, PayloadKeyTypeRef, PointIdType,
     SeqNumberType, VectorElementType,
 };
 
@@ -125,10 +125,10 @@ pub(crate) fn create_field_index(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
     field_name: PayloadKeyTypeRef,
-    field_type: &Option<PayloadSchemaType>,
+    field_schema: Option<&PayloadFieldSchema>,
 ) -> CollectionResult<usize> {
     let res = segments.apply_segments(|write_segment| {
-        write_segment.create_field_index(op_num, field_name, field_type)
+        write_segment.create_field_index(op_num, field_name, field_schema)
     })?;
     Ok(res)
 }
@@ -292,7 +292,7 @@ pub(crate) fn process_field_index_operation(
             &segments.read(),
             op_num,
             &index_data.field_name,
-            &index_data.field_type,
+            index_data.field_type.as_ref(),
         ),
         FieldIndexOperations::DeleteIndex(field_name) => {
             delete_field_index(&segments.read(), op_num, field_name)

commit 24ac939b4a7a518cfdb209f32cff46e0c4c9f491
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Mon Sep 5 15:09:39 2022 +0200

    Sync Points API (#985)
    
    * shard sync operation
    
    * fnt
    
    * tests
    
    * fix test
    
    * Update lib/collection/src/collection_manager/segments_updater.rs
    
    Co-authored-by: Egor Ivkov <e.o.ivkov@gmail.com>
    
    * Update lib/collection/src/collection_manager/segments_updater.rs
    
    Co-authored-by: Egor Ivkov <e.o.ivkov@gmail.com>
    
    * match payload after vector only
    
    * Update lib/collection/src/collection_manager/segments_updater.rs
    
    Co-authored-by: Arnaud Gourlay <arnaud.gourlay@gmail.com>
    
    Co-authored-by: Egor Ivkov <e.o.ivkov@gmail.com>
    Co-authored-by: Arnaud Gourlay <arnaud.gourlay@gmail.com>

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 88fde5857..422edde9c 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -10,8 +10,8 @@ use segment::types::{
 
 use crate::collection_manager::holders::segment_holder::SegmentHolder;
 use crate::operations::payload_ops::PayloadOps;
-use crate::operations::point_ops::{Batch, PointInsertOperations, PointOperations};
-use crate::operations::types::{CollectionError, CollectionResult, VectorType};
+use crate::operations::point_ops::{Batch, PointInsertOperations, PointOperations, PointStruct};
+use crate::operations::types::{CollectionError, CollectionResult};
 use crate::operations::FieldIndexOperations;
 
 /// A collection of functions for updating points and payloads stored in segments
@@ -143,6 +143,11 @@ pub(crate) fn delete_field_index(
     Ok(res)
 }
 
+///
+/// Returns
+/// - Ok(true) if the operation was successful and point replaced existing value
+/// - Ok(false) if the operation was successful and point was inserted
+/// - Err if the operation failed
 fn upsert_with_payload(
     segment: &mut RwLockWriteGuard<dyn SegmentEntry>,
     op_num: SeqNumberType,
@@ -157,39 +162,110 @@ fn upsert_with_payload(
     Ok(res)
 }
 
+/// Sync points within a given range
+///
+/// 1. Retrieve existing points for a range
+/// 2. Remove points, which are not present in the sync operation
+/// 3. Retrieve overlapping points, detect which one of them are changed
+/// 4. Select new points
+/// 5. Upsert points which differ from the stored ones
+///
+/// Returns:
+///     (number of deleted points, number of new points, number of updated points)
+pub(crate) fn sync_points(
+    segments: &SegmentHolder,
+    op_num: SeqNumberType,
+    from_id: Option<PointIdType>,
+    to_id: Option<PointIdType>,
+    points: &[PointStruct],
+) -> CollectionResult<(usize, usize, usize)> {
+    let id_to_point = points
+        .iter()
+        .map(|p| (p.id, p))
+        .collect::<HashMap<PointIdType, &PointStruct>>();
+    let sync_points: HashSet<_> = points.iter().map(|p| p.id).collect();
+    // 1. Retrieve existing points for a range
+    let stored_point_ids: HashSet<_> = segments
+        .iter()
+        .flat_map(|(_, segment)| segment.get().read().read_range(from_id, to_id))
+        .collect();
+    // 2. Remove points, which are not present in the sync operation
+    let points_to_remove: Vec<_> = stored_point_ids.difference(&sync_points).copied().collect();
+    let deleted = delete_points(segments, op_num, points_to_remove.as_slice())?;
+    // 3. Retrieve overlapping points, detect which one of them are changed
+    let existing_point_ids: Vec<_> = stored_point_ids
+        .intersection(&sync_points)
+        .copied()
+        .collect();
+
+    let mut points_to_update: Vec<_> = Vec::new();
+    let _num_updated = segments.read_points(existing_point_ids.as_slice(), |id, segment| {
+        let vector = segment.vector(id)?;
+        let payload = segment.payload(id)?;
+        let point = id_to_point.get(&id).unwrap();
+        if point.vector != vector {
+            points_to_update.push(*point);
+            Ok(true)
+        } else {
+            let payload_match = match point.payload {
+                Some(ref p) => p == &payload,
+                None => Payload::default() == payload,
+            };
+            if !payload_match {
+                points_to_update.push(*point);
+                Ok(true)
+            } else {
+                Ok(false)
+            }
+        }
+    })?;
+
+    // 4. Select new points
+    let num_updated = points_to_update.len();
+    let mut num_new = 0;
+    sync_points
+        .difference(&stored_point_ids)
+        .copied()
+        .for_each(|id| {
+            num_new += 1;
+            points_to_update.push(*id_to_point.get(&id).unwrap());
+        });
+
+    // 5. Upsert points which differ from the stored ones
+    let num_replaced = upsert_points(segments, op_num, points_to_update)?;
+    debug_assert_eq!(num_replaced, num_updated);
+
+    Ok((deleted, num_new, num_updated))
+}
+
 /// Checks point id in each segment, update point if found.
 /// All not found points are inserted into random segment.
 /// Returns: number of updated points.
-pub(crate) fn upsert_points(
-    segments: &RwLock<SegmentHolder>,
+pub(crate) fn upsert_points<'a, T>(
+    segments: &SegmentHolder,
     op_num: SeqNumberType,
-    ids: &[PointIdType],
-    vectors: &[VectorType],
-    payloads: &Option<Vec<Option<Payload>>>,
-) -> CollectionResult<usize> {
-    let vectors_map: HashMap<PointIdType, &VectorType> = ids.iter().cloned().zip(vectors).collect();
-    let payloads_map: HashMap<PointIdType, &Payload> = match payloads {
-        None => Default::default(),
-        Some(payloads_vector) => ids
-            .iter()
-            .clone()
-            .zip(payloads_vector)
-            .filter_map(|(id, payload)| {
-                payload.as_ref().map(|payload_values| (*id, payload_values))
-            })
-            .collect(),
-    };
+    points: T,
+) -> CollectionResult<usize>
+where
+    T: IntoIterator<Item = &'a PointStruct>,
+{
+    let mut ids: Vec<PointIdType> = vec![];
+    let mut points_map: HashMap<PointIdType, &PointStruct> = Default::default();
+    points.into_iter().for_each(|p| {
+        ids.push(p.id);
+        points_map.insert(p.id, p);
+    });
 
-    let segments = segments.read();
     // Update points in writable segments
     let updated_points =
-        segments.apply_points_to_appendable(op_num, ids, |id, write_segment| {
+        segments.apply_points_to_appendable(op_num, &ids, |id, write_segment| {
+            let point = points_map[&id];
             upsert_with_payload(
                 write_segment,
                 op_num,
                 id,
-                vectors_map[&id],
-                payloads_map.get(&id).cloned(),
+                point.vector.as_slice(),
+                point.payload.as_ref(),
             )
         })?;
 
@@ -211,12 +287,13 @@ pub(crate) fn upsert_points(
         let segment_arc = default_write_segment.get();
         let mut write_segment = segment_arc.write();
         for point_id in new_point_ids {
+            let point = points_map[&point_id];
             res += upsert_with_payload(
                 &mut write_segment,
                 op_num,
                 point_id,
-                vectors_map[&point_id],
-                payloads_map.get(&point_id).cloned(),
+                point.vector.as_slice(),
+                point.payload.as_ref(),
             )? as usize;
         }
     };
@@ -232,31 +309,49 @@ pub(crate) fn process_point_operation(
     match point_operation {
         PointOperations::DeletePoints { ids, .. } => delete_points(&segments.read(), op_num, &ids),
         PointOperations::UpsertPoints(operation) => {
-            let (ids, vectors, payloads) = match operation {
+            let points: Vec<_> = match operation {
                 PointInsertOperations::PointsBatch(Batch {
                     ids,
                     vectors,
                     payloads,
-                    ..
-                }) => (ids, vectors, payloads),
-                PointInsertOperations::PointsList(points) => {
-                    let mut ids = vec![];
-                    let mut vectors = vec![];
-                    let mut payloads = vec![];
-                    for point in points {
-                        ids.push(point.id);
-                        vectors.push(point.vector);
-                        payloads.push(point.payload)
+                }) => {
+                    let vectors_iter = ids.into_iter().zip(vectors.into_iter());
+                    match payloads {
+                        None => vectors_iter
+                            .map(|(id, vector)| PointStruct {
+                                id,
+                                vector,
+                                payload: None,
+                            })
+                            .collect(),
+                        Some(payloads) => vectors_iter
+                            .zip(payloads.into_iter())
+                            .map(|((id, vector), payload)| PointStruct {
+                                id,
+                                vector,
+                                payload,
+                            })
+                            .collect(),
                     }
-                    (ids, vectors, Some(payloads))
                 }
+                PointInsertOperations::PointsList(points) => points,
             };
-            let res = upsert_points(segments, op_num, &ids, &vectors, &payloads)?;
+            let res = upsert_points(&segments.read(), op_num, points.iter())?;
             Ok(res)
         }
         PointOperations::DeletePointsByFilter(filter) => {
             delete_points_by_filter(&segments.read(), op_num, &filter)
         }
+        PointOperations::SyncPoints(operation) => {
+            let (deleted, new, updated) = sync_points(
+                &segments.read(),
+                op_num,
+                operation.from_id,
+                operation.to_id,
+                &operation.points,
+            )?;
+            Ok(deleted + new + updated)
+        }
     }
 }
 

commit f6b21861939744e054a861d9771608b7e6b614e7
Author: Ivan Pleshkov <pleshkov.ivan@gmail.com>
Date:   Sun Sep 11 22:59:23 2022 +0400

    [WIP] Many named vectors per point (#958)
    
    * many named vectors per point (segment-level)
    
    * operation result for dim function
    
    * beautifulized vector name
    
    * fix naming bug
    
    * segment version migration
    
    * fmt
    
    * add segment tests
    
    * are you happy clippy
    
    * fix build
    
    * [WIP] many named vectors per point (collection-level) (#975)
    
    * config and search
    
    * fix placeholders for proxy segment move
    
    * remove VectorType from collection
    
    * are you happy fmt
    
    * vectors in grps messages
    
    * create collections with vectors
    
    * segment holder fixes
    
    * are you happy fmt
    
    * remove default vector name placeholders
    
    * are you happy fmt
    
    * are you happy clippy
    
    * fix build
    
    * fix web api
    
    * are you happy clippy
    
    * are you happy fmt
    
    * record vector&vectors
    
    * openapi update
    
    * fix openapi integration tests
    
    * segment builder fix todo
    
    * vector names for update from segment
    
    * remove unwrap
    
    * backward compatibility
    
    * upd openapi
    
    * backward compatible PointStruct
    
    * upd openapi
    
    * fix record back-comp
    
    * fmt
    
    * vector configuration backward compatibility
    
    * fix vetor storage size estimation
    
    * fmt
    
    * multi-vec segment test + index test
    
    * fmt
    
    * api integration tests
    
    * [WIP] Named vectors struct (#1002)
    
    * move to separate file
    
    * named vectors as struct
    
    * use cow
    
    * fix build
    
    * keys iterator
    
    * avoid copy in PointStruct -> get_vectors
    
    * avoid another copy
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 422edde9c..07002320c 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -2,15 +2,16 @@ use std::collections::{HashMap, HashSet};
 
 use itertools::Itertools;
 use parking_lot::{RwLock, RwLockWriteGuard};
+use segment::data_types::named_vectors::NamedVectors;
 use segment::entry::entry_point::{OperationResult, SegmentEntry};
 use segment::types::{
     Filter, Payload, PayloadFieldSchema, PayloadKeyType, PayloadKeyTypeRef, PointIdType,
-    SeqNumberType, VectorElementType,
+    SeqNumberType,
 };
 
 use crate::collection_manager::holders::segment_holder::SegmentHolder;
 use crate::operations::payload_ops::PayloadOps;
-use crate::operations::point_ops::{Batch, PointInsertOperations, PointOperations, PointStruct};
+use crate::operations::point_ops::{PointInsertOperations, PointOperations, PointStruct};
 use crate::operations::types::{CollectionError, CollectionResult};
 use crate::operations::FieldIndexOperations;
 
@@ -152,10 +153,10 @@ fn upsert_with_payload(
     segment: &mut RwLockWriteGuard<dyn SegmentEntry>,
     op_num: SeqNumberType,
     point_id: PointIdType,
-    vector: &[VectorElementType],
+    vectors: &NamedVectors,
     payload: Option<&Payload>,
 ) -> OperationResult<bool> {
-    let mut res = segment.upsert_vector(op_num, point_id, vector)?;
+    let mut res = segment.upsert_vector(op_num, point_id, vectors)?;
     if let Some(full_payload) = payload {
         res &= segment.set_payload(op_num, point_id, full_payload)?;
     }
@@ -200,10 +201,10 @@ pub(crate) fn sync_points(
 
     let mut points_to_update: Vec<_> = Vec::new();
     let _num_updated = segments.read_points(existing_point_ids.as_slice(), |id, segment| {
-        let vector = segment.vector(id)?;
+        let all_vectors = segment.all_vectors(id)?;
         let payload = segment.payload(id)?;
         let point = id_to_point.get(&id).unwrap();
-        if point.vector != vector {
+        if point.get_vectors() != all_vectors {
             points_to_update.push(*point);
             Ok(true)
         } else {
@@ -264,7 +265,7 @@ where
                 write_segment,
                 op_num,
                 id,
-                point.vector.as_slice(),
+                &point.get_vectors(),
                 point.payload.as_ref(),
             )
         })?;
@@ -292,7 +293,7 @@ where
                 &mut write_segment,
                 op_num,
                 point_id,
-                point.vector.as_slice(),
+                &point.get_vectors(),
                 point.payload.as_ref(),
             )? as usize;
         }
@@ -310,25 +311,22 @@ pub(crate) fn process_point_operation(
         PointOperations::DeletePoints { ids, .. } => delete_points(&segments.read(), op_num, &ids),
         PointOperations::UpsertPoints(operation) => {
             let points: Vec<_> = match operation {
-                PointInsertOperations::PointsBatch(Batch {
-                    ids,
-                    vectors,
-                    payloads,
-                }) => {
-                    let vectors_iter = ids.into_iter().zip(vectors.into_iter());
-                    match payloads {
+                PointInsertOperations::PointsBatch(batch) => {
+                    let all_vectors = batch.vectors.into_all_vectors(batch.ids.len());
+                    let vectors_iter = batch.ids.into_iter().zip(all_vectors.into_iter());
+                    match batch.payloads {
                         None => vectors_iter
-                            .map(|(id, vector)| PointStruct {
+                            .map(|(id, vectors)| PointStruct {
                                 id,
-                                vector,
+                                vector: vectors.into(),
                                 payload: None,
                             })
                             .collect(),
                         Some(payloads) => vectors_iter
                             .zip(payloads.into_iter())
-                            .map(|((id, vector), payload)| PointStruct {
+                            .map(|((id, vectors), payload)| PointStruct {
                                 id,
-                                vector,
+                                vector: vectors.into(),
                                 payload,
                             })
                             .collect(),

commit ba26e2f85e36fc1f4258de9351ad4d90082056c0
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Mon Sep 12 17:55:56 2022 +0200

    Faster filtered scroll (#1003)
    
    * faster filtered scroll for low cardinality filters
    
    * add test
    
    * scroll strategy heuristics

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 07002320c..d420c7eed 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -108,7 +108,7 @@ pub(crate) fn clear_payload_by_filter(
     let mut points_to_clear: Vec<PointIdType> = Vec::new();
 
     segments.apply_segments(|s| {
-        let points = s.read_filtered(None, usize::MAX, Some(filter));
+        let points = s.read_filtered(None, None, Some(filter));
         points_to_clear.extend_from_slice(points.as_slice());
         Ok(true)
     })?;

commit dc07b01e1fea5cb9be3579b555be480e30aa3041
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Mon Sep 19 13:51:03 2022 +0200

    remove deprecated fields from API (#1030)
    
    * remove depricated fields from API
    
    * fmt
    
    * upd openapi and integration tests
    
    * fix grpc test
    
    * regenerate storage reference data
    
    * improve docs
    
    Co-authored-by: Arnaud Gourlay <arnaud.gourlay@gmail.com>

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index d420c7eed..4e50a55ca 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -385,7 +385,7 @@ pub(crate) fn process_field_index_operation(
             &segments.read(),
             op_num,
             &index_data.field_name,
-            index_data.field_type.as_ref(),
+            index_data.field_schema.as_ref(),
         ),
         FieldIndexOperations::DeleteIndex(field_name) => {
             delete_field_index(&segments.read(), op_num, field_name)

commit 1a295ac3a099c459d7e5b01c056f84c2a22578e6
Author: Ivan Pleshkov <pleshkov.ivan@gmail.com>
Date:   Tue Sep 27 20:03:11 2022 +0400

    Fix upsert freezes on rust client stress test (#1061)
    
    * use parking lot for wal
    
    * fair unlock
    
    * limit update queue
    
    * Revert "limit update queue"
    
    This reverts commit 7df88870f64571ef92c4f99677f166568e2399c2.
    
    * Limited queue (#1062)
    
    * limit update queue size
    
    * fmt
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 4e50a55ca..064e1d3df 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -297,6 +297,7 @@ where
                 point.payload.as_ref(),
             )? as usize;
         }
+        RwLockWriteGuard::unlock_fair(write_segment);
     };
 
     Ok(res)

commit 9324be1d6038b4ba66ca776eb7e753e69fe5d624
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Mon Oct 17 20:12:10 2022 +0200

    [Replication] Add replicas (#1085)
    
    * Add replicas to ReplicaSet
    
    * unproxify shard & miscs
    
    * no exlusive write locking while adding replicas
    
    * on_optimizer_config_update on Shard with async. recursion
    
    * no exlusive write locking while removing replicas
    
    * shortcut replica set propagation #1101
    
    * remove unused field
    
    * make RequestShardTransfer callback sync.
    
    * promote local & remote to replica state
    
    * fixes for replica sync api (#1123)
    
    * code review
    
    * fix replica set update - fail only if all failed
    
    * Add replica redesign (#1131)
    
    * refactor shard/mod.rs
    
    * wip
    
    * fmt
    
    * it compiles
    
    * temporary disable replica placemeant change on replication factor
    
    * fmt
    
    * finish todos
    
    * small refactoring
    
    * remove change::add
    
    * replica-set -> shard-replica-set
    
    * fmt
    
    * upd openapi
    
    * fix finish transfer logic
    
    * fix existing integration tests
    
    * shard transfer validation
    
    * fmt
    
    * review fixes
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 064e1d3df..81c5ebd0d 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -163,7 +163,7 @@ fn upsert_with_payload(
     Ok(res)
 }
 
-/// Sync points within a given range
+/// Sync points within a given [from_id; to_id) range
 ///
 /// 1. Retrieve existing points for a range
 /// 2. Remove points, which are not present in the sync operation

commit bf480661458cb01af0291adb8992c76b71e2d553
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Thu Oct 20 10:26:33 2022 +0200

    Full reassign payload on upsert (#1148)
    
    * set_full_payload on upsert
    
    * fmt
    
    * fix clippy

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 81c5ebd0d..07e9613d3 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -158,7 +158,7 @@ fn upsert_with_payload(
 ) -> OperationResult<bool> {
     let mut res = segment.upsert_vector(op_num, point_id, vectors)?;
     if let Some(full_payload) = payload {
-        res &= segment.set_payload(op_num, point_id, full_payload)?;
+        res &= segment.set_full_payload(op_num, point_id, full_payload)?;
     }
     Ok(res)
 }

commit 0d130c395a65f13f48d13c6b1db83542e3e7ec82
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Mon Nov 28 13:46:58 2022 +0100

    Full payload update (#1245)
    
    * implement api to fully overwrite payload of the point
    
    * fmt

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 07e9613d3..5072c396a 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -50,6 +50,22 @@ pub(crate) fn delete_points(
     Ok(res)
 }
 
+pub(crate) fn overwrite_payload(
+    segments: &SegmentHolder,
+    op_num: SeqNumberType,
+    payload: &Payload,
+    points: &[PointIdType],
+) -> CollectionResult<usize> {
+    let updated_points =
+        segments.apply_points_to_appendable(op_num, points, |id, write_segment| {
+            write_segment.set_full_payload(op_num, id, payload)?;
+            Ok(true)
+        })?;
+
+    check_unprocessed_points(points, &updated_points)?;
+    Ok(updated_points.len())
+}
+
 pub(crate) fn set_payload(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
@@ -373,6 +389,10 @@ pub(crate) fn process_payload_operation(
         PayloadOps::ClearPayloadByFilter(ref filter) => {
             clear_payload_by_filter(&segments.read(), op_num, filter)
         }
+        PayloadOps::OverwritePayload(sp) => {
+            let payload: Payload = sp.payload;
+            overwrite_payload(&segments.read(), op_num, &payload, &sp.points)
+        }
     }
 }
 

commit 469c098ca4bd690ce79b54420243b9d4e40889d4
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Thu Dec 1 09:16:07 2022 +0100

    Update points by filter (#1249)
    
    * allow to set remove payload by selector
    
    * fmt

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 5072c396a..b7d583b1e 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -66,6 +66,16 @@ pub(crate) fn overwrite_payload(
     Ok(updated_points.len())
 }
 
+pub(crate) fn overwrite_payload_by_filter(
+    segments: &SegmentHolder,
+    op_num: SeqNumberType,
+    payload: &Payload,
+    filter: &Filter,
+) -> CollectionResult<usize> {
+    let affected_points = points_by_filter(segments, filter)?;
+    overwrite_payload(segments, op_num, payload, &affected_points)
+}
+
 pub(crate) fn set_payload(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
@@ -82,6 +92,29 @@ pub(crate) fn set_payload(
     Ok(updated_points.len())
 }
 
+fn points_by_filter(
+    segments: &SegmentHolder,
+    filter: &Filter,
+) -> CollectionResult<Vec<PointIdType>> {
+    let mut affected_points: Vec<PointIdType> = Vec::new();
+    segments.for_each_segment(|s| {
+        let points = s.read_filtered(None, None, Some(filter));
+        affected_points.extend_from_slice(points.as_slice());
+        Ok(true)
+    })?;
+    Ok(affected_points)
+}
+
+pub(crate) fn set_payload_by_filter(
+    segments: &SegmentHolder,
+    op_num: SeqNumberType,
+    payload: &Payload,
+    filter: &Filter,
+) -> CollectionResult<usize> {
+    let affected_points = points_by_filter(segments, filter)?;
+    set_payload(segments, op_num, payload, &affected_points)
+}
+
 pub(crate) fn delete_payload(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
@@ -101,6 +134,16 @@ pub(crate) fn delete_payload(
     Ok(updated_points.len())
 }
 
+pub(crate) fn delete_payload_by_filter(
+    segments: &SegmentHolder,
+    op_num: SeqNumberType,
+    filter: &Filter,
+    keys: &[PayloadKeyType],
+) -> CollectionResult<usize> {
+    let affected_points = points_by_filter(segments, filter)?;
+    delete_payload(segments, op_num, &affected_points, keys)
+}
+
 pub(crate) fn clear_payload(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
@@ -121,13 +164,7 @@ pub(crate) fn clear_payload_by_filter(
     op_num: SeqNumberType,
     filter: &Filter,
 ) -> CollectionResult<usize> {
-    let mut points_to_clear: Vec<PointIdType> = Vec::new();
-
-    segments.apply_segments(|s| {
-        let points = s.read_filtered(None, None, Some(filter));
-        points_to_clear.extend_from_slice(points.as_slice());
-        Ok(true)
-    })?;
+    let points_to_clear = points_by_filter(segments, filter)?;
 
     let updated_points = segments.apply_points_to_appendable(
         op_num,
@@ -378,10 +415,26 @@ pub(crate) fn process_payload_operation(
     match payload_operation {
         PayloadOps::SetPayload(sp) => {
             let payload: Payload = sp.payload;
-            set_payload(&segments.read(), op_num, &payload, &sp.points)
+            if let Some(points) = sp.points {
+                set_payload(&segments.read(), op_num, &payload, &points)
+            } else if let Some(filter) = sp.filter {
+                set_payload_by_filter(&segments.read(), op_num, &payload, &filter)
+            } else {
+                Err(CollectionError::BadRequest {
+                    description: "No points or filter specified".to_string(),
+                })
+            }
         }
         PayloadOps::DeletePayload(dp) => {
-            delete_payload(&segments.read(), op_num, &dp.points, &dp.keys)
+            if let Some(points) = dp.points {
+                delete_payload(&segments.read(), op_num, &points, &dp.keys)
+            } else if let Some(filter) = dp.filter {
+                delete_payload_by_filter(&segments.read(), op_num, &filter, &dp.keys)
+            } else {
+                Err(CollectionError::BadRequest {
+                    description: "No points or filter specified".to_string(),
+                })
+            }
         }
         PayloadOps::ClearPayload { ref points, .. } => {
             clear_payload(&segments.read(), op_num, points)
@@ -391,7 +444,15 @@ pub(crate) fn process_payload_operation(
         }
         PayloadOps::OverwritePayload(sp) => {
             let payload: Payload = sp.payload;
-            overwrite_payload(&segments.read(), op_num, &payload, &sp.points)
+            if let Some(points) = sp.points {
+                overwrite_payload(&segments.read(), op_num, &payload, &points)
+            } else if let Some(filter) = sp.filter {
+                overwrite_payload_by_filter(&segments.read(), op_num, &payload, &filter)
+            } else {
+                Err(CollectionError::BadRequest {
+                    description: "No points or filter specified".to_string(),
+                })
+            }
         }
     }
 }

commit 2d02f209ae9bdbb5140a361c06e469770ab05d73
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Tue Dec 6 12:31:24 2022 +0100

    Missed points on update fix (#1255)
    
    * add context to point-not-found error
    
    * fmt
    
    * add backtrace
    
    * use upgradable lock
    
    * unique points in insert operation
    
    * remove debug code

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index b7d583b1e..975e3a409 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -303,12 +303,9 @@ pub(crate) fn upsert_points<'a, T>(
 where
     T: IntoIterator<Item = &'a PointStruct>,
 {
-    let mut ids: Vec<PointIdType> = vec![];
-    let mut points_map: HashMap<PointIdType, &PointStruct> = Default::default();
-    points.into_iter().for_each(|p| {
-        ids.push(p.id);
-        points_map.insert(p.id, p);
-    });
+    let points_map: HashMap<PointIdType, &PointStruct> =
+        points.into_iter().map(|p| (p.id, p)).collect();
+    let ids: Vec<PointIdType> = points_map.keys().copied().collect();
 
     // Update points in writable segments
     let updated_points =

commit 6a0d90775574d00afc94e8aa567b596fd4e4a15f
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Tue Dec 6 12:31:38 2022 +0100

    include backtrace for service errors (#1256)

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 975e3a409..4a3c85cec 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -328,12 +328,9 @@ where
         .filter(|x| !(updated_points.contains(x)));
 
     {
-        let default_write_segment =
-            segments
-                .random_appendable_segment()
-                .ok_or(CollectionError::ServiceError {
-                    error: "No segments exists, expected at least one".to_string(),
-                })?;
+        let default_write_segment = segments.random_appendable_segment().ok_or_else(|| {
+            CollectionError::service_error("No segments exists, expected at least one".to_string())
+        })?;
 
         let segment_arc = default_write_segment.get();
         let mut write_segment = segment_arc.write();

commit 1c85c9b2359c81897da57ea7dd5e9f0bdbf67791
Author: Tim Vise <tim+github@visee.me>
Date:   Fri Apr 28 10:36:58 2023 +0200

    Add optimizer for many deleted points, make aware of deleted points and vectors (#1758)
    
    * Minor collection optimizer cleanup
    
    * Make optimizers better aware of available vs soft deleted points
    
    * Fix incorrect deleted state on proxy segment for double delete
    
    * Rename upsert_vector to upsert_point, because we work with points
    
    * Refactor point methods for more clear and consistent naming
    
    * Replace internal_size in IdTracker with total_point_count
    
    * Keep track of vector deletion count on storage creation
    
    * Add sparse index optimizer, to optimize indexes with high deletion count
    
    * Add minimum vector count threshold to sparse index optimizer
    
    * Add sparse index optimizer test
    
    * Use consistent naming, write vector in full everywhere
    
    * Simplify vacuum optimizer a bit
    
    * Merge sparse index optimizer into vacuum optimizer
    
    * Improve update_from in segment builder by returning early
    
    * More accurately count vectors in segment optimizer
    
    * Remove random from vacuum optimizer tests to make them more reliable
    
    * Don't expose the total points in segment info, use available points
    
    * Process review feedback
    
    * Compare available vectors against indexed ones in vacuum optimizer
    
    This is much better than using the number of soft-deleted vectors when
    the segment was created for calculations. Not to mention that value had
    other problems as well.
    
    * Remove create_deleted_vector_count field, update vacuum test parameters
    
    * Potentially solve out of bound panic when building index
    
    * Review fixes:
    
    - Propagate deleted flags into payload hnsw building
    - Use `total` number of points for building HNSW instead of number of
      available points
    - minor refactoring of `hnsw_config` copy -> clone
    - Better detection of `indexed_points` in HNSW
    
    * fix assert condition
    
    * Optional named vectors optimizer reveiw 2 (#1794)
    
    * review with Ivan
    
    * fmt
    
    * remove available_vector_count from segment entry
    
    * remove total_point_count from segment entry
    
    ---------
    
    Co-authored-by: Ivan Pleshkov <pleshkov.ivan@gmail.com>
    
    * rollback changes in deleted count in proxy segment
    
    * improve vector threshold detection logic in optimized_segment_builder
    
    * style changes
    
    * fix propagate deleted points to vectors
    
    * Fix typo in method name
    
    ---------
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>
    Co-authored-by: Ivan Pleshkov <pleshkov.ivan@gmail.com>

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 4a3c85cec..cbab61d9d 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -209,7 +209,7 @@ fn upsert_with_payload(
     vectors: &NamedVectors,
     payload: Option<&Payload>,
 ) -> OperationResult<bool> {
-    let mut res = segment.upsert_vector(op_num, point_id, vectors)?;
+    let mut res = segment.upsert_point(op_num, point_id, vectors)?;
     if let Some(full_payload) = payload {
         res &= segment.set_full_payload(op_num, point_id, full_payload)?;
     }

commit 42e2365b17556607c58aafe589681578cad8c18e
Author: Tim Vise <tim+github@visee.me>
Date:   Fri May 5 08:45:42 2023 +0200

    General optional named vector improvements (#1835)
    
    * Remove unnecessary mut
    
    * Simplify creating option with if
    
    * Simplify some returns
    
    * DRY in vector name checking functions
    
    * Use panic with attribute rather than debug_assert false
    
    * Make update vector function plural because we can update multiple

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index cbab61d9d..c58491b05 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -44,10 +44,11 @@ pub(crate) fn delete_points(
     op_num: SeqNumberType,
     ids: &[PointIdType],
 ) -> CollectionResult<usize> {
-    let res = segments.apply_points(ids, |id, _idx, write_segment| {
-        write_segment.delete_point(op_num, id)
-    })?;
-    Ok(res)
+    segments
+        .apply_points(ids, |id, _idx, write_segment| {
+            write_segment.delete_point(op_num, id)
+        })
+        .map_err(Into::into)
 }
 
 pub(crate) fn overwrite_payload(
@@ -125,7 +126,7 @@ pub(crate) fn delete_payload(
         segments.apply_points_to_appendable(op_num, points, |id, write_segment| {
             let mut res = true;
             for key in keys {
-                res = write_segment.delete_payload(op_num, id, key)? && res;
+                res &= write_segment.delete_payload(op_num, id, key)?;
             }
             Ok(res)
         })?;
@@ -181,10 +182,11 @@ pub(crate) fn create_field_index(
     field_name: PayloadKeyTypeRef,
     field_schema: Option<&PayloadFieldSchema>,
 ) -> CollectionResult<usize> {
-    let res = segments.apply_segments(|write_segment| {
-        write_segment.create_field_index(op_num, field_name, field_schema)
-    })?;
-    Ok(res)
+    segments
+        .apply_segments(|write_segment| {
+            write_segment.create_field_index(op_num, field_name, field_schema)
+        })
+        .map_err(Into::into)
 }
 
 pub(crate) fn delete_field_index(
@@ -192,9 +194,9 @@ pub(crate) fn delete_field_index(
     op_num: SeqNumberType,
     field_name: PayloadKeyTypeRef,
 ) -> CollectionResult<usize> {
-    let res = segments
-        .apply_segments(|write_segment| write_segment.delete_field_index(op_num, field_name))?;
-    Ok(res)
+    segments
+        .apply_segments(|write_segment| write_segment.delete_field_index(op_num, field_name))
+        .map_err(Into::into)
 }
 
 ///

commit 5805811ad4b6d41aaa3033c4df36a4fe8536e958
Author: Tim Vise <tim+github@visee.me>
Date:   Fri May 5 15:18:19 2023 +0200

    Add gRPC interface to update/delete optional named vectors (#1816)
    
    * Add segment entry function to update named vectors
    
    * Use already available function to update existing vectors
    
    We already had a segment function to update existing named vectors. This
    change ensure we use that instead of separating it separately. As a
    bonus, this adds support for setting multiple named vectors at once.
    
    * Update set vectors ourselves, don't drop omitted vectors
    
    * Refactor vector updating functions, separate update and replace
    
    * Add basic vector ops, add update/delete functionality to segment updater
    
    * Add internal and public gRPC types and actions for vectors
    
    * Add gRPC API actions
    
    * Reformat
    
    * Add VectorOperations to vector ops, add basic validation
    
    * Validate gRPC vector types
    
    * Validate vector operation structs
    
    * Construct PointIdsList through From trait
    
    * Update gRPC docs
    
    * Use VectorsSelector for vector deletions in gRPC
    
    * Add support for updating multiple points/vectors in update vectors API
    
    * Update gRPC docs
    
    * Fix incorrect gRPC type numbering
    
    * Return point ID error from vector update/delete functions if not found
    
    * Fix disbalanced vectors test

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index c58491b05..5bb0839ab 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -1,3 +1,5 @@
+//! A collection of functions for updating points and payloads stored in segments
+
 use std::collections::{HashMap, HashSet};
 
 use itertools::Itertools;
@@ -13,10 +15,9 @@ use crate::collection_manager::holders::segment_holder::SegmentHolder;
 use crate::operations::payload_ops::PayloadOps;
 use crate::operations::point_ops::{PointInsertOperations, PointOperations, PointStruct};
 use crate::operations::types::{CollectionError, CollectionResult};
+use crate::operations::vector_ops::{PointVectors, VectorOperations};
 use crate::operations::FieldIndexOperations;
 
-/// A collection of functions for updating points and payloads stored in segments
-
 pub(crate) fn check_unprocessed_points(
     points: &[PointIdType],
     processed: &HashSet<PointIdType>,
@@ -51,6 +52,54 @@ pub(crate) fn delete_points(
         .map_err(Into::into)
 }
 
+/// Update the specified named vectors of a point, keeping unspecified vectors intact.
+pub(crate) fn update_vectors(
+    segments: &SegmentHolder,
+    op_num: SeqNumberType,
+    points: &[PointVectors],
+) -> CollectionResult<usize> {
+    let points_map: HashMap<PointIdType, &PointVectors> =
+        points.iter().map(|p| (p.id, p)).collect();
+    let ids: Vec<PointIdType> = points_map.keys().copied().collect();
+
+    let updated_points =
+        segments.apply_points_to_appendable(op_num, &ids, |id, write_segment| {
+            let vectors = points_map[&id].vector.clone().into_all_vectors();
+            write_segment.update_vectors(op_num, id, vectors)
+        })?;
+    check_unprocessed_points(&ids, &updated_points)?;
+    Ok(updated_points.len())
+}
+
+/// Delete the given named vectors for the given points, keeping other vectors intact.
+pub(crate) fn delete_vectors(
+    segments: &SegmentHolder,
+    op_num: SeqNumberType,
+    points: &[PointIdType],
+    vector_names: &[String],
+) -> CollectionResult<usize> {
+    segments
+        .apply_points(points, |id, _idx, write_segment| {
+            let mut res = true;
+            for name in vector_names {
+                res &= write_segment.delete_vector(op_num, id, name)?;
+            }
+            Ok(res)
+        })
+        .map_err(Into::into)
+}
+
+/// Delete the given named vectors for points matching the given filter, keeping otehr vectors intact.
+pub(crate) fn delete_vectors_by_filter(
+    segments: &SegmentHolder,
+    op_num: SeqNumberType,
+    filter: &Filter,
+    vector_names: &[String],
+) -> CollectionResult<usize> {
+    let affected_points = points_by_filter(segments, filter)?;
+    delete_vectors(segments, op_num, &affected_points, vector_names)
+}
+
 pub(crate) fn overwrite_payload(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
@@ -403,6 +452,24 @@ pub(crate) fn process_point_operation(
     }
 }
 
+pub(crate) fn process_vector_operation(
+    segments: &RwLock<SegmentHolder>,
+    op_num: SeqNumberType,
+    vector_operation: VectorOperations,
+) -> CollectionResult<usize> {
+    match vector_operation {
+        VectorOperations::UpdateVectors(operation) => {
+            update_vectors(&segments.read(), op_num, &operation.points)
+        }
+        VectorOperations::DeleteVectors(ids, vector_names) => {
+            delete_vectors(&segments.read(), op_num, &ids.points, &vector_names)
+        }
+        VectorOperations::DeleteVectorsByFilter(filter, vector_names) => {
+            delete_vectors_by_filter(&segments.read(), op_num, &filter, &vector_names)
+        }
+    }
+}
+
 pub(crate) fn process_payload_operation(
     segments: &RwLock<SegmentHolder>,
     op_num: SeqNumberType,

commit 291be89400a850cf937af27e4b65a4e2ca8d4c9e
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Wed May 31 23:15:55 2023 +0200

    Extend `SerdeWal` to better track the last truncated-to WAL index (#1815) (#1868)
    
    * Skip applied WAL operations on shard load
    
    * fixup! Skip applied WAL operations on shard load
    
    - Skip WAL operation up to the *minimal* persisted version
    - Add explanation comment
    
    * fixup! Skip applied WAL operations on shard load
    
    - "beautify" `skip_while` expression
    - handle version `0` corner case (and add explanation)
    
    * Update `Segment::handle_version` comment
    
    * Simplify `Segment::handle_version` a bit
    
    * Refactor `check_unprocessed_points`
    
    * fixup! Refactor `check_unprocessed_points`
    
    Optimized `check_unprocessed_points`
    
    * Revert `LocalShard::load_from_wal` and `check_unprocessed_points` changes
    
    * Downgrade failure in `check_unprocessed_points` to a warning
    
    * Persist/track the first un-truncated index in `SerdeWal`...
    
    ...and use it as first index for WAL operations
    
    * fixup! Downgrade failure in `check_unprocessed_points` to a warning
    
    Downgrade failure in `check_unprocessed_points` to a warning *during `LocalShard::load_from_wal`* 
    
    * fixup! Persist/track the first un-truncated index in `SerdeWal`...
    
    Words...
    
    * Fix a bug in `SerdeWal::truncated_prefix_entries_num` and add explanation comment
    
    * Fix a bug in `SerdeWal::ack`
    
    * prevent unnesessary savings, use json, never decrease ack index
    
    ---------
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 5bb0839ab..038c8e39c 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -2,7 +2,6 @@
 
 use std::collections::{HashMap, HashSet};
 
-use itertools::Itertools;
 use parking_lot::{RwLock, RwLockWriteGuard};
 use segment::data_types::named_vectors::NamedVectors;
 use segment::entry::entry_point::{OperationResult, SegmentEntry};
@@ -22,20 +21,11 @@ pub(crate) fn check_unprocessed_points(
     points: &[PointIdType],
     processed: &HashSet<PointIdType>,
 ) -> CollectionResult<usize> {
-    let unprocessed_points = points
-        .iter()
-        .cloned()
-        .filter(|p| !processed.contains(p))
-        .collect_vec();
-    let missed_point = unprocessed_points.iter().cloned().next();
-
-    // ToDo: check pre-existing points
+    let first_missed_point = points.iter().copied().find(|p| !processed.contains(p));
 
-    match missed_point {
+    match first_missed_point {
         None => Ok(processed.len()),
-        Some(missed_point) => Err(CollectionError::PointNotFound {
-            missed_point_id: missed_point,
-        }),
+        Some(missed_point_id) => Err(CollectionError::PointNotFound { missed_point_id }),
     }
 }
 

commit 0fb05b38eae96948473871920a48e8500a20fe52
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Mon Jul 3 12:38:18 2023 +0200

    handle inconsistent vector storage in sync operation (#2185)

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 038c8e39c..7732c21c0 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -4,7 +4,7 @@ use std::collections::{HashMap, HashSet};
 
 use parking_lot::{RwLock, RwLockWriteGuard};
 use segment::data_types::named_vectors::NamedVectors;
-use segment::entry::entry_point::{OperationResult, SegmentEntry};
+use segment::entry::entry_point::{OperationError, OperationResult, SegmentEntry};
 use segment::types::{
     Filter, Payload, PayloadFieldSchema, PayloadKeyType, PayloadKeyTypeRef, PointIdType,
     SeqNumberType,
@@ -295,7 +295,11 @@ pub(crate) fn sync_points(
 
     let mut points_to_update: Vec<_> = Vec::new();
     let _num_updated = segments.read_points(existing_point_ids.as_slice(), |id, segment| {
-        let all_vectors = segment.all_vectors(id)?;
+        let all_vectors = match segment.all_vectors(id) {
+            Ok(v) => v,
+            Err(OperationError::InconsistentStorage { .. }) => NamedVectors::default(),
+            Err(e) => return Err(e),
+        };
         let payload = segment.payload(id)?;
         let point = id_to_point.get(&id).unwrap();
         if point.get_vectors() != all_vectors {

commit 396714f7faa04ac6a64d63c784adfda25d468737
Author: Ivan Pleshkov <pleshkov.ivan@gmail.com>
Date:   Wed Jul 5 00:30:15 2023 +0200

    Add missed vector preprocess (#2203)
    
    * test missed preprocess after segment update
    
    * missed preprocess
    
    * remove preprocess_named_vectors fn
    
    * are you happy clippy
    
    * fix integration tests
    
    ---------
    
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 7732c21c0..d222255a8 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -247,7 +247,7 @@ fn upsert_with_payload(
     segment: &mut RwLockWriteGuard<dyn SegmentEntry>,
     op_num: SeqNumberType,
     point_id: PointIdType,
-    vectors: &NamedVectors,
+    vectors: NamedVectors,
     payload: Option<&Payload>,
 ) -> OperationResult<bool> {
     let mut res = segment.upsert_point(op_num, point_id, vectors)?;
@@ -360,7 +360,7 @@ where
                 write_segment,
                 op_num,
                 id,
-                &point.get_vectors(),
+                point.get_vectors(),
                 point.payload.as_ref(),
             )
         })?;
@@ -385,7 +385,7 @@ where
                 &mut write_segment,
                 op_num,
                 point_id,
-                &point.get_vectors(),
+                point.get_vectors(),
                 point.payload.as_ref(),
             )? as usize;
         }

commit 462ce6ba051297aac2c32ab0e89de3e1cbb24390
Author: Yaroslav Halchenko <debian@onerussian.com>
Date:   Wed Jul 12 10:30:42 2023 -0400

    codespell: workflow, config, typos fixed (#2248)
    
    * Add github action to codespell master on push and PRs
    
    * Add rudimentary codespell config
    
    * some skips
    
    * fix some ambigous typos
    
    * [DATALAD RUNCMD] run codespell throughout
    
    === Do not change lines below ===
    {
     "chain": [],
     "cmd": "codespell -w",
     "exit": 0,
     "extra_inputs": [],
     "inputs": [],
     "outputs": [],
     "pwd": "."
    }
    ^^^ Do not change lines above ^^^
    
    * Add dev branch as target for the workflow

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index d222255a8..06193d74d 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -79,7 +79,7 @@ pub(crate) fn delete_vectors(
         .map_err(Into::into)
 }
 
-/// Delete the given named vectors for points matching the given filter, keeping otehr vectors intact.
+/// Delete the given named vectors for points matching the given filter, keeping other vectors intact.
 pub(crate) fn delete_vectors_by_filter(
     segments: &SegmentHolder,
     op_num: SeqNumberType,

commit 5a021c51d5786b9281254720bdfd211d2d0d04c2
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Thu Aug 24 16:43:34 2023 +0200

    Fix lints for Clippy 1.72 (#2476)

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 06193d74d..c6dc39fa8 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -406,7 +406,7 @@ pub(crate) fn process_point_operation(
             let points: Vec<_> = match operation {
                 PointInsertOperations::PointsBatch(batch) => {
                     let all_vectors = batch.vectors.into_all_vectors(batch.ids.len());
-                    let vectors_iter = batch.ids.into_iter().zip(all_vectors.into_iter());
+                    let vectors_iter = batch.ids.into_iter().zip(all_vectors);
                     match batch.payloads {
                         None => vectors_iter
                             .map(|(id, vectors)| PointStruct {
@@ -416,7 +416,7 @@ pub(crate) fn process_point_operation(
                             })
                             .collect(),
                         Some(payloads) => vectors_iter
-                            .zip(payloads.into_iter())
+                            .zip(payloads)
                             .map(|((id, vectors), payload)| PointStruct {
                                 id,
                                 vector: vectors.into(),

commit dca96985406c4c7a3b5a5e9c76a49cf5f0973603
Author: Tim Vise <tim+github@visee.me>
Date:   Thu Aug 31 10:03:51 2023 +0200

    Fix missing points, vectors and payload (#2514)
    
    * On segment flush, read-lock all segments to prevent CoW between flushes
    
    For example, we have a point on an immutable segment. If we use a
    set-payload operation, we do copy-on-write. The point from immutable
    segment A is deleted, the updated point is stored on appendable segment
    B.
    
    Because of flush ordering segment B (appendable) is flushed before
    segment A (not-appendable). If the copy-on-write operation happens in
    between, the point is deleted from A but the new point in B is not
    persisted. We cannot recover this by replaying the WAL in case of a
    crash because the point in A does not exist anymore, making
    copy-on-write impossible.
    
    Locking all segments prevents copy-on-write operations from occurring in
    between flushes.
    
    * Return proper status on set payload operations in segment
    
    * Disable propagating point deletions to its vectors, it looses vectors
    
    * Update vector tests after disabling point delete propagation to vectors
    
    * Implement retry with exponential backoff for read-locking all segments
    
    * Use try_read_for rather than sleeping after lock attempts
    
    * Simplify locking many, just lock one by one
    
    Refs: <https://github.com/qdrant/qdrant/pull/2527>
    Co-authored-by: generall <andrey@vasnetsov.com>
    
    * Comment out now obsolete test
    
    * Error handling in method getting segment locks by ID
    
    ---------
    
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index c6dc39fa8..364b13f80 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -98,8 +98,7 @@ pub(crate) fn overwrite_payload(
 ) -> CollectionResult<usize> {
     let updated_points =
         segments.apply_points_to_appendable(op_num, points, |id, write_segment| {
-            write_segment.set_full_payload(op_num, id, payload)?;
-            Ok(true)
+            write_segment.set_full_payload(op_num, id, payload)
         })?;
 
     check_unprocessed_points(points, &updated_points)?;
@@ -124,8 +123,7 @@ pub(crate) fn set_payload(
 ) -> CollectionResult<usize> {
     let updated_points =
         segments.apply_points_to_appendable(op_num, points, |id, write_segment| {
-            write_segment.set_payload(op_num, id, payload)?;
-            Ok(true)
+            write_segment.set_payload(op_num, id, payload)
         })?;
 
     check_unprocessed_points(points, &updated_points)?;

commit 4f983e495db72336b2311dc2abe95a11eab8c620
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Fri Sep 29 16:23:24 2023 +0200

    Promote operation error to dedicated file (#2736)

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 364b13f80..60ff40a6c 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -3,8 +3,9 @@
 use std::collections::{HashMap, HashSet};
 
 use parking_lot::{RwLock, RwLockWriteGuard};
+use segment::common::operation_error::{OperationError, OperationResult};
 use segment::data_types::named_vectors::NamedVectors;
-use segment::entry::entry_point::{OperationError, OperationResult, SegmentEntry};
+use segment::entry::entry_point::SegmentEntry;
 use segment::types::{
     Filter, Payload, PayloadFieldSchema, PayloadKeyType, PayloadKeyTypeRef, PointIdType,
     SeqNumberType,

commit 816b5a7448c7f1e0d81c99e5a31219d00ece6fe5
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Thu Nov 9 15:06:02 2023 +0100

    Shard key routing for update requests (#2909)
    
    * add shard_key into output data structures for points
    
    * fmt
    
    * add shard selector for point update operations
    
    * fix creating index without sharding
    
    * Merge serde attributes
    
    * Code review changes
    
    * review fixes
    
    * upd openapi
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 60ff40a6c..427bda385 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -13,7 +13,7 @@ use segment::types::{
 
 use crate::collection_manager::holders::segment_holder::SegmentHolder;
 use crate::operations::payload_ops::PayloadOps;
-use crate::operations::point_ops::{PointInsertOperations, PointOperations, PointStruct};
+use crate::operations::point_ops::{PointInsertOperationsInternal, PointOperations, PointStruct};
 use crate::operations::types::{CollectionError, CollectionResult};
 use crate::operations::vector_ops::{PointVectors, VectorOperations};
 use crate::operations::FieldIndexOperations;
@@ -403,7 +403,7 @@ pub(crate) fn process_point_operation(
         PointOperations::DeletePoints { ids, .. } => delete_points(&segments.read(), op_num, &ids),
         PointOperations::UpsertPoints(operation) => {
             let points: Vec<_> = match operation {
-                PointInsertOperations::PointsBatch(batch) => {
+                PointInsertOperationsInternal::PointsBatch(batch) => {
                     let all_vectors = batch.vectors.into_all_vectors(batch.ids.len());
                     let vectors_iter = batch.ids.into_iter().zip(all_vectors);
                     match batch.payloads {
@@ -424,7 +424,7 @@ pub(crate) fn process_point_operation(
                             .collect(),
                     }
                 }
-                PointInsertOperations::PointsList(points) => points,
+                PointInsertOperationsInternal::PointsList(points) => points,
             };
             let res = upsert_points(&segments.read(), op_num, points.iter())?;
             Ok(res)

commit 7ba801545540af06f2e0cee3b2c653b697082c57
Author: Tim Vise <tim+github@visee.me>
Date:   Fri Nov 24 13:02:42 2023 +0100

    Remove redundant copy from sum iterator (#3085)
    
    * Remove redundant copy from sum iterator
    
    * Remove another redundant copied call

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 427bda385..3dbe3d594 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -321,13 +321,10 @@ pub(crate) fn sync_points(
     // 4. Select new points
     let num_updated = points_to_update.len();
     let mut num_new = 0;
-    sync_points
-        .difference(&stored_point_ids)
-        .copied()
-        .for_each(|id| {
-            num_new += 1;
-            points_to_update.push(*id_to_point.get(&id).unwrap());
-        });
+    sync_points.difference(&stored_point_ids).for_each(|id| {
+        num_new += 1;
+        points_to_update.push(*id_to_point.get(id).unwrap());
+    });
 
     // 5. Upsert points which differ from the stored ones
     let num_replaced = upsert_points(segments, op_num, points_to_update)?;

commit 41b7a55e168e6aa6116740624a81a3010ff669db
Author: Tim Vise <tim+github@visee.me>
Date:   Mon Jan 15 10:08:06 2024 +0100

    Fix multiple vector updates on same point in batch not working (#3386)
    
    * Fix merge of vector operations, don't drop earlier updates on same point
    
    * Add unit test for vector struct merging
    
    * Add integration test for fix, also covering bug report

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 3dbe3d594..252abedcf 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -49,8 +49,16 @@ pub(crate) fn update_vectors(
     op_num: SeqNumberType,
     points: &[PointVectors],
 ) -> CollectionResult<usize> {
-    let points_map: HashMap<PointIdType, &PointVectors> =
-        points.iter().map(|p| (p.id, p)).collect();
+    // Build a map of vectors to update per point, merge updates on same point ID
+    let points_map: HashMap<PointIdType, PointVectors> =
+        points
+            .iter()
+            .fold(HashMap::with_capacity(points.len()), |mut map, p| {
+                map.entry(p.id)
+                    .and_modify(|e| e.vector.merge(p.vector.clone()))
+                    .or_insert_with(|| p.clone());
+                map
+            });
     let ids: Vec<PointIdType> = points_map.keys().copied().collect();
 
     let updated_points =

commit 87b541bb41560adf4609190cc0a7c1ed1da6e2f3
Author: shylock <tcath2s@gmail.com>
Date:   Thu Feb 15 22:15:05 2024 +0800

    Feat/set payload by key (#3548)
    
    * Support set by key in low level.
    
    * Rename key field.
    
    * Format.
    
    * Pass key.
    
    * Format.
    
    * Test.
    
    * Clippy.
    
    * Fix ci lint.
    
    * Check grpc consistency.
    
    * Update openapi.
    
    * Fix empty key test case.
    
    * Support array index.
    
    * Format.
    
    * Add test for non exists key.
    
    * Clippy fix.
    
    * Add idempotence test.
    
    * Update index by updated payload.
    
    * Add ut for utils.
    
    * Add ut for 1 level key.
    
    * Fix ut.
    
    * Support no exits key.
    
    * Fix test result.
    
    * Fix after rebase
    
    * handle wildcart insertion into non-existing array
    
    * avoid double read of payload during update
    
    * fix missing removing data from index in case if set_payload removes indexed field
    
    ---------
    
    Co-authored-by: Shylock Hg <shylock@DESKTOP-40I855A>
    Co-authored-by: Albert Safin <xzfcpw@gmail.com>
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 252abedcf..d0b015b04 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -129,10 +129,11 @@ pub(crate) fn set_payload(
     op_num: SeqNumberType,
     payload: &Payload,
     points: &[PointIdType],
+    key: &Option<String>,
 ) -> CollectionResult<usize> {
     let updated_points =
         segments.apply_points_to_appendable(op_num, points, |id, write_segment| {
-            write_segment.set_payload(op_num, id, payload)
+            write_segment.set_payload(op_num, id, payload, key)
         })?;
 
     check_unprocessed_points(points, &updated_points)?;
@@ -157,9 +158,10 @@ pub(crate) fn set_payload_by_filter(
     op_num: SeqNumberType,
     payload: &Payload,
     filter: &Filter,
+    key: &Option<String>,
 ) -> CollectionResult<usize> {
     let affected_points = points_by_filter(segments, filter)?;
-    set_payload(segments, op_num, payload, &affected_points)
+    set_payload(segments, op_num, payload, &affected_points, key)
 }
 
 pub(crate) fn delete_payload(
@@ -477,9 +479,9 @@ pub(crate) fn process_payload_operation(
         PayloadOps::SetPayload(sp) => {
             let payload: Payload = sp.payload;
             if let Some(points) = sp.points {
-                set_payload(&segments.read(), op_num, &payload, &points)
+                set_payload(&segments.read(), op_num, &payload, &points, &sp.key)
             } else if let Some(filter) = sp.filter {
-                set_payload_by_filter(&segments.read(), op_num, &payload, &filter)
+                set_payload_by_filter(&segments.read(), op_num, &payload, &filter, &sp.key)
             } else {
                 Err(CollectionError::BadRequest {
                     description: "No points or filter specified".to_string(),

commit 3beb4e3b4ff4b3f9585337f4e5b0826a14e247b6
Author: xzfc <5121426+xzfc@users.noreply.github.com>
Date:   Fri Feb 23 14:38:40 2024 +0000

    Introduce JsonPathString (#3674)
    
    * Introduce JsonPathString
    
    * Fix fomatting

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index d0b015b04..d03da26d0 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -6,6 +6,7 @@ use parking_lot::{RwLock, RwLockWriteGuard};
 use segment::common::operation_error::{OperationError, OperationResult};
 use segment::data_types::named_vectors::NamedVectors;
 use segment::entry::entry_point::SegmentEntry;
+use segment::json_path::JsonPath;
 use segment::types::{
     Filter, Payload, PayloadFieldSchema, PayloadKeyType, PayloadKeyTypeRef, PointIdType,
     SeqNumberType,
@@ -129,7 +130,7 @@ pub(crate) fn set_payload(
     op_num: SeqNumberType,
     payload: &Payload,
     points: &[PointIdType],
-    key: &Option<String>,
+    key: &Option<JsonPath>,
 ) -> CollectionResult<usize> {
     let updated_points =
         segments.apply_points_to_appendable(op_num, points, |id, write_segment| {
@@ -158,7 +159,7 @@ pub(crate) fn set_payload_by_filter(
     op_num: SeqNumberType,
     payload: &Payload,
     filter: &Filter,
-    key: &Option<String>,
+    key: &Option<JsonPath>,
 ) -> CollectionResult<usize> {
     let affected_points = points_by_filter(segments, filter)?;
     set_payload(segments, op_num, payload, &affected_points, key)

commit d05634bad09436ffd81dd375ba064b3f1d180c71
Author: xzfc <5121426+xzfc@users.noreply.github.com>
Date:   Mon Feb 26 14:28:31 2024 +0000

    Don't rebuild HNSW when updating non-indexed fields (#3611)

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index d03da26d0..98d1b0384 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -6,7 +6,7 @@ use parking_lot::{RwLock, RwLockWriteGuard};
 use segment::common::operation_error::{OperationError, OperationResult};
 use segment::data_types::named_vectors::NamedVectors;
 use segment::entry::entry_point::SegmentEntry;
-use segment::json_path::JsonPath;
+use segment::json_path::{JsonPath, JsonPathInterface};
 use segment::types::{
     Filter, Payload, PayloadFieldSchema, PayloadKeyType, PayloadKeyTypeRef, PointIdType,
     SeqNumberType,
@@ -38,9 +38,11 @@ pub(crate) fn delete_points(
     ids: &[PointIdType],
 ) -> CollectionResult<usize> {
     segments
-        .apply_points(ids, |id, _idx, write_segment| {
-            write_segment.delete_point(op_num, id)
-        })
+        .apply_points(
+            ids,
+            |_| (),
+            |id, _idx, write_segment, ()| write_segment.delete_point(op_num, id),
+        )
         .map_err(Into::into)
 }
 
@@ -62,11 +64,15 @@ pub(crate) fn update_vectors(
             });
     let ids: Vec<PointIdType> = points_map.keys().copied().collect();
 
-    let updated_points =
-        segments.apply_points_to_appendable(op_num, &ids, |id, write_segment| {
+    let updated_points = segments.apply_points_with_conditional_move(
+        op_num,
+        &ids,
+        |id, write_segment| {
             let vectors = points_map[&id].vector.clone().into_all_vectors();
             write_segment.update_vectors(op_num, id, vectors)
-        })?;
+        },
+        |_| false,
+    )?;
     check_unprocessed_points(&ids, &updated_points)?;
     Ok(updated_points.len())
 }
@@ -79,13 +85,17 @@ pub(crate) fn delete_vectors(
     vector_names: &[String],
 ) -> CollectionResult<usize> {
     segments
-        .apply_points(points, |id, _idx, write_segment| {
-            let mut res = true;
-            for name in vector_names {
-                res &= write_segment.delete_vector(op_num, id, name)?;
-            }
-            Ok(res)
-        })
+        .apply_points(
+            points,
+            |_| (),
+            |id, _idx, write_segment, ()| {
+                let mut res = true;
+                for name in vector_names {
+                    res &= write_segment.delete_vector(op_num, id, name)?;
+                }
+                Ok(res)
+            },
+        )
         .map_err(Into::into)
 }
 
@@ -106,10 +116,12 @@ pub(crate) fn overwrite_payload(
     payload: &Payload,
     points: &[PointIdType],
 ) -> CollectionResult<usize> {
-    let updated_points =
-        segments.apply_points_to_appendable(op_num, points, |id, write_segment| {
-            write_segment.set_full_payload(op_num, id, payload)
-        })?;
+    let updated_points = segments.apply_points_with_conditional_move(
+        op_num,
+        points,
+        |id, write_segment| write_segment.set_full_payload(op_num, id, payload),
+        |segment| segment.get_indexed_fields().is_empty(),
+    )?;
 
     check_unprocessed_points(points, &updated_points)?;
     Ok(updated_points.len())
@@ -132,15 +144,48 @@ pub(crate) fn set_payload(
     points: &[PointIdType],
     key: &Option<JsonPath>,
 ) -> CollectionResult<usize> {
-    let updated_points =
-        segments.apply_points_to_appendable(op_num, points, |id, write_segment| {
-            write_segment.set_payload(op_num, id, payload, key)
-        })?;
+    let updated_points = segments.apply_points_with_conditional_move(
+        op_num,
+        points,
+        |id, write_segment| write_segment.set_payload(op_num, id, payload, key),
+        |segment| safe_to_set_payload(&segment.get_indexed_fields(), payload, key),
+    )?;
 
     check_unprocessed_points(points, &updated_points)?;
     Ok(updated_points.len())
 }
 
+fn safe_to_set_payload<P: JsonPathInterface>(
+    indexed_fields: &HashMap<P, PayloadFieldSchema>,
+    payload: &Payload,
+    key: &Option<P>,
+) -> bool {
+    if key.is_some() {
+        // Not supported yet
+        return false;
+    }
+    // Set is safe when the provided payload doesn't intersect with any of the indexed fields.
+    // Note that if we have, e.g., an index on "a.c" and the payload being set is `{"a": {"b": 1}}`,
+    // it is not safe because the whole value of "a" is being replaced.
+    indexed_fields
+        .keys()
+        .all(|path| !payload.0.contains_key(path.head()))
+}
+
+fn safe_to_delete_payload_keys<P: JsonPathInterface>(
+    indexed_fields: &HashMap<P, PayloadFieldSchema>,
+    keys_to_delete: &[P],
+) -> bool {
+    // Deletion is safe when the keys being deleted don't intersect with any of the indexed fields.
+    // Note that if we have, e.g., indexed field "a.b", then it is not safe to delete any of of "a",
+    // "a.b", or "a.b.c".
+    indexed_fields.keys().all(|indexed_path| {
+        keys_to_delete
+            .iter()
+            .all(|path_to_delete| !indexed_path.check_include_pattern(path_to_delete))
+    })
+}
+
 fn points_by_filter(
     segments: &SegmentHolder,
     filter: &Filter,
@@ -171,14 +216,18 @@ pub(crate) fn delete_payload(
     points: &[PointIdType],
     keys: &[PayloadKeyType],
 ) -> CollectionResult<usize> {
-    let updated_points =
-        segments.apply_points_to_appendable(op_num, points, |id, write_segment| {
+    let updated_points = segments.apply_points_with_conditional_move(
+        op_num,
+        points,
+        |id, write_segment| {
             let mut res = true;
             for key in keys {
                 res &= write_segment.delete_payload(op_num, id, key)?;
             }
             Ok(res)
-        })?;
+        },
+        |segment| safe_to_delete_payload_keys(&segment.get_indexed_fields(), keys),
+    )?;
 
     check_unprocessed_points(points, &updated_points)?;
     Ok(updated_points.len())
@@ -199,10 +248,12 @@ pub(crate) fn clear_payload(
     op_num: SeqNumberType,
     points: &[PointIdType],
 ) -> CollectionResult<usize> {
-    let updated_points =
-        segments.apply_points_to_appendable(op_num, points, |id, write_segment| {
-            write_segment.clear_payload(op_num, id)
-        })?;
+    let updated_points = segments.apply_points_with_conditional_move(
+        op_num,
+        points,
+        |id, write_segment| write_segment.clear_payload(op_num, id),
+        |segment| segment.get_indexed_fields().is_empty(),
+    )?;
 
     check_unprocessed_points(points, &updated_points)?;
     Ok(updated_points.len())
@@ -216,10 +267,11 @@ pub(crate) fn clear_payload_by_filter(
 ) -> CollectionResult<usize> {
     let points_to_clear = points_by_filter(segments, filter)?;
 
-    let updated_points = segments.apply_points_to_appendable(
+    let updated_points = segments.apply_points_with_conditional_move(
         op_num,
         points_to_clear.as_slice(),
         |id, write_segment| write_segment.clear_payload(op_num, id),
+        |segment| segment.get_indexed_fields().is_empty(),
     )?;
 
     Ok(updated_points.len())
@@ -360,8 +412,10 @@ where
     let ids: Vec<PointIdType> = points_map.keys().copied().collect();
 
     // Update points in writable segments
-    let updated_points =
-        segments.apply_points_to_appendable(op_num, &ids, |id, write_segment| {
+    let updated_points = segments.apply_points_with_conditional_move(
+        op_num,
+        &ids,
+        |id, write_segment| {
             let point = points_map[&id];
             upsert_with_payload(
                 write_segment,
@@ -370,7 +424,9 @@ where
                 point.get_vectors(),
                 point.payload.as_ref(),
             )
-        })?;
+        },
+        |_| false,
+    )?;
 
     let mut res = updated_points.len();
     // Insert new points, which was not updated or existed
@@ -552,3 +608,76 @@ pub(crate) fn delete_points_by_filter(
     })?;
     Ok(deleted)
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    fn make_indexed_fields(keys: &[&str]) -> HashMap<PayloadKeyType, PayloadFieldSchema> {
+        keys.iter()
+            .map(|&s| {
+                (
+                    s.parse().unwrap(),
+                    PayloadFieldSchema::FieldType(segment::types::PayloadSchemaType::Integer),
+                )
+            })
+            .collect()
+    }
+
+    #[test]
+    fn test_safe_to_set_payload() {
+        assert!(safe_to_set_payload(
+            &make_indexed_fields(&[]),
+            &serde_json::json!({"a": 1}).into(),
+            &None,
+        ));
+        assert!(safe_to_set_payload(
+            &make_indexed_fields(&["a", "b"]),
+            &serde_json::json!({"c": 1, "d": 1}).into(),
+            &None,
+        ));
+        assert!(!safe_to_set_payload(
+            &make_indexed_fields(&["a", "b"]),
+            &serde_json::json!({"b": 1, "c": 1}).into(),
+            &None,
+        ));
+        assert!(!safe_to_set_payload(
+            &make_indexed_fields(&["a.x"]),
+            &serde_json::json!({"a": {"y": 1}}).into(),
+            &None,
+        ));
+        assert!(safe_to_set_payload(
+            &make_indexed_fields(&["a.x"]),
+            &serde_json::json!({"b": {"x": 1}}).into(),
+            &None,
+        ));
+    }
+
+    #[test]
+    fn test_safe_to_delete_payload_keys() {
+        assert!(safe_to_delete_payload_keys(
+            &make_indexed_fields(&[]),
+            &["a".parse().unwrap()],
+        ));
+        assert!(safe_to_delete_payload_keys(
+            &make_indexed_fields(&["a", "b"]),
+            &["c".parse().unwrap(), "d".parse().unwrap()],
+        ));
+        assert!(!safe_to_delete_payload_keys(
+            &make_indexed_fields(&["a", "b"]),
+            &["a".parse().unwrap(), "c".parse().unwrap()],
+        ));
+        assert!(!safe_to_delete_payload_keys(
+            &make_indexed_fields(&["a.b"]),
+            &["a".parse().unwrap()]
+        ));
+        assert!(!safe_to_delete_payload_keys(
+            &make_indexed_fields(&["a.b"]),
+            &["a.b".parse().unwrap()]
+        ));
+        assert!(!safe_to_delete_payload_keys(
+            &make_indexed_fields(&["a.b"]),
+            &["a.b.c".parse().unwrap()]
+        ));
+    }
+}

commit ea59ff6577076769620599fe8d3bd500c8d5eae0
Author: xzfc <5121426+xzfc@users.noreply.github.com>
Date:   Thu Feb 29 16:09:39 2024 +0000

    Use new safe_to_set/safe_to_remove functions (#3722)
    
    * Switch to JsonPathV2
    
    * Use new safe_to_set/safe_to_remove functions, fix PayloadIndex::assign
    
    * minor review fixes
    
    * Rename and inverse
    
    ---------
    
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 98d1b0384..974692b70 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -2,11 +2,12 @@
 
 use std::collections::{HashMap, HashSet};
 
+use itertools::iproduct;
 use parking_lot::{RwLock, RwLockWriteGuard};
 use segment::common::operation_error::{OperationError, OperationResult};
 use segment::data_types::named_vectors::NamedVectors;
 use segment::entry::entry_point::SegmentEntry;
-use segment::json_path::{JsonPath, JsonPathInterface};
+use segment::json_path::JsonPath;
 use segment::types::{
     Filter, Payload, PayloadFieldSchema, PayloadKeyType, PayloadKeyTypeRef, PointIdType,
     SeqNumberType,
@@ -148,44 +149,17 @@ pub(crate) fn set_payload(
         op_num,
         points,
         |id, write_segment| write_segment.set_payload(op_num, id, payload, key),
-        |segment| safe_to_set_payload(&segment.get_indexed_fields(), payload, key),
+        |segment| {
+            segment.get_indexed_fields().keys().all(|indexed_path| {
+                !indexed_path.is_affected_by_value_set(&payload.0, key.as_ref())
+            })
+        },
     )?;
 
     check_unprocessed_points(points, &updated_points)?;
     Ok(updated_points.len())
 }
 
-fn safe_to_set_payload<P: JsonPathInterface>(
-    indexed_fields: &HashMap<P, PayloadFieldSchema>,
-    payload: &Payload,
-    key: &Option<P>,
-) -> bool {
-    if key.is_some() {
-        // Not supported yet
-        return false;
-    }
-    // Set is safe when the provided payload doesn't intersect with any of the indexed fields.
-    // Note that if we have, e.g., an index on "a.c" and the payload being set is `{"a": {"b": 1}}`,
-    // it is not safe because the whole value of "a" is being replaced.
-    indexed_fields
-        .keys()
-        .all(|path| !payload.0.contains_key(path.head()))
-}
-
-fn safe_to_delete_payload_keys<P: JsonPathInterface>(
-    indexed_fields: &HashMap<P, PayloadFieldSchema>,
-    keys_to_delete: &[P],
-) -> bool {
-    // Deletion is safe when the keys being deleted don't intersect with any of the indexed fields.
-    // Note that if we have, e.g., indexed field "a.b", then it is not safe to delete any of of "a",
-    // "a.b", or "a.b.c".
-    indexed_fields.keys().all(|indexed_path| {
-        keys_to_delete
-            .iter()
-            .all(|path_to_delete| !indexed_path.check_include_pattern(path_to_delete))
-    })
-}
-
 fn points_by_filter(
     segments: &SegmentHolder,
     filter: &Filter,
@@ -226,7 +200,13 @@ pub(crate) fn delete_payload(
             }
             Ok(res)
         },
-        |segment| safe_to_delete_payload_keys(&segment.get_indexed_fields(), keys),
+        |segment| {
+            iproduct!(segment.get_indexed_fields().keys(), keys).all(
+                |(indexed_path, path_to_delete)| {
+                    !indexed_path.is_affected_by_value_remove(path_to_delete)
+                },
+            )
+        },
     )?;
 
     check_unprocessed_points(points, &updated_points)?;
@@ -608,76 +588,3 @@ pub(crate) fn delete_points_by_filter(
     })?;
     Ok(deleted)
 }
-
-#[cfg(test)]
-mod tests {
-    use super::*;
-
-    fn make_indexed_fields(keys: &[&str]) -> HashMap<PayloadKeyType, PayloadFieldSchema> {
-        keys.iter()
-            .map(|&s| {
-                (
-                    s.parse().unwrap(),
-                    PayloadFieldSchema::FieldType(segment::types::PayloadSchemaType::Integer),
-                )
-            })
-            .collect()
-    }
-
-    #[test]
-    fn test_safe_to_set_payload() {
-        assert!(safe_to_set_payload(
-            &make_indexed_fields(&[]),
-            &serde_json::json!({"a": 1}).into(),
-            &None,
-        ));
-        assert!(safe_to_set_payload(
-            &make_indexed_fields(&["a", "b"]),
-            &serde_json::json!({"c": 1, "d": 1}).into(),
-            &None,
-        ));
-        assert!(!safe_to_set_payload(
-            &make_indexed_fields(&["a", "b"]),
-            &serde_json::json!({"b": 1, "c": 1}).into(),
-            &None,
-        ));
-        assert!(!safe_to_set_payload(
-            &make_indexed_fields(&["a.x"]),
-            &serde_json::json!({"a": {"y": 1}}).into(),
-            &None,
-        ));
-        assert!(safe_to_set_payload(
-            &make_indexed_fields(&["a.x"]),
-            &serde_json::json!({"b": {"x": 1}}).into(),
-            &None,
-        ));
-    }
-
-    #[test]
-    fn test_safe_to_delete_payload_keys() {
-        assert!(safe_to_delete_payload_keys(
-            &make_indexed_fields(&[]),
-            &["a".parse().unwrap()],
-        ));
-        assert!(safe_to_delete_payload_keys(
-            &make_indexed_fields(&["a", "b"]),
-            &["c".parse().unwrap(), "d".parse().unwrap()],
-        ));
-        assert!(!safe_to_delete_payload_keys(
-            &make_indexed_fields(&["a", "b"]),
-            &["a".parse().unwrap(), "c".parse().unwrap()],
-        ));
-        assert!(!safe_to_delete_payload_keys(
-            &make_indexed_fields(&["a.b"]),
-            &["a".parse().unwrap()]
-        ));
-        assert!(!safe_to_delete_payload_keys(
-            &make_indexed_fields(&["a.b"]),
-            &["a.b".parse().unwrap()]
-        ));
-        assert!(!safe_to_delete_payload_keys(
-            &make_indexed_fields(&["a.b"]),
-            &["a.b.c".parse().unwrap()]
-        ));
-    }
-}

commit db5399f9e47cfe9d740645ec2f27e8751444882b
Author: Ivan Pleshkov <pleshkov.ivan@gmail.com>
Date:   Mon Mar 18 13:31:55 2024 +0100

    Use rest vector type as non segment part (#3829)
    
    * use rest vector type as non-segment part
    
    * add todo
    
    * switch into -> from
    
    * review remarks
    
    * review remarks

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 974692b70..68f8ba9a2 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -6,6 +6,7 @@ use itertools::iproduct;
 use parking_lot::{RwLock, RwLockWriteGuard};
 use segment::common::operation_error::{OperationError, OperationResult};
 use segment::data_types::named_vectors::NamedVectors;
+use segment::data_types::vectors::{BatchVectorStruct, VectorStruct};
 use segment::entry::entry_point::SegmentEntry;
 use segment::json_path::JsonPath;
 use segment::types::{
@@ -69,7 +70,8 @@ pub(crate) fn update_vectors(
         op_num,
         &ids,
         |id, write_segment| {
-            let vectors = points_map[&id].vector.clone().into_all_vectors();
+            let vectors: VectorStruct = points_map[&id].vector.clone().into();
+            let vectors = vectors.into_all_vectors();
             write_segment.update_vectors(op_num, id, vectors)
         },
         |_| false,
@@ -448,13 +450,14 @@ pub(crate) fn process_point_operation(
         PointOperations::UpsertPoints(operation) => {
             let points: Vec<_> = match operation {
                 PointInsertOperationsInternal::PointsBatch(batch) => {
-                    let all_vectors = batch.vectors.into_all_vectors(batch.ids.len());
+                    let batch_vectors: BatchVectorStruct = batch.vectors.into();
+                    let all_vectors = batch_vectors.into_all_vectors(batch.ids.len());
                     let vectors_iter = batch.ids.into_iter().zip(all_vectors);
                     match batch.payloads {
                         None => vectors_iter
                             .map(|(id, vectors)| PointStruct {
                                 id,
-                                vector: vectors.into(),
+                                vector: VectorStruct::from(vectors).into(),
                                 payload: None,
                             })
                             .collect(),
@@ -462,7 +465,7 @@ pub(crate) fn process_point_operation(
                             .zip(payloads)
                             .map(|((id, vectors), payload)| PointStruct {
                                 id,
-                                vector: vectors.into(),
+                                vector: VectorStruct::from(vectors).into(),
                                 payload,
                             })
                             .collect(),

commit 48cd6ebd09c6cb838490023a2124867f000a7e40
Author: Tim Vise <tim+github@visee.me>
Date:   Tue Apr 30 10:42:19 2024 +0200

    Number of replaced points may also be less (#4098)

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 68f8ba9a2..5b10e02be 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -373,7 +373,7 @@ pub(crate) fn sync_points(
 
     // 5. Upsert points which differ from the stored ones
     let num_replaced = upsert_points(segments, op_num, points_to_update)?;
-    debug_assert_eq!(num_replaced, num_updated);
+    debug_assert!(num_replaced <= num_updated, "number of replaced points cannot be greater than points to update ({num_replaced} <= {num_updated})");
 
     Ok((deleted, num_new, num_updated))
 }

commit 8e6f8d4575a4613f7d863f76088b3096c9d7be77
Author: Tim Vise <tim+github@visee.me>
Date:   Tue May 28 13:31:54 2024 +0200

    On shard load, ensure we have any appendable segments (#4342)
    
    * Extract logic for creating temporary segment during segment proxying
    
    * Simplify check for having an appendable segment
    
    * Fix incorrect documentation
    
    * When loading shard, ensure we have any appendable segments or create one
    
    * Use correct parameter name
    
    * In debug builds, crash when there's no appendable segment on start

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 5b10e02be..79ad5470c 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -418,9 +418,12 @@ where
         .filter(|x| !(updated_points.contains(x)));
 
     {
-        let default_write_segment = segments.random_appendable_segment().ok_or_else(|| {
-            CollectionError::service_error("No segments exists, expected at least one".to_string())
-        })?;
+        let default_write_segment =
+            segments
+                .random_appendable_segment()
+                .ok_or(CollectionError::service_error(
+                    "No appendable segments exists, expected at least one",
+                ))?;
 
         let segment_arc = default_write_segment.get();
         let mut write_segment = segment_arc.write();

commit ac9313e00bc9fffebbacc4672d1cb157b2178063
Author: Tim Vise <tim+github@visee.me>
Date:   Tue Jun 11 12:59:05 2024 +0200

    When selecting a segment for writing, select the smallest one (#4440)
    
    * Preallocate list of entires to prevent some unnecessary reallocations
    
    * Implement Copy for OptimizerThresholds
    
    * Add shard holder function get smallest segment
    
    * Take the smallest segment in the segments updater
    
    * Add test to assert inserting into smallest segment
    
    * Fix compilation warnings

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 79ad5470c..70137baa9 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -420,7 +420,7 @@ where
     {
         let default_write_segment =
             segments
-                .random_appendable_segment()
+                .smallest_appendable_segment()
                 .ok_or(CollectionError::service_error(
                     "No appendable segments exists, expected at least one",
                 ))?;

commit 49a9d05e7c180c2a4828686a54b9a7a8fbc946f3
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Tue Jun 18 20:38:24 2024 +0200

    Fix multivector for unnamed vectors (#4482)
    
    * minor conversion improvement
    
    * use NamedVectors in update_vectors
    
    * remove merge from VectorStruct
    
    * rename Multi -> Named in vector struct
    
    * add multi-dense vectors option into VectorStruct
    
    * generate openapi
    
    * rename VectorStruct -> VectorStructInternal
    
    * add conversion for anonymous multivec in grpc
    
    * renames for BatchVectorStruct
    
    * implement multi-dense for batch
    
    * allow multi-dense in batch upserts
    
    * test and fixes

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 70137baa9..617b7120e 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -6,7 +6,7 @@ use itertools::iproduct;
 use parking_lot::{RwLock, RwLockWriteGuard};
 use segment::common::operation_error::{OperationError, OperationResult};
 use segment::data_types::named_vectors::NamedVectors;
-use segment::data_types::vectors::{BatchVectorStruct, VectorStruct};
+use segment::data_types::vectors::{BatchVectorStructInternal, VectorStructInternal};
 use segment::entry::entry_point::SegmentEntry;
 use segment::json_path::JsonPath;
 use segment::types::{
@@ -52,26 +52,25 @@ pub(crate) fn delete_points(
 pub(crate) fn update_vectors(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
-    points: &[PointVectors],
+    points: Vec<PointVectors>,
 ) -> CollectionResult<usize> {
     // Build a map of vectors to update per point, merge updates on same point ID
-    let points_map: HashMap<PointIdType, PointVectors> =
-        points
-            .iter()
-            .fold(HashMap::with_capacity(points.len()), |mut map, p| {
-                map.entry(p.id)
-                    .and_modify(|e| e.vector.merge(p.vector.clone()))
-                    .or_insert_with(|| p.clone());
-                map
-            });
+    let mut points_map: HashMap<PointIdType, NamedVectors> = HashMap::new();
+    for point in points {
+        let PointVectors { id, vector } = point;
+        let named_vector = NamedVectors::from(vector);
+
+        let entry = points_map.entry(id).or_default();
+        entry.merge(named_vector);
+    }
+
     let ids: Vec<PointIdType> = points_map.keys().copied().collect();
 
     let updated_points = segments.apply_points_with_conditional_move(
         op_num,
         &ids,
         |id, write_segment| {
-            let vectors: VectorStruct = points_map[&id].vector.clone().into();
-            let vectors = vectors.into_all_vectors();
+            let vectors = points_map[&id].clone();
             write_segment.update_vectors(op_num, id, vectors)
         },
         |_| false,
@@ -453,14 +452,14 @@ pub(crate) fn process_point_operation(
         PointOperations::UpsertPoints(operation) => {
             let points: Vec<_> = match operation {
                 PointInsertOperationsInternal::PointsBatch(batch) => {
-                    let batch_vectors: BatchVectorStruct = batch.vectors.into();
+                    let batch_vectors: BatchVectorStructInternal = batch.vectors.into();
                     let all_vectors = batch_vectors.into_all_vectors(batch.ids.len());
                     let vectors_iter = batch.ids.into_iter().zip(all_vectors);
                     match batch.payloads {
                         None => vectors_iter
                             .map(|(id, vectors)| PointStruct {
                                 id,
-                                vector: VectorStruct::from(vectors).into(),
+                                vector: VectorStructInternal::from(vectors).into(),
                                 payload: None,
                             })
                             .collect(),
@@ -468,7 +467,7 @@ pub(crate) fn process_point_operation(
                             .zip(payloads)
                             .map(|((id, vectors), payload)| PointStruct {
                                 id,
-                                vector: VectorStruct::from(vectors).into(),
+                                vector: VectorStructInternal::from(vectors).into(),
                                 payload,
                             })
                             .collect(),
@@ -502,7 +501,7 @@ pub(crate) fn process_vector_operation(
 ) -> CollectionResult<usize> {
     match vector_operation {
         VectorOperations::UpdateVectors(operation) => {
-            update_vectors(&segments.read(), op_num, &operation.points)
+            update_vectors(&segments.read(), op_num, operation.points)
         }
         VectorOperations::DeleteVectors(ids, vector_names) => {
             delete_vectors(&segments.read(), op_num, &ids.points, &vector_names)

commit 07c278ad51084c98adf9a7093619ffc5a73f87c9
Author: xzfc <5121426+xzfc@users.noreply.github.com>
Date:   Mon Jul 22 08:19:19 2024 +0000

    Enable some of the pedantic clippy lints (#4715)
    
    * Use workspace lints
    
    * Enable lint: manual_let_else
    
    * Enable lint: enum_glob_use
    
    * Enable lint: filter_map_next
    
    * Enable lint: ref_as_ptr
    
    * Enable lint: ref_option_ref
    
    * Enable lint: manual_is_variant_and
    
    * Enable lint: flat_map_option
    
    * Enable lint: inefficient_to_string
    
    * Enable lint: implicit_clone
    
    * Enable lint: inconsistent_struct_constructor
    
    * Enable lint: unnecessary_wraps
    
    * Enable lint: needless_continue
    
    * Enable lint: unused_self
    
    * Enable lint: from_iter_instead_of_collect
    
    * Enable lint: uninlined_format_args
    
    * Enable lint: doc_link_with_quotes
    
    * Enable lint: needless_raw_string_hashes
    
    * Enable lint: used_underscore_binding
    
    * Enable lint: ptr_as_ptr
    
    * Enable lint: explicit_into_iter_loop
    
    * Enable lint: cast_lossless

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 617b7120e..4e86d05e2 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -428,13 +428,13 @@ where
         let mut write_segment = segment_arc.write();
         for point_id in new_point_ids {
             let point = points_map[&point_id];
-            res += upsert_with_payload(
+            res += usize::from(upsert_with_payload(
                 &mut write_segment,
                 op_num,
                 point_id,
                 point.get_vectors(),
                 point.payload.as_ref(),
-            )? as usize;
+            )?);
         }
         RwLockWriteGuard::unlock_fair(write_segment);
     };

commit 3a8c111cb8926f0faa8db8c1e33b1a7cf36ea33e
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Wed Jul 31 11:09:08 2024 +0200

    Non blocking filtered deletion (#4780)
    
    * Delete by filter in batches
    
    * Use pop()
    
    * attempt 2 of refactoring (#4784)
    
    ---------
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 4e86d05e2..4e18f6ed5 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -580,16 +580,49 @@ pub(crate) fn process_field_index_operation(
     }
 }
 
+/// Max amount of points to delete in a batched deletion iteration.
+const DELETION_BATCH_SIZE: usize = 512;
+
 /// Deletes points from all segments matching the given filter
 pub(crate) fn delete_points_by_filter(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
     filter: &Filter,
 ) -> CollectionResult<usize> {
-    let mut deleted = 0;
-    segments.apply_segments(|s| {
-        deleted += s.delete_filtered(op_num, filter)?;
+    let mut total_deleted = 0;
+
+    let mut points_to_delete: HashMap<_, _> = segments
+        .iter()
+        .map(|(segment_id, segment)| {
+            (
+                *segment_id,
+                segment.get().read().read_filtered(None, None, Some(filter)),
+            )
+        })
+        .collect();
+
+    segments.apply_segments_batched(|s, segment_id| {
+        let Some(curr_points) = points_to_delete.get_mut(&segment_id) else {
+            return Ok(false);
+        };
+        if curr_points.is_empty() {
+            return Ok(false);
+        }
+
+        let mut deleted_in_batch = 0;
+        while let Some(point_id) = curr_points.pop() {
+            if s.delete_point(op_num, point_id)? {
+                total_deleted += 1;
+                deleted_in_batch += 1;
+            }
+
+            if deleted_in_batch >= DELETION_BATCH_SIZE {
+                break;
+            }
+        }
+
         Ok(true)
     })?;
-    Ok(deleted)
+
+    Ok(total_deleted)
 }

commit 6b85f1fd7f537e08a77d2e309b2d7696d3772ecb
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Wed Jul 31 20:02:09 2024 +0200

    Merge pull request #4781
    
    * Non blocking set payload by filter
    
    * Make more payload functions non-blocking

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 4e18f6ed5..04f6c0d02 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -112,21 +112,30 @@ pub(crate) fn delete_vectors_by_filter(
     delete_vectors(segments, op_num, &affected_points, vector_names)
 }
 
+/// Batch size when modifying payload.
+const PAYLOAD_OP_BATCH_SIZE: usize = 512;
+
 pub(crate) fn overwrite_payload(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
     payload: &Payload,
     points: &[PointIdType],
 ) -> CollectionResult<usize> {
-    let updated_points = segments.apply_points_with_conditional_move(
-        op_num,
-        points,
-        |id, write_segment| write_segment.set_full_payload(op_num, id, payload),
-        |segment| segment.get_indexed_fields().is_empty(),
-    )?;
+    let mut total_updated_points = 0;
 
-    check_unprocessed_points(points, &updated_points)?;
-    Ok(updated_points.len())
+    for batch in points.chunks(PAYLOAD_OP_BATCH_SIZE) {
+        let updated_points = segments.apply_points_with_conditional_move(
+            op_num,
+            batch,
+            |id, write_segment| write_segment.set_full_payload(op_num, id, payload),
+            |segment| segment.get_indexed_fields().is_empty(),
+        )?;
+
+        total_updated_points += updated_points.len();
+        check_unprocessed_points(batch, &updated_points)?;
+    }
+
+    Ok(total_updated_points)
 }
 
 pub(crate) fn overwrite_payload_by_filter(
@@ -146,19 +155,25 @@ pub(crate) fn set_payload(
     points: &[PointIdType],
     key: &Option<JsonPath>,
 ) -> CollectionResult<usize> {
-    let updated_points = segments.apply_points_with_conditional_move(
-        op_num,
-        points,
-        |id, write_segment| write_segment.set_payload(op_num, id, payload, key),
-        |segment| {
-            segment.get_indexed_fields().keys().all(|indexed_path| {
-                !indexed_path.is_affected_by_value_set(&payload.0, key.as_ref())
-            })
-        },
-    )?;
+    let mut total_updated_points = 0;
 
-    check_unprocessed_points(points, &updated_points)?;
-    Ok(updated_points.len())
+    for chunk in points.chunks(PAYLOAD_OP_BATCH_SIZE) {
+        let updated_points = segments.apply_points_with_conditional_move(
+            op_num,
+            chunk,
+            |id, write_segment| write_segment.set_payload(op_num, id, payload, key),
+            |segment| {
+                segment.get_indexed_fields().keys().all(|indexed_path| {
+                    !indexed_path.is_affected_by_value_set(&payload.0, key.as_ref())
+                })
+            },
+        )?;
+
+        check_unprocessed_points(chunk, &updated_points)?;
+        total_updated_points += updated_points.len();
+    }
+
+    Ok(total_updated_points)
 }
 
 fn points_by_filter(
@@ -191,27 +206,33 @@ pub(crate) fn delete_payload(
     points: &[PointIdType],
     keys: &[PayloadKeyType],
 ) -> CollectionResult<usize> {
-    let updated_points = segments.apply_points_with_conditional_move(
-        op_num,
-        points,
-        |id, write_segment| {
-            let mut res = true;
-            for key in keys {
-                res &= write_segment.delete_payload(op_num, id, key)?;
-            }
-            Ok(res)
-        },
-        |segment| {
-            iproduct!(segment.get_indexed_fields().keys(), keys).all(
-                |(indexed_path, path_to_delete)| {
-                    !indexed_path.is_affected_by_value_remove(path_to_delete)
-                },
-            )
-        },
-    )?;
+    let mut total_deleted_points = 0;
 
-    check_unprocessed_points(points, &updated_points)?;
-    Ok(updated_points.len())
+    for batch in points.chunks(PAYLOAD_OP_BATCH_SIZE) {
+        let updated_points = segments.apply_points_with_conditional_move(
+            op_num,
+            batch,
+            |id, write_segment| {
+                let mut res = true;
+                for key in keys {
+                    res &= write_segment.delete_payload(op_num, id, key)?;
+                }
+                Ok(res)
+            },
+            |segment| {
+                iproduct!(segment.get_indexed_fields().keys(), keys).all(
+                    |(indexed_path, path_to_delete)| {
+                        !indexed_path.is_affected_by_value_remove(path_to_delete)
+                    },
+                )
+            },
+        )?;
+
+        check_unprocessed_points(batch, &updated_points)?;
+        total_deleted_points += updated_points.len();
+    }
+
+    Ok(total_deleted_points)
 }
 
 pub(crate) fn delete_payload_by_filter(
@@ -229,15 +250,20 @@ pub(crate) fn clear_payload(
     op_num: SeqNumberType,
     points: &[PointIdType],
 ) -> CollectionResult<usize> {
-    let updated_points = segments.apply_points_with_conditional_move(
-        op_num,
-        points,
-        |id, write_segment| write_segment.clear_payload(op_num, id),
-        |segment| segment.get_indexed_fields().is_empty(),
-    )?;
+    let mut total_updated_points = 0;
 
-    check_unprocessed_points(points, &updated_points)?;
-    Ok(updated_points.len())
+    for batch in points.chunks(PAYLOAD_OP_BATCH_SIZE) {
+        let updated_points = segments.apply_points_with_conditional_move(
+            op_num,
+            batch,
+            |id, write_segment| write_segment.clear_payload(op_num, id),
+            |segment| segment.get_indexed_fields().is_empty(),
+        )?;
+        check_unprocessed_points(batch, &updated_points)?;
+        total_updated_points += updated_points.len();
+    }
+
+    Ok(total_updated_points)
 }
 
 /// Clear Payloads from all segments matching the given filter
@@ -248,14 +274,19 @@ pub(crate) fn clear_payload_by_filter(
 ) -> CollectionResult<usize> {
     let points_to_clear = points_by_filter(segments, filter)?;
 
-    let updated_points = segments.apply_points_with_conditional_move(
-        op_num,
-        points_to_clear.as_slice(),
-        |id, write_segment| write_segment.clear_payload(op_num, id),
-        |segment| segment.get_indexed_fields().is_empty(),
-    )?;
+    let mut total_updated_points = 0;
 
-    Ok(updated_points.len())
+    for batch in points_to_clear.chunks(PAYLOAD_OP_BATCH_SIZE) {
+        let updated_points = segments.apply_points_with_conditional_move(
+            op_num,
+            batch,
+            |id, write_segment| write_segment.clear_payload(op_num, id),
+            |segment| segment.get_indexed_fields().is_empty(),
+        )?;
+        total_updated_points += updated_points.len();
+    }
+
+    Ok(total_updated_points)
 }
 
 pub(crate) fn create_field_index(

commit 882c74dbcf20e823954e5e640ff4f920650f7886
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Wed Jul 31 20:04:35 2024 +0200

    Non blocking vector operations (#4787)
    
    * Non blocking set payload by filter
    
    * Make more payload functions non-blocking
    
    * Non blocking vector operations

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 04f6c0d02..3798c2335 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -39,13 +39,19 @@ pub(crate) fn delete_points(
     op_num: SeqNumberType,
     ids: &[PointIdType],
 ) -> CollectionResult<usize> {
-    segments
-        .apply_points(
-            ids,
+    let mut total_deleted_points = 0;
+
+    for batch in ids.chunks(VECTOR_OP_BATCH_SIZE) {
+        let deleted_points = segments.apply_points(
+            batch,
             |_| (),
             |id, _idx, write_segment, ()| write_segment.delete_point(op_num, id),
-        )
-        .map_err(Into::into)
+        )?;
+
+        total_deleted_points += deleted_points;
+    }
+
+    Ok(total_deleted_points)
 }
 
 /// Update the specified named vectors of a point, keeping unspecified vectors intact.
@@ -66,19 +72,26 @@ pub(crate) fn update_vectors(
 
     let ids: Vec<PointIdType> = points_map.keys().copied().collect();
 
-    let updated_points = segments.apply_points_with_conditional_move(
-        op_num,
-        &ids,
-        |id, write_segment| {
-            let vectors = points_map[&id].clone();
-            write_segment.update_vectors(op_num, id, vectors)
-        },
-        |_| false,
-    )?;
-    check_unprocessed_points(&ids, &updated_points)?;
-    Ok(updated_points.len())
+    let mut total_updated_points = 0;
+    for batch in ids.chunks(VECTOR_OP_BATCH_SIZE) {
+        let updated_points = segments.apply_points_with_conditional_move(
+            op_num,
+            batch,
+            |id, write_segment| {
+                let vectors = points_map[&id].clone();
+                write_segment.update_vectors(op_num, id, vectors)
+            },
+            |_| false,
+        )?;
+        check_unprocessed_points(batch, &updated_points)?;
+        total_updated_points += updated_points.len();
+    }
+
+    Ok(total_updated_points)
 }
 
+const VECTOR_OP_BATCH_SIZE: usize = 512;
+
 /// Delete the given named vectors for the given points, keeping other vectors intact.
 pub(crate) fn delete_vectors(
     segments: &SegmentHolder,
@@ -86,9 +99,11 @@ pub(crate) fn delete_vectors(
     points: &[PointIdType],
     vector_names: &[String],
 ) -> CollectionResult<usize> {
-    segments
-        .apply_points(
-            points,
+    let mut total_deleted_points = 0;
+
+    for batch in points.chunks(VECTOR_OP_BATCH_SIZE) {
+        let deleted_points = segments.apply_points(
+            batch,
             |_| (),
             |id, _idx, write_segment, ()| {
                 let mut res = true;
@@ -97,8 +112,12 @@ pub(crate) fn delete_vectors(
                 }
                 Ok(res)
             },
-        )
-        .map_err(Into::into)
+        )?;
+
+        total_deleted_points += deleted_points;
+    }
+
+    Ok(total_deleted_points)
 }
 
 /// Delete the given named vectors for points matching the given filter, keeping other vectors intact.

commit 10b05c3ed84024f4aeaad5e97e24bd0b0ec421d2
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Mon Aug 5 19:05:45 2024 +0200

    Make scroll cancellable (#4827)
    
    * Make scroll cancellable
    
    * comments and fix
    
    * better comment

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 3798c2335..57adecbaa 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -1,6 +1,7 @@
 //! A collection of functions for updating points and payloads stored in segments
 
 use std::collections::{HashMap, HashSet};
+use std::sync::atomic::AtomicBool;
 
 use itertools::iproduct;
 use parking_lot::{RwLock, RwLockWriteGuard};
@@ -200,8 +201,10 @@ fn points_by_filter(
     filter: &Filter,
 ) -> CollectionResult<Vec<PointIdType>> {
     let mut affected_points: Vec<PointIdType> = Vec::new();
+    // we dont want to cancel this filtered read
+    let is_stopped = AtomicBool::new(false);
     segments.for_each_segment(|s| {
-        let points = s.read_filtered(None, None, Some(filter));
+        let points = s.read_filtered(None, None, Some(filter), &is_stopped);
         affected_points.extend_from_slice(points.as_slice());
         Ok(true)
     })?;
@@ -640,13 +643,17 @@ pub(crate) fn delete_points_by_filter(
     filter: &Filter,
 ) -> CollectionResult<usize> {
     let mut total_deleted = 0;
-
+    // we dont want to cancel this filtered read
+    let is_stopped = AtomicBool::new(false);
     let mut points_to_delete: HashMap<_, _> = segments
         .iter()
         .map(|(segment_id, segment)| {
             (
                 *segment_id,
-                segment.get().read().read_filtered(None, None, Some(filter)),
+                segment
+                    .get()
+                    .read()
+                    .read_filtered(None, None, Some(filter), &is_stopped),
             )
         })
         .collect();

commit c7da6ae36c455a67859dbc2a9f1e3ce274645121
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Thu Aug 8 12:41:33 2024 +0200

    Non blocking retrieve with timeout and cancellation support (#4844)
    
    * Non blocking retrieve with timeout and cancellation support
    
    * apply timeout for extra retrieve in rescoring

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 57adecbaa..93c81d9fd 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -390,30 +390,33 @@ pub(crate) fn sync_points(
         .collect();
 
     let mut points_to_update: Vec<_> = Vec::new();
-    let _num_updated = segments.read_points(existing_point_ids.as_slice(), |id, segment| {
-        let all_vectors = match segment.all_vectors(id) {
-            Ok(v) => v,
-            Err(OperationError::InconsistentStorage { .. }) => NamedVectors::default(),
-            Err(e) => return Err(e),
-        };
-        let payload = segment.payload(id)?;
-        let point = id_to_point.get(&id).unwrap();
-        if point.get_vectors() != all_vectors {
-            points_to_update.push(*point);
-            Ok(true)
-        } else {
-            let payload_match = match point.payload {
-                Some(ref p) => p == &payload,
-                None => Payload::default() == payload,
+    // we dont want to cancel this filtered read
+    let is_stopped = AtomicBool::new(false);
+    let _num_updated =
+        segments.read_points(existing_point_ids.as_slice(), &is_stopped, |id, segment| {
+            let all_vectors = match segment.all_vectors(id) {
+                Ok(v) => v,
+                Err(OperationError::InconsistentStorage { .. }) => NamedVectors::default(),
+                Err(e) => return Err(e),
             };
-            if !payload_match {
+            let payload = segment.payload(id)?;
+            let point = id_to_point.get(&id).unwrap();
+            if point.get_vectors() != all_vectors {
                 points_to_update.push(*point);
                 Ok(true)
             } else {
-                Ok(false)
+                let payload_match = match point.payload {
+                    Some(ref p) => p == &payload,
+                    None => Payload::default() == payload,
+                };
+                if !payload_match {
+                    points_to_update.push(*point);
+                    Ok(true)
+                } else {
+                    Ok(false)
+                }
             }
-        }
-    })?;
+        })?;
 
     // 4. Select new points
     let num_updated = points_to_update.len();

commit 8802660275cf22cb096b4d78734e3043869e67d7
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Fri Aug 16 09:33:30 2024 +0200

    Storage improvement (#4855)
    
    * improve segment updating
    
    * api improvement
    
    * remove unneeded function
    
    * Update lib/collection/src/collection_manager/holders/segment_holder.rs
    
    Co-authored-by: Tim Vise <tim+github@visee.me>
    
    * fix ci
    
    * add test for cow operation
    
    * review: simplify callback
    
    ---------
    
    Co-authored-by: Tim Vise <tim+github@visee.me>
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 93c81d9fd..f76e17e9b 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -82,6 +82,11 @@ pub(crate) fn update_vectors(
                 let vectors = points_map[&id].clone();
                 write_segment.update_vectors(op_num, id, vectors)
             },
+            |id, owned_vectors, _| {
+                for (vector_name, vector_ref) in points_map[&id].iter() {
+                    owned_vectors.insert(vector_name.to_string(), vector_ref.to_owned());
+                }
+            },
             |_| false,
         )?;
         check_unprocessed_points(batch, &updated_points)?;
@@ -148,6 +153,9 @@ pub(crate) fn overwrite_payload(
             op_num,
             batch,
             |id, write_segment| write_segment.set_full_payload(op_num, id, payload),
+            |_, _, old_payload| {
+                *old_payload = payload.clone();
+            },
             |segment| segment.get_indexed_fields().is_empty(),
         )?;
 
@@ -182,6 +190,7 @@ pub(crate) fn set_payload(
             op_num,
             chunk,
             |id, write_segment| write_segment.set_payload(op_num, id, payload, key),
+            |_, _, old_payload| old_payload.merge(payload),
             |segment| {
                 segment.get_indexed_fields().keys().all(|indexed_path| {
                     !indexed_path.is_affected_by_value_set(&payload.0, key.as_ref())
@@ -241,6 +250,11 @@ pub(crate) fn delete_payload(
                 }
                 Ok(res)
             },
+            |_, _, payload| {
+                for key in keys {
+                    payload.remove(key);
+                }
+            },
             |segment| {
                 iproduct!(segment.get_indexed_fields().keys(), keys).all(
                     |(indexed_path, path_to_delete)| {
@@ -279,6 +293,7 @@ pub(crate) fn clear_payload(
             op_num,
             batch,
             |id, write_segment| write_segment.clear_payload(op_num, id),
+            |_, _, payload| payload.0.clear(),
             |segment| segment.get_indexed_fields().is_empty(),
         )?;
         check_unprocessed_points(batch, &updated_points)?;
@@ -303,6 +318,7 @@ pub(crate) fn clear_payload_by_filter(
             op_num,
             batch,
             |id, write_segment| write_segment.clear_payload(op_num, id),
+            |_, _, payload| payload.0.clear(),
             |segment| segment.get_indexed_fields().is_empty(),
         )?;
         total_updated_points += updated_points.len();
@@ -462,6 +478,15 @@ where
                 point.payload.as_ref(),
             )
         },
+        |id, vectors, old_payload| {
+            let point = points_map[&id];
+            for (name, vec) in point.get_vectors() {
+                vectors.insert(name.to_string(), vec.to_owned());
+            }
+            if let Some(payload) = &point.payload {
+                *old_payload = payload.clone();
+            }
+        },
         |_| false,
     )?;
 

commit 96158c6f27c8a5d4366ecb88118f1808a6dd642f
Author: Luis Cosso <luis.cossio@qdrant.com>
Date:   Fri Aug 23 08:30:45 2024 -0400

    fix: Non-blocking payload index building (#4941)
    
    * separate index creation into build and apply
    
    * check version in Segment impl of `build_field_index`
    
    * add wait to issues test
    
    * fix consensus tests

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index f76e17e9b..dfa8f7fa9 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -335,7 +335,15 @@ pub(crate) fn create_field_index(
 ) -> CollectionResult<usize> {
     segments
         .apply_segments(|write_segment| {
-            write_segment.create_field_index(op_num, field_name, field_schema)
+            let Some((schema, index)) =
+                write_segment.build_field_index(op_num, field_name, field_schema)?
+            else {
+                return Ok(false);
+            };
+
+            write_segment.with_upgraded(|segment| {
+                segment.apply_field_index(op_num, field_name.to_owned(), schema, index)
+            })
         })
         .map_err(Into::into)
 }
@@ -346,7 +354,9 @@ pub(crate) fn delete_field_index(
     field_name: PayloadKeyTypeRef,
 ) -> CollectionResult<usize> {
     segments
-        .apply_segments(|write_segment| write_segment.delete_field_index(op_num, field_name))
+        .apply_segments(|write_segment| {
+            write_segment.with_upgraded(|segment| segment.delete_field_index(op_num, field_name))
+        })
         .map_err(Into::into)
 }
 

commit 6ae3b53907fa940eba4bac4f42e3885c44477092
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Mon Sep 23 11:59:08 2024 +0200

    Fix set payload by key on non appendable segment (#5113)

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index dfa8f7fa9..a99e69a36 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -190,7 +190,10 @@ pub(crate) fn set_payload(
             op_num,
             chunk,
             |id, write_segment| write_segment.set_payload(op_num, id, payload, key),
-            |_, _, old_payload| old_payload.merge(payload),
+            |_, _, old_payload| match key {
+                Some(key) => old_payload.merge_by_key(payload, key),
+                None => old_payload.merge(payload),
+            },
             |segment| {
                 segment.get_indexed_fields().keys().all(|indexed_path| {
                     !indexed_path.is_affected_by_value_set(&payload.0, key.as_ref())

commit 1d0ee7ea32043598f8b240e6a3a52be20663fa44
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Wed Oct 9 10:15:46 2024 +0200

    Inference interface in REST and gRPC (#5165)
    
    * include document & image objects into grpc API
    
    * introduce image and object to rest api
    
    * minor refactoring
    
    * rename Vector -> VectorInternal
    
    * decompose vector data structures
    
    * add schema
    
    * fmt
    
    * grpc docs
    
    * fix conversion
    
    * fix clippy
    
    * fix another conversion
    
    * rename VectorInput -> VectorInputInternal
    
    * replace grpc TryFrom with async functions
    
    * fmt
    
    * replace rest TryFrom with async functions
    
    * add image and object into query rest
    
    * separate inference related conversions
    
    * move json-related conversions into a separate file
    
    * move vector-related transformations into a separate file
    
    * move more vector related-conversions into dedicated module

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index a99e69a36..a39c85e26 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -17,9 +17,11 @@ use segment::types::{
 
 use crate::collection_manager::holders::segment_holder::SegmentHolder;
 use crate::operations::payload_ops::PayloadOps;
-use crate::operations::point_ops::{PointInsertOperationsInternal, PointOperations, PointStruct};
+use crate::operations::point_ops::{
+    PointInsertOperationsInternal, PointOperations, PointStructPersisted,
+};
 use crate::operations::types::{CollectionError, CollectionResult};
-use crate::operations::vector_ops::{PointVectors, VectorOperations};
+use crate::operations::vector_ops::{PointVectorsPersisted, VectorOperations};
 use crate::operations::FieldIndexOperations;
 
 pub(crate) fn check_unprocessed_points(
@@ -59,12 +61,12 @@ pub(crate) fn delete_points(
 pub(crate) fn update_vectors(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
-    points: Vec<PointVectors>,
+    points: Vec<PointVectorsPersisted>,
 ) -> CollectionResult<usize> {
     // Build a map of vectors to update per point, merge updates on same point ID
     let mut points_map: HashMap<PointIdType, NamedVectors> = HashMap::new();
     for point in points {
-        let PointVectors { id, vector } = point;
+        let PointVectorsPersisted { id, vector } = point;
         let named_vector = NamedVectors::from(vector);
 
         let entry = points_map.entry(id).or_default();
@@ -397,12 +399,9 @@ pub(crate) fn sync_points(
     op_num: SeqNumberType,
     from_id: Option<PointIdType>,
     to_id: Option<PointIdType>,
-    points: &[PointStruct],
+    points: &[PointStructPersisted],
 ) -> CollectionResult<(usize, usize, usize)> {
-    let id_to_point = points
-        .iter()
-        .map(|p| (p.id, p))
-        .collect::<HashMap<PointIdType, &PointStruct>>();
+    let id_to_point: HashMap<PointIdType, _> = points.iter().map(|p| (p.id, p)).collect();
     let sync_points: HashSet<_> = points.iter().map(|p| p.id).collect();
     // 1. Retrieve existing points for a range
     let stored_point_ids: HashSet<_> = segments
@@ -471,10 +470,9 @@ pub(crate) fn upsert_points<'a, T>(
     points: T,
 ) -> CollectionResult<usize>
 where
-    T: IntoIterator<Item = &'a PointStruct>,
+    T: IntoIterator<Item = &'a PointStructPersisted>,
 {
-    let points_map: HashMap<PointIdType, &PointStruct> =
-        points.into_iter().map(|p| (p.id, p)).collect();
+    let points_map: HashMap<PointIdType, _> = points.into_iter().map(|p| (p.id, p)).collect();
     let ids: Vec<PointIdType> = points_map.keys().copied().collect();
 
     // Update points in writable segments
@@ -546,12 +544,12 @@ pub(crate) fn process_point_operation(
         PointOperations::UpsertPoints(operation) => {
             let points: Vec<_> = match operation {
                 PointInsertOperationsInternal::PointsBatch(batch) => {
-                    let batch_vectors: BatchVectorStructInternal = batch.vectors.into();
+                    let batch_vectors = BatchVectorStructInternal::from(batch.vectors);
                     let all_vectors = batch_vectors.into_all_vectors(batch.ids.len());
                     let vectors_iter = batch.ids.into_iter().zip(all_vectors);
                     match batch.payloads {
                         None => vectors_iter
-                            .map(|(id, vectors)| PointStruct {
+                            .map(|(id, vectors)| PointStructPersisted {
                                 id,
                                 vector: VectorStructInternal::from(vectors).into(),
                                 payload: None,
@@ -559,7 +557,7 @@ pub(crate) fn process_point_operation(
                             .collect(),
                         Some(payloads) => vectors_iter
                             .zip(payloads)
-                            .map(|((id, vectors), payload)| PointStruct {
+                            .map(|((id, vectors), payload)| PointStructPersisted {
                                 id,
                                 vector: VectorStructInternal::from(vectors).into(),
                                 payload,

commit 753c4b5d2e7b3875d106d1f1739b28b6ab1dbeda
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Thu Oct 17 10:46:46 2024 +0200

    Fix eager backtrace generation (#5251)

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index a39c85e26..a5dbc06c5 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -503,18 +503,12 @@ where
 
     let mut res = updated_points.len();
     // Insert new points, which was not updated or existed
-    let new_point_ids = ids
-        .iter()
-        .cloned()
-        .filter(|x| !(updated_points.contains(x)));
+    let new_point_ids = ids.iter().cloned().filter(|x| !updated_points.contains(x));
 
     {
-        let default_write_segment =
-            segments
-                .smallest_appendable_segment()
-                .ok_or(CollectionError::service_error(
-                    "No appendable segments exists, expected at least one",
-                ))?;
+        let default_write_segment = segments.smallest_appendable_segment().ok_or_else(|| {
+            CollectionError::service_error("No appendable segments exists, expected at least one")
+        })?;
 
         let segment_arc = default_write_segment.get();
         let mut write_segment = segment_arc.write();

commit 037cdf175618f9056666a7ff57bf7f3d17305bb5
Author: Tim Vise <tim+github@visee.me>
Date:   Fri Nov 29 10:47:32 2024 +0100

    Minor improvements during testing (#5541)

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index a5dbc06c5..5f773efa7 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -365,6 +365,7 @@ pub(crate) fn delete_field_index(
         .map_err(Into::into)
 }
 
+/// Upsert to a point ID with the specified vectors and payload in the given segment.
 ///
 /// Returns
 /// - Ok(true) if the operation was successful and point replaced existing value
@@ -384,7 +385,7 @@ fn upsert_with_payload(
     Ok(res)
 }
 
-/// Sync points within a given [from_id; to_id) range
+/// Sync points within a given [from_id; to_id) range.
 ///
 /// 1. Retrieve existing points for a range
 /// 2. Remove points, which are not present in the sync operation
@@ -503,7 +504,7 @@ where
 
     let mut res = updated_points.len();
     // Insert new points, which was not updated or existed
-    let new_point_ids = ids.iter().cloned().filter(|x| !updated_points.contains(x));
+    let new_point_ids = ids.iter().copied().filter(|x| !updated_points.contains(x));
 
     {
         let default_write_segment = segments.smallest_appendable_segment().ok_or_else(|| {

commit 38f478ddf7a9d03a1c783c5599f3b6ae33a05195
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Thu Jan 16 14:25:55 2025 +0100

    Measure payload read IO (#5773)
    
    * Measure read io for payload storage
    
    * Add Hardware Counter to update functions
    
    * Fix tests and benches
    
    * Rename (some) *_measured functions back to original

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 5f773efa7..b42c26f18 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -3,6 +3,7 @@
 use std::collections::{HashMap, HashSet};
 use std::sync::atomic::AtomicBool;
 
+use common::counter::hardware_counter::HardwareCounterCell;
 use itertools::iproduct;
 use parking_lot::{RwLock, RwLockWriteGuard};
 use segment::common::operation_error::{OperationError, OperationResult};
@@ -41,6 +42,7 @@ pub(crate) fn delete_points(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
     ids: &[PointIdType],
+    hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     let mut total_deleted_points = 0;
 
@@ -48,7 +50,7 @@ pub(crate) fn delete_points(
         let deleted_points = segments.apply_points(
             batch,
             |_| (),
-            |id, _idx, write_segment, ()| write_segment.delete_point(op_num, id),
+            |id, _idx, write_segment, ()| write_segment.delete_point(op_num, id, hw_counter),
         )?;
 
         total_deleted_points += deleted_points;
@@ -62,6 +64,7 @@ pub(crate) fn update_vectors(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
     points: Vec<PointVectorsPersisted>,
+    hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     // Build a map of vectors to update per point, merge updates on same point ID
     let mut points_map: HashMap<PointIdType, NamedVectors> = HashMap::new();
@@ -82,7 +85,7 @@ pub(crate) fn update_vectors(
             batch,
             |id, write_segment| {
                 let vectors = points_map[&id].clone();
-                write_segment.update_vectors(op_num, id, vectors)
+                write_segment.update_vectors(op_num, id, vectors, hw_counter)
             },
             |id, owned_vectors, _| {
                 for (vector_name, vector_ref) in points_map[&id].iter() {
@@ -90,6 +93,7 @@ pub(crate) fn update_vectors(
                 }
             },
             |_| false,
+            hw_counter,
         )?;
         check_unprocessed_points(batch, &updated_points)?;
         total_updated_points += updated_points.len();
@@ -106,6 +110,7 @@ pub(crate) fn delete_vectors(
     op_num: SeqNumberType,
     points: &[PointIdType],
     vector_names: &[String],
+    hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     let mut total_deleted_points = 0;
 
@@ -116,7 +121,7 @@ pub(crate) fn delete_vectors(
             |id, _idx, write_segment, ()| {
                 let mut res = true;
                 for name in vector_names {
-                    res &= write_segment.delete_vector(op_num, id, name)?;
+                    res &= write_segment.delete_vector(op_num, id, name, hw_counter)?;
                 }
                 Ok(res)
             },
@@ -134,9 +139,10 @@ pub(crate) fn delete_vectors_by_filter(
     op_num: SeqNumberType,
     filter: &Filter,
     vector_names: &[String],
+    hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     let affected_points = points_by_filter(segments, filter)?;
-    delete_vectors(segments, op_num, &affected_points, vector_names)
+    delete_vectors(segments, op_num, &affected_points, vector_names, hw_counter)
 }
 
 /// Batch size when modifying payload.
@@ -147,6 +153,7 @@ pub(crate) fn overwrite_payload(
     op_num: SeqNumberType,
     payload: &Payload,
     points: &[PointIdType],
+    hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     let mut total_updated_points = 0;
 
@@ -154,11 +161,12 @@ pub(crate) fn overwrite_payload(
         let updated_points = segments.apply_points_with_conditional_move(
             op_num,
             batch,
-            |id, write_segment| write_segment.set_full_payload(op_num, id, payload),
+            |id, write_segment| write_segment.set_full_payload(op_num, id, payload, hw_counter),
             |_, _, old_payload| {
                 *old_payload = payload.clone();
             },
             |segment| segment.get_indexed_fields().is_empty(),
+            hw_counter,
         )?;
 
         total_updated_points += updated_points.len();
@@ -173,9 +181,10 @@ pub(crate) fn overwrite_payload_by_filter(
     op_num: SeqNumberType,
     payload: &Payload,
     filter: &Filter,
+    hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     let affected_points = points_by_filter(segments, filter)?;
-    overwrite_payload(segments, op_num, payload, &affected_points)
+    overwrite_payload(segments, op_num, payload, &affected_points, hw_counter)
 }
 
 pub(crate) fn set_payload(
@@ -184,6 +193,7 @@ pub(crate) fn set_payload(
     payload: &Payload,
     points: &[PointIdType],
     key: &Option<JsonPath>,
+    hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     let mut total_updated_points = 0;
 
@@ -191,7 +201,7 @@ pub(crate) fn set_payload(
         let updated_points = segments.apply_points_with_conditional_move(
             op_num,
             chunk,
-            |id, write_segment| write_segment.set_payload(op_num, id, payload, key),
+            |id, write_segment| write_segment.set_payload(op_num, id, payload, key, hw_counter),
             |_, _, old_payload| match key {
                 Some(key) => old_payload.merge_by_key(payload, key),
                 None => old_payload.merge(payload),
@@ -201,6 +211,7 @@ pub(crate) fn set_payload(
                     !indexed_path.is_affected_by_value_set(&payload.0, key.as_ref())
                 })
             },
+            hw_counter,
         )?;
 
         check_unprocessed_points(chunk, &updated_points)?;
@@ -231,9 +242,10 @@ pub(crate) fn set_payload_by_filter(
     payload: &Payload,
     filter: &Filter,
     key: &Option<JsonPath>,
+    hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     let affected_points = points_by_filter(segments, filter)?;
-    set_payload(segments, op_num, payload, &affected_points, key)
+    set_payload(segments, op_num, payload, &affected_points, key, hw_counter)
 }
 
 pub(crate) fn delete_payload(
@@ -241,6 +253,7 @@ pub(crate) fn delete_payload(
     op_num: SeqNumberType,
     points: &[PointIdType],
     keys: &[PayloadKeyType],
+    hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     let mut total_deleted_points = 0;
 
@@ -251,7 +264,7 @@ pub(crate) fn delete_payload(
             |id, write_segment| {
                 let mut res = true;
                 for key in keys {
-                    res &= write_segment.delete_payload(op_num, id, key)?;
+                    res &= write_segment.delete_payload(op_num, id, key, hw_counter)?;
                 }
                 Ok(res)
             },
@@ -267,6 +280,7 @@ pub(crate) fn delete_payload(
                     },
                 )
             },
+            hw_counter,
         )?;
 
         check_unprocessed_points(batch, &updated_points)?;
@@ -281,15 +295,17 @@ pub(crate) fn delete_payload_by_filter(
     op_num: SeqNumberType,
     filter: &Filter,
     keys: &[PayloadKeyType],
+    hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     let affected_points = points_by_filter(segments, filter)?;
-    delete_payload(segments, op_num, &affected_points, keys)
+    delete_payload(segments, op_num, &affected_points, keys, hw_counter)
 }
 
 pub(crate) fn clear_payload(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
     points: &[PointIdType],
+    hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     let mut total_updated_points = 0;
 
@@ -297,9 +313,10 @@ pub(crate) fn clear_payload(
         let updated_points = segments.apply_points_with_conditional_move(
             op_num,
             batch,
-            |id, write_segment| write_segment.clear_payload(op_num, id),
+            |id, write_segment| write_segment.clear_payload(op_num, id, hw_counter),
             |_, _, payload| payload.0.clear(),
             |segment| segment.get_indexed_fields().is_empty(),
+            hw_counter,
         )?;
         check_unprocessed_points(batch, &updated_points)?;
         total_updated_points += updated_points.len();
@@ -313,6 +330,7 @@ pub(crate) fn clear_payload_by_filter(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
     filter: &Filter,
+    hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     let points_to_clear = points_by_filter(segments, filter)?;
 
@@ -322,9 +340,10 @@ pub(crate) fn clear_payload_by_filter(
         let updated_points = segments.apply_points_with_conditional_move(
             op_num,
             batch,
-            |id, write_segment| write_segment.clear_payload(op_num, id),
+            |id, write_segment| write_segment.clear_payload(op_num, id, hw_counter),
             |_, _, payload| payload.0.clear(),
             |segment| segment.get_indexed_fields().is_empty(),
+            hw_counter,
         )?;
         total_updated_points += updated_points.len();
     }
@@ -377,10 +396,11 @@ fn upsert_with_payload(
     point_id: PointIdType,
     vectors: NamedVectors,
     payload: Option<&Payload>,
+    hw_counter: &HardwareCounterCell,
 ) -> OperationResult<bool> {
-    let mut res = segment.upsert_point(op_num, point_id, vectors)?;
+    let mut res = segment.upsert_point(op_num, point_id, vectors, hw_counter)?;
     if let Some(full_payload) = payload {
-        res &= segment.set_full_payload(op_num, point_id, full_payload)?;
+        res &= segment.set_full_payload(op_num, point_id, full_payload, hw_counter)?;
     }
     Ok(res)
 }
@@ -401,6 +421,7 @@ pub(crate) fn sync_points(
     from_id: Option<PointIdType>,
     to_id: Option<PointIdType>,
     points: &[PointStructPersisted],
+    hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<(usize, usize, usize)> {
     let id_to_point: HashMap<PointIdType, _> = points.iter().map(|p| (p.id, p)).collect();
     let sync_points: HashSet<_> = points.iter().map(|p| p.id).collect();
@@ -411,7 +432,7 @@ pub(crate) fn sync_points(
         .collect();
     // 2. Remove points, which are not present in the sync operation
     let points_to_remove: Vec<_> = stored_point_ids.difference(&sync_points).copied().collect();
-    let deleted = delete_points(segments, op_num, points_to_remove.as_slice())?;
+    let deleted = delete_points(segments, op_num, points_to_remove.as_slice(), hw_counter)?;
     // 3. Retrieve overlapping points, detect which one of them are changed
     let existing_point_ids: Vec<_> = stored_point_ids
         .intersection(&sync_points)
@@ -428,7 +449,7 @@ pub(crate) fn sync_points(
                 Err(OperationError::InconsistentStorage { .. }) => NamedVectors::default(),
                 Err(e) => return Err(e),
             };
-            let payload = segment.payload(id)?;
+            let payload = segment.payload(id, hw_counter)?;
             let point = id_to_point.get(&id).unwrap();
             if point.get_vectors() != all_vectors {
                 points_to_update.push(*point);
@@ -456,7 +477,7 @@ pub(crate) fn sync_points(
     });
 
     // 5. Upsert points which differ from the stored ones
-    let num_replaced = upsert_points(segments, op_num, points_to_update)?;
+    let num_replaced = upsert_points(segments, op_num, points_to_update, hw_counter)?;
     debug_assert!(num_replaced <= num_updated, "number of replaced points cannot be greater than points to update ({num_replaced} <= {num_updated})");
 
     Ok((deleted, num_new, num_updated))
@@ -469,6 +490,7 @@ pub(crate) fn upsert_points<'a, T>(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
     points: T,
+    hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize>
 where
     T: IntoIterator<Item = &'a PointStructPersisted>,
@@ -488,6 +510,7 @@ where
                 id,
                 point.get_vectors(),
                 point.payload.as_ref(),
+                hw_counter,
             )
         },
         |id, vectors, old_payload| {
@@ -500,6 +523,7 @@ where
             }
         },
         |_| false,
+        hw_counter,
     )?;
 
     let mut res = updated_points.len();
@@ -521,6 +545,7 @@ where
                 point_id,
                 point.get_vectors(),
                 point.payload.as_ref(),
+                hw_counter,
             )?);
         }
         RwLockWriteGuard::unlock_fair(write_segment);
@@ -533,9 +558,12 @@ pub(crate) fn process_point_operation(
     segments: &RwLock<SegmentHolder>,
     op_num: SeqNumberType,
     point_operation: PointOperations,
+    hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     match point_operation {
-        PointOperations::DeletePoints { ids, .. } => delete_points(&segments.read(), op_num, &ids),
+        PointOperations::DeletePoints { ids, .. } => {
+            delete_points(&segments.read(), op_num, &ids, hw_counter)
+        }
         PointOperations::UpsertPoints(operation) => {
             let points: Vec<_> = match operation {
                 PointInsertOperationsInternal::PointsBatch(batch) => {
@@ -562,11 +590,11 @@ pub(crate) fn process_point_operation(
                 }
                 PointInsertOperationsInternal::PointsList(points) => points,
             };
-            let res = upsert_points(&segments.read(), op_num, points.iter())?;
+            let res = upsert_points(&segments.read(), op_num, points.iter(), hw_counter)?;
             Ok(res)
         }
         PointOperations::DeletePointsByFilter(filter) => {
-            delete_points_by_filter(&segments.read(), op_num, &filter)
+            delete_points_by_filter(&segments.read(), op_num, &filter, hw_counter)
         }
         PointOperations::SyncPoints(operation) => {
             let (deleted, new, updated) = sync_points(
@@ -575,6 +603,7 @@ pub(crate) fn process_point_operation(
                 operation.from_id,
                 operation.to_id,
                 &operation.points,
+                hw_counter,
             )?;
             Ok(deleted + new + updated)
         }
@@ -585,16 +614,21 @@ pub(crate) fn process_vector_operation(
     segments: &RwLock<SegmentHolder>,
     op_num: SeqNumberType,
     vector_operation: VectorOperations,
+    hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     match vector_operation {
         VectorOperations::UpdateVectors(operation) => {
-            update_vectors(&segments.read(), op_num, operation.points)
-        }
-        VectorOperations::DeleteVectors(ids, vector_names) => {
-            delete_vectors(&segments.read(), op_num, &ids.points, &vector_names)
+            update_vectors(&segments.read(), op_num, operation.points, hw_counter)
         }
+        VectorOperations::DeleteVectors(ids, vector_names) => delete_vectors(
+            &segments.read(),
+            op_num,
+            &ids.points,
+            &vector_names,
+            hw_counter,
+        ),
         VectorOperations::DeleteVectorsByFilter(filter, vector_names) => {
-            delete_vectors_by_filter(&segments.read(), op_num, &filter, &vector_names)
+            delete_vectors_by_filter(&segments.read(), op_num, &filter, &vector_names, hw_counter)
         }
     }
 }
@@ -603,14 +637,29 @@ pub(crate) fn process_payload_operation(
     segments: &RwLock<SegmentHolder>,
     op_num: SeqNumberType,
     payload_operation: PayloadOps,
+    hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     match payload_operation {
         PayloadOps::SetPayload(sp) => {
             let payload: Payload = sp.payload;
             if let Some(points) = sp.points {
-                set_payload(&segments.read(), op_num, &payload, &points, &sp.key)
+                set_payload(
+                    &segments.read(),
+                    op_num,
+                    &payload,
+                    &points,
+                    &sp.key,
+                    hw_counter,
+                )
             } else if let Some(filter) = sp.filter {
-                set_payload_by_filter(&segments.read(), op_num, &payload, &filter, &sp.key)
+                set_payload_by_filter(
+                    &segments.read(),
+                    op_num,
+                    &payload,
+                    &filter,
+                    &sp.key,
+                    hw_counter,
+                )
             } else {
                 Err(CollectionError::BadRequest {
                     description: "No points or filter specified".to_string(),
@@ -619,9 +668,9 @@ pub(crate) fn process_payload_operation(
         }
         PayloadOps::DeletePayload(dp) => {
             if let Some(points) = dp.points {
-                delete_payload(&segments.read(), op_num, &points, &dp.keys)
+                delete_payload(&segments.read(), op_num, &points, &dp.keys, hw_counter)
             } else if let Some(filter) = dp.filter {
-                delete_payload_by_filter(&segments.read(), op_num, &filter, &dp.keys)
+                delete_payload_by_filter(&segments.read(), op_num, &filter, &dp.keys, hw_counter)
             } else {
                 Err(CollectionError::BadRequest {
                     description: "No points or filter specified".to_string(),
@@ -629,17 +678,17 @@ pub(crate) fn process_payload_operation(
             }
         }
         PayloadOps::ClearPayload { ref points, .. } => {
-            clear_payload(&segments.read(), op_num, points)
+            clear_payload(&segments.read(), op_num, points, hw_counter)
         }
         PayloadOps::ClearPayloadByFilter(ref filter) => {
-            clear_payload_by_filter(&segments.read(), op_num, filter)
+            clear_payload_by_filter(&segments.read(), op_num, filter, hw_counter)
         }
         PayloadOps::OverwritePayload(sp) => {
             let payload: Payload = sp.payload;
             if let Some(points) = sp.points {
-                overwrite_payload(&segments.read(), op_num, &payload, &points)
+                overwrite_payload(&segments.read(), op_num, &payload, &points, hw_counter)
             } else if let Some(filter) = sp.filter {
-                overwrite_payload_by_filter(&segments.read(), op_num, &payload, &filter)
+                overwrite_payload_by_filter(&segments.read(), op_num, &payload, &filter, hw_counter)
             } else {
                 Err(CollectionError::BadRequest {
                     description: "No points or filter specified".to_string(),
@@ -675,6 +724,7 @@ pub(crate) fn delete_points_by_filter(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
     filter: &Filter,
+    hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     let mut total_deleted = 0;
     // we dont want to cancel this filtered read
@@ -702,7 +752,7 @@ pub(crate) fn delete_points_by_filter(
 
         let mut deleted_in_batch = 0;
         while let Some(point_id) = curr_points.pop() {
-            if s.delete_point(op_num, point_id)? {
+            if s.delete_point(op_num, point_id, hw_counter)? {
                 total_deleted += 1;
                 deleted_in_batch += 1;
             }

commit e85a9f18b4f5219799c3625c2d3d19c5b3be4ed5
Author: xzfc <5121426+xzfc@users.noreply.github.com>
Date:   Fri Jan 24 01:29:01 2025 +0000

    Add `VectorName` type alias (#5763)
    
    * Add VectorName/VectorNameBuf type aliases [1/2]
    
    * Add VectorName/VectorNameBuf type aliases [2/2]

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index b42c26f18..e6301c8f2 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -13,7 +13,7 @@ use segment::entry::entry_point::SegmentEntry;
 use segment::json_path::JsonPath;
 use segment::types::{
     Filter, Payload, PayloadFieldSchema, PayloadKeyType, PayloadKeyTypeRef, PointIdType,
-    SeqNumberType,
+    SeqNumberType, VectorNameBuf,
 };
 
 use crate::collection_manager::holders::segment_holder::SegmentHolder;
@@ -89,7 +89,7 @@ pub(crate) fn update_vectors(
             },
             |id, owned_vectors, _| {
                 for (vector_name, vector_ref) in points_map[&id].iter() {
-                    owned_vectors.insert(vector_name.to_string(), vector_ref.to_owned());
+                    owned_vectors.insert(vector_name.to_owned(), vector_ref.to_owned());
                 }
             },
             |_| false,
@@ -109,7 +109,7 @@ pub(crate) fn delete_vectors(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
     points: &[PointIdType],
-    vector_names: &[String],
+    vector_names: &[VectorNameBuf],
     hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     let mut total_deleted_points = 0;
@@ -138,7 +138,7 @@ pub(crate) fn delete_vectors_by_filter(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
     filter: &Filter,
-    vector_names: &[String],
+    vector_names: &[VectorNameBuf],
     hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     let affected_points = points_by_filter(segments, filter)?;
@@ -516,7 +516,7 @@ where
         |id, vectors, old_payload| {
             let point = points_map[&id];
             for (name, vec) in point.get_vectors() {
-                vectors.insert(name.to_string(), vec.to_owned());
+                vectors.insert(name.into(), vec.to_owned());
             }
             if let Some(payload) = &point.payload {
                 *old_payload = payload.clone();

commit 97743b1b625d42f73955ecb32d54ca34ea3a5cb7
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Fri Jan 24 16:33:44 2025 +0100

    Propagate hardware counter for more functions (#5844)
    
    * Propagate hardware counter for more functions
    
    * Minor improvements
    
    * use vector_query_contexts hardware_counter

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index e6301c8f2..89231f13e 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -141,7 +141,7 @@ pub(crate) fn delete_vectors_by_filter(
     vector_names: &[VectorNameBuf],
     hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
-    let affected_points = points_by_filter(segments, filter)?;
+    let affected_points = points_by_filter(segments, filter, hw_counter)?;
     delete_vectors(segments, op_num, &affected_points, vector_names, hw_counter)
 }
 
@@ -183,7 +183,7 @@ pub(crate) fn overwrite_payload_by_filter(
     filter: &Filter,
     hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
-    let affected_points = points_by_filter(segments, filter)?;
+    let affected_points = points_by_filter(segments, filter, hw_counter)?;
     overwrite_payload(segments, op_num, payload, &affected_points, hw_counter)
 }
 
@@ -224,12 +224,13 @@ pub(crate) fn set_payload(
 fn points_by_filter(
     segments: &SegmentHolder,
     filter: &Filter,
+    hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<Vec<PointIdType>> {
     let mut affected_points: Vec<PointIdType> = Vec::new();
     // we dont want to cancel this filtered read
     let is_stopped = AtomicBool::new(false);
     segments.for_each_segment(|s| {
-        let points = s.read_filtered(None, None, Some(filter), &is_stopped);
+        let points = s.read_filtered(None, None, Some(filter), &is_stopped, hw_counter);
         affected_points.extend_from_slice(points.as_slice());
         Ok(true)
     })?;
@@ -244,7 +245,7 @@ pub(crate) fn set_payload_by_filter(
     key: &Option<JsonPath>,
     hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
-    let affected_points = points_by_filter(segments, filter)?;
+    let affected_points = points_by_filter(segments, filter, hw_counter)?;
     set_payload(segments, op_num, payload, &affected_points, key, hw_counter)
 }
 
@@ -297,7 +298,7 @@ pub(crate) fn delete_payload_by_filter(
     keys: &[PayloadKeyType],
     hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
-    let affected_points = points_by_filter(segments, filter)?;
+    let affected_points = points_by_filter(segments, filter, hw_counter)?;
     delete_payload(segments, op_num, &affected_points, keys, hw_counter)
 }
 
@@ -332,7 +333,7 @@ pub(crate) fn clear_payload_by_filter(
     filter: &Filter,
     hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
-    let points_to_clear = points_by_filter(segments, filter)?;
+    let points_to_clear = points_by_filter(segments, filter, hw_counter)?;
 
     let mut total_updated_points = 0;
 
@@ -734,10 +735,13 @@ pub(crate) fn delete_points_by_filter(
         .map(|(segment_id, segment)| {
             (
                 *segment_id,
-                segment
-                    .get()
-                    .read()
-                    .read_filtered(None, None, Some(filter), &is_stopped),
+                segment.get().read().read_filtered(
+                    None,
+                    None,
+                    Some(filter),
+                    &is_stopped,
+                    hw_counter,
+                ),
             )
         })
         .collect();

commit 4453015046fd71f94f6e9fbb109c873c32815dad
Author: Tim Vise <tim+github@visee.me>
Date:   Mon Feb 10 11:07:49 2025 +0100

    Apply point deletions to all point versions (#5956)
    
    * Extract point in segment selection logic, find just latest or all
    
    * Add only_latest_version flag to apply_points
    
    * The apply_points_with_conditional_move function does not delete points
    
    * Flip boolean, only_latest_version to all_point_versions
    
    * Link to GitHub PR describing bug
    
    * Add test, assert point delete applies to all point versions
    
    * split into two functions
    
    * nit: calculate default capacity outside of loop
    
    * When deleting point vector, only apply to latest point version
    
    ---------
    
    Co-authored-by: Luis Cosso <luis.cossio@outlook.com>

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 89231f13e..7091b7b79 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -51,6 +51,8 @@ pub(crate) fn delete_points(
             batch,
             |_| (),
             |id, _idx, write_segment, ()| write_segment.delete_point(op_num, id, hw_counter),
+            // Apply point delete to all point versions
+            true,
         )?;
 
         total_deleted_points += deleted_points;
@@ -125,6 +127,8 @@ pub(crate) fn delete_vectors(
                 }
                 Ok(res)
             },
+            // Only apply operation to latest point versions, operation does not delete points
+            false,
         )?;
 
         total_deleted_points += deleted_points;

commit 0323923d840e847ba8ac7a992ef8db6e15dc925a
Author: Tim Vise <tim+github@visee.me>
Date:   Tue Feb 11 14:32:23 2025 +0100

    Always delete old point versions when applying point operation (#5962)
    
    * Always delete old point versions when applying point operation
    
    * Add helper struct for updates/deletes, make selection more readable
    
    * Cleanup
    
    * Add debug assertion
    
    * If having multiple point versions, delete first, then apply operations
    
    * Deleted old points with their existing point version, extend test
    
    * Don't delete other points with the same version for now, add test case
    
    * Remove helper struct since updates and deletes are separated again
    
    * Smarter capacity estimate
    
    * apply change to all points with latest version
    
    * Use small vec extra allocations
    
    * Eliminate delete closure, delete internally and pass hardware counter
    
    * Update comment
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>
    
    ---------
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 7091b7b79..89231f13e 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -51,8 +51,6 @@ pub(crate) fn delete_points(
             batch,
             |_| (),
             |id, _idx, write_segment, ()| write_segment.delete_point(op_num, id, hw_counter),
-            // Apply point delete to all point versions
-            true,
         )?;
 
         total_deleted_points += deleted_points;
@@ -127,8 +125,6 @@ pub(crate) fn delete_vectors(
                 }
                 Ok(res)
             },
-            // Only apply operation to latest point versions, operation does not delete points
-            false,
         )?;
 
         total_deleted_points += deleted_points;

commit 8ad2b34265448ec01b89d4093de5fbb1a86dcd4d
Author: Tim Vise <tim+github@visee.me>
Date:   Tue Feb 25 11:21:25 2025 +0100

    Bump Rust edition to 2024 (#6042)
    
    * Bump Rust edition to 2024
    
    * gen is a reserved keyword now
    
    * Remove ref mut on references
    
    * Mark extern C as unsafe
    
    * Wrap unsafe function bodies in unsafe block
    
    * Geo hash implements Copy, don't reference but pass by value instead
    
    * Replace secluded self import with parent
    
    * Update execute_cluster_read_operation with new match semantics
    
    * Fix lifetime issue
    
    * Replace map_or with is_none_or
    
    * set_var is unsafe now
    
    * Reformat

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 89231f13e..b9ad47172 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -17,13 +17,13 @@ use segment::types::{
 };
 
 use crate::collection_manager::holders::segment_holder::SegmentHolder;
+use crate::operations::FieldIndexOperations;
 use crate::operations::payload_ops::PayloadOps;
 use crate::operations::point_ops::{
     PointInsertOperationsInternal, PointOperations, PointStructPersisted,
 };
 use crate::operations::types::{CollectionError, CollectionResult};
 use crate::operations::vector_ops::{PointVectorsPersisted, VectorOperations};
-use crate::operations::FieldIndexOperations;
 
 pub(crate) fn check_unprocessed_points(
     points: &[PointIdType],
@@ -479,7 +479,10 @@ pub(crate) fn sync_points(
 
     // 5. Upsert points which differ from the stored ones
     let num_replaced = upsert_points(segments, op_num, points_to_update, hw_counter)?;
-    debug_assert!(num_replaced <= num_updated, "number of replaced points cannot be greater than points to update ({num_replaced} <= {num_updated})");
+    debug_assert!(
+        num_replaced <= num_updated,
+        "number of replaced points cannot be greater than points to update ({num_replaced} <= {num_updated})",
+    );
 
     Ok((deleted, num_new, num_updated))
 }

commit 55cc50c6ddc3da9047a91b8465223aed26066da7
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Tue Mar 11 11:13:21 2025 +0100

    Remove code duplication for clear payload (#6130)

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index b9ad47172..66ec33bc5 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -334,22 +334,7 @@ pub(crate) fn clear_payload_by_filter(
     hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     let points_to_clear = points_by_filter(segments, filter, hw_counter)?;
-
-    let mut total_updated_points = 0;
-
-    for batch in points_to_clear.chunks(PAYLOAD_OP_BATCH_SIZE) {
-        let updated_points = segments.apply_points_with_conditional_move(
-            op_num,
-            batch,
-            |id, write_segment| write_segment.clear_payload(op_num, id, hw_counter),
-            |_, _, payload| payload.0.clear(),
-            |segment| segment.get_indexed_fields().is_empty(),
-            hw_counter,
-        )?;
-        total_updated_points += updated_points.len();
-    }
-
-    Ok(total_updated_points)
+    clear_payload(segments, op_num, &points_to_clear, hw_counter)
 }
 
 pub(crate) fn create_field_index(

commit 5cd7239b61d1a6944984132283f762850275670f
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Mon Mar 24 19:39:17 2025 +0100

    Measure Payload Index IO Writes (#6137)
    
    * Prepare measurement of index creation + Remove vector deletion
    measurement
    
    * add hw_counter to add_point functions
    
    * Adjust add_point(..) function signatures
    
    * Add new measurement type: payload index IO write
    
    * Measure payload index IO writes
    
    * Some Hw measurement performance improvements
    
    * Review remarks
    
    * Fix measurements in distributed setups
    
    * review fixes
    
    ---------
    
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 66ec33bc5..0fdc697c6 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -110,7 +110,6 @@ pub(crate) fn delete_vectors(
     op_num: SeqNumberType,
     points: &[PointIdType],
     vector_names: &[VectorNameBuf],
-    hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     let mut total_deleted_points = 0;
 
@@ -121,7 +120,7 @@ pub(crate) fn delete_vectors(
             |id, _idx, write_segment, ()| {
                 let mut res = true;
                 for name in vector_names {
-                    res &= write_segment.delete_vector(op_num, id, name, hw_counter)?;
+                    res &= write_segment.delete_vector(op_num, id, name)?;
                 }
                 Ok(res)
             },
@@ -142,7 +141,7 @@ pub(crate) fn delete_vectors_by_filter(
     hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     let affected_points = points_by_filter(segments, filter, hw_counter)?;
-    delete_vectors(segments, op_num, &affected_points, vector_names, hw_counter)
+    delete_vectors(segments, op_num, &affected_points, vector_names)
 }
 
 /// Batch size when modifying payload.
@@ -342,11 +341,12 @@ pub(crate) fn create_field_index(
     op_num: SeqNumberType,
     field_name: PayloadKeyTypeRef,
     field_schema: Option<&PayloadFieldSchema>,
+    hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     segments
         .apply_segments(|write_segment| {
             let Some((schema, index)) =
-                write_segment.build_field_index(op_num, field_name, field_schema)?
+                write_segment.build_field_index(op_num, field_name, field_schema, hw_counter)?
             else {
                 return Ok(false);
             };
@@ -609,13 +609,9 @@ pub(crate) fn process_vector_operation(
         VectorOperations::UpdateVectors(operation) => {
             update_vectors(&segments.read(), op_num, operation.points, hw_counter)
         }
-        VectorOperations::DeleteVectors(ids, vector_names) => delete_vectors(
-            &segments.read(),
-            op_num,
-            &ids.points,
-            &vector_names,
-            hw_counter,
-        ),
+        VectorOperations::DeleteVectors(ids, vector_names) => {
+            delete_vectors(&segments.read(), op_num, &ids.points, &vector_names)
+        }
         VectorOperations::DeleteVectorsByFilter(filter, vector_names) => {
             delete_vectors_by_filter(&segments.read(), op_num, &filter, &vector_names, hw_counter)
         }
@@ -691,6 +687,7 @@ pub(crate) fn process_field_index_operation(
     segments: &RwLock<SegmentHolder>,
     op_num: SeqNumberType,
     field_index_operation: &FieldIndexOperations,
+    hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     match field_index_operation {
         FieldIndexOperations::CreateIndex(index_data) => create_field_index(
@@ -698,6 +695,7 @@ pub(crate) fn process_field_index_operation(
             op_num,
             &index_data.field_name,
             index_data.field_schema.as_ref(),
+            hw_counter,
         ),
         FieldIndexOperations::DeleteIndex(field_name) => {
             delete_field_index(&segments.read(), op_num, field_name)

commit e59d395d80ade92eef58c220adb576548e5e21a7
Author: Tim Vise <tim+github@visee.me>
Date:   Thu Apr 17 23:11:35 2025 +0200

    Use ahash for maps/sets holding point IDs, offsets or similar (#6388)

diff --git a/lib/collection/src/collection_manager/segments_updater.rs b/lib/collection/src/collection_manager/segments_updater.rs
index 0fdc697c6..2ed5c8e7d 100644
--- a/lib/collection/src/collection_manager/segments_updater.rs
+++ b/lib/collection/src/collection_manager/segments_updater.rs
@@ -1,8 +1,8 @@
 //! A collection of functions for updating points and payloads stored in segments
 
-use std::collections::{HashMap, HashSet};
 use std::sync::atomic::AtomicBool;
 
+use ahash::{AHashMap, AHashSet};
 use common::counter::hardware_counter::HardwareCounterCell;
 use itertools::iproduct;
 use parking_lot::{RwLock, RwLockWriteGuard};
@@ -27,7 +27,7 @@ use crate::operations::vector_ops::{PointVectorsPersisted, VectorOperations};
 
 pub(crate) fn check_unprocessed_points(
     points: &[PointIdType],
-    processed: &HashSet<PointIdType>,
+    processed: &AHashSet<PointIdType>,
 ) -> CollectionResult<usize> {
     let first_missed_point = points.iter().copied().find(|p| !processed.contains(p));
 
@@ -67,7 +67,7 @@ pub(crate) fn update_vectors(
     hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     // Build a map of vectors to update per point, merge updates on same point ID
-    let mut points_map: HashMap<PointIdType, NamedVectors> = HashMap::new();
+    let mut points_map: AHashMap<PointIdType, NamedVectors> = AHashMap::new();
     for point in points {
         let PointVectorsPersisted { id, vector } = point;
         let named_vector = NamedVectors::from(vector);
@@ -409,10 +409,10 @@ pub(crate) fn sync_points(
     points: &[PointStructPersisted],
     hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<(usize, usize, usize)> {
-    let id_to_point: HashMap<PointIdType, _> = points.iter().map(|p| (p.id, p)).collect();
-    let sync_points: HashSet<_> = points.iter().map(|p| p.id).collect();
+    let id_to_point: AHashMap<PointIdType, _> = points.iter().map(|p| (p.id, p)).collect();
+    let sync_points: AHashSet<_> = points.iter().map(|p| p.id).collect();
     // 1. Retrieve existing points for a range
-    let stored_point_ids: HashSet<_> = segments
+    let stored_point_ids: AHashSet<_> = segments
         .iter()
         .flat_map(|(_, segment)| segment.get().read().read_range(from_id, to_id))
         .collect();
@@ -484,7 +484,7 @@ pub(crate) fn upsert_points<'a, T>(
 where
     T: IntoIterator<Item = &'a PointStructPersisted>,
 {
-    let points_map: HashMap<PointIdType, _> = points.into_iter().map(|p| (p.id, p)).collect();
+    let points_map: AHashMap<PointIdType, _> = points.into_iter().map(|p| (p.id, p)).collect();
     let ids: Vec<PointIdType> = points_map.keys().copied().collect();
 
     // Update points in writable segments
@@ -716,7 +716,7 @@ pub(crate) fn delete_points_by_filter(
     let mut total_deleted = 0;
     // we dont want to cancel this filtered read
     let is_stopped = AtomicBool::new(false);
-    let mut points_to_delete: HashMap<_, _> = segments
+    let mut points_to_delete: AHashMap<_, _> = segments
         .iter()
         .map(|(segment_id, segment)| {
             (

