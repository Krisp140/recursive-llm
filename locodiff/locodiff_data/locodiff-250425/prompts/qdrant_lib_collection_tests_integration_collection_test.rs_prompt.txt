# Instructions

You are being benchmarked. You will see the output of a git log command, and from that must infer the current state of a file. Think carefully, as you must output the exact state of the file to earn full marks.

**Important:** Your goal is to reproduce the file's content *exactly* as it exists at the final commit, even if the code appears broken, buggy, or contains obvious errors. Do **not** try to "fix" the code. Attempting to correct issues will result in a poor score, as this benchmark evaluates your ability to reproduce the precise state of the file based on its history.

# Required Response Format

Wrap the content of the file in triple backticks (```). Any text outside the final closing backticks will be ignored. End your response after outputting the closing backticks.

# Example Response

```python
#!/usr/bin/env python
print('Hello, world!')
```

# File History

> git log -p --cc --topo-order --reverse -- lib/collection/tests/integration/collection_test.rs

commit 5303b8f6a73d3b9459635980e0cf2c06bf089c40
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Wed Jun 7 11:32:00 2023 +0200

    merge integration binaries (collection) (#2037)

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
new file mode 100644
index 000000000..7387b559a
--- /dev/null
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -0,0 +1,491 @@
+use std::collections::HashSet;
+
+use collection::operations::payload_ops::{PayloadOps, SetPayload};
+use collection::operations::point_ops::{Batch, PointOperations, PointStruct, WriteOrdering};
+use collection::operations::types::{
+    CountRequest, PointRequest, RecommendRequest, ScrollRequest, SearchRequest, UpdateStatus,
+};
+use collection::operations::CollectionUpdateOperations;
+use collection::recommendations::recommend_by;
+use itertools::Itertools;
+use segment::data_types::vectors::VectorStruct;
+use segment::types::{
+    Condition, FieldCondition, Filter, HasIdCondition, Payload, PointIdType, WithPayloadInterface,
+};
+use tempfile::Builder;
+
+use crate::common::{load_local_collection, simple_collection_fixture, N_SHARDS};
+
+#[tokio::test]
+async fn test_collection_updater() {
+    test_collection_updater_with_shards(1).await;
+    test_collection_updater_with_shards(N_SHARDS).await;
+}
+
+async fn test_collection_updater_with_shards(shard_number: u32) {
+    let collection_dir = Builder::new().prefix("collection").tempdir().unwrap();
+
+    let mut collection = simple_collection_fixture(collection_dir.path(), shard_number).await;
+
+    let insert_points = CollectionUpdateOperations::PointOperation(
+        Batch {
+            ids: vec![0, 1, 2, 3, 4]
+                .into_iter()
+                .map(|x| x.into())
+                .collect_vec(),
+            vectors: vec![
+                vec![1.0, 0.0, 1.0, 1.0],
+                vec![1.0, 0.0, 1.0, 0.0],
+                vec![1.0, 1.0, 1.0, 1.0],
+                vec![1.0, 1.0, 0.0, 1.0],
+                vec![1.0, 0.0, 0.0, 0.0],
+            ]
+            .into(),
+            payloads: None,
+        }
+        .into(),
+    );
+
+    let insert_result = collection
+        .update_from_client(insert_points, true, WriteOrdering::default())
+        .await;
+
+    match insert_result {
+        Ok(res) => {
+            assert_eq!(res.status, UpdateStatus::Completed)
+        }
+        Err(err) => panic!("operation failed: {err:?}"),
+    }
+
+    let search_request = SearchRequest {
+        vector: vec![1.0, 1.0, 1.0, 1.0].into(),
+        with_payload: None,
+        with_vector: None,
+        filter: None,
+        params: None,
+        limit: 3,
+        offset: 0,
+        score_threshold: None,
+    };
+
+    let search_res = collection.search(search_request, None, None).await;
+
+    match search_res {
+        Ok(res) => {
+            assert_eq!(res.len(), 3);
+            assert_eq!(res[0].id, 2.into());
+            assert!(res[0].payload.is_none());
+        }
+        Err(err) => panic!("search failed: {err:?}"),
+    }
+    collection.before_drop().await;
+}
+
+#[tokio::test]
+async fn test_collection_search_with_payload_and_vector() {
+    test_collection_search_with_payload_and_vector_with_shards(1).await;
+    test_collection_search_with_payload_and_vector_with_shards(N_SHARDS).await;
+}
+
+async fn test_collection_search_with_payload_and_vector_with_shards(shard_number: u32) {
+    let collection_dir = Builder::new().prefix("collection").tempdir().unwrap();
+
+    let mut collection = simple_collection_fixture(collection_dir.path(), shard_number).await;
+
+    let insert_points = CollectionUpdateOperations::PointOperation(
+        Batch {
+            ids: vec![0.into(), 1.into()],
+            vectors: vec![vec![1.0, 0.0, 1.0, 1.0], vec![1.0, 0.0, 1.0, 0.0]].into(),
+            payloads: serde_json::from_str(
+                r#"[{ "k": { "type": "keyword", "value": "v1" } }, { "k": "v2" , "v": "v3"}]"#,
+            )
+            .unwrap(),
+        }
+        .into(),
+    );
+
+    let insert_result = collection
+        .update_from_client(insert_points, true, WriteOrdering::default())
+        .await;
+
+    match insert_result {
+        Ok(res) => {
+            assert_eq!(res.status, UpdateStatus::Completed)
+        }
+        Err(err) => panic!("operation failed: {err:?}"),
+    }
+
+    let search_request = SearchRequest {
+        vector: vec![1.0, 0.0, 1.0, 1.0].into(),
+        with_payload: Some(WithPayloadInterface::Bool(true)),
+        with_vector: Some(true.into()),
+        filter: None,
+        params: None,
+        limit: 3,
+        offset: 0,
+        score_threshold: None,
+    };
+
+    let search_res = collection.search(search_request, None, None).await;
+
+    match search_res {
+        Ok(res) => {
+            assert_eq!(res.len(), 2);
+            assert_eq!(res[0].id, 0.into());
+            assert_eq!(res[0].payload.as_ref().unwrap().len(), 1);
+            match &res[0].vector {
+                Some(VectorStruct::Single(v)) => assert_eq!(v, &vec![1.0, 0.0, 1.0, 1.0]),
+                _ => panic!("vector is not returned"),
+            }
+        }
+        Err(err) => panic!("search failed: {err:?}"),
+    }
+
+    let count_request = CountRequest {
+        filter: Some(Filter::new_must(Condition::Field(FieldCondition {
+            key: "k".to_string(),
+            r#match: Some(serde_json::from_str(r#"{ "value": "v2" }"#).unwrap()),
+            range: None,
+            geo_bounding_box: None,
+            geo_radius: None,
+            values_count: None,
+        }))),
+        exact: true,
+    };
+
+    let count_res = collection.count(count_request, None).await.unwrap();
+    assert_eq!(count_res.count, 1);
+
+    collection.before_drop().await;
+}
+
+// FIXME: dos not work
+#[tokio::test]
+async fn test_collection_loading() {
+    test_collection_loading_with_shards(1).await;
+    test_collection_loading_with_shards(N_SHARDS).await;
+}
+
+async fn test_collection_loading_with_shards(shard_number: u32) {
+    let collection_dir = Builder::new().prefix("collection").tempdir().unwrap();
+
+    {
+        let mut collection = simple_collection_fixture(collection_dir.path(), shard_number).await;
+        let insert_points = CollectionUpdateOperations::PointOperation(
+            Batch {
+                ids: vec![0, 1, 2, 3, 4]
+                    .into_iter()
+                    .map(|x| x.into())
+                    .collect_vec(),
+                vectors: vec![
+                    vec![1.0, 0.0, 1.0, 1.0],
+                    vec![1.0, 0.0, 1.0, 0.0],
+                    vec![1.0, 1.0, 1.0, 1.0],
+                    vec![1.0, 1.0, 0.0, 1.0],
+                    vec![1.0, 0.0, 0.0, 0.0],
+                ]
+                .into(),
+                payloads: None,
+            }
+            .into(),
+        );
+
+        collection
+            .update_from_client(insert_points, true, WriteOrdering::default())
+            .await
+            .unwrap();
+
+        let payload: Payload = serde_json::from_str(r#"{"color":"red"}"#).unwrap();
+
+        let assign_payload =
+            CollectionUpdateOperations::PayloadOperation(PayloadOps::SetPayload(SetPayload {
+                payload,
+                points: Some(vec![2.into(), 3.into()]),
+                filter: None,
+            }));
+
+        collection
+            .update_from_client(assign_payload, true, WriteOrdering::default())
+            .await
+            .unwrap();
+        collection.before_drop().await;
+    }
+
+    let collection_path = collection_dir.path();
+    let mut loaded_collection = load_local_collection(
+        "test".to_string(),
+        collection_path,
+        &collection_path.join("snapshots"),
+    )
+    .await;
+    let request = PointRequest {
+        ids: vec![1.into(), 2.into()],
+        with_payload: Some(WithPayloadInterface::Bool(true)),
+        with_vector: true.into(),
+    };
+    let retrieved = loaded_collection
+        .retrieve(request, None, None)
+        .await
+        .unwrap();
+
+    assert_eq!(retrieved.len(), 2);
+
+    for record in retrieved {
+        if record.id == 2.into() {
+            let non_empty_payload = record.payload.unwrap();
+
+            assert_eq!(non_empty_payload.len(), 1)
+        }
+    }
+    println!("Function end");
+    loaded_collection.before_drop().await;
+}
+
+#[test]
+fn test_deserialization() {
+    let insert_points = CollectionUpdateOperations::PointOperation(
+        Batch {
+            ids: vec![0.into(), 1.into()],
+            vectors: vec![vec![1.0, 0.0, 1.0, 1.0], vec![1.0, 0.0, 1.0, 0.0]].into(),
+            payloads: None,
+        }
+        .into(),
+    );
+    let json_str = serde_json::to_string_pretty(&insert_points).unwrap();
+
+    let _read_obj: CollectionUpdateOperations = serde_json::from_str(&json_str).unwrap();
+
+    let crob_bytes = rmp_serde::to_vec(&insert_points).unwrap();
+
+    let _read_obj2: CollectionUpdateOperations = rmp_serde::from_slice(&crob_bytes).unwrap();
+}
+
+#[test]
+fn test_deserialization2() {
+    let insert_points = CollectionUpdateOperations::PointOperation(
+        vec![
+            PointStruct {
+                id: 0.into(),
+                vector: vec![1.0, 0.0, 1.0, 1.0].into(),
+                payload: None,
+            },
+            PointStruct {
+                id: 1.into(),
+                vector: vec![1.0, 0.0, 1.0, 0.0].into(),
+                payload: None,
+            },
+        ]
+        .into(),
+    );
+
+    let json_str = serde_json::to_string_pretty(&insert_points).unwrap();
+
+    let _read_obj: CollectionUpdateOperations = serde_json::from_str(&json_str).unwrap();
+
+    let raw_bytes = rmp_serde::to_vec(&insert_points).unwrap();
+
+    let _read_obj2: CollectionUpdateOperations = rmp_serde::from_slice(&raw_bytes).unwrap();
+}
+
+// Request to find points sent to all shards but they might not have a particular id, so they will return an error
+#[tokio::test]
+async fn test_recommendation_api() {
+    test_recommendation_api_with_shards(1).await;
+    test_recommendation_api_with_shards(N_SHARDS).await;
+}
+
+async fn test_recommendation_api_with_shards(shard_number: u32) {
+    let collection_dir = Builder::new().prefix("collection").tempdir().unwrap();
+    let mut collection = simple_collection_fixture(collection_dir.path(), shard_number).await;
+
+    let insert_points = CollectionUpdateOperations::PointOperation(
+        Batch {
+            ids: vec![0, 1, 2, 3, 4, 5, 6, 7, 8]
+                .into_iter()
+                .map(|x| x.into())
+                .collect_vec(),
+            vectors: vec![
+                vec![0.0, 0.0, 1.0, 1.0],
+                vec![1.0, 0.0, 0.0, 0.0],
+                vec![1.0, 0.0, 0.0, 0.0],
+                vec![0.0, 1.0, 0.0, 0.0],
+                vec![0.0, 1.0, 0.0, 0.0],
+                vec![0.0, 0.0, 1.0, 0.0],
+                vec![0.0, 0.0, 1.0, 0.0],
+                vec![0.0, 0.0, 0.0, 1.0],
+                vec![0.0, 0.0, 0.0, 1.0],
+            ]
+            .into(),
+            payloads: None,
+        }
+        .into(),
+    );
+
+    collection
+        .update_from_client(insert_points, true, WriteOrdering::default())
+        .await
+        .unwrap();
+    let result = recommend_by(
+        RecommendRequest {
+            positive: vec![0.into()],
+            negative: vec![8.into()],
+            limit: 5,
+            ..Default::default()
+        },
+        &collection,
+        |_name| async { unreachable!("Should not be called in this test") },
+        None,
+    )
+    .await
+    .unwrap();
+    assert!(!result.is_empty());
+    let top1 = &result[0];
+
+    assert!(top1.id == 5.into() || top1.id == 6.into());
+    collection.before_drop().await;
+}
+
+#[tokio::test]
+async fn test_read_api() {
+    test_read_api_with_shards(1).await;
+    test_read_api_with_shards(N_SHARDS).await;
+}
+
+async fn test_read_api_with_shards(shard_number: u32) {
+    let collection_dir = Builder::new().prefix("collection").tempdir().unwrap();
+    let mut collection = simple_collection_fixture(collection_dir.path(), shard_number).await;
+
+    let insert_points = CollectionUpdateOperations::PointOperation(PointOperations::UpsertPoints(
+        Batch {
+            ids: vec![0, 1, 2, 3, 4, 5, 6, 7, 8]
+                .into_iter()
+                .map(|x| x.into())
+                .collect_vec(),
+            vectors: vec![
+                vec![0.0, 0.0, 1.0, 1.0],
+                vec![1.0, 0.0, 0.0, 0.0],
+                vec![1.0, 0.0, 0.0, 0.0],
+                vec![0.0, 1.0, 0.0, 0.0],
+                vec![0.0, 1.0, 0.0, 0.0],
+                vec![0.0, 0.0, 1.0, 0.0],
+                vec![0.0, 0.0, 1.0, 0.0],
+                vec![0.0, 0.0, 0.0, 1.0],
+                vec![0.0, 0.0, 0.0, 1.0],
+            ]
+            .into(),
+            payloads: None,
+        }
+        .into(),
+    ));
+
+    collection
+        .update_from_client(insert_points, true, WriteOrdering::default())
+        .await
+        .unwrap();
+
+    let result = collection
+        .scroll_by(
+            ScrollRequest {
+                offset: None,
+                limit: Some(2),
+                filter: None,
+                with_payload: Some(WithPayloadInterface::Bool(true)),
+                with_vector: false.into(),
+            },
+            None,
+            None,
+        )
+        .await
+        .unwrap();
+
+    assert_eq!(result.next_page_offset, Some(2.into()));
+    assert_eq!(result.points.len(), 2);
+    collection.before_drop().await;
+}
+
+#[tokio::test]
+async fn test_collection_delete_points_by_filter() {
+    test_collection_delete_points_by_filter_with_shards(1).await;
+    test_collection_delete_points_by_filter_with_shards(N_SHARDS).await;
+}
+
+async fn test_collection_delete_points_by_filter_with_shards(shard_number: u32) {
+    let collection_dir = Builder::new().prefix("collection").tempdir().unwrap();
+
+    let mut collection = simple_collection_fixture(collection_dir.path(), shard_number).await;
+
+    let insert_points = CollectionUpdateOperations::PointOperation(
+        Batch {
+            ids: vec![0, 1, 2, 3, 4]
+                .into_iter()
+                .map(|x| x.into())
+                .collect_vec(),
+            vectors: vec![
+                vec![1.0, 0.0, 1.0, 1.0],
+                vec![1.0, 0.0, 1.0, 0.0],
+                vec![1.0, 1.0, 1.0, 1.0],
+                vec![1.0, 1.0, 0.0, 1.0],
+                vec![1.0, 0.0, 0.0, 0.0],
+            ]
+            .into(),
+            payloads: None,
+        }
+        .into(),
+    );
+
+    let insert_result = collection
+        .update_from_client(insert_points, true, WriteOrdering::default())
+        .await;
+
+    match insert_result {
+        Ok(res) => {
+            assert_eq!(res.status, UpdateStatus::Completed)
+        }
+        Err(err) => panic!("operation failed: {err:?}"),
+    }
+
+    // delete points with id (0, 3)
+    let to_be_deleted: HashSet<PointIdType> = vec![0.into(), 3.into()].into_iter().collect();
+    let delete_filter = segment::types::Filter {
+        should: None,
+        must: Some(vec![Condition::HasId(HasIdCondition::from(to_be_deleted))]),
+        must_not: None,
+    };
+
+    let delete_points = CollectionUpdateOperations::PointOperation(
+        PointOperations::DeletePointsByFilter(delete_filter),
+    );
+
+    let delete_result = collection
+        .update_from_client(delete_points, true, WriteOrdering::default())
+        .await;
+
+    match delete_result {
+        Ok(res) => {
+            assert_eq!(res.status, UpdateStatus::Completed)
+        }
+        Err(err) => panic!("operation failed: {err:?}"),
+    }
+
+    let result = collection
+        .scroll_by(
+            ScrollRequest {
+                offset: None,
+                limit: Some(10),
+                filter: None,
+                with_payload: Some(WithPayloadInterface::Bool(false)),
+                with_vector: false.into(),
+            },
+            None,
+            None,
+        )
+        .await
+        .unwrap();
+
+    // check if we only have 3 out of 5 points left and that the point id were really deleted
+    assert_eq!(result.points.len(), 3);
+    assert_eq!(result.points.get(0).unwrap().id, 1.into());
+    assert_eq!(result.points.get(1).unwrap().id, 2.into());
+    assert_eq!(result.points.get(2).unwrap().id, 4.into());
+    collection.before_drop().await;
+}

commit 84ff963f03a3efb849eae55ff2311c6fea9d6554
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Tue Jun 27 10:52:31 2023 +0200

    Remove before_drop mechanism (#2144)
    
    * Remove before_drop mechanism
    
    * name scoped thread for troubleshooting
    
    * Do early return with try
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index 7387b559a..dee001a95 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -16,7 +16,7 @@ use tempfile::Builder;
 
 use crate::common::{load_local_collection, simple_collection_fixture, N_SHARDS};
 
-#[tokio::test]
+#[tokio::test(flavor = "multi_thread")]
 async fn test_collection_updater() {
     test_collection_updater_with_shards(1).await;
     test_collection_updater_with_shards(N_SHARDS).await;
@@ -25,7 +25,7 @@ async fn test_collection_updater() {
 async fn test_collection_updater_with_shards(shard_number: u32) {
     let collection_dir = Builder::new().prefix("collection").tempdir().unwrap();
 
-    let mut collection = simple_collection_fixture(collection_dir.path(), shard_number).await;
+    let collection = simple_collection_fixture(collection_dir.path(), shard_number).await;
 
     let insert_points = CollectionUpdateOperations::PointOperation(
         Batch {
@@ -78,10 +78,9 @@ async fn test_collection_updater_with_shards(shard_number: u32) {
         }
         Err(err) => panic!("search failed: {err:?}"),
     }
-    collection.before_drop().await;
 }
 
-#[tokio::test]
+#[tokio::test(flavor = "multi_thread")]
 async fn test_collection_search_with_payload_and_vector() {
     test_collection_search_with_payload_and_vector_with_shards(1).await;
     test_collection_search_with_payload_and_vector_with_shards(N_SHARDS).await;
@@ -90,7 +89,7 @@ async fn test_collection_search_with_payload_and_vector() {
 async fn test_collection_search_with_payload_and_vector_with_shards(shard_number: u32) {
     let collection_dir = Builder::new().prefix("collection").tempdir().unwrap();
 
-    let mut collection = simple_collection_fixture(collection_dir.path(), shard_number).await;
+    let collection = simple_collection_fixture(collection_dir.path(), shard_number).await;
 
     let insert_points = CollectionUpdateOperations::PointOperation(
         Batch {
@@ -155,12 +154,10 @@ async fn test_collection_search_with_payload_and_vector_with_shards(shard_number
 
     let count_res = collection.count(count_request, None).await.unwrap();
     assert_eq!(count_res.count, 1);
-
-    collection.before_drop().await;
 }
 
 // FIXME: dos not work
-#[tokio::test]
+#[tokio::test(flavor = "multi_thread")]
 async fn test_collection_loading() {
     test_collection_loading_with_shards(1).await;
     test_collection_loading_with_shards(N_SHARDS).await;
@@ -170,7 +167,7 @@ async fn test_collection_loading_with_shards(shard_number: u32) {
     let collection_dir = Builder::new().prefix("collection").tempdir().unwrap();
 
     {
-        let mut collection = simple_collection_fixture(collection_dir.path(), shard_number).await;
+        let collection = simple_collection_fixture(collection_dir.path(), shard_number).await;
         let insert_points = CollectionUpdateOperations::PointOperation(
             Batch {
                 ids: vec![0, 1, 2, 3, 4]
@@ -208,11 +205,10 @@ async fn test_collection_loading_with_shards(shard_number: u32) {
             .update_from_client(assign_payload, true, WriteOrdering::default())
             .await
             .unwrap();
-        collection.before_drop().await;
     }
 
     let collection_path = collection_dir.path();
-    let mut loaded_collection = load_local_collection(
+    let loaded_collection = load_local_collection(
         "test".to_string(),
         collection_path,
         &collection_path.join("snapshots"),
@@ -238,7 +234,6 @@ async fn test_collection_loading_with_shards(shard_number: u32) {
         }
     }
     println!("Function end");
-    loaded_collection.before_drop().await;
 }
 
 #[test]
@@ -288,7 +283,7 @@ fn test_deserialization2() {
 }
 
 // Request to find points sent to all shards but they might not have a particular id, so they will return an error
-#[tokio::test]
+#[tokio::test(flavor = "multi_thread")]
 async fn test_recommendation_api() {
     test_recommendation_api_with_shards(1).await;
     test_recommendation_api_with_shards(N_SHARDS).await;
@@ -296,7 +291,7 @@ async fn test_recommendation_api() {
 
 async fn test_recommendation_api_with_shards(shard_number: u32) {
     let collection_dir = Builder::new().prefix("collection").tempdir().unwrap();
-    let mut collection = simple_collection_fixture(collection_dir.path(), shard_number).await;
+    let collection = simple_collection_fixture(collection_dir.path(), shard_number).await;
 
     let insert_points = CollectionUpdateOperations::PointOperation(
         Batch {
@@ -342,10 +337,9 @@ async fn test_recommendation_api_with_shards(shard_number: u32) {
     let top1 = &result[0];
 
     assert!(top1.id == 5.into() || top1.id == 6.into());
-    collection.before_drop().await;
 }
 
-#[tokio::test]
+#[tokio::test(flavor = "multi_thread")]
 async fn test_read_api() {
     test_read_api_with_shards(1).await;
     test_read_api_with_shards(N_SHARDS).await;
@@ -353,7 +347,7 @@ async fn test_read_api() {
 
 async fn test_read_api_with_shards(shard_number: u32) {
     let collection_dir = Builder::new().prefix("collection").tempdir().unwrap();
-    let mut collection = simple_collection_fixture(collection_dir.path(), shard_number).await;
+    let collection = simple_collection_fixture(collection_dir.path(), shard_number).await;
 
     let insert_points = CollectionUpdateOperations::PointOperation(PointOperations::UpsertPoints(
         Batch {
@@ -400,10 +394,9 @@ async fn test_read_api_with_shards(shard_number: u32) {
 
     assert_eq!(result.next_page_offset, Some(2.into()));
     assert_eq!(result.points.len(), 2);
-    collection.before_drop().await;
 }
 
-#[tokio::test]
+#[tokio::test(flavor = "multi_thread")]
 async fn test_collection_delete_points_by_filter() {
     test_collection_delete_points_by_filter_with_shards(1).await;
     test_collection_delete_points_by_filter_with_shards(N_SHARDS).await;
@@ -412,7 +405,7 @@ async fn test_collection_delete_points_by_filter() {
 async fn test_collection_delete_points_by_filter_with_shards(shard_number: u32) {
     let collection_dir = Builder::new().prefix("collection").tempdir().unwrap();
 
-    let mut collection = simple_collection_fixture(collection_dir.path(), shard_number).await;
+    let collection = simple_collection_fixture(collection_dir.path(), shard_number).await;
 
     let insert_points = CollectionUpdateOperations::PointOperation(
         Batch {
@@ -487,5 +480,4 @@ async fn test_collection_delete_points_by_filter_with_shards(shard_number: u32)
     assert_eq!(result.points.get(0).unwrap().id, 1.into());
     assert_eq!(result.points.get(1).unwrap().id, 2.into());
     assert_eq!(result.points.get(2).unwrap().id, 4.into());
-    collection.before_drop().await;
 }

commit bd40a58e65e58ba5cfea79be5603faf88dc62248
Author: Zein Wen <85084498+zzzz-vincent@users.noreply.github.com>
Date:   Mon Jul 17 03:36:50 2023 -0700

    Add geo_polygon filter to proto interface, complete conversion fn, and add an integration test (#2188)

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index dee001a95..c40ed7796 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -148,6 +148,7 @@ async fn test_collection_search_with_payload_and_vector_with_shards(shard_number
             geo_bounding_box: None,
             geo_radius: None,
             values_count: None,
+            geo_polygon: None,
         }))),
         exact: true,
     };

commit 0f9a42ef53bed4ef3ba4486fb3f0799c5ae1070b
Author: Tim Visée <tim+github@visee.me>
Date:   Wed Oct 4 14:45:15 2023 +0200

    Change local shard stuck in Initializing to Active on load (#2759)
    
    * Change local shard stuck in Initializing to Active on load
    
    * Add test for changing local initializing shard to active on load
    
    * Spelling fix
    
    * Remove function that is too specialized
    
    * Reuse variable for settings distributed parameter
    
    * Only set shard state from init to active on load if not distributed

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index c40ed7796..950d4768d 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -1,4 +1,5 @@
 use std::collections::HashSet;
+use std::fs::File;
 
 use collection::operations::payload_ops::{PayloadOps, SetPayload};
 use collection::operations::point_ops::{Batch, PointOperations, PointStruct, WriteOrdering};
@@ -7,6 +8,7 @@ use collection::operations::types::{
 };
 use collection::operations::CollectionUpdateOperations;
 use collection::recommendations::recommend_by;
+use collection::shards::replica_set::{ReplicaSetState, ReplicaState};
 use itertools::Itertools;
 use segment::data_types::vectors::VectorStruct;
 use segment::types::{
@@ -157,7 +159,7 @@ async fn test_collection_search_with_payload_and_vector_with_shards(shard_number
     assert_eq!(count_res.count, 1);
 }
 
-// FIXME: dos not work
+// FIXME: does not work
 #[tokio::test(flavor = "multi_thread")]
 async fn test_collection_loading() {
     test_collection_loading_with_shards(1).await;
@@ -482,3 +484,45 @@ async fn test_collection_delete_points_by_filter_with_shards(shard_number: u32)
     assert_eq!(result.points.get(1).unwrap().id, 2.into());
     assert_eq!(result.points.get(2).unwrap().id, 4.into());
 }
+
+#[tokio::test(flavor = "multi_thread")]
+async fn test_collection_local_load_initializing_not_stuck() {
+    let collection_dir = Builder::new().prefix("collection").tempdir().unwrap();
+
+    // Create and unload collection
+    simple_collection_fixture(collection_dir.path(), 1).await;
+
+    // Modify replica state file on disk, set state to Initializing
+    // This is to simulate a situation where a collection was not fully created, we cannot create
+    // this situation through our collection interface
+    {
+        let replica_state_path = collection_dir.path().join("0/replica_state.json");
+        let replica_state_file = File::open(&replica_state_path).unwrap();
+        let mut replica_set_state: ReplicaSetState =
+            serde_json::from_reader(replica_state_file).unwrap();
+
+        for peer_id in replica_set_state.peers().into_keys() {
+            replica_set_state.set_peer_state(peer_id, ReplicaState::Initializing);
+        }
+
+        let replica_state_file = File::create(&replica_state_path).unwrap();
+        serde_json::to_writer(replica_state_file, &replica_set_state).unwrap();
+    }
+
+    // Reload collection
+    let collection_path = collection_dir.path();
+    let loaded_collection = load_local_collection(
+        "test".to_string(),
+        collection_path,
+        &collection_path.join("snapshots"),
+    )
+    .await;
+
+    // Local replica must be in Active state after loading (all replicas are local)
+    let loaded_state = loaded_collection.state().await;
+    for shard_info in loaded_state.shards.values() {
+        for replica_state in shard_info.replicas.values() {
+            assert_eq!(replica_state, &ReplicaState::Active);
+        }
+    }
+}

commit 87524275d4ff940145ff0110932f2b4d64f987b9
Author: Luis Cossío <luis.cossio@qdrant.com>
Date:   Thu Nov 2 12:45:46 2023 -0400

    Expose timeout query param for search requests (#2748)
    
    * add timeout query param for search requests
    
    * enable timeout for recommend requests
    
    * Add query timeout for group by requests
    
    * update openapi models
    
    * Don't decrease timeout after recommend preprocessing
    
    * Add openapi test
    
    * code review
    
    * add timeout to individual group by requests, non-decreasing
    
    * handle timeout for discover
    
    * Update timeout field tag in SearchBatchPoints
    message

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index 950d4768d..6e3830b68 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -70,7 +70,7 @@ async fn test_collection_updater_with_shards(shard_number: u32) {
         score_threshold: None,
     };
 
-    let search_res = collection.search(search_request, None, None).await;
+    let search_res = collection.search(search_request, None, None, None).await;
 
     match search_res {
         Ok(res) => {
@@ -127,7 +127,7 @@ async fn test_collection_search_with_payload_and_vector_with_shards(shard_number
         score_threshold: None,
     };
 
-    let search_res = collection.search(search_request, None, None).await;
+    let search_res = collection.search(search_request, None, None, None).await;
 
     match search_res {
         Ok(res) => {
@@ -333,6 +333,7 @@ async fn test_recommendation_api_with_shards(shard_number: u32) {
         &collection,
         |_name| async { unreachable!("Should not be called in this test") },
         None,
+        None,
     )
     .await
     .unwrap();

commit 816b5a7448c7f1e0d81c99e5a31219d00ece6fe5
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Thu Nov 9 15:06:02 2023 +0100

    Shard key routing for update requests (#2909)
    
    * add shard_key into output data structures for points
    
    * fmt
    
    * add shard selector for point update operations
    
    * fix creating index without sharding
    
    * Merge serde attributes
    
    * Code review changes
    
    * review fixes
    
    * upd openapi
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index 6e3830b68..f8f7fc6d8 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -1,7 +1,7 @@
 use std::collections::HashSet;
 use std::fs::File;
 
-use collection::operations::payload_ops::{PayloadOps, SetPayload};
+use collection::operations::payload_ops::{PayloadOps, SetPayloadOp};
 use collection::operations::point_ops::{Batch, PointOperations, PointStruct, WriteOrdering};
 use collection::operations::types::{
     CountRequest, PointRequest, RecommendRequest, ScrollRequest, SearchRequest, UpdateStatus,
@@ -49,7 +49,7 @@ async fn test_collection_updater_with_shards(shard_number: u32) {
     );
 
     let insert_result = collection
-        .update_from_client(insert_points, true, WriteOrdering::default())
+        .update_from_client_simple(insert_points, true, WriteOrdering::default())
         .await;
 
     match insert_result {
@@ -106,7 +106,7 @@ async fn test_collection_search_with_payload_and_vector_with_shards(shard_number
     );
 
     let insert_result = collection
-        .update_from_client(insert_points, true, WriteOrdering::default())
+        .update_from_client_simple(insert_points, true, WriteOrdering::default())
         .await;
 
     match insert_result {
@@ -191,21 +191,21 @@ async fn test_collection_loading_with_shards(shard_number: u32) {
         );
 
         collection
-            .update_from_client(insert_points, true, WriteOrdering::default())
+            .update_from_client_simple(insert_points, true, WriteOrdering::default())
             .await
             .unwrap();
 
         let payload: Payload = serde_json::from_str(r#"{"color":"red"}"#).unwrap();
 
         let assign_payload =
-            CollectionUpdateOperations::PayloadOperation(PayloadOps::SetPayload(SetPayload {
+            CollectionUpdateOperations::PayloadOperation(PayloadOps::SetPayload(SetPayloadOp {
                 payload,
                 points: Some(vec![2.into(), 3.into()]),
                 filter: None,
             }));
 
         collection
-            .update_from_client(assign_payload, true, WriteOrdering::default())
+            .update_from_client_simple(assign_payload, true, WriteOrdering::default())
             .await
             .unwrap();
     }
@@ -320,7 +320,7 @@ async fn test_recommendation_api_with_shards(shard_number: u32) {
     );
 
     collection
-        .update_from_client(insert_points, true, WriteOrdering::default())
+        .update_from_client_simple(insert_points, true, WriteOrdering::default())
         .await
         .unwrap();
     let result = recommend_by(
@@ -377,7 +377,7 @@ async fn test_read_api_with_shards(shard_number: u32) {
     ));
 
     collection
-        .update_from_client(insert_points, true, WriteOrdering::default())
+        .update_from_client_simple(insert_points, true, WriteOrdering::default())
         .await
         .unwrap();
 
@@ -431,7 +431,7 @@ async fn test_collection_delete_points_by_filter_with_shards(shard_number: u32)
     );
 
     let insert_result = collection
-        .update_from_client(insert_points, true, WriteOrdering::default())
+        .update_from_client_simple(insert_points, true, WriteOrdering::default())
         .await;
 
     match insert_result {
@@ -454,7 +454,7 @@ async fn test_collection_delete_points_by_filter_with_shards(shard_number: u32)
     );
 
     let delete_result = collection
-        .update_from_client(delete_points, true, WriteOrdering::default())
+        .update_from_client_simple(delete_points, true, WriteOrdering::default())
         .await;
 
     match delete_result {

commit cae3c45bf5d08ef6900cb88891b72f0b0bbf154e
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Fri Nov 10 18:38:01 2023 +0100

    Remove deprecated search methods (#2970)
    
    * remove duplicated search methods, introduced for compatibility in last version
    
    * Use `with_capacity` rather than a manual reserve
    
    * explicit Arc clones
    
    * get rid of batching by runs of same strategy
    
    * avoid refactor in group by
    
    * more explicit arc clones, remove one .expect()
    
    * little extra refactor on recommendations.rs
    
    * refactor grouping_test.rs too
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>
    Co-authored-by: Luis Cossío <luis.cossio@outlook.com>

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index f8f7fc6d8..21ecaf93d 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -70,7 +70,9 @@ async fn test_collection_updater_with_shards(shard_number: u32) {
         score_threshold: None,
     };
 
-    let search_res = collection.search(search_request, None, None, None).await;
+    let search_res = collection
+        .search(search_request.into(), None, None, None)
+        .await;
 
     match search_res {
         Ok(res) => {
@@ -127,7 +129,9 @@ async fn test_collection_search_with_payload_and_vector_with_shards(shard_number
         score_threshold: None,
     };
 
-    let search_res = collection.search(search_request, None, None, None).await;
+    let search_res = collection
+        .search(search_request.into(), None, None, None)
+        .await;
 
     match search_res {
         Ok(res) => {

commit 2810672598fcba5aac80077daf469791475d1b5e
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Tue Nov 14 16:47:05 2023 +0100

    Huge refactoring to make read requests aware of shard key selector (#3004)
    
    * huge refactoring to make read requests avare of shard key selector
    
    * fix integration test
    
    * review fixes
    
    * allow lookup_from specific shards

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index 21ecaf93d..f0b4c6e36 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -3,8 +3,10 @@ use std::fs::File;
 
 use collection::operations::payload_ops::{PayloadOps, SetPayloadOp};
 use collection::operations::point_ops::{Batch, PointOperations, PointStruct, WriteOrdering};
+use collection::operations::shard_selector_internal::ShardSelectorInternal;
 use collection::operations::types::{
-    CountRequest, PointRequest, RecommendRequest, ScrollRequest, SearchRequest, UpdateStatus,
+    CountRequestInternal, PointRequestInternal, RecommendRequestInternal, ScrollRequestInternal,
+    SearchRequestInternal, UpdateStatus,
 };
 use collection::operations::CollectionUpdateOperations;
 use collection::recommendations::recommend_by;
@@ -59,7 +61,7 @@ async fn test_collection_updater_with_shards(shard_number: u32) {
         Err(err) => panic!("operation failed: {err:?}"),
     }
 
-    let search_request = SearchRequest {
+    let search_request = SearchRequestInternal {
         vector: vec![1.0, 1.0, 1.0, 1.0].into(),
         with_payload: None,
         with_vector: None,
@@ -71,7 +73,12 @@ async fn test_collection_updater_with_shards(shard_number: u32) {
     };
 
     let search_res = collection
-        .search(search_request.into(), None, None, None)
+        .search(
+            search_request.into(),
+            None,
+            &ShardSelectorInternal::All,
+            None,
+        )
         .await;
 
     match search_res {
@@ -118,7 +125,7 @@ async fn test_collection_search_with_payload_and_vector_with_shards(shard_number
         Err(err) => panic!("operation failed: {err:?}"),
     }
 
-    let search_request = SearchRequest {
+    let search_request = SearchRequestInternal {
         vector: vec![1.0, 0.0, 1.0, 1.0].into(),
         with_payload: Some(WithPayloadInterface::Bool(true)),
         with_vector: Some(true.into()),
@@ -130,7 +137,12 @@ async fn test_collection_search_with_payload_and_vector_with_shards(shard_number
     };
 
     let search_res = collection
-        .search(search_request.into(), None, None, None)
+        .search(
+            search_request.into(),
+            None,
+            &ShardSelectorInternal::All,
+            None,
+        )
         .await;
 
     match search_res {
@@ -146,7 +158,7 @@ async fn test_collection_search_with_payload_and_vector_with_shards(shard_number
         Err(err) => panic!("search failed: {err:?}"),
     }
 
-    let count_request = CountRequest {
+    let count_request = CountRequestInternal {
         filter: Some(Filter::new_must(Condition::Field(FieldCondition {
             key: "k".to_string(),
             r#match: Some(serde_json::from_str(r#"{ "value": "v2" }"#).unwrap()),
@@ -159,7 +171,10 @@ async fn test_collection_search_with_payload_and_vector_with_shards(shard_number
         exact: true,
     };
 
-    let count_res = collection.count(count_request, None).await.unwrap();
+    let count_res = collection
+        .count(count_request, None, &ShardSelectorInternal::All)
+        .await
+        .unwrap();
     assert_eq!(count_res.count, 1);
 }
 
@@ -221,13 +236,13 @@ async fn test_collection_loading_with_shards(shard_number: u32) {
         &collection_path.join("snapshots"),
     )
     .await;
-    let request = PointRequest {
+    let request = PointRequestInternal {
         ids: vec![1.into(), 2.into()],
         with_payload: Some(WithPayloadInterface::Bool(true)),
         with_vector: true.into(),
     };
     let retrieved = loaded_collection
-        .retrieve(request, None, None)
+        .retrieve(request, None, &ShardSelectorInternal::All)
         .await
         .unwrap();
 
@@ -328,7 +343,7 @@ async fn test_recommendation_api_with_shards(shard_number: u32) {
         .await
         .unwrap();
     let result = recommend_by(
-        RecommendRequest {
+        RecommendRequestInternal {
             positive: vec![0.into()],
             negative: vec![8.into()],
             limit: 5,
@@ -337,6 +352,7 @@ async fn test_recommendation_api_with_shards(shard_number: u32) {
         &collection,
         |_name| async { unreachable!("Should not be called in this test") },
         None,
+        ShardSelectorInternal::All,
         None,
     )
     .await
@@ -387,7 +403,7 @@ async fn test_read_api_with_shards(shard_number: u32) {
 
     let result = collection
         .scroll_by(
-            ScrollRequest {
+            ScrollRequestInternal {
                 offset: None,
                 limit: Some(2),
                 filter: None,
@@ -395,7 +411,7 @@ async fn test_read_api_with_shards(shard_number: u32) {
                 with_vector: false.into(),
             },
             None,
-            None,
+            &ShardSelectorInternal::All,
         )
         .await
         .unwrap();
@@ -470,7 +486,7 @@ async fn test_collection_delete_points_by_filter_with_shards(shard_number: u32)
 
     let result = collection
         .scroll_by(
-            ScrollRequest {
+            ScrollRequestInternal {
                 offset: None,
                 limit: Some(10),
                 filter: None,
@@ -478,7 +494,7 @@ async fn test_collection_delete_points_by_filter_with_shards(shard_number: u32)
                 with_vector: false.into(),
             },
             None,
-            None,
+            &ShardSelectorInternal::All,
         )
         .await
         .unwrap();

commit 3fd3ff215aefede19b7f6ce566f7680b4b53dac3
Author: Luis Cossío <luis.cossio@qdrant.com>
Date:   Wed Nov 22 15:05:54 2023 -0300

    refactor: turn offset into an option (#3082)
    
    * refactor: make offset optional
    
    * update openapi
    
    * add simple test

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index f0b4c6e36..145a13e46 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -68,7 +68,7 @@ async fn test_collection_updater_with_shards(shard_number: u32) {
         filter: None,
         params: None,
         limit: 3,
-        offset: 0,
+        offset: None,
         score_threshold: None,
     };
 
@@ -132,7 +132,7 @@ async fn test_collection_search_with_payload_and_vector_with_shards(shard_number
         filter: None,
         params: None,
         limit: 3,
-        offset: 0,
+        offset: None,
         score_threshold: None,
     };
 

commit 3fc1f9656418995d21d156bd83f6f3611a99ee96
Author: Ivan Pleshkov <pleshkov.ivan@gmail.com>
Date:   Fri Dec 1 13:10:58 2023 +0100

    Sparse index segment and collection config (#2802)
    
    * quantization storage as separate entity
    
    sparse index try to extend segment types
    
    fix build
    
    fix async scorer
    
    codespell
    
    update openapi
    
    update vector index
    
    remove code duplications
    
    more fixes
    
    more fixes
    
    fix build
    
    fix deserialization test
    
    remove transform_into
    
    are you happy clippy
    
    update openapi
    
    update openapi
    
    are you happy clippy
    
    fix build
    
    optional serialize
    
    more defaults
    
    update openapi
    
    fix comments
    
    generic transpose_map_into_named_vector
    
    rename fields in tests
    
    remove obsolete parts
    
    only named sparse config
    
    VectorStruct without unnamed sparse
    
    NamedVectorStruct without unnamed sparse
    
    remove obsolete test
    
    update openapi
    
    mmap index
    
    revert preprocess function
    
    are you happy fmt
    
    update openapi
    
    fix build
    
    fix tests
    
    are you happy fmt
    
    fix for client generation
    
    fix sparse segment creation
    
    fix basic sparse test
    
    fix conflicts
    
    remove obsolete convertion
    
    fix build
    
    config diffs
    
    update openapi
    
    review remarks
    
    update openapi
    
    fix batch upsert
    
    add failing test showing bad ids matching
    
    fix sparse vector insertion
    
    remove on_disk flag
    
    update openapi
    
    revert debug assert
    
    simplify conversions
    
    update openapi
    
    remove on disk storage flag
    
    update openapi
    
    default for vector config
    
    update openapi comment
    
    remove diffs
    
    update openapi
    
    * enable consensus test
    
    * add comment
    
    * update openapi

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index 145a13e46..be747360d 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -150,8 +150,9 @@ async fn test_collection_search_with_payload_and_vector_with_shards(shard_number
             assert_eq!(res.len(), 2);
             assert_eq!(res[0].id, 0.into());
             assert_eq!(res[0].payload.as_ref().unwrap().len(), 1);
+            let vec = vec![1.0, 0.0, 1.0, 1.0];
             match &res[0].vector {
-                Some(VectorStruct::Single(v)) => assert_eq!(v, &vec![1.0, 0.0, 1.0, 1.0]),
+                Some(VectorStruct::Single(v)) => assert_eq!(v.clone(), vec),
                 _ => panic!("vector is not returned"),
             }
         }

commit 680574347f3b3dd6f604f452b80734a8c6f2f7c6
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Mon Dec 25 14:26:21 2023 +0100

    Fix clippy 1.75 (#3270)

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index be747360d..39078f1f4 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -502,7 +502,7 @@ async fn test_collection_delete_points_by_filter_with_shards(shard_number: u32)
 
     // check if we only have 3 out of 5 points left and that the point id were really deleted
     assert_eq!(result.points.len(), 3);
-    assert_eq!(result.points.get(0).unwrap().id, 1.into());
+    assert_eq!(result.points.first().unwrap().id, 1.into());
     assert_eq!(result.points.get(1).unwrap().id, 2.into());
     assert_eq!(result.points.get(2).unwrap().id, 4.into());
 }

commit 2f76603ddfbe5f995443c1e5e85c2d9345a55db0
Author: xzfc <5121426+xzfc@users.noreply.github.com>
Date:   Wed Jan 31 10:14:31 2024 +0000

    DateTime payload index (#3395)
    
    * Datetime payload index
    
    * Introduce IndexMapItem
    
    * Drop FieldIndex::DatetimeIndex
    
    * Rename OpenAPI struct names
    
    * Switch to microseconds
    
    * Validate and serialize grpc timestamps
    
    * Add tests with different timezones
    
    * minor review fixes
    
    * Revert "Drop FieldIndex::DatetimeIndex"
    
    This reverts commit d55f251afdbb418ef732a3e6799b92f924fc3035.
    
    * Revert "Introduce IndexMapItem"
    
    This reverts commit c5255f6b1aafa2b9552bac5d1811f9e826eb8d61.
    
    * fix: back to microseconds after reverts
    
    * extract range conversion from boxed checker fn
    
    * add log to deps
    
    * don't run macro doctest
    
    * no_run -> ignore
    
    * remove prost-types in favor of prost-wkt-types
    
    * better assertion on test_payload_indexing.py
    
    * propagate unparsable datetime
    
    ---------
    
    Co-authored-by: Luis Cossío <luis.cossio@outlook.com>

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index 39078f1f4..2ba6ff3a0 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -160,15 +160,9 @@ async fn test_collection_search_with_payload_and_vector_with_shards(shard_number
     }
 
     let count_request = CountRequestInternal {
-        filter: Some(Filter::new_must(Condition::Field(FieldCondition {
-            key: "k".to_string(),
-            r#match: Some(serde_json::from_str(r#"{ "value": "v2" }"#).unwrap()),
-            range: None,
-            geo_bounding_box: None,
-            geo_radius: None,
-            values_count: None,
-            geo_polygon: None,
-        }))),
+        filter: Some(Filter::new_must(Condition::Field(
+            FieldCondition::new_match("k", serde_json::from_str(r#"{ "value": "v2" }"#).unwrap()),
+        ))),
         exact: true,
     };
 

commit 320b7f2621f08d08fa6fbd1e8f82a277610af81c
Author: Luis Cossío <luis.cossio@qdrant.com>
Date:   Sun Feb 4 14:46:22 2024 -0300

    `order_by` in scroll (#3493)
    
    * first PR implementation (#2865)
    
    - fetch offset id
    - restructure tests
    - only let order_by with numeric
    - introduce order_by interface
    
    cargo fmt
    
    update openapi
    
    calculate range to fetch using offset + limit, do some cleanup
    
    enable index validation, fix test
    
    Fix pagination
    
    add e2e tests
    
    make test a little more strict
    
    select numeric index on read_ordered_filtered
    
    add filtering test 🫨
    
    fix filtering on order-by
    
    fix pip requirements
    
    add grpc interface, make read_ordered_filtered fallible
    
    fmt
    
    small optimization of `with_payload` and `with_vector`
    
    refactor common logic of point_ops and local_shard_operations
    
    Make filtering test harder and fix limit for worst case
    
    update openapi
    
    small clarity refactor
    
    avoid extra allocation when sorting with offset
    
    stream from numeric index btree instead of calculating range
    
    use payload to store order-by value, instead of modifying Record interface
    
    various fixes:
    - fix ordering at collection level, when merging shard results
    - fix offset at segment level, to take into account also value offset
    - make rust tests pass
    
    remove unused histogram changes
    
    fix error messages and make has_range_index exhaustive
    
    remove unused From impl
    
    Move OrderBy and Direction to segment::data_types::order_by
    
    Refactor normal scroll_by in local_shard_operations.rs
    
    More cleanup + rename OrderableRead to StreamWithValue
    
    empty commit
    
    optimization for merging results from shards and segments
    
    fix case of multi-valued fields
    
    fix IntegerIndexParams name after rebase
    
    precompute offset key
    
    use extracted `read_by_id_stream`
    
    Expose value_offset to user
    - rename offset -> value_offset
    - extract offset value fetching logic
    
    * remove offset functionality when using order_by
    
    * include order_by in ForwardProxyShard
    
    * extra nits
    
    * remove histogram changes
    
    * more nits
    
    * self review
    
    * resolve conflicts after rebase, not enable order-by with datetime index schema
    
    * make grpc start_from value extendable
    
    * gen grpc docs
    
    ---------
    
    Co-authored-by: kwkr <kawka.maciej.93@gmail.com>
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index 2ba6ff3a0..8c76b6ba0 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -1,21 +1,24 @@
-use std::collections::HashSet;
+use std::collections::{HashMap, HashSet};
 use std::fs::File;
 
 use collection::operations::payload_ops::{PayloadOps, SetPayloadOp};
 use collection::operations::point_ops::{Batch, PointOperations, PointStruct, WriteOrdering};
 use collection::operations::shard_selector_internal::ShardSelectorInternal;
 use collection::operations::types::{
-    CountRequestInternal, PointRequestInternal, RecommendRequestInternal, ScrollRequestInternal,
-    SearchRequestInternal, UpdateStatus,
+    CountRequestInternal, OrderByInterface, PointRequestInternal, RecommendRequestInternal,
+    ScrollRequestInternal, SearchRequestInternal, UpdateStatus,
 };
 use collection::operations::CollectionUpdateOperations;
 use collection::recommendations::recommend_by;
 use collection::shards::replica_set::{ReplicaSetState, ReplicaState};
 use itertools::Itertools;
+use segment::data_types::order_by::{Direction, OrderBy};
 use segment::data_types::vectors::VectorStruct;
 use segment::types::{
-    Condition, FieldCondition, Filter, HasIdCondition, Payload, PointIdType, WithPayloadInterface,
+    Condition, ExtendedPointId, FieldCondition, Filter, HasIdCondition, Payload,
+    PayloadFieldSchema, PayloadSchemaType, PointIdType, WithPayloadInterface,
 };
+use serde_json::Map;
 use tempfile::Builder;
 
 use crate::common::{load_local_collection, simple_collection_fixture, N_SHARDS};
@@ -404,6 +407,7 @@ async fn test_read_api_with_shards(shard_number: u32) {
                 filter: None,
                 with_payload: Some(WithPayloadInterface::Bool(true)),
                 with_vector: false.into(),
+                order_by: None,
             },
             None,
             &ShardSelectorInternal::All,
@@ -415,6 +419,289 @@ async fn test_read_api_with_shards(shard_number: u32) {
     assert_eq!(result.points.len(), 2);
 }
 
+#[tokio::test(flavor = "multi_thread")]
+async fn test_ordered_read_api() {
+    test_ordered_scroll_api_with_shards(1).await;
+    test_ordered_scroll_api_with_shards(N_SHARDS).await;
+}
+
+async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
+    let collection_dir = Builder::new().prefix("collection").tempdir().unwrap();
+    let collection = simple_collection_fixture(collection_dir.path(), shard_number).await;
+
+    const PRICE_FLOAT_KEY: &str = "price_float";
+    const PRICE_INT_KEY: &str = "price_int";
+    const MULTI_VALUE_KEY: &str = "multi_value";
+
+    let get_payload = |value: f64| -> Option<Payload> {
+        let mut payload_map = Map::new();
+        payload_map.insert(PRICE_FLOAT_KEY.to_string(), (value).into());
+        payload_map.insert(PRICE_INT_KEY.to_string(), (value as i64).into());
+        payload_map.insert(
+            MULTI_VALUE_KEY.to_string(),
+            vec![value, value + 20.0].into(),
+        );
+        Some(Payload(payload_map))
+    };
+
+    let payloads: Vec<Option<Payload>> = vec![
+        get_payload(11.0),
+        get_payload(10.0),
+        get_payload(9.0),
+        get_payload(8.0),
+        get_payload(7.0),
+        get_payload(6.0),
+        get_payload(5.0),
+        get_payload(5.0),
+        get_payload(5.0),
+        get_payload(5.0),
+        get_payload(4.0),
+        get_payload(3.0),
+        get_payload(2.0),
+        get_payload(1.0),
+    ];
+
+    let insert_points = CollectionUpdateOperations::PointOperation(PointOperations::UpsertPoints(
+        Batch {
+            ids: vec![0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
+                .into_iter()
+                .map(|x| x.into())
+                .collect_vec(),
+            vectors: vec![
+                vec![0.0, 0.0, 1.0, 1.0],
+                vec![1.0, 0.0, 0.0, 0.0],
+                vec![1.0, 0.0, 0.0, 0.0],
+                vec![0.0, 1.0, 0.0, 0.0],
+                vec![0.0, 1.0, 0.0, 0.0],
+                vec![0.0, 0.0, 1.0, 0.0],
+                vec![0.0, 0.0, 1.0, 0.0],
+                vec![0.0, 0.0, 0.0, 1.0],
+                vec![0.0, 0.0, 0.0, 1.0],
+                vec![0.0, 1.0, 1.0, 1.0],
+                vec![0.0, 1.0, 1.0, 1.0],
+                vec![0.0, 1.0, 1.0, 1.0],
+                vec![0.0, 1.0, 1.0, 1.0],
+                vec![1.0, 1.0, 1.0, 1.0],
+            ]
+            .into(),
+            payloads: Some(payloads),
+        }
+        .into(),
+    ));
+
+    collection
+        .update_from_client_simple(insert_points, true, WriteOrdering::default())
+        .await
+        .unwrap();
+
+    collection
+        .create_payload_index_with_wait(
+            PRICE_FLOAT_KEY.to_string(),
+            PayloadFieldSchema::FieldType(PayloadSchemaType::Float),
+            true,
+        )
+        .await
+        .unwrap();
+
+    collection
+        .create_payload_index_with_wait(
+            PRICE_INT_KEY.to_string(),
+            PayloadFieldSchema::FieldType(PayloadSchemaType::Integer),
+            true,
+        )
+        .await
+        .unwrap();
+
+    collection
+        .create_payload_index_with_wait(
+            MULTI_VALUE_KEY.to_string(),
+            PayloadFieldSchema::FieldType(PayloadSchemaType::Float),
+            true,
+        )
+        .await
+        .unwrap();
+
+    ///////// Test single-valued fields ///////////
+    for key in [PRICE_FLOAT_KEY, PRICE_INT_KEY].into_iter() {
+        let result_asc = collection
+            .scroll_by(
+                ScrollRequestInternal {
+                    offset: None,
+                    limit: Some(3),
+                    filter: None,
+                    with_payload: Some(WithPayloadInterface::Bool(true)),
+                    with_vector: false.into(),
+                    order_by: Some(OrderByInterface::Struct(OrderBy {
+                        key: key.into(),
+                        direction: Some(Direction::Asc),
+                        start_from: None,
+                    })),
+                },
+                None,
+                &ShardSelectorInternal::All,
+            )
+            .await
+            .unwrap();
+
+        assert_eq!(result_asc.points.len(), 3);
+        assert_eq!(result_asc.next_page_offset, None);
+        assert!(result_asc.points.iter().tuple_windows().all(|(a, b)| {
+            let a = a.payload.as_ref().unwrap();
+            let b = b.payload.as_ref().unwrap();
+            let a = a.0.get(key).unwrap().as_f64();
+            let b = b.0.get(key).unwrap().as_f64();
+            a <= b
+        }));
+
+        let result_desc = collection
+            .scroll_by(
+                ScrollRequestInternal {
+                    offset: None,
+                    limit: Some(5),
+                    filter: None,
+                    with_payload: Some(WithPayloadInterface::Bool(true)),
+                    with_vector: false.into(),
+                    order_by: Some(OrderByInterface::Struct(OrderBy {
+                        key: key.into(),
+                        direction: Some(Direction::Desc),
+                        start_from: None,
+                    })),
+                },
+                None,
+                &ShardSelectorInternal::All,
+            )
+            .await
+            .unwrap();
+
+        assert_eq!(result_desc.points.len(), 5);
+        assert_eq!(result_desc.next_page_offset, None);
+        assert!(
+            result_desc.points.iter().tuple_windows().all(|(a, b)| {
+                let a = a.payload.as_ref().unwrap();
+                let b = b.payload.as_ref().unwrap();
+                let a = a.0.get(key).unwrap().as_f64();
+                let b = b.0.get(key).unwrap().as_f64();
+                a >= b
+            }),
+            "got: {:#?}",
+            result_desc.points
+        );
+
+        let asc_already_seen: HashSet<_> = result_asc.points.iter().map(|x| x.id).collect();
+
+        dbg!(&asc_already_seen);
+        let asc_second_page = collection
+            .scroll_by(
+                ScrollRequestInternal {
+                    offset: None,
+                    limit: Some(5),
+                    filter: Some(Filter::new_must_not(Condition::HasId(
+                        HasIdCondition::from(asc_already_seen),
+                    ))),
+                    with_payload: Some(WithPayloadInterface::Bool(true)),
+                    with_vector: false.into(),
+                    order_by: Some(OrderByInterface::Struct(OrderBy {
+                        key: key.into(),
+                        direction: Some(Direction::Asc),
+                        start_from: None,
+                    })),
+                },
+                None,
+                &ShardSelectorInternal::All,
+            )
+            .await
+            .unwrap();
+
+        let asc_second_page_points = asc_second_page
+            .points
+            .iter()
+            .map(|x| x.id)
+            .collect::<HashSet<_>>();
+        let valid_asc_second_page_points = [10, 9, 8, 7, 6]
+            .into_iter()
+            .map(|x| x.into())
+            .collect::<HashSet<ExtendedPointId>>();
+        assert_eq!(asc_second_page.points.len(), 5);
+        assert!(asc_second_page_points.is_subset(&valid_asc_second_page_points));
+
+        let desc_already_seen: HashSet<_> = result_desc.points.iter().map(|x| x.id).collect();
+
+        dbg!(&desc_already_seen);
+
+        let desc_second_page = collection
+            .scroll_by(
+                ScrollRequestInternal {
+                    offset: None,
+                    limit: Some(4),
+                    filter: Some(Filter::new_must_not(Condition::HasId(
+                        HasIdCondition::from(desc_already_seen),
+                    ))),
+                    with_payload: Some(WithPayloadInterface::Bool(true)),
+                    with_vector: false.into(),
+                    order_by: Some(OrderByInterface::Struct(OrderBy {
+                        key: key.into(),
+                        direction: Some(Direction::Desc),
+                        start_from: None,
+                    })),
+                },
+                None,
+                &ShardSelectorInternal::All,
+            )
+            .await
+            .unwrap();
+
+        let desc_second_page_points = desc_second_page
+            .points
+            .iter()
+            .map(|x| x.id)
+            .collect::<HashSet<_>>();
+
+        let valid_desc_second_page_points = [5, 6, 7, 8, 9]
+            .into_iter()
+            .map(|x| x.into())
+            .collect::<HashSet<ExtendedPointId>>();
+
+        assert_eq!(desc_second_page.points.len(), 4);
+        assert!(
+            desc_second_page_points.is_subset(&valid_desc_second_page_points),
+            "expected: {:?}, got: {:?}",
+            valid_desc_second_page_points,
+            desc_second_page_points
+        );
+    }
+
+    ///////// Test multi-valued field ///////////
+    let result_multi = collection
+        .scroll_by(
+            ScrollRequestInternal {
+                offset: None,
+                limit: Some(100),
+                filter: None,
+                with_payload: Some(WithPayloadInterface::Bool(true)),
+                with_vector: false.into(),
+                order_by: Some(OrderByInterface::Key(MULTI_VALUE_KEY.into())),
+            },
+            None,
+            &ShardSelectorInternal::All,
+        )
+        .await
+        .unwrap();
+
+    assert!(result_multi
+        .points
+        .iter()
+        .fold(HashMap::<PointIdType, usize, _>::new(), |mut acc, point| {
+            acc.entry(point.id)
+                .and_modify(|x| {
+                    *x += 1;
+                })
+                .or_insert(1);
+            acc
+        })
+        .values()
+        .all(|&x| x == 2));
+}
+
 #[tokio::test(flavor = "multi_thread")]
 async fn test_collection_delete_points_by_filter() {
     test_collection_delete_points_by_filter_with_shards(1).await;
@@ -487,6 +774,7 @@ async fn test_collection_delete_points_by_filter_with_shards(shard_number: u32)
                 filter: None,
                 with_payload: Some(WithPayloadInterface::Bool(false)),
                 with_vector: false.into(),
+                order_by: None,
             },
             None,
             &ShardSelectorInternal::All,

commit 112b5d66f32d24dd9a064e286567fa7aa810eac5
Author: ding-young <lsyhime@snu.ac.kr>
Date:   Mon Feb 12 00:43:43 2024 +0900

    Support Min should clause (#3331) (#3466)
    
    * Add min_should field in Filter struct
    
    * min_should clause checks whether at least given number (min_count) of conditions are met
    * modify test cases due to change in Filter struct (set min_should: None)
    * add simple condition check unit test
    * docs, cardinality estimation, grpc not implemented yet
    
    * Add min_should field in Filter struct
    
    * min_should clause checks whether at least given number (min_count) of conditions are met
    * modify test cases due to change in Filter struct (set min_should: None)
    * add simple condition check unit test
    
    * Impl min_should clause in REST API
    
    * perform cardinality estimation by estimating cardinalities of intersection and combining as union
    * add openapi spec with docs update
    * add integration test
    
    * Impl min_should clause in gRPC
    
    * Cargo fmt & clippy
    
    * Fix minor comments
    
    * add equivalence test between min_should and must
    
    * shortcut at min_count matches
    
    * use `Filter::new_*` whenever possible
    
    * Add missing min_should field
    
    * Fix gRPC field ordering & remove deny_unknown_fields
    
    * Empty commit
    
    ---------
    
    Co-authored-by: Luis Cossío <luis.cossio@outlook.com>

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index 8c76b6ba0..45848da9e 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -745,11 +745,8 @@ async fn test_collection_delete_points_by_filter_with_shards(shard_number: u32)
 
     // delete points with id (0, 3)
     let to_be_deleted: HashSet<PointIdType> = vec![0.into(), 3.into()].into_iter().collect();
-    let delete_filter = segment::types::Filter {
-        should: None,
-        must: Some(vec![Condition::HasId(HasIdCondition::from(to_be_deleted))]),
-        must_not: None,
-    };
+    let delete_filter =
+        segment::types::Filter::new_must(Condition::HasId(HasIdCondition::from(to_be_deleted)));
 
     let delete_points = CollectionUpdateOperations::PointOperation(
         PointOperations::DeletePointsByFilter(delete_filter),

commit 87b541bb41560adf4609190cc0a7c1ed1da6e2f3
Author: shylock <tcath2s@gmail.com>
Date:   Thu Feb 15 22:15:05 2024 +0800

    Feat/set payload by key (#3548)
    
    * Support set by key in low level.
    
    * Rename key field.
    
    * Format.
    
    * Pass key.
    
    * Format.
    
    * Test.
    
    * Clippy.
    
    * Fix ci lint.
    
    * Check grpc consistency.
    
    * Update openapi.
    
    * Fix empty key test case.
    
    * Support array index.
    
    * Format.
    
    * Add test for non exists key.
    
    * Clippy fix.
    
    * Add idempotence test.
    
    * Update index by updated payload.
    
    * Add ut for utils.
    
    * Add ut for 1 level key.
    
    * Fix ut.
    
    * Support no exits key.
    
    * Fix test result.
    
    * Fix after rebase
    
    * handle wildcart insertion into non-existing array
    
    * avoid double read of payload during update
    
    * fix missing removing data from index in case if set_payload removes indexed field
    
    ---------
    
    Co-authored-by: Shylock Hg <shylock@DESKTOP-40I855A>
    Co-authored-by: Albert Safin <xzfcpw@gmail.com>
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index 45848da9e..f6045a4de 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -219,6 +219,7 @@ async fn test_collection_loading_with_shards(shard_number: u32) {
                 payload,
                 points: Some(vec![2.into(), 3.into()]),
                 filter: None,
+                key: None,
             }));
 
         collection

commit 3beb4e3b4ff4b3f9585337f4e5b0826a14e247b6
Author: xzfc <5121426+xzfc@users.noreply.github.com>
Date:   Fri Feb 23 14:38:40 2024 +0000

    Introduce JsonPathString (#3674)
    
    * Introduce JsonPathString
    
    * Fix fomatting

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index f6045a4de..36040c7f6 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -164,7 +164,10 @@ async fn test_collection_search_with_payload_and_vector_with_shards(shard_number
 
     let count_request = CountRequestInternal {
         filter: Some(Filter::new_must(Condition::Field(
-            FieldCondition::new_match("k", serde_json::from_str(r#"{ "value": "v2" }"#).unwrap()),
+            FieldCondition::new_match(
+                "k".parse().unwrap(),
+                serde_json::from_str(r#"{ "value": "v2" }"#).unwrap(),
+            ),
         ))),
         exact: true,
     };
@@ -497,7 +500,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
 
     collection
         .create_payload_index_with_wait(
-            PRICE_FLOAT_KEY.to_string(),
+            PRICE_FLOAT_KEY.parse().unwrap(),
             PayloadFieldSchema::FieldType(PayloadSchemaType::Float),
             true,
         )
@@ -506,7 +509,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
 
     collection
         .create_payload_index_with_wait(
-            PRICE_INT_KEY.to_string(),
+            PRICE_INT_KEY.parse().unwrap(),
             PayloadFieldSchema::FieldType(PayloadSchemaType::Integer),
             true,
         )
@@ -515,7 +518,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
 
     collection
         .create_payload_index_with_wait(
-            MULTI_VALUE_KEY.to_string(),
+            MULTI_VALUE_KEY.parse().unwrap(),
             PayloadFieldSchema::FieldType(PayloadSchemaType::Float),
             true,
         )
@@ -533,7 +536,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
                     with_payload: Some(WithPayloadInterface::Bool(true)),
                     with_vector: false.into(),
                     order_by: Some(OrderByInterface::Struct(OrderBy {
-                        key: key.into(),
+                        key: key.parse().unwrap(),
                         direction: Some(Direction::Asc),
                         start_from: None,
                     })),
@@ -563,7 +566,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
                     with_payload: Some(WithPayloadInterface::Bool(true)),
                     with_vector: false.into(),
                     order_by: Some(OrderByInterface::Struct(OrderBy {
-                        key: key.into(),
+                        key: key.parse().unwrap(),
                         direction: Some(Direction::Desc),
                         start_from: None,
                     })),
@@ -602,7 +605,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
                     with_payload: Some(WithPayloadInterface::Bool(true)),
                     with_vector: false.into(),
                     order_by: Some(OrderByInterface::Struct(OrderBy {
-                        key: key.into(),
+                        key: key.parse().unwrap(),
                         direction: Some(Direction::Asc),
                         start_from: None,
                     })),
@@ -640,7 +643,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
                     with_payload: Some(WithPayloadInterface::Bool(true)),
                     with_vector: false.into(),
                     order_by: Some(OrderByInterface::Struct(OrderBy {
-                        key: key.into(),
+                        key: key.parse().unwrap(),
                         direction: Some(Direction::Desc),
                         start_from: None,
                     })),
@@ -680,7 +683,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
                 filter: None,
                 with_payload: Some(WithPayloadInterface::Bool(true)),
                 with_vector: false.into(),
-                order_by: Some(OrderByInterface::Key(MULTI_VALUE_KEY.into())),
+                order_by: Some(OrderByInterface::Key(MULTI_VALUE_KEY.parse().unwrap())),
             },
             None,
             &ShardSelectorInternal::All,

commit db5399f9e47cfe9d740645ec2f27e8751444882b
Author: Ivan Pleshkov <pleshkov.ivan@gmail.com>
Date:   Mon Mar 18 13:31:55 2024 +0100

    Use rest vector type as non segment part (#3829)
    
    * use rest vector type as non-segment part
    
    * add todo
    
    * switch into -> from
    
    * review remarks
    
    * review remarks

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index 36040c7f6..872ce906f 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -13,7 +13,7 @@ use collection::recommendations::recommend_by;
 use collection::shards::replica_set::{ReplicaSetState, ReplicaState};
 use itertools::Itertools;
 use segment::data_types::order_by::{Direction, OrderBy};
-use segment::data_types::vectors::VectorStruct;
+use segment::data_types::vectors::{BatchVectorStruct, VectorStruct};
 use segment::types::{
     Condition, ExtendedPointId, FieldCondition, Filter, HasIdCondition, Payload,
     PayloadFieldSchema, PayloadSchemaType, PointIdType, WithPayloadInterface,
@@ -40,13 +40,13 @@ async fn test_collection_updater_with_shards(shard_number: u32) {
                 .into_iter()
                 .map(|x| x.into())
                 .collect_vec(),
-            vectors: vec![
+            vectors: BatchVectorStruct::from(vec![
                 vec![1.0, 0.0, 1.0, 1.0],
                 vec![1.0, 0.0, 1.0, 0.0],
                 vec![1.0, 1.0, 1.0, 1.0],
                 vec![1.0, 1.0, 0.0, 1.0],
                 vec![1.0, 0.0, 0.0, 0.0],
-            ]
+            ])
             .into(),
             payloads: None,
         }
@@ -108,7 +108,11 @@ async fn test_collection_search_with_payload_and_vector_with_shards(shard_number
     let insert_points = CollectionUpdateOperations::PointOperation(
         Batch {
             ids: vec![0.into(), 1.into()],
-            vectors: vec![vec![1.0, 0.0, 1.0, 1.0], vec![1.0, 0.0, 1.0, 0.0]].into(),
+            vectors: BatchVectorStruct::from(vec![
+                vec![1.0, 0.0, 1.0, 1.0],
+                vec![1.0, 0.0, 1.0, 0.0],
+            ])
+            .into(),
             payloads: serde_json::from_str(
                 r#"[{ "k": { "type": "keyword", "value": "v1" } }, { "k": "v2" , "v": "v3"}]"#,
             )
@@ -197,13 +201,13 @@ async fn test_collection_loading_with_shards(shard_number: u32) {
                     .into_iter()
                     .map(|x| x.into())
                     .collect_vec(),
-                vectors: vec![
+                vectors: BatchVectorStruct::from(vec![
                     vec![1.0, 0.0, 1.0, 1.0],
                     vec![1.0, 0.0, 1.0, 0.0],
                     vec![1.0, 1.0, 1.0, 1.0],
                     vec![1.0, 1.0, 0.0, 1.0],
                     vec![1.0, 0.0, 0.0, 0.0],
-                ]
+                ])
                 .into(),
                 payloads: None,
             }
@@ -265,7 +269,11 @@ fn test_deserialization() {
     let insert_points = CollectionUpdateOperations::PointOperation(
         Batch {
             ids: vec![0.into(), 1.into()],
-            vectors: vec![vec![1.0, 0.0, 1.0, 1.0], vec![1.0, 0.0, 1.0, 0.0]].into(),
+            vectors: BatchVectorStruct::from(vec![
+                vec![1.0, 0.0, 1.0, 1.0],
+                vec![1.0, 0.0, 1.0, 0.0],
+            ])
+            .into(),
             payloads: None,
         }
         .into(),
@@ -285,12 +293,12 @@ fn test_deserialization2() {
         vec![
             PointStruct {
                 id: 0.into(),
-                vector: vec![1.0, 0.0, 1.0, 1.0].into(),
+                vector: VectorStruct::from(vec![1.0, 0.0, 1.0, 1.0]).into(),
                 payload: None,
             },
             PointStruct {
                 id: 1.into(),
-                vector: vec![1.0, 0.0, 1.0, 0.0].into(),
+                vector: VectorStruct::from(vec![1.0, 0.0, 1.0, 0.0]).into(),
                 payload: None,
             },
         ]
@@ -323,7 +331,7 @@ async fn test_recommendation_api_with_shards(shard_number: u32) {
                 .into_iter()
                 .map(|x| x.into())
                 .collect_vec(),
-            vectors: vec![
+            vectors: BatchVectorStruct::from(vec![
                 vec![0.0, 0.0, 1.0, 1.0],
                 vec![1.0, 0.0, 0.0, 0.0],
                 vec![1.0, 0.0, 0.0, 0.0],
@@ -333,7 +341,7 @@ async fn test_recommendation_api_with_shards(shard_number: u32) {
                 vec![0.0, 0.0, 1.0, 0.0],
                 vec![0.0, 0.0, 0.0, 1.0],
                 vec![0.0, 0.0, 0.0, 1.0],
-            ]
+            ])
             .into(),
             payloads: None,
         }
@@ -381,7 +389,7 @@ async fn test_read_api_with_shards(shard_number: u32) {
                 .into_iter()
                 .map(|x| x.into())
                 .collect_vec(),
-            vectors: vec![
+            vectors: BatchVectorStruct::from(vec![
                 vec![0.0, 0.0, 1.0, 1.0],
                 vec![1.0, 0.0, 0.0, 0.0],
                 vec![1.0, 0.0, 0.0, 0.0],
@@ -391,7 +399,7 @@ async fn test_read_api_with_shards(shard_number: u32) {
                 vec![0.0, 0.0, 1.0, 0.0],
                 vec![0.0, 0.0, 0.0, 1.0],
                 vec![0.0, 0.0, 0.0, 1.0],
-            ]
+            ])
             .into(),
             payloads: None,
         }
@@ -471,7 +479,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
                 .into_iter()
                 .map(|x| x.into())
                 .collect_vec(),
-            vectors: vec![
+            vectors: BatchVectorStruct::from(vec![
                 vec![0.0, 0.0, 1.0, 1.0],
                 vec![1.0, 0.0, 0.0, 0.0],
                 vec![1.0, 0.0, 0.0, 0.0],
@@ -486,7 +494,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
                 vec![0.0, 1.0, 1.0, 1.0],
                 vec![0.0, 1.0, 1.0, 1.0],
                 vec![1.0, 1.0, 1.0, 1.0],
-            ]
+            ])
             .into(),
             payloads: Some(payloads),
         }
@@ -723,13 +731,13 @@ async fn test_collection_delete_points_by_filter_with_shards(shard_number: u32)
                 .into_iter()
                 .map(|x| x.into())
                 .collect_vec(),
-            vectors: vec![
+            vectors: BatchVectorStruct::from(vec![
                 vec![1.0, 0.0, 1.0, 1.0],
                 vec![1.0, 0.0, 1.0, 0.0],
                 vec![1.0, 1.0, 1.0, 1.0],
                 vec![1.0, 1.0, 0.0, 1.0],
                 vec![1.0, 0.0, 0.0, 0.0],
-            ]
+            ])
             .into(),
             payloads: None,
         }

commit 0cbc6a4578a13494e0af25e724a7a737b2916a53
Author: Luis Cossío <luis.cossio@qdrant.com>
Date:   Fri May 24 08:23:12 2024 -0400

    universal-query: user-facing gRPC and REST request types (#4303)
    
    * add grpc `QueryPoints`
    
    * add rest types
    
    * move to REST types to `api::rest::schema`, decouple `OrderByInterface` and `RecommendStrategy`
    
    * openapi changes
    
    * add score_threshold param
    
    * decouple `ShardKeySelector`, add shard key param
    
    * remove commented-out code
    
    * new codespell

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index 872ce906f..ef3a1b198 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -1,12 +1,13 @@
 use std::collections::{HashMap, HashSet};
 use std::fs::File;
 
+use api::rest::OrderByInterface;
 use collection::operations::payload_ops::{PayloadOps, SetPayloadOp};
 use collection::operations::point_ops::{Batch, PointOperations, PointStruct, WriteOrdering};
 use collection::operations::shard_selector_internal::ShardSelectorInternal;
 use collection::operations::types::{
-    CountRequestInternal, OrderByInterface, PointRequestInternal, RecommendRequestInternal,
-    ScrollRequestInternal, SearchRequestInternal, UpdateStatus,
+    CountRequestInternal, PointRequestInternal, RecommendRequestInternal, ScrollRequestInternal,
+    SearchRequestInternal, UpdateStatus,
 };
 use collection::operations::CollectionUpdateOperations;
 use collection::recommendations::recommend_by;

commit 020da0635696b60a53b0261a0ee111bfc71197c6
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Tue Jun 18 07:54:59 2024 +0200

    universal-query: add lookups to query API definition (#4479)
    
    * universal-query: add with_lookup to query API definition
    
    * fix consistency doc check
    
    * lookup_from and not lookup_with
    
    * clean import
    
    * improve doc for lookup location wrt using
    
    * Suggestions for descriptions
    
    ---------
    
    Co-authored-by: Luis Cossío <luis.cossio@outlook.com>

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index ef3a1b198..5abb52426 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -1,13 +1,13 @@
 use std::collections::{HashMap, HashSet};
 use std::fs::File;
 
-use api::rest::OrderByInterface;
+use api::rest::{OrderByInterface, SearchRequestInternal};
 use collection::operations::payload_ops::{PayloadOps, SetPayloadOp};
 use collection::operations::point_ops::{Batch, PointOperations, PointStruct, WriteOrdering};
 use collection::operations::shard_selector_internal::ShardSelectorInternal;
 use collection::operations::types::{
     CountRequestInternal, PointRequestInternal, RecommendRequestInternal, ScrollRequestInternal,
-    SearchRequestInternal, UpdateStatus,
+    UpdateStatus,
 };
 use collection::operations::CollectionUpdateOperations;
 use collection::recommendations::recommend_by;

commit 49a9d05e7c180c2a4828686a54b9a7a8fbc946f3
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Tue Jun 18 20:38:24 2024 +0200

    Fix multivector for unnamed vectors (#4482)
    
    * minor conversion improvement
    
    * use NamedVectors in update_vectors
    
    * remove merge from VectorStruct
    
    * rename Multi -> Named in vector struct
    
    * add multi-dense vectors option into VectorStruct
    
    * generate openapi
    
    * rename VectorStruct -> VectorStructInternal
    
    * add conversion for anonymous multivec in grpc
    
    * renames for BatchVectorStruct
    
    * implement multi-dense for batch
    
    * allow multi-dense in batch upserts
    
    * test and fixes

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index 5abb52426..edf4d2d64 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -14,7 +14,7 @@ use collection::recommendations::recommend_by;
 use collection::shards::replica_set::{ReplicaSetState, ReplicaState};
 use itertools::Itertools;
 use segment::data_types::order_by::{Direction, OrderBy};
-use segment::data_types::vectors::{BatchVectorStruct, VectorStruct};
+use segment::data_types::vectors::{BatchVectorStructInternal, VectorStructInternal};
 use segment::types::{
     Condition, ExtendedPointId, FieldCondition, Filter, HasIdCondition, Payload,
     PayloadFieldSchema, PayloadSchemaType, PointIdType, WithPayloadInterface,
@@ -41,7 +41,7 @@ async fn test_collection_updater_with_shards(shard_number: u32) {
                 .into_iter()
                 .map(|x| x.into())
                 .collect_vec(),
-            vectors: BatchVectorStruct::from(vec![
+            vectors: BatchVectorStructInternal::from(vec![
                 vec![1.0, 0.0, 1.0, 1.0],
                 vec![1.0, 0.0, 1.0, 0.0],
                 vec![1.0, 1.0, 1.0, 1.0],
@@ -109,7 +109,7 @@ async fn test_collection_search_with_payload_and_vector_with_shards(shard_number
     let insert_points = CollectionUpdateOperations::PointOperation(
         Batch {
             ids: vec![0.into(), 1.into()],
-            vectors: BatchVectorStruct::from(vec![
+            vectors: BatchVectorStructInternal::from(vec![
                 vec![1.0, 0.0, 1.0, 1.0],
                 vec![1.0, 0.0, 1.0, 0.0],
             ])
@@ -160,7 +160,7 @@ async fn test_collection_search_with_payload_and_vector_with_shards(shard_number
             assert_eq!(res[0].payload.as_ref().unwrap().len(), 1);
             let vec = vec![1.0, 0.0, 1.0, 1.0];
             match &res[0].vector {
-                Some(VectorStruct::Single(v)) => assert_eq!(v.clone(), vec),
+                Some(VectorStructInternal::Single(v)) => assert_eq!(v.clone(), vec),
                 _ => panic!("vector is not returned"),
             }
         }
@@ -202,7 +202,7 @@ async fn test_collection_loading_with_shards(shard_number: u32) {
                     .into_iter()
                     .map(|x| x.into())
                     .collect_vec(),
-                vectors: BatchVectorStruct::from(vec![
+                vectors: BatchVectorStructInternal::from(vec![
                     vec![1.0, 0.0, 1.0, 1.0],
                     vec![1.0, 0.0, 1.0, 0.0],
                     vec![1.0, 1.0, 1.0, 1.0],
@@ -270,7 +270,7 @@ fn test_deserialization() {
     let insert_points = CollectionUpdateOperations::PointOperation(
         Batch {
             ids: vec![0.into(), 1.into()],
-            vectors: BatchVectorStruct::from(vec![
+            vectors: BatchVectorStructInternal::from(vec![
                 vec![1.0, 0.0, 1.0, 1.0],
                 vec![1.0, 0.0, 1.0, 0.0],
             ])
@@ -294,12 +294,12 @@ fn test_deserialization2() {
         vec![
             PointStruct {
                 id: 0.into(),
-                vector: VectorStruct::from(vec![1.0, 0.0, 1.0, 1.0]).into(),
+                vector: VectorStructInternal::from(vec![1.0, 0.0, 1.0, 1.0]).into(),
                 payload: None,
             },
             PointStruct {
                 id: 1.into(),
-                vector: VectorStruct::from(vec![1.0, 0.0, 1.0, 0.0]).into(),
+                vector: VectorStructInternal::from(vec![1.0, 0.0, 1.0, 0.0]).into(),
                 payload: None,
             },
         ]
@@ -332,7 +332,7 @@ async fn test_recommendation_api_with_shards(shard_number: u32) {
                 .into_iter()
                 .map(|x| x.into())
                 .collect_vec(),
-            vectors: BatchVectorStruct::from(vec![
+            vectors: BatchVectorStructInternal::from(vec![
                 vec![0.0, 0.0, 1.0, 1.0],
                 vec![1.0, 0.0, 0.0, 0.0],
                 vec![1.0, 0.0, 0.0, 0.0],
@@ -390,7 +390,7 @@ async fn test_read_api_with_shards(shard_number: u32) {
                 .into_iter()
                 .map(|x| x.into())
                 .collect_vec(),
-            vectors: BatchVectorStruct::from(vec![
+            vectors: BatchVectorStructInternal::from(vec![
                 vec![0.0, 0.0, 1.0, 1.0],
                 vec![1.0, 0.0, 0.0, 0.0],
                 vec![1.0, 0.0, 0.0, 0.0],
@@ -480,7 +480,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
                 .into_iter()
                 .map(|x| x.into())
                 .collect_vec(),
-            vectors: BatchVectorStruct::from(vec![
+            vectors: BatchVectorStructInternal::from(vec![
                 vec![0.0, 0.0, 1.0, 1.0],
                 vec![1.0, 0.0, 0.0, 0.0],
                 vec![1.0, 0.0, 0.0, 0.0],
@@ -732,7 +732,7 @@ async fn test_collection_delete_points_by_filter_with_shards(shard_number: u32)
                 .into_iter()
                 .map(|x| x.into())
                 .collect_vec(),
-            vectors: BatchVectorStruct::from(vec![
+            vectors: BatchVectorStructInternal::from(vec![
                 vec![1.0, 0.0, 1.0, 1.0],
                 vec![1.0, 0.0, 1.0, 0.0],
                 vec![1.0, 1.0, 1.0, 1.0],

commit 07c278ad51084c98adf9a7093619ffc5a73f87c9
Author: xzfc <5121426+xzfc@users.noreply.github.com>
Date:   Mon Jul 22 08:19:19 2024 +0000

    Enable some of the pedantic clippy lints (#4715)
    
    * Use workspace lints
    
    * Enable lint: manual_let_else
    
    * Enable lint: enum_glob_use
    
    * Enable lint: filter_map_next
    
    * Enable lint: ref_as_ptr
    
    * Enable lint: ref_option_ref
    
    * Enable lint: manual_is_variant_and
    
    * Enable lint: flat_map_option
    
    * Enable lint: inefficient_to_string
    
    * Enable lint: implicit_clone
    
    * Enable lint: inconsistent_struct_constructor
    
    * Enable lint: unnecessary_wraps
    
    * Enable lint: needless_continue
    
    * Enable lint: unused_self
    
    * Enable lint: from_iter_instead_of_collect
    
    * Enable lint: uninlined_format_args
    
    * Enable lint: doc_link_with_quotes
    
    * Enable lint: needless_raw_string_hashes
    
    * Enable lint: used_underscore_binding
    
    * Enable lint: ptr_as_ptr
    
    * Enable lint: explicit_into_iter_loop
    
    * Enable lint: cast_lossless

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index edf4d2d64..f89286066 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -535,7 +535,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
         .unwrap();
 
     ///////// Test single-valued fields ///////////
-    for key in [PRICE_FLOAT_KEY, PRICE_INT_KEY].into_iter() {
+    for key in [PRICE_FLOAT_KEY, PRICE_INT_KEY] {
         let result_asc = collection
             .scroll_by(
                 ScrollRequestInternal {
@@ -677,9 +677,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
         assert_eq!(desc_second_page.points.len(), 4);
         assert!(
             desc_second_page_points.is_subset(&valid_desc_second_page_points),
-            "expected: {:?}, got: {:?}",
-            valid_desc_second_page_points,
-            desc_second_page_points
+            "expected: {valid_desc_second_page_points:?}, got: {desc_second_page_points:?}"
         );
     }
 

commit 93f04a3f49d625d7b55a6b393e3ba202b8cbb0be
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Tue Aug 6 18:59:23 2024 +0200

    Add and fix missing timeout in the retrieve APIs (#4842)
    
    * Add and fix missing timeout in the retrieve APIs
    
    * no need for extra timeout handling as single ops are doing it
    
    * don't surface timeout too high in shard transfer infra
    
    * add todos for future work
    
    * update timeout after first query

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index f89286066..40570005d 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -178,7 +178,7 @@ async fn test_collection_search_with_payload_and_vector_with_shards(shard_number
     };
 
     let count_res = collection
-        .count(count_request, None, &ShardSelectorInternal::All)
+        .count(count_request, None, &ShardSelectorInternal::All, None)
         .await
         .unwrap();
     assert_eq!(count_res.count, 1);
@@ -249,7 +249,7 @@ async fn test_collection_loading_with_shards(shard_number: u32) {
         with_vector: true.into(),
     };
     let retrieved = loaded_collection
-        .retrieve(request, None, &ShardSelectorInternal::All)
+        .retrieve(request, None, &ShardSelectorInternal::All, None)
         .await
         .unwrap();
 
@@ -424,6 +424,7 @@ async fn test_read_api_with_shards(shard_number: u32) {
             },
             None,
             &ShardSelectorInternal::All,
+            None,
         )
         .await
         .unwrap();
@@ -552,6 +553,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
                 },
                 None,
                 &ShardSelectorInternal::All,
+                None,
             )
             .await
             .unwrap();
@@ -582,6 +584,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
                 },
                 None,
                 &ShardSelectorInternal::All,
+                None,
             )
             .await
             .unwrap();
@@ -621,6 +624,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
                 },
                 None,
                 &ShardSelectorInternal::All,
+                None,
             )
             .await
             .unwrap();
@@ -659,6 +663,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
                 },
                 None,
                 &ShardSelectorInternal::All,
+                None,
             )
             .await
             .unwrap();
@@ -694,6 +699,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
             },
             None,
             &ShardSelectorInternal::All,
+            None,
         )
         .await
         .unwrap();
@@ -786,6 +792,7 @@ async fn test_collection_delete_points_by_filter_with_shards(shard_number: u32)
             },
             None,
             &ShardSelectorInternal::All,
+            None,
         )
         .await
         .unwrap();

commit 1d0ee7ea32043598f8b240e6a3a52be20663fa44
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Wed Oct 9 10:15:46 2024 +0200

    Inference interface in REST and gRPC (#5165)
    
    * include document & image objects into grpc API
    
    * introduce image and object to rest api
    
    * minor refactoring
    
    * rename Vector -> VectorInternal
    
    * decompose vector data structures
    
    * add schema
    
    * fmt
    
    * grpc docs
    
    * fix conversion
    
    * fix clippy
    
    * fix another conversion
    
    * rename VectorInput -> VectorInputInternal
    
    * replace grpc TryFrom with async functions
    
    * fmt
    
    * replace rest TryFrom with async functions
    
    * add image and object into query rest
    
    * separate inference related conversions
    
    * move json-related conversions into a separate file
    
    * move vector-related transformations into a separate file
    
    * move more vector related-conversions into dedicated module

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index 40570005d..82fe60801 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -3,7 +3,10 @@ use std::fs::File;
 
 use api::rest::{OrderByInterface, SearchRequestInternal};
 use collection::operations::payload_ops::{PayloadOps, SetPayloadOp};
-use collection::operations::point_ops::{Batch, PointOperations, PointStruct, WriteOrdering};
+use collection::operations::point_ops::{
+    BatchPersisted, BatchVectorStructPersisted, PointInsertOperationsInternal, PointOperations,
+    PointStructPersisted, VectorStructPersisted, WriteOrdering,
+};
 use collection::operations::shard_selector_internal::ShardSelectorInternal;
 use collection::operations::types::{
     CountRequestInternal, PointRequestInternal, RecommendRequestInternal, ScrollRequestInternal,
@@ -14,7 +17,7 @@ use collection::recommendations::recommend_by;
 use collection::shards::replica_set::{ReplicaSetState, ReplicaState};
 use itertools::Itertools;
 use segment::data_types::order_by::{Direction, OrderBy};
-use segment::data_types::vectors::{BatchVectorStructInternal, VectorStructInternal};
+use segment::data_types::vectors::VectorStructInternal;
 use segment::types::{
     Condition, ExtendedPointId, FieldCondition, Filter, HasIdCondition, Payload,
     PayloadFieldSchema, PayloadSchemaType, PointIdType, WithPayloadInterface,
@@ -35,24 +38,24 @@ async fn test_collection_updater_with_shards(shard_number: u32) {
 
     let collection = simple_collection_fixture(collection_dir.path(), shard_number).await;
 
-    let insert_points = CollectionUpdateOperations::PointOperation(
-        Batch {
-            ids: vec![0, 1, 2, 3, 4]
-                .into_iter()
-                .map(|x| x.into())
-                .collect_vec(),
-            vectors: BatchVectorStructInternal::from(vec![
-                vec![1.0, 0.0, 1.0, 1.0],
-                vec![1.0, 0.0, 1.0, 0.0],
-                vec![1.0, 1.0, 1.0, 1.0],
-                vec![1.0, 1.0, 0.0, 1.0],
-                vec![1.0, 0.0, 0.0, 0.0],
-            ])
-            .into(),
-            payloads: None,
-        }
-        .into(),
-    );
+    let batch = BatchPersisted {
+        ids: vec![0, 1, 2, 3, 4]
+            .into_iter()
+            .map(|x| x.into())
+            .collect_vec(),
+        vectors: BatchVectorStructPersisted::Single(vec![
+            vec![1.0, 0.0, 1.0, 1.0],
+            vec![1.0, 0.0, 1.0, 0.0],
+            vec![1.0, 1.0, 1.0, 1.0],
+            vec![1.0, 1.0, 0.0, 1.0],
+            vec![1.0, 0.0, 0.0, 0.0],
+        ]),
+        payloads: None,
+    };
+
+    let insert_points = CollectionUpdateOperations::PointOperation(PointOperations::UpsertPoints(
+        PointInsertOperationsInternal::from(batch),
+    ));
 
     let insert_result = collection
         .update_from_client_simple(insert_points, true, WriteOrdering::default())
@@ -106,21 +109,21 @@ async fn test_collection_search_with_payload_and_vector_with_shards(shard_number
 
     let collection = simple_collection_fixture(collection_dir.path(), shard_number).await;
 
-    let insert_points = CollectionUpdateOperations::PointOperation(
-        Batch {
-            ids: vec![0.into(), 1.into()],
-            vectors: BatchVectorStructInternal::from(vec![
-                vec![1.0, 0.0, 1.0, 1.0],
-                vec![1.0, 0.0, 1.0, 0.0],
-            ])
-            .into(),
-            payloads: serde_json::from_str(
-                r#"[{ "k": { "type": "keyword", "value": "v1" } }, { "k": "v2" , "v": "v3"}]"#,
-            )
-            .unwrap(),
-        }
-        .into(),
-    );
+    let batch = BatchPersisted {
+        ids: vec![0.into(), 1.into()],
+        vectors: BatchVectorStructPersisted::Single(vec![
+            vec![1.0, 0.0, 1.0, 1.0],
+            vec![1.0, 0.0, 1.0, 0.0],
+        ]),
+        payloads: serde_json::from_str(
+            r#"[{ "k": { "type": "keyword", "value": "v1" } }, { "k": "v2" , "v": "v3"}]"#,
+        )
+        .unwrap(),
+    };
+
+    let insert_points = CollectionUpdateOperations::PointOperation(PointOperations::UpsertPoints(
+        PointInsertOperationsInternal::from(batch),
+    ));
 
     let insert_result = collection
         .update_from_client_simple(insert_points, true, WriteOrdering::default())
@@ -196,23 +199,24 @@ async fn test_collection_loading_with_shards(shard_number: u32) {
 
     {
         let collection = simple_collection_fixture(collection_dir.path(), shard_number).await;
+
+        let batch = BatchPersisted {
+            ids: vec![0, 1, 2, 3, 4]
+                .into_iter()
+                .map(|x| x.into())
+                .collect_vec(),
+            vectors: BatchVectorStructPersisted::Single(vec![
+                vec![1.0, 0.0, 1.0, 1.0],
+                vec![1.0, 0.0, 1.0, 0.0],
+                vec![1.0, 1.0, 1.0, 1.0],
+                vec![1.0, 1.0, 0.0, 1.0],
+                vec![1.0, 0.0, 0.0, 0.0],
+            ]),
+            payloads: None,
+        };
+
         let insert_points = CollectionUpdateOperations::PointOperation(
-            Batch {
-                ids: vec![0, 1, 2, 3, 4]
-                    .into_iter()
-                    .map(|x| x.into())
-                    .collect_vec(),
-                vectors: BatchVectorStructInternal::from(vec![
-                    vec![1.0, 0.0, 1.0, 1.0],
-                    vec![1.0, 0.0, 1.0, 0.0],
-                    vec![1.0, 1.0, 1.0, 1.0],
-                    vec![1.0, 1.0, 0.0, 1.0],
-                    vec![1.0, 0.0, 0.0, 0.0],
-                ])
-                .into(),
-                payloads: None,
-            }
-            .into(),
+            PointOperations::UpsertPoints(PointInsertOperationsInternal::from(batch)),
         );
 
         collection
@@ -267,18 +271,18 @@ async fn test_collection_loading_with_shards(shard_number: u32) {
 
 #[test]
 fn test_deserialization() {
-    let insert_points = CollectionUpdateOperations::PointOperation(
-        Batch {
-            ids: vec![0.into(), 1.into()],
-            vectors: BatchVectorStructInternal::from(vec![
-                vec![1.0, 0.0, 1.0, 1.0],
-                vec![1.0, 0.0, 1.0, 0.0],
-            ])
-            .into(),
-            payloads: None,
-        }
-        .into(),
-    );
+    let batch = BatchPersisted {
+        ids: vec![0.into(), 1.into()],
+        vectors: BatchVectorStructPersisted::Single(vec![
+            vec![1.0, 0.0, 1.0, 1.0],
+            vec![1.0, 0.0, 1.0, 0.0],
+        ]),
+        payloads: None,
+    };
+
+    let insert_points = CollectionUpdateOperations::PointOperation(PointOperations::UpsertPoints(
+        PointInsertOperationsInternal::from(batch),
+    ));
     let json_str = serde_json::to_string_pretty(&insert_points).unwrap();
 
     let _read_obj: CollectionUpdateOperations = serde_json::from_str(&json_str).unwrap();
@@ -290,21 +294,22 @@ fn test_deserialization() {
 
 #[test]
 fn test_deserialization2() {
-    let insert_points = CollectionUpdateOperations::PointOperation(
-        vec![
-            PointStruct {
-                id: 0.into(),
-                vector: VectorStructInternal::from(vec![1.0, 0.0, 1.0, 1.0]).into(),
-                payload: None,
-            },
-            PointStruct {
-                id: 1.into(),
-                vector: VectorStructInternal::from(vec![1.0, 0.0, 1.0, 0.0]).into(),
-                payload: None,
-            },
-        ]
-        .into(),
-    );
+    let points = vec![
+        PointStructPersisted {
+            id: 0.into(),
+            vector: VectorStructPersisted::from(vec![1.0, 0.0, 1.0, 1.0]),
+            payload: None,
+        },
+        PointStructPersisted {
+            id: 1.into(),
+            vector: VectorStructPersisted::from(vec![1.0, 0.0, 1.0, 0.0]),
+            payload: None,
+        },
+    ];
+
+    let insert_points = CollectionUpdateOperations::PointOperation(PointOperations::UpsertPoints(
+        PointInsertOperationsInternal::from(points),
+    ));
 
     let json_str = serde_json::to_string_pretty(&insert_points).unwrap();
 
@@ -326,28 +331,28 @@ async fn test_recommendation_api_with_shards(shard_number: u32) {
     let collection_dir = Builder::new().prefix("collection").tempdir().unwrap();
     let collection = simple_collection_fixture(collection_dir.path(), shard_number).await;
 
-    let insert_points = CollectionUpdateOperations::PointOperation(
-        Batch {
-            ids: vec![0, 1, 2, 3, 4, 5, 6, 7, 8]
-                .into_iter()
-                .map(|x| x.into())
-                .collect_vec(),
-            vectors: BatchVectorStructInternal::from(vec![
-                vec![0.0, 0.0, 1.0, 1.0],
-                vec![1.0, 0.0, 0.0, 0.0],
-                vec![1.0, 0.0, 0.0, 0.0],
-                vec![0.0, 1.0, 0.0, 0.0],
-                vec![0.0, 1.0, 0.0, 0.0],
-                vec![0.0, 0.0, 1.0, 0.0],
-                vec![0.0, 0.0, 1.0, 0.0],
-                vec![0.0, 0.0, 0.0, 1.0],
-                vec![0.0, 0.0, 0.0, 1.0],
-            ])
-            .into(),
-            payloads: None,
-        }
-        .into(),
-    );
+    let batch = BatchPersisted {
+        ids: vec![0, 1, 2, 3, 4, 5, 6, 7, 8]
+            .into_iter()
+            .map(|x| x.into())
+            .collect_vec(),
+        vectors: BatchVectorStructPersisted::Single(vec![
+            vec![0.0, 0.0, 1.0, 1.0],
+            vec![1.0, 0.0, 0.0, 0.0],
+            vec![1.0, 0.0, 0.0, 0.0],
+            vec![0.0, 1.0, 0.0, 0.0],
+            vec![0.0, 1.0, 0.0, 0.0],
+            vec![0.0, 0.0, 1.0, 0.0],
+            vec![0.0, 0.0, 1.0, 0.0],
+            vec![0.0, 0.0, 0.0, 1.0],
+            vec![0.0, 0.0, 0.0, 1.0],
+        ]),
+        payloads: None,
+    };
+
+    let insert_points = CollectionUpdateOperations::PointOperation(PointOperations::UpsertPoints(
+        PointInsertOperationsInternal::from(batch),
+    ));
 
     collection
         .update_from_client_simple(insert_points, true, WriteOrdering::default())
@@ -384,27 +389,27 @@ async fn test_read_api_with_shards(shard_number: u32) {
     let collection_dir = Builder::new().prefix("collection").tempdir().unwrap();
     let collection = simple_collection_fixture(collection_dir.path(), shard_number).await;
 
+    let batch = BatchPersisted {
+        ids: vec![0, 1, 2, 3, 4, 5, 6, 7, 8]
+            .into_iter()
+            .map(|x| x.into())
+            .collect_vec(),
+        vectors: BatchVectorStructPersisted::Single(vec![
+            vec![0.0, 0.0, 1.0, 1.0],
+            vec![1.0, 0.0, 0.0, 0.0],
+            vec![1.0, 0.0, 0.0, 0.0],
+            vec![0.0, 1.0, 0.0, 0.0],
+            vec![0.0, 1.0, 0.0, 0.0],
+            vec![0.0, 0.0, 1.0, 0.0],
+            vec![0.0, 0.0, 1.0, 0.0],
+            vec![0.0, 0.0, 0.0, 1.0],
+            vec![0.0, 0.0, 0.0, 1.0],
+        ]),
+        payloads: None,
+    };
+
     let insert_points = CollectionUpdateOperations::PointOperation(PointOperations::UpsertPoints(
-        Batch {
-            ids: vec![0, 1, 2, 3, 4, 5, 6, 7, 8]
-                .into_iter()
-                .map(|x| x.into())
-                .collect_vec(),
-            vectors: BatchVectorStructInternal::from(vec![
-                vec![0.0, 0.0, 1.0, 1.0],
-                vec![1.0, 0.0, 0.0, 0.0],
-                vec![1.0, 0.0, 0.0, 0.0],
-                vec![0.0, 1.0, 0.0, 0.0],
-                vec![0.0, 1.0, 0.0, 0.0],
-                vec![0.0, 0.0, 1.0, 0.0],
-                vec![0.0, 0.0, 1.0, 0.0],
-                vec![0.0, 0.0, 0.0, 1.0],
-                vec![0.0, 0.0, 0.0, 1.0],
-            ])
-            .into(),
-            payloads: None,
-        }
-        .into(),
+        PointInsertOperationsInternal::from(batch),
     ));
 
     collection
@@ -475,32 +480,32 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
         get_payload(1.0),
     ];
 
+    let batch = BatchPersisted {
+        ids: vec![0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
+            .into_iter()
+            .map(|x| x.into())
+            .collect_vec(),
+        vectors: BatchVectorStructPersisted::Single(vec![
+            vec![0.0, 0.0, 1.0, 1.0],
+            vec![1.0, 0.0, 0.0, 0.0],
+            vec![1.0, 0.0, 0.0, 0.0],
+            vec![0.0, 1.0, 0.0, 0.0],
+            vec![0.0, 1.0, 0.0, 0.0],
+            vec![0.0, 0.0, 1.0, 0.0],
+            vec![0.0, 0.0, 1.0, 0.0],
+            vec![0.0, 0.0, 0.0, 1.0],
+            vec![0.0, 0.0, 0.0, 1.0],
+            vec![0.0, 1.0, 1.0, 1.0],
+            vec![0.0, 1.0, 1.0, 1.0],
+            vec![0.0, 1.0, 1.0, 1.0],
+            vec![0.0, 1.0, 1.0, 1.0],
+            vec![1.0, 1.0, 1.0, 1.0],
+        ]),
+        payloads: Some(payloads),
+    };
+
     let insert_points = CollectionUpdateOperations::PointOperation(PointOperations::UpsertPoints(
-        Batch {
-            ids: vec![0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
-                .into_iter()
-                .map(|x| x.into())
-                .collect_vec(),
-            vectors: BatchVectorStructInternal::from(vec![
-                vec![0.0, 0.0, 1.0, 1.0],
-                vec![1.0, 0.0, 0.0, 0.0],
-                vec![1.0, 0.0, 0.0, 0.0],
-                vec![0.0, 1.0, 0.0, 0.0],
-                vec![0.0, 1.0, 0.0, 0.0],
-                vec![0.0, 0.0, 1.0, 0.0],
-                vec![0.0, 0.0, 1.0, 0.0],
-                vec![0.0, 0.0, 0.0, 1.0],
-                vec![0.0, 0.0, 0.0, 1.0],
-                vec![0.0, 1.0, 1.0, 1.0],
-                vec![0.0, 1.0, 1.0, 1.0],
-                vec![0.0, 1.0, 1.0, 1.0],
-                vec![0.0, 1.0, 1.0, 1.0],
-                vec![1.0, 1.0, 1.0, 1.0],
-            ])
-            .into(),
-            payloads: Some(payloads),
-        }
-        .into(),
+        PointInsertOperationsInternal::from(batch),
     ));
 
     collection
@@ -730,24 +735,24 @@ async fn test_collection_delete_points_by_filter_with_shards(shard_number: u32)
 
     let collection = simple_collection_fixture(collection_dir.path(), shard_number).await;
 
-    let insert_points = CollectionUpdateOperations::PointOperation(
-        Batch {
-            ids: vec![0, 1, 2, 3, 4]
-                .into_iter()
-                .map(|x| x.into())
-                .collect_vec(),
-            vectors: BatchVectorStructInternal::from(vec![
-                vec![1.0, 0.0, 1.0, 1.0],
-                vec![1.0, 0.0, 1.0, 0.0],
-                vec![1.0, 1.0, 1.0, 1.0],
-                vec![1.0, 1.0, 0.0, 1.0],
-                vec![1.0, 0.0, 0.0, 0.0],
-            ])
-            .into(),
-            payloads: None,
-        }
-        .into(),
-    );
+    let batch = BatchPersisted {
+        ids: vec![0, 1, 2, 3, 4]
+            .into_iter()
+            .map(|x| x.into())
+            .collect_vec(),
+        vectors: BatchVectorStructPersisted::Single(vec![
+            vec![1.0, 0.0, 1.0, 1.0],
+            vec![1.0, 0.0, 1.0, 0.0],
+            vec![1.0, 1.0, 1.0, 1.0],
+            vec![1.0, 1.0, 0.0, 1.0],
+            vec![1.0, 0.0, 0.0, 0.0],
+        ]),
+        payloads: None,
+    };
+
+    let insert_points = CollectionUpdateOperations::PointOperation(PointOperations::UpsertPoints(
+        PointInsertOperationsInternal::from(batch),
+    ));
 
     let insert_result = collection
         .update_from_client_simple(insert_points, true, WriteOrdering::default())

commit c1d0a8d61e2d825770fa65a05cbf085e20a4e7a9
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Tue Oct 29 22:15:37 2024 +0100

    Populate hardware counter to REST API (#5308)
    
    * populate hardware counter
    
    * make consume semantic explicit
    
    * Merge pull request #5328
    
    * add hardware info to more endpoints
    
    ---------
    
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index 82fe60801..fbc464f01 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -15,6 +15,7 @@ use collection::operations::types::{
 use collection::operations::CollectionUpdateOperations;
 use collection::recommendations::recommend_by;
 use collection::shards::replica_set::{ReplicaSetState, ReplicaState};
+use common::counter::hardware_accumulator::HwMeasurementAcc;
 use itertools::Itertools;
 use segment::data_types::order_by::{Direction, OrderBy};
 use segment::data_types::vectors::VectorStructInternal;
@@ -85,6 +86,7 @@ async fn test_collection_updater_with_shards(shard_number: u32) {
             None,
             &ShardSelectorInternal::All,
             None,
+            HwMeasurementAcc::new(),
         )
         .await;
 
@@ -153,6 +155,7 @@ async fn test_collection_search_with_payload_and_vector_with_shards(shard_number
             None,
             &ShardSelectorInternal::All,
             None,
+            HwMeasurementAcc::new(),
         )
         .await;
 
@@ -181,7 +184,13 @@ async fn test_collection_search_with_payload_and_vector_with_shards(shard_number
     };
 
     let count_res = collection
-        .count(count_request, None, &ShardSelectorInternal::All, None)
+        .count(
+            count_request,
+            None,
+            &ShardSelectorInternal::All,
+            None,
+            HwMeasurementAcc::new(),
+        )
         .await
         .unwrap();
     assert_eq!(count_res.count, 1);
@@ -370,6 +379,7 @@ async fn test_recommendation_api_with_shards(shard_number: u32) {
         None,
         ShardSelectorInternal::All,
         None,
+        HwMeasurementAcc::new(),
     )
     .await
     .unwrap();

commit b4e30ad6bfff857e212b7d63079783569e572267
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Sun Nov 17 12:46:01 2024 +0100

    Per collection hardware measurements  (#5453)
    
    * Add HwMeasurementCollector
    
    * Add hardware reporting to TOC + RequestHwCounter
    
    * Pass HwMeasurementAcc by reference + Update accumulation
    
    * Update tests and benchmarks
    
    * update REST API
    
    * Update gRPC API
    
    * codespell
    
    * Adjust internal API
    
    * improve docs
    
    * introduce drain to the HwMeasurementAcc
    
    * fmt
    
    * use drain to report to the collection counter
    
    * implement hw metrics drain for internal and external queries
    
    * fix drinage
    
    * refactor rest models: move away from grpc crate
    
    * fmt
    
    * implement usage reporting to collection acc for rest api
    
    ---------
    
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index fbc464f01..6d15369fb 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -80,15 +80,17 @@ async fn test_collection_updater_with_shards(shard_number: u32) {
         score_threshold: None,
     };
 
+    let hw_acc = HwMeasurementAcc::new();
     let search_res = collection
         .search(
             search_request.into(),
             None,
             &ShardSelectorInternal::All,
             None,
-            HwMeasurementAcc::new(),
+            &hw_acc,
         )
         .await;
+    hw_acc.discard();
 
     match search_res {
         Ok(res) => {
@@ -149,15 +151,17 @@ async fn test_collection_search_with_payload_and_vector_with_shards(shard_number
         score_threshold: None,
     };
 
+    let hw_acc = HwMeasurementAcc::new();
     let search_res = collection
         .search(
             search_request.into(),
             None,
             &ShardSelectorInternal::All,
             None,
-            HwMeasurementAcc::new(),
+            &hw_acc,
         )
         .await;
+    hw_acc.discard();
 
     match search_res {
         Ok(res) => {
@@ -183,17 +187,19 @@ async fn test_collection_search_with_payload_and_vector_with_shards(shard_number
         exact: true,
     };
 
+    let hw_acc = HwMeasurementAcc::new();
     let count_res = collection
         .count(
             count_request,
             None,
             &ShardSelectorInternal::All,
             None,
-            HwMeasurementAcc::new(),
+            &hw_acc,
         )
         .await
         .unwrap();
     assert_eq!(count_res.count, 1);
+    hw_acc.discard();
 }
 
 // FIXME: does not work
@@ -363,6 +369,7 @@ async fn test_recommendation_api_with_shards(shard_number: u32) {
         PointInsertOperationsInternal::from(batch),
     ));
 
+    let hw_acc = HwMeasurementAcc::new();
     collection
         .update_from_client_simple(insert_points, true, WriteOrdering::default())
         .await
@@ -379,12 +386,13 @@ async fn test_recommendation_api_with_shards(shard_number: u32) {
         None,
         ShardSelectorInternal::All,
         None,
-        HwMeasurementAcc::new(),
+        &hw_acc,
     )
     .await
     .unwrap();
     assert!(!result.is_empty());
     let top1 = &result[0];
+    hw_acc.discard();
 
     assert!(top1.id == 5.into() || top1.id == 6.into());
 }

commit 5aee24cc089b0ddedacb80c508e33d40fcea1950
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Tue Dec 10 12:12:36 2024 +0100

    Timeout aware hardware counter (#5555)
    
    * Make hardware counting timeout aware
    
    * improve test
    
    * rebuild everything
    
    * fmt
    
    * post-rebase fixes
    
    * upd tests
    
    * fix tests
    
    ---------
    
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index 6d15369fb..70bf15a2c 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -87,10 +87,9 @@ async fn test_collection_updater_with_shards(shard_number: u32) {
             None,
             &ShardSelectorInternal::All,
             None,
-            &hw_acc,
+            hw_acc,
         )
         .await;
-    hw_acc.discard();
 
     match search_res {
         Ok(res) => {
@@ -158,10 +157,9 @@ async fn test_collection_search_with_payload_and_vector_with_shards(shard_number
             None,
             &ShardSelectorInternal::All,
             None,
-            &hw_acc,
+            hw_acc,
         )
         .await;
-    hw_acc.discard();
 
     match search_res {
         Ok(res) => {
@@ -194,12 +192,11 @@ async fn test_collection_search_with_payload_and_vector_with_shards(shard_number
             None,
             &ShardSelectorInternal::All,
             None,
-            &hw_acc,
+            hw_acc,
         )
         .await
         .unwrap();
     assert_eq!(count_res.count, 1);
-    hw_acc.discard();
 }
 
 // FIXME: does not work
@@ -386,13 +383,12 @@ async fn test_recommendation_api_with_shards(shard_number: u32) {
         None,
         ShardSelectorInternal::All,
         None,
-        &hw_acc,
+        hw_acc,
     )
     .await
     .unwrap();
     assert!(!result.is_empty());
     let top1 = &result[0];
-    hw_acc.discard();
 
     assert!(top1.id == 5.into() || top1.id == 6.into());
 }

commit 38f478ddf7a9d03a1c783c5599f3b6ae33a05195
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Thu Jan 16 14:25:55 2025 +0100

    Measure payload read IO (#5773)
    
    * Measure read io for payload storage
    
    * Add Hardware Counter to update functions
    
    * Fix tests and benches
    
    * Rename (some) *_measured functions back to original

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index 70bf15a2c..2dafc4060 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -265,7 +265,13 @@ async fn test_collection_loading_with_shards(shard_number: u32) {
         with_vector: true.into(),
     };
     let retrieved = loaded_collection
-        .retrieve(request, None, &ShardSelectorInternal::All, None)
+        .retrieve(
+            request,
+            None,
+            &ShardSelectorInternal::All,
+            None,
+            HwMeasurementAcc::new(),
+        )
         .await
         .unwrap();
 
@@ -444,6 +450,7 @@ async fn test_read_api_with_shards(shard_number: u32) {
             None,
             &ShardSelectorInternal::All,
             None,
+            HwMeasurementAcc::new(),
         )
         .await
         .unwrap();
@@ -573,6 +580,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
                 None,
                 &ShardSelectorInternal::All,
                 None,
+                HwMeasurementAcc::new(),
             )
             .await
             .unwrap();
@@ -604,6 +612,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
                 None,
                 &ShardSelectorInternal::All,
                 None,
+                HwMeasurementAcc::new(),
             )
             .await
             .unwrap();
@@ -644,6 +653,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
                 None,
                 &ShardSelectorInternal::All,
                 None,
+                HwMeasurementAcc::new(),
             )
             .await
             .unwrap();
@@ -683,6 +693,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
                 None,
                 &ShardSelectorInternal::All,
                 None,
+                HwMeasurementAcc::new(),
             )
             .await
             .unwrap();
@@ -719,6 +730,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
             None,
             &ShardSelectorInternal::All,
             None,
+            HwMeasurementAcc::new(),
         )
         .await
         .unwrap();
@@ -812,6 +824,7 @@ async fn test_collection_delete_points_by_filter_with_shards(shard_number: u32)
             None,
             &ShardSelectorInternal::All,
             None,
+            HwMeasurementAcc::new(),
         )
         .await
         .unwrap();

commit 8ad2b34265448ec01b89d4093de5fbb1a86dcd4d
Author: Tim Visée <tim+github@visee.me>
Date:   Tue Feb 25 11:21:25 2025 +0100

    Bump Rust edition to 2024 (#6042)
    
    * Bump Rust edition to 2024
    
    * gen is a reserved keyword now
    
    * Remove ref mut on references
    
    * Mark extern C as unsafe
    
    * Wrap unsafe function bodies in unsafe block
    
    * Geo hash implements Copy, don't reference but pass by value instead
    
    * Replace secluded self import with parent
    
    * Update execute_cluster_read_operation with new match semantics
    
    * Fix lifetime issue
    
    * Replace map_or with is_none_or
    
    * set_var is unsafe now
    
    * Reformat

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index 2dafc4060..8e49f378e 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -2,6 +2,7 @@ use std::collections::{HashMap, HashSet};
 use std::fs::File;
 
 use api::rest::{OrderByInterface, SearchRequestInternal};
+use collection::operations::CollectionUpdateOperations;
 use collection::operations::payload_ops::{PayloadOps, SetPayloadOp};
 use collection::operations::point_ops::{
     BatchPersisted, BatchVectorStructPersisted, PointInsertOperationsInternal, PointOperations,
@@ -12,7 +13,6 @@ use collection::operations::types::{
     CountRequestInternal, PointRequestInternal, RecommendRequestInternal, ScrollRequestInternal,
     UpdateStatus,
 };
-use collection::operations::CollectionUpdateOperations;
 use collection::recommendations::recommend_by;
 use collection::shards::replica_set::{ReplicaSetState, ReplicaState};
 use common::counter::hardware_accumulator::HwMeasurementAcc;
@@ -26,7 +26,7 @@ use segment::types::{
 use serde_json::Map;
 use tempfile::Builder;
 
-use crate::common::{load_local_collection, simple_collection_fixture, N_SHARDS};
+use crate::common::{N_SHARDS, load_local_collection, simple_collection_fixture};
 
 #[tokio::test(flavor = "multi_thread")]
 async fn test_collection_updater() {
@@ -735,19 +735,21 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
         .await
         .unwrap();
 
-    assert!(result_multi
-        .points
-        .iter()
-        .fold(HashMap::<PointIdType, usize, _>::new(), |mut acc, point| {
-            acc.entry(point.id)
-                .and_modify(|x| {
-                    *x += 1;
-                })
-                .or_insert(1);
-            acc
-        })
-        .values()
-        .all(|&x| x == 2));
+    assert!(
+        result_multi
+            .points
+            .iter()
+            .fold(HashMap::<PointIdType, usize, _>::new(), |mut acc, point| {
+                acc.entry(point.id)
+                    .and_modify(|x| {
+                        *x += 1;
+                    })
+                    .or_insert(1);
+                acc
+            })
+            .values()
+            .all(|&x| x == 2),
+    );
 }
 
 #[tokio::test(flavor = "multi_thread")]

commit 78f0428f3e23b41cb5702b7aa6caab5564f4af26
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Thu Mar 6 15:03:23 2025 +0100

    Measure hardware IO for update operations (#5922)
    
    * Measure update operations hardware IO
    
    * Add support for distributed setups
    
    * also measure update_local
    
    * Add consensus tests for HW metrics of update operations
    
    * add test for upserting without waiting
    
    * Disable HW usage reporting when not waiting for update API
    
    * Review remarks
    
    * Fix resharding collecting hw measurements
    
    * Fix metric type
    
    * New struct HardwareData for better accumulation
    
    * Ensure we always apply CPU multiplier
    
    * Apply suggestions from code review
    
    * Update src/actix/api/update_api.rs
    
    Co-authored-by: Tim Visée <tim+github@visee.me>
    
    * Fix assert_with_upper_bound_error threshold calculation.
    
    * Clarifying why we don't measure shard cleanup
    
    ---------
    
    Co-authored-by: Tim Visée <tim+github@visee.me>

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index 8e49f378e..ebad119aa 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -58,8 +58,9 @@ async fn test_collection_updater_with_shards(shard_number: u32) {
         PointInsertOperationsInternal::from(batch),
     ));
 
+    let hw_counter = HwMeasurementAcc::new();
     let insert_result = collection
-        .update_from_client_simple(insert_points, true, WriteOrdering::default())
+        .update_from_client_simple(insert_points, true, WriteOrdering::default(), hw_counter)
         .await;
 
     match insert_result {
@@ -128,8 +129,9 @@ async fn test_collection_search_with_payload_and_vector_with_shards(shard_number
         PointInsertOperationsInternal::from(batch),
     ));
 
+    let hw_counter = HwMeasurementAcc::new();
     let insert_result = collection
-        .update_from_client_simple(insert_points, true, WriteOrdering::default())
+        .update_from_client_simple(insert_points, true, WriteOrdering::default(), hw_counter)
         .await;
 
     match insert_result {
@@ -231,8 +233,9 @@ async fn test_collection_loading_with_shards(shard_number: u32) {
             PointOperations::UpsertPoints(PointInsertOperationsInternal::from(batch)),
         );
 
+        let hw_counter = HwMeasurementAcc::new();
         collection
-            .update_from_client_simple(insert_points, true, WriteOrdering::default())
+            .update_from_client_simple(insert_points, true, WriteOrdering::default(), hw_counter)
             .await
             .unwrap();
 
@@ -246,8 +249,9 @@ async fn test_collection_loading_with_shards(shard_number: u32) {
                 key: None,
             }));
 
+        let hw_counter = HwMeasurementAcc::new();
         collection
-            .update_from_client_simple(assign_payload, true, WriteOrdering::default())
+            .update_from_client_simple(assign_payload, true, WriteOrdering::default(), hw_counter)
             .await
             .unwrap();
     }
@@ -374,7 +378,12 @@ async fn test_recommendation_api_with_shards(shard_number: u32) {
 
     let hw_acc = HwMeasurementAcc::new();
     collection
-        .update_from_client_simple(insert_points, true, WriteOrdering::default())
+        .update_from_client_simple(
+            insert_points,
+            true,
+            WriteOrdering::default(),
+            hw_acc.clone(),
+        )
         .await
         .unwrap();
     let result = recommend_by(
@@ -432,8 +441,9 @@ async fn test_read_api_with_shards(shard_number: u32) {
         PointInsertOperationsInternal::from(batch),
     ));
 
+    let hw_counter = HwMeasurementAcc::new();
     collection
-        .update_from_client_simple(insert_points, true, WriteOrdering::default())
+        .update_from_client_simple(insert_points, true, WriteOrdering::default(), hw_counter)
         .await
         .unwrap();
 
@@ -529,8 +539,9 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
         PointInsertOperationsInternal::from(batch),
     ));
 
+    let hw_counter = HwMeasurementAcc::new();
     collection
-        .update_from_client_simple(insert_points, true, WriteOrdering::default())
+        .update_from_client_simple(insert_points, true, WriteOrdering::default(), hw_counter)
         .await
         .unwrap();
 
@@ -782,8 +793,14 @@ async fn test_collection_delete_points_by_filter_with_shards(shard_number: u32)
         PointInsertOperationsInternal::from(batch),
     ));
 
+    let hw_counter = HwMeasurementAcc::new();
     let insert_result = collection
-        .update_from_client_simple(insert_points, true, WriteOrdering::default())
+        .update_from_client_simple(
+            insert_points,
+            true,
+            WriteOrdering::default(),
+            hw_counter.clone(),
+        )
         .await;
 
     match insert_result {
@@ -803,7 +820,7 @@ async fn test_collection_delete_points_by_filter_with_shards(shard_number: u32)
     );
 
     let delete_result = collection
-        .update_from_client_simple(delete_points, true, WriteOrdering::default())
+        .update_from_client_simple(delete_points, true, WriteOrdering::default(), hw_counter)
         .await;
 
     match delete_result {

commit 5cd7239b61d1a6944984132283f762850275670f
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Mon Mar 24 19:39:17 2025 +0100

    Measure Payload Index IO Writes (#6137)
    
    * Prepare measurement of index creation + Remove vector deletion
    measurement
    
    * add hw_counter to add_point functions
    
    * Adjust add_point(..) function signatures
    
    * Add new measurement type: payload index IO write
    
    * Measure payload index IO writes
    
    * Some Hw measurement performance improvements
    
    * Review remarks
    
    * Fix measurements in distributed setups
    
    * review fixes
    
    ---------
    
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index ebad119aa..2211cc849 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -541,7 +541,12 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
 
     let hw_counter = HwMeasurementAcc::new();
     collection
-        .update_from_client_simple(insert_points, true, WriteOrdering::default(), hw_counter)
+        .update_from_client_simple(
+            insert_points,
+            true,
+            WriteOrdering::default(),
+            hw_counter.clone(),
+        )
         .await
         .unwrap();
 
@@ -550,6 +555,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
             PRICE_FLOAT_KEY.parse().unwrap(),
             PayloadFieldSchema::FieldType(PayloadSchemaType::Float),
             true,
+            hw_counter.clone(),
         )
         .await
         .unwrap();
@@ -559,6 +565,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
             PRICE_INT_KEY.parse().unwrap(),
             PayloadFieldSchema::FieldType(PayloadSchemaType::Integer),
             true,
+            hw_counter.clone(),
         )
         .await
         .unwrap();
@@ -568,6 +575,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
             MULTI_VALUE_KEY.parse().unwrap(),
             PayloadFieldSchema::FieldType(PayloadSchemaType::Float),
             true,
+            hw_counter.clone(),
         )
         .await
         .unwrap();

commit 6e4a0ad22723d78546ce81efe3cb734be4ed1b79
Author: Luis Cossío <luis.cossio@qdrant.com>
Date:   Thu Mar 27 10:07:43 2025 -0300

    [order_by] Deprecate using `order_value` from payload, dedup by `(order_value, id)` tuple (#6233)
    
    * deprecate inserting order_value in payload, use the one in the struct instead
    
    * fix for test
    
    * dedup by (order_value, id) tuple
    
    * Scroll dedup test must also consider order value
    
    * Update assert message, include order value
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index 2211cc849..42089eee2 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -646,7 +646,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
                 let b = b.0.get(key).unwrap().as_f64();
                 a >= b
             }),
-            "got: {:#?}",
+            "Expected descending order when using {key} key, got: {:#?}",
             result_desc.points
         );
 

commit 89667860dd1c27ade1a99393c9c6387131af9dc3
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Tue Apr 1 13:01:10 2025 +0200

    Minor cleanups (#6291)

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index 42089eee2..e8b7168a3 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -485,7 +485,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
 
     let get_payload = |value: f64| -> Option<Payload> {
         let mut payload_map = Map::new();
-        payload_map.insert(PRICE_FLOAT_KEY.to_string(), (value).into());
+        payload_map.insert(PRICE_FLOAT_KEY.to_string(), value.into());
         payload_map.insert(PRICE_INT_KEY.to_string(), (value as i64).into());
         payload_map.insert(
             MULTI_VALUE_KEY.to_string(),

commit e59d395d80ade92eef58c220adb576548e5e21a7
Author: Tim Visée <tim+github@visee.me>
Date:   Thu Apr 17 23:11:35 2025 +0200

    Use ahash for maps/sets holding point IDs, offsets or similar (#6388)

diff --git a/lib/collection/tests/integration/collection_test.rs b/lib/collection/tests/integration/collection_test.rs
index e8b7168a3..13acdd173 100644
--- a/lib/collection/tests/integration/collection_test.rs
+++ b/lib/collection/tests/integration/collection_test.rs
@@ -1,6 +1,7 @@
 use std::collections::{HashMap, HashSet};
 use std::fs::File;
 
+use ahash::AHashSet;
 use api::rest::{OrderByInterface, SearchRequestInternal};
 use collection::operations::CollectionUpdateOperations;
 use collection::operations::payload_ops::{PayloadOps, SetPayloadOp};
@@ -650,7 +651,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
             result_desc.points
         );
 
-        let asc_already_seen: HashSet<_> = result_asc.points.iter().map(|x| x.id).collect();
+        let asc_already_seen: AHashSet<_> = result_asc.points.iter().map(|x| x.id).collect();
 
         dbg!(&asc_already_seen);
         let asc_second_page = collection
@@ -689,7 +690,7 @@ async fn test_ordered_scroll_api_with_shards(shard_number: u32) {
         assert_eq!(asc_second_page.points.len(), 5);
         assert!(asc_second_page_points.is_subset(&valid_asc_second_page_points));
 
-        let desc_already_seen: HashSet<_> = result_desc.points.iter().map(|x| x.id).collect();
+        let desc_already_seen: AHashSet<_> = result_desc.points.iter().map(|x| x.id).collect();
 
         dbg!(&desc_already_seen);
 
@@ -819,7 +820,7 @@ async fn test_collection_delete_points_by_filter_with_shards(shard_number: u32)
     }
 
     // delete points with id (0, 3)
-    let to_be_deleted: HashSet<PointIdType> = vec![0.into(), 3.into()].into_iter().collect();
+    let to_be_deleted: AHashSet<PointIdType> = vec![0.into(), 3.into()].into_iter().collect();
     let delete_filter =
         segment::types::Filter::new_must(Condition::HasId(HasIdCondition::from(to_be_deleted)));
 

