# Instructions

You are being benchmarked. You will see the output of a git log command, and from that must infer the current state of a file. Think carefully, as you must output the exact state of the file to earn full marks.

**Important:** Your goal is to reproduce the file's content *exactly* as it exists at the final commit, even if the code appears broken, buggy, or contains obvious errors. Do **not** try to "fix" the code. Attempting to correct issues will result in a poor score, as this benchmark evaluates your ability to reproduce the precise state of the file based on its history.

# Required Response Format

Wrap the content of the file in triple backticks (```). Any text outside the final closing backticks will be ignored. End your response after outputting the closing backticks.

# Example Response

```python
#!/usr/bin/env python
print('Hello, world!')
```

# File History

> git log -p --cc --topo-order --reverse -- lib/collection/src/collection/mod.rs

commit 6ad970577bdb52116880878a7046bbf2c3d91331
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Thu Oct 5 03:52:52 2023 +0200

    Split `collection.rs` into sub-modules (#2762)
    
    * Rename `collection.rs` to `collection/mod.rs`
    
    * Extract `collection/collection_ops.rs`
    
    * Extract `collection/point_ops.rs`
    
    * Extract `collection/search.rs`
    
    * Extract `collection/shard_transfer.rs`
    
    * Extract `collection/snapshots.rs`
    
    * fixup! Extract `collection/collection_ops.rs`
    
    * Reorder items in `collection/mod.rs` (1/?)
    
    * Reorder items in `collection/mod.rs` (2/?)
    
    * Reorder items in `collection/mod.rs` (3/?)
    
    * Reorder items in `collection/mod.rs` (4/?)
    
    * Reorder items in `collection/mod.rs` (5/?)
    
    * Reorder items in `collection/mod.rs` (6/?)
    
    * Reorder items in `collection/mod.rs` (7/?)
    
    * Reorder items in `collection/mod.rs` (8/?)
    
    * Reorder items in `collection/collection_ops.rs` (1/2)
    
    * Reorder items in `collection/collection_ops.rs` (2/2)
    
    * Add explicit imports to `collection/collection_ops.rs`
    
    * Add explicit imports to `collection/point_ops.rs`
    
    * Reorder items in `collection/search.rs` (1/2)
    
    * Reorder items in `collection/search.rs` (2/2)
    
    * Add explicit imports to `collection/search.rs`
    
    * Reorder items in `collection/shard_transfer.rs` (1/3)
    
    * Reorder items in `collection/shard_transfer.rs` (2/3)
    
    * Reorder items in `collection/shard_transfer.rs` (3/3)
    
    * Add explicit imports to `collection/shard_transfer.rs`
    
    * Reorder items in `collection/snapshots.rs` (1/?)
    
    * Reorder items in `collection/snapshots.rs` (2/?)
    
    * Reorder items in `collection/snapshots.rs` (3/?)
    
    * Reorder items in `collection/snapshots.rs` (4/?)
    
    * Add explicit imports to `collection/snapshots.rs`
    
    * Optimize imports in `collection/mod.rs`

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
new file mode 100644
index 000000000..e25b15626
--- /dev/null
+++ b/lib/collection/src/collection/mod.rs
@@ -0,0 +1,543 @@
+mod collection_ops;
+mod point_ops;
+mod search;
+mod shard_transfer;
+mod snapshots;
+
+use std::collections::HashSet;
+use std::ops::Deref;
+use std::path::{Path, PathBuf};
+use std::sync::Arc;
+use std::time::Duration;
+
+use segment::common::version::StorageVersion;
+use semver::Version;
+use tokio::runtime::Handle;
+use tokio::sync::{Mutex, RwLock, RwLockWriteGuard};
+
+use crate::collection_state::{ShardInfo, State};
+use crate::common::is_ready::IsReady;
+use crate::config::CollectionConfig;
+use crate::hash_ring::HashRing;
+use crate::operations::shared_storage_config::SharedStorageConfig;
+use crate::operations::types::{CollectionError, CollectionResult, NodeType};
+use crate::shards::channel_service::ChannelService;
+use crate::shards::collection_shard_distribution::CollectionShardDistribution;
+use crate::shards::replica_set::ReplicaState::{Active, Dead, Initializing, Listener};
+use crate::shards::replica_set::{ChangePeerState, ReplicaState, ShardReplicaSet};
+use crate::shards::shard::{PeerId, ShardId};
+use crate::shards::shard_holder::{shard_not_found_error, LockedShardHolder, ShardHolder};
+use crate::shards::transfer::shard_transfer::{check_transfer_conflicts_strict, ShardTransfer};
+use crate::shards::transfer::transfer_tasks_pool::TransferTasksPool;
+use crate::shards::{replica_set, CollectionId, HASH_RING_SHARD_SCALE};
+use crate::telemetry::CollectionTelemetry;
+
+/// Collection's data is split into several shards.
+pub struct Collection {
+    pub(crate) id: CollectionId,
+    pub(crate) shards_holder: Arc<LockedShardHolder>,
+    pub(crate) collection_config: Arc<RwLock<CollectionConfig>>,
+    pub(crate) shared_storage_config: Arc<SharedStorageConfig>,
+    this_peer_id: PeerId,
+    path: PathBuf,
+    snapshots_path: PathBuf,
+    channel_service: ChannelService,
+    transfer_tasks: Mutex<TransferTasksPool>,
+    request_shard_transfer_cb: RequestShardTransfer,
+    #[allow(dead_code)] //Might be useful in case of repartition implementation
+    notify_peer_failure_cb: ChangePeerState,
+    init_time: Duration,
+    // One-way boolean flag that is set to true when the collection is fully initialized
+    // i.e. all shards are activated for the first time.
+    is_initialized: Arc<IsReady>,
+    // Lock to temporary block collection update operations while the collection is being migrated.
+    // Lock is acquired for read on update operation and can be acquired for write externally,
+    // which will block all update operations until the lock is released.
+    updates_lock: RwLock<()>,
+    // Update runtime handle.
+    update_runtime: Handle,
+}
+
+pub type RequestShardTransfer = Arc<dyn Fn(ShardTransfer) + Send + Sync>;
+
+pub type OnTransferFailure = Arc<dyn Fn(ShardTransfer, CollectionId, &str) + Send + Sync>;
+pub type OnTransferSuccess = Arc<dyn Fn(ShardTransfer, CollectionId) + Send + Sync>;
+
+impl Collection {
+    #[allow(clippy::too_many_arguments)]
+    pub async fn new(
+        name: CollectionId,
+        this_peer_id: PeerId,
+        path: &Path,
+        snapshots_path: &Path,
+        collection_config: &CollectionConfig,
+        shared_storage_config: Arc<SharedStorageConfig>,
+        shard_distribution: CollectionShardDistribution,
+        channel_service: ChannelService,
+        on_replica_failure: ChangePeerState,
+        request_shard_transfer: RequestShardTransfer,
+        search_runtime: Option<Handle>,
+        update_runtime: Option<Handle>,
+    ) -> Result<Self, CollectionError> {
+        let start_time = std::time::Instant::now();
+
+        let mut shard_holder = ShardHolder::new(path, HashRing::fair(HASH_RING_SHARD_SCALE))?;
+
+        let shared_collection_config = Arc::new(RwLock::new(collection_config.clone()));
+        for (shard_id, mut peers) in shard_distribution.shards {
+            let is_local = peers.remove(&this_peer_id);
+
+            let replica_set = ShardReplicaSet::build(
+                shard_id,
+                name.clone(),
+                this_peer_id,
+                is_local,
+                peers,
+                on_replica_failure.clone(),
+                path,
+                shared_collection_config.clone(),
+                shared_storage_config.clone(),
+                channel_service.clone(),
+                update_runtime.clone().unwrap_or_else(Handle::current),
+                search_runtime.clone().unwrap_or_else(Handle::current),
+            )
+            .await?;
+
+            shard_holder.add_shard(shard_id, replica_set);
+        }
+
+        let locked_shard_holder = Arc::new(LockedShardHolder::new(shard_holder));
+
+        // Once the config is persisted - the collection is considered to be successfully created.
+        CollectionVersion::save(path)?;
+        collection_config.save(path)?;
+
+        Ok(Self {
+            id: name.clone(),
+            shards_holder: locked_shard_holder,
+            collection_config: shared_collection_config,
+            shared_storage_config,
+            this_peer_id,
+            path: path.to_owned(),
+            snapshots_path: snapshots_path.to_owned(),
+            channel_service,
+            transfer_tasks: Mutex::new(TransferTasksPool::new(name.clone())),
+            request_shard_transfer_cb: request_shard_transfer.clone(),
+            notify_peer_failure_cb: on_replica_failure.clone(),
+            init_time: start_time.elapsed(),
+            is_initialized: Arc::new(Default::default()),
+            updates_lock: RwLock::new(()),
+            update_runtime: update_runtime.unwrap_or_else(Handle::current),
+        })
+    }
+
+    #[allow(clippy::too_many_arguments)]
+    pub async fn load(
+        collection_id: CollectionId,
+        this_peer_id: PeerId,
+        path: &Path,
+        snapshots_path: &Path,
+        shared_storage_config: Arc<SharedStorageConfig>,
+        channel_service: ChannelService,
+        on_replica_failure: replica_set::ChangePeerState,
+        request_shard_transfer: RequestShardTransfer,
+        search_runtime: Option<Handle>,
+        update_runtime: Option<Handle>,
+    ) -> Self {
+        let start_time = std::time::Instant::now();
+        let stored_version = CollectionVersion::load(path)
+            .expect("Can't read collection version")
+            .parse()
+            .expect("Failed to parse stored collection version as semver");
+
+        let app_version: Version = CollectionVersion::current()
+            .parse()
+            .expect("Failed to parse current collection version as semver");
+
+        if stored_version > app_version {
+            panic!("Collection version is greater than application version");
+        }
+
+        if stored_version != app_version {
+            if Self::can_upgrade_storage(&stored_version, &app_version) {
+                log::info!("Migrating collection {stored_version} -> {app_version}");
+                CollectionVersion::save(path)
+                    .unwrap_or_else(|err| panic!("Can't save collection version {err}"));
+            } else {
+                log::error!("Cannot upgrade version {stored_version} to {app_version}.");
+                panic!("Cannot upgrade version {stored_version} to {app_version}. Try to use older version of Qdrant first.");
+            }
+        }
+
+        let collection_config = CollectionConfig::load(path).unwrap_or_else(|err| {
+            panic!(
+                "Can't read collection config due to {}\nat {}",
+                err,
+                path.to_str().unwrap(),
+            )
+        });
+        collection_config.validate_and_warn();
+
+        let ring = HashRing::fair(HASH_RING_SHARD_SCALE);
+        let mut shard_holder = ShardHolder::new(path, ring).expect("Can not create shard holder");
+
+        let shared_collection_config = Arc::new(RwLock::new(collection_config.clone()));
+
+        shard_holder
+            .load_shards(
+                path,
+                &collection_id,
+                shared_collection_config.clone(),
+                shared_storage_config.clone(),
+                channel_service.clone(),
+                on_replica_failure.clone(),
+                this_peer_id,
+                update_runtime.clone().unwrap_or_else(Handle::current),
+                search_runtime.clone().unwrap_or_else(Handle::current),
+            )
+            .await;
+
+        let locked_shard_holder = Arc::new(LockedShardHolder::new(shard_holder));
+
+        Self {
+            id: collection_id.clone(),
+            shards_holder: locked_shard_holder,
+            collection_config: shared_collection_config,
+            shared_storage_config,
+            this_peer_id,
+            path: path.to_owned(),
+            snapshots_path: snapshots_path.to_owned(),
+            channel_service,
+            transfer_tasks: Mutex::new(TransferTasksPool::new(collection_id.clone())),
+            request_shard_transfer_cb: request_shard_transfer.clone(),
+            notify_peer_failure_cb: on_replica_failure,
+            init_time: start_time.elapsed(),
+            is_initialized: Arc::new(Default::default()),
+            updates_lock: RwLock::new(()),
+            update_runtime: update_runtime.unwrap_or_else(Handle::current),
+        }
+    }
+
+    /// Check if stored version have consequent version.
+    /// If major version is different, then it is not compatible.
+    /// If the difference in consecutive versions is greater than 1 in patch,
+    /// then the collection is not compatible with the current version.
+    ///
+    /// Example:
+    ///   0.4.0 -> 0.4.1 = true
+    ///   0.4.0 -> 0.4.2 = false
+    ///   0.4.0 -> 0.5.0 = false
+    ///   0.4.0 -> 0.5.1 = false
+    pub fn can_upgrade_storage(stored: &Version, app: &Version) -> bool {
+        if stored.major != app.major {
+            return false;
+        }
+        if stored.minor != app.minor {
+            return false;
+        }
+        if stored.patch + 1 < app.patch {
+            return false;
+        }
+        true
+    }
+
+    pub fn name(&self) -> String {
+        self.id.clone()
+    }
+
+    /// Return a list of local shards, present on this peer
+    pub async fn get_local_shards(&self) -> Vec<ShardId> {
+        self.shards_holder.read().await.get_local_shards().await
+    }
+
+    pub async fn contains_shard(&self, shard_id: ShardId) -> bool {
+        self.shards_holder.read().await.contains_shard(&shard_id)
+    }
+
+    pub async fn set_shard_replica_state(
+        &self,
+        shard_id: ShardId,
+        peer_id: PeerId,
+        state: ReplicaState,
+        from_state: Option<ReplicaState>,
+    ) -> CollectionResult<()> {
+        let shard_holder = self.shards_holder.read().await;
+        let replica_set = shard_holder
+            .get_shard(&shard_id)
+            .ok_or_else(|| shard_not_found_error(shard_id))?;
+
+        log::debug!(
+            "Changing shard {}:{shard_id} replica state from {:?} to {state:?}",
+            self.id,
+            replica_set.peer_state(&peer_id),
+        );
+
+        // Validation:
+        // 0. Check that `from_state` matches current state
+
+        if from_state.is_some() {
+            let current_state = replica_set.peer_state(&peer_id);
+            if current_state != from_state {
+                return Err(CollectionError::bad_input(format!(
+                    "Replica {peer_id} of shard {shard_id} has state {current_state:?}, but expected {from_state:?}"
+                )));
+            }
+        }
+
+        // 1. Do not deactivate the last active replica
+
+        if state != ReplicaState::Active {
+            let active_replicas: HashSet<_> = replica_set
+                .peers()
+                .into_iter()
+                .filter_map(|(peer, state)| {
+                    if state == ReplicaState::Active {
+                        Some(peer)
+                    } else {
+                        None
+                    }
+                })
+                .collect();
+            if active_replicas.len() == 1 && active_replicas.contains(&peer_id) {
+                return Err(CollectionError::bad_input(format!(
+                    "Cannot deactivate the last active replica {peer_id} of shard {shard_id}"
+                )));
+            }
+        }
+
+        replica_set
+            .ensure_replica_with_state(&peer_id, state)
+            .await?;
+
+        if state == ReplicaState::Dead {
+            // Terminate transfer if source or target replicas are now dead
+            let related_transfers = shard_holder.get_related_transfers(&shard_id, &peer_id);
+            for transfer in related_transfers {
+                self._abort_shard_transfer(transfer.key(), &shard_holder)
+                    .await?;
+            }
+        }
+
+        if !self.is_initialized.check_ready() {
+            // If not initialized yet, we need to check if it was initialized by this call
+            let state = self.state().await;
+            let mut is_fully_active = true;
+            for (_shard_id, shard_info) in state.shards {
+                if shard_info
+                    .replicas
+                    .into_iter()
+                    .any(|(_peer_id, state)| state != ReplicaState::Active)
+                {
+                    is_fully_active = false;
+                    break;
+                }
+            }
+            if is_fully_active {
+                self.is_initialized.make_ready();
+            }
+        }
+
+        // Try to request shard transfer if replicas on the current peer are dead
+        if state == ReplicaState::Dead && self.this_peer_id == peer_id {
+            let transfer_from = replica_set
+                .peers()
+                .into_iter()
+                .find(|(_, state)| state == &ReplicaState::Active)
+                .map(|(peer_id, _)| peer_id);
+            if let Some(transfer_from) = transfer_from {
+                self.request_shard_transfer(ShardTransfer {
+                    shard_id,
+                    from: transfer_from,
+                    to: self.this_peer_id,
+                    sync: true,
+                })
+            } else {
+                log::warn!("No alive replicas to recover shard {shard_id}");
+            }
+        }
+
+        Ok(())
+    }
+
+    pub async fn state(&self) -> State {
+        let shards_holder = self.shards_holder.read().await;
+        let transfers = shards_holder.shard_transfers.read().clone();
+        State {
+            config: self.collection_config.read().await.clone(),
+            shards: shards_holder
+                .get_shards()
+                .map(|(shard_id, replicas)| {
+                    let shard_info = ShardInfo {
+                        replicas: replicas.peers(),
+                    };
+                    (*shard_id, shard_info)
+                })
+                .collect(),
+            transfers,
+        }
+    }
+
+    pub async fn apply_state(
+        &self,
+        state: State,
+        this_peer_id: PeerId,
+        abort_transfer: impl FnMut(ShardTransfer),
+    ) -> CollectionResult<()> {
+        state.apply(this_peer_id, self, abort_transfer).await
+    }
+
+    pub async fn remove_shards_at_peer(&self, peer_id: PeerId) -> CollectionResult<()> {
+        self.shards_holder
+            .read()
+            .await
+            .remove_shards_at_peer(peer_id)
+            .await
+    }
+
+    pub async fn sync_local_state(
+        &self,
+        on_transfer_failure: OnTransferFailure,
+        on_transfer_success: OnTransferSuccess,
+        on_finish_init: ChangePeerState,
+        on_convert_to_listener: ChangePeerState,
+        on_convert_from_listener: ChangePeerState,
+    ) -> CollectionResult<()> {
+        // Check for disabled replicas
+        let shard_holder = self.shards_holder.read().await;
+        for replica_set in shard_holder.all_shards() {
+            replica_set.sync_local_state().await?;
+        }
+
+        // Check for un-reported finished transfers
+        let outgoing_transfers = shard_holder
+            .get_outgoing_transfers(&self.this_peer_id)
+            .await;
+        let tasks_lock = self.transfer_tasks.lock().await;
+        for transfer in outgoing_transfers {
+            match tasks_lock.get_task_result(&transfer.key()) {
+                None => {
+                    if !tasks_lock.check_if_still_running(&transfer.key()) {
+                        log::debug!(
+                            "Transfer {:?} does not exist, but not reported as cancelled. Reporting now.",
+                            transfer.key()
+                        );
+                        on_transfer_failure(transfer, self.name(), "transfer task does not exist");
+                    }
+                }
+                Some(true) => {
+                    log::debug!(
+                        "Transfer {:?} is finished successfully, but not reported. Reporting now.",
+                        transfer.key()
+                    );
+                    on_transfer_success(transfer, self.name());
+                }
+                Some(false) => {
+                    log::debug!(
+                        "Transfer {:?} is failed, but not reported as failed. Reporting now.",
+                        transfer.key()
+                    );
+                    on_transfer_failure(transfer, self.name(), "transfer failed");
+                }
+            }
+        }
+
+        // Check for proper replica states
+        for replica_set in shard_holder.all_shards() {
+            let this_peer_id = &replica_set.this_peer_id();
+            let shard_id = replica_set.shard_id;
+
+            let peers = replica_set.peers();
+            let this_peer_state = peers.get(this_peer_id).copied();
+            let is_last_active = peers.values().filter(|state| **state == Active).count() == 1;
+
+            if this_peer_state == Some(Initializing) {
+                // It is possible, that collection creation didn't report
+                // Try to activate shard, as the collection clearly exists
+                on_finish_init(*this_peer_id, shard_id);
+                continue;
+            }
+
+            if self.shared_storage_config.node_type == NodeType::Listener {
+                if this_peer_state == Some(Active) && !is_last_active {
+                    // Convert active node from active to listener
+                    on_convert_to_listener(*this_peer_id, shard_id);
+                    continue;
+                }
+            } else if this_peer_state == Some(Listener) {
+                // Convert listener node to active
+                on_convert_from_listener(*this_peer_id, shard_id);
+                continue;
+            }
+
+            if this_peer_state != Some(Dead) || replica_set.is_dummy().await {
+                continue; // All good
+            }
+
+            // Try to find dead replicas with no active transfers
+            let transfers = shard_holder.get_transfers(|_| true).await;
+
+            // Try to find a replica to transfer from
+            for replica_id in replica_set.active_remote_shards().await {
+                let transfer = ShardTransfer {
+                    from: replica_id,
+                    to: *this_peer_id,
+                    shard_id,
+                    sync: true,
+                };
+                if check_transfer_conflicts_strict(&transfer, transfers.iter()).is_some() {
+                    continue; // this transfer won't work
+                }
+                log::debug!(
+                    "Recovering shard {}:{} on peer {} by requesting it from {}",
+                    self.name(),
+                    shard_id,
+                    this_peer_id,
+                    replica_id
+                );
+                self.request_shard_transfer(transfer);
+                break;
+            }
+        }
+
+        Ok(())
+    }
+
+    pub async fn get_telemetry_data(&self) -> CollectionTelemetry {
+        let (shards_telemetry, transfers) = {
+            let mut shards_telemetry = Vec::new();
+            let shards_holder = self.shards_holder.read().await;
+            for shard in shards_holder.all_shards() {
+                shards_telemetry.push(shard.get_telemetry_data().await)
+            }
+            (shards_telemetry, shards_holder.get_shard_transfer_info())
+        };
+
+        CollectionTelemetry {
+            id: self.name(),
+            init_time_ms: self.init_time.as_millis() as u64,
+            config: self.collection_config.read().await.clone(),
+            shards: shards_telemetry,
+            transfers,
+        }
+    }
+
+    pub async fn lock_updates(&self) -> RwLockWriteGuard<()> {
+        self.updates_lock.write().await
+    }
+
+    pub fn wait_collection_initiated(&self, timeout: Duration) -> bool {
+        self.is_initialized.await_ready_for_timeout(timeout)
+    }
+
+    pub fn request_shard_transfer(&self, shard_transfer: ShardTransfer) {
+        self.request_shard_transfer_cb.deref()(shard_transfer)
+    }
+}
+
+struct CollectionVersion;
+
+impl StorageVersion for CollectionVersion {
+    fn current() -> String {
+        env!("CARGO_PKG_VERSION").to_string()
+    }
+}

commit a0f5634209d5d1942af9b7f501ffaf67d4505e24
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Fri Oct 13 13:36:13 2023 +0200

    Shard key (#2808)
    
    * introduce shard_key to shard info API and shard holder
    
    * Remove obsolete move
    
    * Update OpenAPI and gRPC specification
    
    * fix shard key mapping condition
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index e25b15626..d65d365cb 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -103,7 +103,7 @@ impl Collection {
             )
             .await?;
 
-            shard_holder.add_shard(shard_id, replica_set);
+            shard_holder.add_shard(shard_id, replica_set, None)?;
         }
 
         let locked_shard_holder = Arc::new(LockedShardHolder::new(shard_holder));

commit 5bdf5e4bfe5ca2de32aff603a755ad263d5eca56
Author: Tim Visée <tim+github@visee.me>
Date:   Mon Oct 30 12:44:39 2023 +0100

    Shard snapshot transfer integration (#2467)
    
    * Clone inside blocks
    
    * Add shard transfer method to distinguish between batching and snapshots
    
    * Add stub method to drive snapshot transfer
    
    * Store remote shard in forward proxy, merge unproxy methods
    
    * On snapshot shard transfer, create a shard snapshot
    
    * Unify logic for unproxifying forward and queue proxy
    
    * Error snapshot transfer if shard is not a queue proxy
    
    * Add remote shard function to request remote HTTP port
    
    * Handle all specific shard types when proxifying
    
    * Allow queue proxy for some shard holder snapshot methods
    
    * Bring local and remote shard snapshot transfer URLs into transfer logic
    
    * Expose optional shard transfer method parameter in REST and gRPC API
    
    * Expose shard transfer method in list of active transfers
    
    * Fix off-by-one error in queue proxy shard batch transfer logic
    
    * Do not set max ack version for WAL twice, already set when finalizing
    
    * Merge comment for two similar calls
    
    * Use reqwest client to transfer and recover shard snapshot on remote
    
    Using the reqwest client should be temporary. We better switch to a gRPC
    call here eventually to use our existing channels. That way we don't
    require an extra HTTP client (and dependency) just for this.
    
    * Send queue proxy updates to remote when shard is transferred
    
    * On shard queue transfer, set max WAL ack to last transferred
    
    * Add safe queue proxy destructor, skip destructing in error
    
    This adds a finalize method to safely destruct a queue proxy shard. It
    ensures that all remaining updates are transferred to the remote, and
    that the max acknowledged version for our WAL is released. Only then is
    the queue proxy shard destructed unwrapping the inner local shard.
    
    Our unproxify logic now ensures that the queue proxy shard remains if
    transferring the updates fails.
    
    * Clean up method driving shard snapshot transfer a bit
    
    * Change default shard transfer method to stream records
    
    This changes the default transfer method to stream records rather than
    using a snaphsot transfer. We can switch this once snapshot transfer is
    fully integrated.
    
    * Improve error handling, don't panic but return proper error
    
    * Do not unwrap in type conversions
    
    * Update OpenAPI and gRPC specification
    
    * Resolve and remove some TODOs
    
    * During shard snapshot transfer, use REST port from config
    
    * Always release max acknowledged WAL version on queue proxy finalize
    
    * Rework queue unproxying, transform into forward proxy to handle errors
    
    When a queue or forward proxy shard needs to be unproxified into a local
    shard again we typically don't have room to handle errors. A queue proxy
    shard may error if it fails to send updates to the remote shard, while a
    forward proxy does not fail at all when transforming.
    
    We now transfer queued updates before a shard is unproxified. This
    allows for proper error handling. After everything is transferred the
    shard is transformed into a forward proxy which can eventually be safely
    unproxified later.
    
    * Add trace logging for transferring queue proxy updates in batch
    
    * Simplify snapshot method conversion from gRPC
    
    * Remove remote shard parameter
    
    * Add safe guard to queue proxy handler, panic in debug if not finalized
    
    * Improve safety and architecture of queue proxy shard
    
    Switch from an explicit finalized flag to an outer-inner architecture.
    This improves the interface and robustness of the type.
    
    * Do not panic on drop if already unwinding
    
    * Make REST port interface in channel service for local node explicitly
    
    * Recover shard on remote over gRPC, remove reqwest client
    
    * Use shard transfer priority for shard snapshot recovery
    
    * Remove obsolete comment
    
    * Simplify qualified path with use
    
    * Don't construct URLs ourselves as a string, use `parse` and `set_port`
    
    * Use `set_path` when building shard download URL
    
    * Fix error handling in queue to forward proxy transformation
    
    Before, we didn't handle finalization errors properly. If this failed,
    tie shard would be lost.  With this change the queue proxy shard is put
    back.
    
    * Set default shard transfer method to stream records, eliminate panics
    
    * Fix shard snapshot transfer not correctly aborting due to queue proxy
    
    When a shard transfer fails (for any reason), the transfer is aborted.
    If we still have a queue proxy shard it should also be reverted, and
    collected updates should be forgotten. Before this change it would try
    to send all collected updates to the remote, even if the transfer
    failed.
    
    * Review fixes
    
    Co-authored-by: Roman Titov <ffuugoo@users.noreply.github.com>
    
    * Review fixes
    
    Co-authored-by: Roman Titov <ffuugoo@users.noreply.github.com>
    
    * Initiate forward and queue proxy shard in specialized transfer methods
    
    Co-authored-by: Roman Titov <ffuugoo@users.noreply.github.com>
    
    * Add consensus interface to shard transfer, repurpose dispatcher (#2873)
    
    * Add shard transfer consensus interface
    
    * Integrate shard transfer consensus interface into toc and transfer logic
    
    * Repurpose dispatcher for getting consensus into shard transfer
    
    * Derive clone
    
    * Mark consensus as unused for now
    
    * Use custom dispatcher with weak ref to prevent Arc cycle for ToC
    
    * Add comment on why a weak reference is used
    
    * Do exhaustive match in shard unproxy logic
    
    * Restructure match statement, use match if
    
    * When queue proxifying shard, allow forward proxy state if same remote
    
    * Before retrying a shard transfer after error, destruct queue proxy
    
    * Synchronize consensus across all nodes for shard snapshot transfer (#2874)
    
    * Move await consensus commit functions into channel service
    
    * Add shard consensus method to synchronize consensus across all nodes
    
    * Move transfer config, channels and local address into snapshot transfer
    
    * Await other nodes to reach consensus before finalizing shard transfer
    
    * Do not fail right away awaiting consensus if still on older term
    
    Instead, give the node time to reach the same term.
    
    * Fix `await_commit_on_all_peers` not catching peer errors properly
    
    * Change return type of `wait_for_consensus_commit` to `Result`
    
    This is of course more conventional, and automatically sets `must_use`.
    
    * Explicitly note number of peers when awaiting consensus
    
    * Before consensus sync, wait for local shard to reach partial state
    
    * Fix timeout error handling when waiting for replica set state
    
    * Wait for replica set to have remote in partial state instead
    
    * Set `(Partial)Snapshot` states for shard snapshot transfer through consensus (#2881)
    
    * When doing a shard snapshot transfer, set shard to `PartialSnapshot`
    
    * Add shard transfer method to set shard state to partial
    
    It currently uses a naive implementation. Using a custom consensus
    operation to also confirm a transfer is still active will be implemented
    later.
    
    * Add consensus snapshot transfer operation to change shard to partial
    
    The operation `ShardTransferOperations::SnapshotRecovered` is called
    after the shard snapshot is recovered on the remote and it progresses
    the transfer further.
    
    The operation sets the shard state from `PartialSnapshot` to `Partial`
    and ensures the transfer is still active.
    
    * Confirm consensus put shard into partial state, retry 3 times
    
    * Get replica set once
    
    * Add extensive shard snapshot transfer process docs, clean up function
    
    * Fix typo
    
    * Review suggestion
    
    Co-authored-by: Luis Cossío <luis.cossio@qdrant.com>
    
    * Add delay between consensus confirmation retries
    
    * Rename retry timeout to retry delay
    
    ---------
    
    Co-authored-by: Luis Cossío <luis.cossio@qdrant.com>
    
    * On replicate shard, remember specified method
    
    ---------
    
    Co-authored-by: Roman Titov <ffuugoo@users.noreply.github.com>
    Co-authored-by: Luis Cossío <luis.cossio@qdrant.com>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index d65d365cb..fe619e85e 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -350,6 +350,7 @@ impl Collection {
                     from: transfer_from,
                     to: self.this_peer_id,
                     sync: true,
+                    method: None,
                 })
             } else {
                 log::warn!("No alive replicas to recover shard {shard_id}");
@@ -483,6 +484,7 @@ impl Collection {
                     to: *this_peer_id,
                     shard_id,
                     sync: true,
+                    method: None,
                 };
                 if check_transfer_conflicts_strict(&transfer, transfers.iter()).is_some() {
                     continue; // this transfer won't work

commit b1eb5a2a687ab81236fc34202fcd119d142dcc1e
Author: Tim Visée <tim+github@visee.me>
Date:   Mon Oct 30 13:18:16 2023 +0100

    Improve shard snapshot transfer replica set state synchronization (#2884)
    
    * Add `WaitForShardState` gRPC call definition
    
    * Implement logic for `WaitForShardState` call
    
    * In next shard snapshot transfer stage, wait for remote to reach `Partial`
    
    * In shard snapshot transfer, synchronize consensus as last step
    
    We don't have to synchronize consensus right away. Instead we just
    confirm that the remote shard has reached `Partial` state. Then we
    transform the queue proxy shard into the forward proxy.
    
    Right before we finalize the transfer we do want to synchronize
    consensus. First make sure the shard has reached `Partial` state in our
    local replica set. Then synchronize all other nodes to make sure they
    reach at least the same consensus state.
    
    * Reformat internal collection service definition

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index fe619e85e..db4fa0d88 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -254,6 +254,24 @@ impl Collection {
         self.shards_holder.read().await.contains_shard(&shard_id)
     }
 
+    pub async fn wait_local_shard_replica_state(
+        &self,
+        shard_id: ShardId,
+        state: ReplicaState,
+        timeout: Duration,
+    ) -> CollectionResult<()> {
+        let shard_holder_read = self.shards_holder.read().await;
+
+        let shard = shard_holder_read.get_shard(&shard_id);
+        let Some(replica_set) = shard else {
+            return Err(CollectionError::NotFound {
+                what: "Shard {shard_id}".into(),
+            });
+        };
+
+        replica_set.wait_for_local_state(state, timeout).await
+    }
+
     pub async fn set_shard_replica_state(
         &self,
         shard_id: ShardId,

commit 7f304340a0b067661c12aa73964c1fbb2e7d5f1b
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Thu Nov 2 12:27:28 2023 +0100

    Shard key - consensus (#2811)
    
    * wip: implement consensus operations to create and delete shard keys
    
    * Fix typo in variable name
    
    * wip: methods to add shard keys
    
    * fmt
    
    * api for submitting creating and deleting shard keys operations into consensus
    
    * fmt
    
    * apply shard mapping in with the collection state transfer
    
    * handle single-to-cluster transition
    
    * fix api structure and bugs
    
    * fmt
    
    * rename param
    
    * review fixes
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index db4fa0d88..a101e3395 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -2,7 +2,9 @@ mod collection_ops;
 mod point_ops;
 mod search;
 mod shard_transfer;
+mod sharding_keys;
 mod snapshots;
+mod state_management;
 
 use std::collections::HashSet;
 use std::ops::Deref;
@@ -18,7 +20,6 @@ use tokio::sync::{Mutex, RwLock, RwLockWriteGuard};
 use crate::collection_state::{ShardInfo, State};
 use crate::common::is_ready::IsReady;
 use crate::config::CollectionConfig;
-use crate::hash_ring::HashRing;
 use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{CollectionError, CollectionResult, NodeType};
 use crate::shards::channel_service::ChannelService;
@@ -29,7 +30,7 @@ use crate::shards::shard::{PeerId, ShardId};
 use crate::shards::shard_holder::{shard_not_found_error, LockedShardHolder, ShardHolder};
 use crate::shards::transfer::shard_transfer::{check_transfer_conflicts_strict, ShardTransfer};
 use crate::shards::transfer::transfer_tasks_pool::TransferTasksPool;
-use crate::shards::{replica_set, CollectionId, HASH_RING_SHARD_SCALE};
+use crate::shards::{replica_set, CollectionId};
 use crate::telemetry::CollectionTelemetry;
 
 /// Collection's data is split into several shards.
@@ -56,6 +57,8 @@ pub struct Collection {
     updates_lock: RwLock<()>,
     // Update runtime handle.
     update_runtime: Handle,
+    // Search runtime handle.
+    search_runtime: Handle,
 }
 
 pub type RequestShardTransfer = Arc<dyn Fn(ShardTransfer) + Send + Sync>;
@@ -81,7 +84,7 @@ impl Collection {
     ) -> Result<Self, CollectionError> {
         let start_time = std::time::Instant::now();
 
-        let mut shard_holder = ShardHolder::new(path, HashRing::fair(HASH_RING_SHARD_SCALE))?;
+        let mut shard_holder = ShardHolder::new(path)?;
 
         let shared_collection_config = Arc::new(RwLock::new(collection_config.clone()));
         for (shard_id, mut peers) in shard_distribution.shards {
@@ -128,6 +131,7 @@ impl Collection {
             is_initialized: Arc::new(Default::default()),
             updates_lock: RwLock::new(()),
             update_runtime: update_runtime.unwrap_or_else(Handle::current),
+            search_runtime: search_runtime.unwrap_or_else(Handle::current),
         })
     }
 
@@ -178,8 +182,7 @@ impl Collection {
         });
         collection_config.validate_and_warn();
 
-        let ring = HashRing::fair(HASH_RING_SHARD_SCALE);
-        let mut shard_holder = ShardHolder::new(path, ring).expect("Can not create shard holder");
+        let mut shard_holder = ShardHolder::new(path).expect("Can not create shard holder");
 
         let shared_collection_config = Arc::new(RwLock::new(collection_config.clone()));
 
@@ -215,6 +218,7 @@ impl Collection {
             is_initialized: Arc::new(Default::default()),
             updates_lock: RwLock::new(()),
             update_runtime: update_runtime.unwrap_or_else(Handle::current),
+            search_runtime: search_runtime.unwrap_or_else(Handle::current),
         }
     }
 
@@ -393,18 +397,10 @@ impl Collection {
                 })
                 .collect(),
             transfers,
+            shards_key_mapping: shards_holder.get_shard_key_to_ids_mapping(),
         }
     }
 
-    pub async fn apply_state(
-        &self,
-        state: State,
-        this_peer_id: PeerId,
-        abort_transfer: impl FnMut(ShardTransfer),
-    ) -> CollectionResult<()> {
-        state.apply(this_peer_id, self, abort_transfer).await
-    }
-
     pub async fn remove_shards_at_peer(&self, peer_id: PeerId) -> CollectionResult<()> {
         self.shards_holder
             .read()

commit 816b5a7448c7f1e0d81c99e5a31219d00ece6fe5
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Thu Nov 9 15:06:02 2023 +0100

    Shard key routing for update requests (#2909)
    
    * add shard_key into output data structures for points
    
    * fmt
    
    * add shard selector for point update operations
    
    * fix creating index without sharding
    
    * Merge serde attributes
    
    * Code review changes
    
    * review fixes
    
    * upd openapi
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index a101e3395..7ed1a479e 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -13,6 +13,7 @@ use std::sync::Arc;
 use std::time::Duration;
 
 use segment::common::version::StorageVersion;
+use segment::types::ShardKey;
 use semver::Version;
 use tokio::runtime::Handle;
 use tokio::sync::{Mutex, RwLock, RwLockWriteGuard};
@@ -249,6 +250,16 @@ impl Collection {
         self.id.clone()
     }
 
+    pub async fn get_shard_keys(&self) -> Vec<ShardKey> {
+        self.shards_holder
+            .read()
+            .await
+            .get_shard_key_to_ids_mapping()
+            .keys()
+            .cloned()
+            .collect()
+    }
+
     /// Return a list of local shards, present on this peer
     pub async fn get_local_shards(&self) -> Vec<ShardId> {
         self.shards_holder.read().await.get_local_shards().await

commit d3aada0e9644975b94409fd79c94e990643614a0
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Fri Nov 10 17:23:30 2023 +0100

    Shard key index consistency (#2938)
    
    * WIP: collection-level storage for payload indexe scheme
    
    * introduce consensus-level operation for creating payload index
    
    * make operation_id optional in the UpdateResult
    
    * set payload index in newly created shards
    
    * upd api definitions
    
    * include payload index schema into collection consensus state
    
    * include payload index schema into shard snapshot
    
    * review fixes

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 7ed1a479e..ff139e442 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -1,4 +1,5 @@
 mod collection_ops;
+pub mod payload_index_schema;
 mod point_ops;
 mod search;
 mod shard_transfer;
@@ -18,11 +19,13 @@ use semver::Version;
 use tokio::runtime::Handle;
 use tokio::sync::{Mutex, RwLock, RwLockWriteGuard};
 
+use crate::collection::payload_index_schema::PayloadIndexSchema;
 use crate::collection_state::{ShardInfo, State};
 use crate::common::is_ready::IsReady;
 use crate::config::CollectionConfig;
 use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{CollectionError, CollectionResult, NodeType};
+use crate::save_on_disk::SaveOnDisk;
 use crate::shards::channel_service::ChannelService;
 use crate::shards::collection_shard_distribution::CollectionShardDistribution;
 use crate::shards::replica_set::ReplicaState::{Active, Dead, Initializing, Listener};
@@ -40,6 +43,7 @@ pub struct Collection {
     pub(crate) shards_holder: Arc<LockedShardHolder>,
     pub(crate) collection_config: Arc<RwLock<CollectionConfig>>,
     pub(crate) shared_storage_config: Arc<SharedStorageConfig>,
+    pub(crate) payload_index_schema: SaveOnDisk<PayloadIndexSchema>,
     this_peer_id: PeerId,
     path: PathBuf,
     snapshots_path: PathBuf,
@@ -116,10 +120,13 @@ impl Collection {
         CollectionVersion::save(path)?;
         collection_config.save(path)?;
 
+        let payload_index_schema = Self::load_payload_index_schema(path)?;
+
         Ok(Self {
             id: name.clone(),
             shards_holder: locked_shard_holder,
             collection_config: shared_collection_config,
+            payload_index_schema,
             shared_storage_config,
             this_peer_id,
             path: path.to_owned(),
@@ -203,10 +210,14 @@ impl Collection {
 
         let locked_shard_holder = Arc::new(LockedShardHolder::new(shard_holder));
 
+        let payload_index_schema = Self::load_payload_index_schema(path)
+            .expect("Can't load or initialize payload index schema");
+
         Self {
             id: collection_id.clone(),
             shards_holder: locked_shard_holder,
             collection_config: shared_collection_config,
+            payload_index_schema,
             shared_storage_config,
             this_peer_id,
             path: path.to_owned(),
@@ -409,6 +420,7 @@ impl Collection {
                 .collect(),
             transfers,
             shards_key_mapping: shards_holder.get_shard_key_to_ids_mapping(),
+            payload_index_schema: self.payload_index_schema.read().clone(),
         }
     }
 

commit 9b177d56320c9b65884c3e1af81d05cc3c2f34cb
Author: Tim Visée <tim+github@visee.me>
Date:   Fri Nov 17 14:23:35 2023 +0100

    Refactor shard transfer logic (#2991)
    
    * Extract shard transfer implementations into modules
    
    * Extract shard transfer helpers into module
    
    * Extract functions driving shard transfer into module
    
    * Move implementation below struct definition

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index ff139e442..54f9ddc35 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -32,8 +32,9 @@ use crate::shards::replica_set::ReplicaState::{Active, Dead, Initializing, Liste
 use crate::shards::replica_set::{ChangePeerState, ReplicaState, ShardReplicaSet};
 use crate::shards::shard::{PeerId, ShardId};
 use crate::shards::shard_holder::{shard_not_found_error, LockedShardHolder, ShardHolder};
-use crate::shards::transfer::shard_transfer::{check_transfer_conflicts_strict, ShardTransfer};
+use crate::shards::transfer::helpers::check_transfer_conflicts_strict;
 use crate::shards::transfer::transfer_tasks_pool::TransferTasksPool;
+use crate::shards::transfer::ShardTransfer;
 use crate::shards::{replica_set, CollectionId};
 use crate::telemetry::CollectionTelemetry;
 

commit f14aa4d6213816737a2e5a6aa36574e28e7da7be
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Tue Nov 21 16:23:04 2023 +0100

    Abort shard transfer during `sync_local_state`, if transfer-sender node failed (#2975, #3012)

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 54f9ddc35..e8557da2f 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -53,6 +53,7 @@ pub struct Collection {
     request_shard_transfer_cb: RequestShardTransfer,
     #[allow(dead_code)] //Might be useful in case of repartition implementation
     notify_peer_failure_cb: ChangePeerState,
+    abort_shard_transfer_cb: replica_set::AbortShardTransfer,
     init_time: Duration,
     // One-way boolean flag that is set to true when the collection is fully initialized
     // i.e. all shards are activated for the first time.
@@ -85,6 +86,7 @@ impl Collection {
         channel_service: ChannelService,
         on_replica_failure: ChangePeerState,
         request_shard_transfer: RequestShardTransfer,
+        abort_shard_transfer: replica_set::AbortShardTransfer,
         search_runtime: Option<Handle>,
         update_runtime: Option<Handle>,
     ) -> Result<Self, CollectionError> {
@@ -103,6 +105,7 @@ impl Collection {
                 is_local,
                 peers,
                 on_replica_failure.clone(),
+                abort_shard_transfer.clone(),
                 path,
                 shared_collection_config.clone(),
                 shared_storage_config.clone(),
@@ -136,6 +139,7 @@ impl Collection {
             transfer_tasks: Mutex::new(TransferTasksPool::new(name.clone())),
             request_shard_transfer_cb: request_shard_transfer.clone(),
             notify_peer_failure_cb: on_replica_failure.clone(),
+            abort_shard_transfer_cb: abort_shard_transfer,
             init_time: start_time.elapsed(),
             is_initialized: Arc::new(Default::default()),
             updates_lock: RwLock::new(()),
@@ -154,6 +158,7 @@ impl Collection {
         channel_service: ChannelService,
         on_replica_failure: replica_set::ChangePeerState,
         request_shard_transfer: RequestShardTransfer,
+        abort_shard_transfer: replica_set::AbortShardTransfer,
         search_runtime: Option<Handle>,
         update_runtime: Option<Handle>,
     ) -> Self {
@@ -203,6 +208,7 @@ impl Collection {
                 shared_storage_config.clone(),
                 channel_service.clone(),
                 on_replica_failure.clone(),
+                abort_shard_transfer.clone(),
                 this_peer_id,
                 update_runtime.clone().unwrap_or_else(Handle::current),
                 search_runtime.clone().unwrap_or_else(Handle::current),
@@ -227,6 +233,7 @@ impl Collection {
             transfer_tasks: Mutex::new(TransferTasksPool::new(collection_id.clone())),
             request_shard_transfer_cb: request_shard_transfer.clone(),
             notify_peer_failure_cb: on_replica_failure,
+            abort_shard_transfer_cb: abort_shard_transfer,
             init_time: start_time.elapsed(),
             is_initialized: Arc::new(Default::default()),
             updates_lock: RwLock::new(()),
@@ -443,14 +450,18 @@ impl Collection {
     ) -> CollectionResult<()> {
         // Check for disabled replicas
         let shard_holder = self.shards_holder.read().await;
+
+        let get_shard_transfers = |shard_id, from| {
+            shard_holder
+                .get_transfers(|transfer| transfer.shard_id == shard_id && transfer.from == from)
+        };
+
         for replica_set in shard_holder.all_shards() {
-            replica_set.sync_local_state().await?;
+            replica_set.sync_local_state(get_shard_transfers).await?;
         }
 
         // Check for un-reported finished transfers
-        let outgoing_transfers = shard_holder
-            .get_outgoing_transfers(&self.this_peer_id)
-            .await;
+        let outgoing_transfers = shard_holder.get_outgoing_transfers(&self.this_peer_id);
         let tasks_lock = self.transfer_tasks.lock().await;
         for transfer in outgoing_transfers {
             match tasks_lock.get_task_result(&transfer.key()) {
@@ -513,7 +524,7 @@ impl Collection {
             }
 
             // Try to find dead replicas with no active transfers
-            let transfers = shard_holder.get_transfers(|_| true).await;
+            let transfers = shard_holder.get_transfers(|_| true);
 
             // Try to find a replica to transfer from
             for replica_id in replica_set.active_remote_shards().await {

commit e085c4dbc12f32715d13afe14f163d4d40f7751c
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Tue Nov 21 17:19:24 2023 +0100

    Add a health check before starting shard transfer during `sync_local_state` (#2977, #3026)

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index e8557da2f..41ae660d4 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -535,9 +535,25 @@ impl Collection {
                     sync: true,
                     method: None,
                 };
+
                 if check_transfer_conflicts_strict(&transfer, transfers.iter()).is_some() {
                     continue; // this transfer won't work
                 }
+
+                // TODO: Should we, maybe, throttle/backoff this requests a bit?
+                if let Err(err) = replica_set.health_check(replica_id).await {
+                    // TODO: This is rather verbose, not sure if we want to log this at all... :/
+                    log::trace!(
+                        "Replica {replica_id}/{}:{} is not available \
+                         to request shard transfer from: \
+                         {err}",
+                        self.id,
+                        replica_set.shard_id,
+                    );
+
+                    continue;
+                }
+
                 log::debug!(
                     "Recovering shard {}:{} on peer {} by requesting it from {}",
                     self.name(),
@@ -545,6 +561,7 @@ impl Collection {
                     this_peer_id,
                     replica_id
                 );
+
                 self.request_shard_transfer(transfer);
                 break;
             }

commit 0866f1c07dccab2f684c012a1e54084a4e2727f3
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Tue Nov 21 19:45:58 2023 +0100

    Add backoff when notifying peer failure during `sync_local_state` (#2942)

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 41ae660d4..7601fcb83 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -457,7 +457,7 @@ impl Collection {
         };
 
         for replica_set in shard_holder.all_shards() {
-            replica_set.sync_local_state(get_shard_transfers).await?;
+            replica_set.sync_local_state(get_shard_transfers)?;
         }
 
         // Check for un-reported finished transfers

commit 680165bda7dea4e5df00a5151a03f8ee0b700f47
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Tue Nov 28 15:22:29 2023 +0100

    fix awaiting of the consensus thread to actually await entry to be ap… (#3103)
    
    * fix awaiting of the consensus thread to actually await entry to be applied
    
    * use last_applied_entry

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 7601fcb83..b9f6a5f26 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -112,6 +112,7 @@ impl Collection {
                 channel_service.clone(),
                 update_runtime.clone().unwrap_or_else(Handle::current),
                 search_runtime.clone().unwrap_or_else(Handle::current),
+                None,
             )
             .await?;
 

commit 835da459cbac57efaf3887b74eefa7a336654698
Author: Tim Visée <tim+github@visee.me>
Date:   Fri Jan 26 16:09:43 2024 +0100

    Limit automatic shard transfers (stateless) (#3458)
    
    * Do not select ourselves when finding auto shard transfer source
    
    * Limit automatic shard transfers with basic transfer count check
    
    * Extract shard recovery transfer request logic to separate function
    
    * Add global shard transfer tracker structure to allow rate limiting
    
    * Count and limit incoming and outgoing shard transfers separately
    
    * Make automatic shard transfer limit configurable
    
    * Move shard transfer tracker from global to collection level
    
    * Comment out new config parameters
    
    * Fix incorrect comment
    
    * Fix missing space in log message
    
    * Fix negated condition
    
    * Remove logic for requesting shard transfer on replica state change
    
    * Check shard transfer limits in consensus sync, use consensus state
    
    Instead of bothering with tracking proposed shard transfers, this now
    purely relies on state that is already in consensus. Each time we sync
    consensus, we request the maximum number of shard transfers up to the
    limit at that time.
    
    * Remove now obsolete shard transfer tracker
    
    * Revert now obsolete changes
    
    * Improve transfer IO counting comment, revert now obsolete code
    
    * Fix typos
    
    * Fix flipped variables, don't take self reference on Copyable type
    
    * Update lib/collection/src/collection/shard_transfer.rs
    
    Co-authored-by: Roman Titov <ffuugoo@users.noreply.github.com>
    
    * Handle incoming/outgoing transfer counts separately, don't tuple it
    
    * Improve loop for counting incoming/outgoing transfers
    
    * Remove unused test function
    
    * Add consensus tests for automatic shard transfer limits
    
    * Apply suggestions from code review
    
    Co-authored-by: Roman Titov <ffuugoo@users.noreply.github.com>
    
    * Remove debug lines from new test
    
    * Reorder last test a bit to resolve flakyness
    
    * We can have one more transfer for recovery on the other alive node
    
    * Attempt to reduce test flakyness, more points and more frequent polling
    
    * Update config/config.yaml
    
    Co-authored-by: Luis Cossío <luis.cossio@qdrant.com>
    
    * Explicitly note default shard transfer limit in configuration
    
    * Use default for shard transfer IO everywhere
    
    * Rename transfer limit check function to be more explicit
    
    ---------
    
    Co-authored-by: Roman Titov <ffuugoo@users.noreply.github.com>
    Co-authored-by: Luis Cossío <luis.cossio@qdrant.com>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index b9f6a5f26..aa5fc8da2 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -7,7 +7,7 @@ mod sharding_keys;
 mod snapshots;
 mod state_management;
 
-use std::collections::HashSet;
+use std::collections::{HashMap, HashSet};
 use std::ops::Deref;
 use std::path::{Path, PathBuf};
 use std::sync::Arc;
@@ -390,26 +390,6 @@ impl Collection {
             }
         }
 
-        // Try to request shard transfer if replicas on the current peer are dead
-        if state == ReplicaState::Dead && self.this_peer_id == peer_id {
-            let transfer_from = replica_set
-                .peers()
-                .into_iter()
-                .find(|(_, state)| state == &ReplicaState::Active)
-                .map(|(peer_id, _)| peer_id);
-            if let Some(transfer_from) = transfer_from {
-                self.request_shard_transfer(ShardTransfer {
-                    shard_id,
-                    from: transfer_from,
-                    to: self.this_peer_id,
-                    sync: true,
-                    method: None,
-                })
-            } else {
-                log::warn!("No alive replicas to recover shard {shard_id}");
-            }
-        }
-
         Ok(())
     }
 
@@ -492,6 +472,12 @@ impl Collection {
             }
         }
 
+        // Count how many transfers we are now proposing
+        // We must track this here so we can reference it when checking for tranfser limits,
+        // because transfers we propose now will not be in the consensus state within the lifetime
+        // of this function
+        let mut proposed = HashMap::<PeerId, usize>::new();
+
         // Check for proper replica states
         for replica_set in shard_holder.all_shards() {
             let this_peer_id = &replica_set.this_peer_id();
@@ -527,6 +513,14 @@ impl Collection {
             // Try to find dead replicas with no active transfers
             let transfers = shard_holder.get_transfers(|_| true);
 
+            // Respect shard transfer limit, consider already proposed transfers in our counts
+            let (mut incoming, outgoing) = shard_holder.count_shard_transfer_io(this_peer_id);
+            incoming += proposed.get(this_peer_id).copied().unwrap_or(0);
+            if self.check_auto_shard_transfer_limit(incoming, outgoing) {
+                log::trace!("Postponing automatic shard {shard_id} transfer to stay below limit on this node (incoming: {incoming}, outgoing: {outgoing})");
+                continue;
+            }
+
             // Try to find a replica to transfer from
             for replica_id in replica_set.active_remote_shards().await {
                 let transfer = ShardTransfer {
@@ -541,6 +535,14 @@ impl Collection {
                     continue; // this transfer won't work
                 }
 
+                // Respect shard transfer limit, consider already proposed transfers in our counts
+                let (incoming, mut outgoing) = shard_holder.count_shard_transfer_io(&replica_id);
+                outgoing += proposed.get(&replica_id).copied().unwrap_or(0);
+                if self.check_auto_shard_transfer_limit(incoming, outgoing) {
+                    log::trace!("Postponing automatic shard {shard_id} transfer to stay below limit on peer {replica_id} (incoming: {incoming}, outgoing: {outgoing})");
+                    continue;
+                }
+
                 // TODO: Should we, maybe, throttle/backoff this requests a bit?
                 if let Err(err) = replica_set.health_check(replica_id).await {
                     // TODO: This is rather verbose, not sure if we want to log this at all... :/
@@ -551,18 +553,17 @@ impl Collection {
                         self.id,
                         replica_set.shard_id,
                     );
-
                     continue;
                 }
 
                 log::debug!(
-                    "Recovering shard {}:{} on peer {} by requesting it from {}",
+                    "Recovering shard {}:{shard_id} on peer {this_peer_id} by requesting it from {replica_id}",
                     self.name(),
-                    shard_id,
-                    this_peer_id,
-                    replica_id
                 );
 
+                // Update our counters for proposed transfers, then request (propose) shard transfer
+                *proposed.entry(transfer.from).or_default() += 1;
+                *proposed.entry(transfer.to).or_default() += 1;
                 self.request_shard_transfer(transfer);
                 break;
             }

commit defd14dcbffd3471c2912a72df97c3815c1800eb
Author: Tim Visée <tim+github@visee.me>
Date:   Thu Dec 21 20:22:04 2023 +0000

    Add config property to set default shard transfer method (#3255)
    
    * Add config option to set default automatic shard transfer method
    
    * Also use configured shard transfer method if user doesn't specify
    
    This is not for automatic shard transfers, but when a user initiates a
    transfer manually.
    
    * Improve comment for shard transfer method configuration
    
    * Fix test compilation

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index aa5fc8da2..71b31d21e 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -528,7 +528,7 @@ impl Collection {
                     to: *this_peer_id,
                     shard_id,
                     sync: true,
-                    method: None,
+                    method: self.shared_storage_config.default_shard_transfer_method,
                 };
 
                 if check_transfer_conflicts_strict(&transfer, transfers.iter()).is_some() {

commit 975809b3bc7246c0ec4551886a3e2478f9c2cd9c
Author: Tim Visée <tim+github@visee.me>
Date:   Mon Jan 22 21:05:44 2024 +0100

    For automatic shard transfers, prevent no shard transfer method warning (#3436)
    
    * On automatic shard transfers, always select some transfer method
    
    * Set default shard transfer method in all places we do automatic transfer

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 71b31d21e..d514113fc 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -528,7 +528,12 @@ impl Collection {
                     to: *this_peer_id,
                     shard_id,
                     sync: true,
-                    method: self.shared_storage_config.default_shard_transfer_method,
+                    // For automatic shard transfers, always select some default method from this point on
+                    method: Some(
+                        self.shared_storage_config
+                            .default_shard_transfer_method
+                            .unwrap_or_default(),
+                    ),
                 };
 
                 if check_transfer_conflicts_strict(&transfer, transfers.iter()).is_some() {

commit 4fce0037f330fc244be93bd2a8da08a97648563d
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Wed Jan 31 09:00:42 2024 +0100

    Make update API cancel safe (#3367)
    
    * WIP: `tokio::spawn` update API request handlers [skip ci]
    
    * WIP: `cancel::future::spawn_cancel_on_drop` update API request handlers [skip ci]
    
    * WIP: Make update API cancel-safe [skip ci]
    
    TODO:
    - Fix tests
    - Evaluate and resolve TODOs
    
    * Fix tests
    
    * Fix benches
    
    * WIP: Simplify cancel safety implementation
    
    * Document and annotate cancel safety guarantees of update API
    
    - Also fix tests after simplifying update API cancel safety impl
    - And add a few `cancel::future::cancel_on_token` calls here and there
    
    * Further simplify cancel safety implementation
    
    No more cancellation tokens! 🎉
    
    * Resolve cancel safety TODO
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index d514113fc..db8764b49 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -61,7 +61,7 @@ pub struct Collection {
     // Lock to temporary block collection update operations while the collection is being migrated.
     // Lock is acquired for read on update operation and can be acquired for write externally,
     // which will block all update operations until the lock is released.
-    updates_lock: RwLock<()>,
+    updates_lock: Arc<RwLock<()>>,
     // Update runtime handle.
     update_runtime: Handle,
     // Search runtime handle.
@@ -142,8 +142,8 @@ impl Collection {
             notify_peer_failure_cb: on_replica_failure.clone(),
             abort_shard_transfer_cb: abort_shard_transfer,
             init_time: start_time.elapsed(),
-            is_initialized: Arc::new(Default::default()),
-            updates_lock: RwLock::new(()),
+            is_initialized: Default::default(),
+            updates_lock: Default::default(),
             update_runtime: update_runtime.unwrap_or_else(Handle::current),
             search_runtime: search_runtime.unwrap_or_else(Handle::current),
         })
@@ -236,8 +236,8 @@ impl Collection {
             notify_peer_failure_cb: on_replica_failure,
             abort_shard_transfer_cb: abort_shard_transfer,
             init_time: start_time.elapsed(),
-            is_initialized: Arc::new(Default::default()),
-            updates_lock: RwLock::new(()),
+            is_initialized: Default::default(),
+            updates_lock: Default::default(),
             update_runtime: update_runtime.unwrap_or_else(Handle::current),
             search_runtime: search_runtime.unwrap_or_else(Handle::current),
         }

commit 19514265330ac9a1049b9439517deb104a5a19ed
Author: Tim Visée <tim+github@visee.me>
Date:   Wed Jan 31 11:56:34 2024 +0100

    Dynamic CPU saturation internals (#3364)
    
    * Move CPU count function to common, fix wrong CPU count in visited list
    
    * Change default number of rayon threads to 8
    
    * Use CPU budget and CPU permits for optimizer tasks to limit utilization
    
    * Respect configured thread limits, use new sane defaults in config
    
    * Fix spelling issues
    
    * Fix test compilation error
    
    * Improve breaking if there is no CPU budget
    
    * Block optimizations until CPU budget, fix potentially getting stuck
    
    Our optimization worker now blocks until CPU budget is available to
    perform the task.
    
    Fix potential issue where optimization worker could get stuck. This
    would happen if no optimization task is started because there's no
    available CPU budget. This ensures the worker is woken up again to
    retry.
    
    * Utilize n-1 CPUs with optimization tasks
    
    * Better handle situations where CPU budget is drained
    
    * Dynamically scale rayon CPU count based on CPU size
    
    * Fix incorrect default for max_indexing_threads conversion
    
    * Respect max_indexing_threads for collection
    
    * Make max_indexing_threads optional, use none to set no limit
    
    * Update property documentation and comments
    
    * Property max_optimization_threads is per shard, not per collection
    
    * If we reached shard optimization limit, skip further checks
    
    * Add remaining TODOs
    
    * Fix spelling mistake
    
    * Align gRPC comment blocks
    
    * Fix compilation errors since last rebase
    
    * Make tests aware of CPU budget
    
    * Use new CPU budget calculation function everywhere
    
    * Make CPU budget configurable in settings, move static budget to common
    
    * Do not use static CPU budget, instance it and pass it through
    
    * Update CPU budget description
    
    * Move heuristic into defaults
    
    * Fix spelling issues
    
    * Move cpu_budget property to a better place
    
    * Move some things around
    
    * Minor review improvements
    
    * Use range match statement for CPU count heuristics
    
    * Systems with 1 or 2 CPUs do not keep cores unallocated by default
    
    * Fix compilation errors since last rebase
    
    * Update lib/segment/src/types.rs
    
    Co-authored-by: Luis Cossío <luis.cossio@qdrant.com>
    
    * Update lib/storage/src/content_manager/toc/transfer.rs
    
    Co-authored-by: Luis Cossío <luis.cossio@qdrant.com>
    
    * Rename cpu_budget to optimizer_cpu_budget
    
    * Update OpenAPI specification
    
    * Require at least half of the desired CPUs for optimizers
    
    This prevents running optimizations with just one CPU, which could be
    very slow.
    
    * Don't use wildcard in CPU heuristic match statements
    
    * Rename cpu_budget setting to optimizer_cpu_budget
    
    * Update CPU budget comments
    
    * Spell acquire correctly
    
    * Change if-else into match
    
    Co-authored-by: Luis Cossío <luis.cossio@qdrant.com>
    
    * Rename max_rayon_threads to num_rayon_threads, add explanation
    
    * Explain limit in update handler
    
    * Remove numbers for automatic selection of indexing threads
    
    * Inline max_workers variable
    
    * Remove CPU budget from ShardTransferConsensus trait, it is in collection
    
    * small allow(dead_code) => cfg(test)
    
    * Remove now obsolete lazy_static
    
    * Fix incorrect CPU calculation in CPU saturation test
    
    * Make waiting for CPU budget async, don't block current thread
    
    * Prevent deadlock on optimizer signal channel
    
    Do not block the optimization worker task anymore to wait for CPU budget
    to be available. That prevents our optimizer signal channel from being
    drained, blocking incoming updates because the cannot send another
    optimizer signal. Now, prevent blocking this task all together and
    retrigger the optimizers separately when CPU budget is available again.
    
    * Fix incorrect CPU calculation in optimization cancel test
    
    * Rename CPU budget wait function to notify
    
    * Detach API changes from CPU saturation internals
    
    This allows us to merge into a patch version of Qdrant. We can
    reintroduce the API changes in the upcoming minor release to make all of
    it fully functional.
    
    ---------
    
    Co-authored-by: Luis Cossío <luis.cossio@qdrant.com>
    Co-authored-by: Luis Cossío <luis.cossio@outlook.com>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index db8764b49..aa6dfb5df 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -13,6 +13,7 @@ use std::path::{Path, PathBuf};
 use std::sync::Arc;
 use std::time::Duration;
 
+use common::cpu::CpuBudget;
 use segment::common::version::StorageVersion;
 use segment::types::ShardKey;
 use semver::Version;
@@ -66,6 +67,7 @@ pub struct Collection {
     update_runtime: Handle,
     // Search runtime handle.
     search_runtime: Handle,
+    optimizer_cpu_budget: CpuBudget,
 }
 
 pub type RequestShardTransfer = Arc<dyn Fn(ShardTransfer) + Send + Sync>;
@@ -89,6 +91,7 @@ impl Collection {
         abort_shard_transfer: replica_set::AbortShardTransfer,
         search_runtime: Option<Handle>,
         update_runtime: Option<Handle>,
+        optimizer_cpu_budget: CpuBudget,
     ) -> Result<Self, CollectionError> {
         let start_time = std::time::Instant::now();
 
@@ -112,6 +115,7 @@ impl Collection {
                 channel_service.clone(),
                 update_runtime.clone().unwrap_or_else(Handle::current),
                 search_runtime.clone().unwrap_or_else(Handle::current),
+                optimizer_cpu_budget.clone(),
                 None,
             )
             .await?;
@@ -146,6 +150,7 @@ impl Collection {
             updates_lock: Default::default(),
             update_runtime: update_runtime.unwrap_or_else(Handle::current),
             search_runtime: search_runtime.unwrap_or_else(Handle::current),
+            optimizer_cpu_budget,
         })
     }
 
@@ -162,6 +167,7 @@ impl Collection {
         abort_shard_transfer: replica_set::AbortShardTransfer,
         search_runtime: Option<Handle>,
         update_runtime: Option<Handle>,
+        optimizer_cpu_budget: CpuBudget,
     ) -> Self {
         let start_time = std::time::Instant::now();
         let stored_version = CollectionVersion::load(path)
@@ -213,6 +219,7 @@ impl Collection {
                 this_peer_id,
                 update_runtime.clone().unwrap_or_else(Handle::current),
                 search_runtime.clone().unwrap_or_else(Handle::current),
+                optimizer_cpu_budget.clone(),
             )
             .await;
 
@@ -240,6 +247,7 @@ impl Collection {
             updates_lock: Default::default(),
             update_runtime: update_runtime.unwrap_or_else(Handle::current),
             search_runtime: search_runtime.unwrap_or_else(Handle::current),
+            optimizer_cpu_budget,
         }
     }
 

commit f08df0b22e28c22c0f1eddeba0343c516c4939a7
Author: Tim Visée <tim+github@visee.me>
Date:   Fri Feb 9 16:57:12 2024 +0100

    Add endpoint to request recovery point for remote shard (#3510)
    
    * Add initial gRPC call for requesting WAL recovery point for remote shard
    
    * Add remote shard method to request WAL recovery point
    
    * Add recovery point type in gRPC, use it in recovery point functions
    
    * Add function to extend recovery point with missing clocks from clock map
    
    * Add new gRPC type for recovery point clocks
    
    * Remove atomic loading, because we use regular integers now

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index aa6dfb5df..ba7145ea4 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -29,6 +29,7 @@ use crate::operations::types::{CollectionError, CollectionResult, NodeType};
 use crate::save_on_disk::SaveOnDisk;
 use crate::shards::channel_service::ChannelService;
 use crate::shards::collection_shard_distribution::CollectionShardDistribution;
+use crate::shards::local_shard::clock_map::RecoveryPoint;
 use crate::shards::replica_set::ReplicaState::{Active, Dead, Initializing, Listener};
 use crate::shards::replica_set::{ChangePeerState, ReplicaState, ShardReplicaSet};
 use crate::shards::shard::{PeerId, ShardId};
@@ -401,6 +402,19 @@ impl Collection {
         Ok(())
     }
 
+    pub async fn shard_recovery_point(&self, shard_id: ShardId) -> CollectionResult<RecoveryPoint> {
+        let shard_holder_read = self.shards_holder.read().await;
+
+        let shard = shard_holder_read.get_shard(&shard_id);
+        let Some(replica_set) = shard else {
+            return Err(CollectionError::NotFound {
+                what: "Shard {shard_id}".into(),
+            });
+        };
+
+        replica_set.shard_recovery_point().await
+    }
+
     pub async fn state(&self) -> State {
         let shards_holder = self.shards_holder.read().await;
         let transfers = shards_holder.shard_transfers.read().clone();

commit d39a483017d14971051e30be5023dd4e969163b6
Author: xzfc <5121426+xzfc@users.noreply.github.com>
Date:   Tue Feb 20 14:55:57 2024 +0000

    Refactor: introduce details level enum (#3612)

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index ba7145ea4..b053ca0d9 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -14,6 +14,7 @@ use std::sync::Arc;
 use std::time::Duration;
 
 use common::cpu::CpuBudget;
+use common::types::TelemetryDetail;
 use segment::common::version::StorageVersion;
 use segment::types::ShardKey;
 use semver::Version;
@@ -599,12 +600,12 @@ impl Collection {
         Ok(())
     }
 
-    pub async fn get_telemetry_data(&self) -> CollectionTelemetry {
+    pub async fn get_telemetry_data(&self, detail: TelemetryDetail) -> CollectionTelemetry {
         let (shards_telemetry, transfers) = {
             let mut shards_telemetry = Vec::new();
             let shards_holder = self.shards_holder.read().await;
             for shard in shards_holder.all_shards() {
-                shards_telemetry.push(shard.get_telemetry_data().await)
+                shards_telemetry.push(shard.get_telemetry_data(detail).await)
             }
             (shards_telemetry, shards_holder.get_shard_transfer_info())
         };

commit bb8dcb15c8ef694d86a68df718b850f8a65ec8aa
Author: Tim Visée <tim+github@visee.me>
Date:   Thu Feb 22 13:29:09 2024 +0100

    Add gRPC API to set shard cutoff point (#3661)
    
    * Add functions to propagate updating cutoff point from collection level
    
    * Add gRPC endpoint to set cutoff point
    
    * Lock highest and cutoff clock maps separately

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index b053ca0d9..c675fcbb2 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -416,6 +416,23 @@ impl Collection {
         replica_set.shard_recovery_point().await
     }
 
+    pub async fn update_shard_cutoff_point(
+        &self,
+        shard_id: ShardId,
+        cutoff: &RecoveryPoint,
+    ) -> CollectionResult<()> {
+        let shard_holder_read = self.shards_holder.read().await;
+
+        let shard = shard_holder_read.get_shard(&shard_id);
+        let Some(replica_set) = shard else {
+            return Err(CollectionError::NotFound {
+                what: "Shard {shard_id}".into(),
+            });
+        };
+
+        replica_set.update_shard_cutoff_point(cutoff).await
+    }
+
     pub async fn state(&self) -> State {
         let shards_holder = self.shards_holder.read().await;
         let transfers = shards_holder.shard_transfers.read().clone();

commit 2bdfa360e10ae2305731a0d3a6e9107273f760fd
Author: xzfc <5121426+xzfc@users.noreply.github.com>
Date:   Fri Feb 23 10:40:44 2024 +0000

    Report shard transfer progress (#3555)
    
    * Shard transfer progress
    
    * Add test
    
    * Round to two decimals
    
    * Use ringbuffer crate
    
    * Text-based human readable report

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index c675fcbb2..f00d1deb6 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -36,7 +36,7 @@ use crate::shards::replica_set::{ChangePeerState, ReplicaState, ShardReplicaSet}
 use crate::shards::shard::{PeerId, ShardId};
 use crate::shards::shard_holder::{shard_not_found_error, LockedShardHolder, ShardHolder};
 use crate::shards::transfer::helpers::check_transfer_conflicts_strict;
-use crate::shards::transfer::transfer_tasks_pool::TransferTasksPool;
+use crate::shards::transfer::transfer_tasks_pool::{TaskResult, TransferTasksPool};
 use crate::shards::transfer::ShardTransfer;
 use crate::shards::{replica_set, CollectionId};
 use crate::telemetry::CollectionTelemetry;
@@ -485,24 +485,26 @@ impl Collection {
         let outgoing_transfers = shard_holder.get_outgoing_transfers(&self.this_peer_id);
         let tasks_lock = self.transfer_tasks.lock().await;
         for transfer in outgoing_transfers {
-            match tasks_lock.get_task_result(&transfer.key()) {
+            match tasks_lock
+                .get_task_status(&transfer.key())
+                .map(|s| s.result)
+            {
                 None => {
-                    if !tasks_lock.check_if_still_running(&transfer.key()) {
-                        log::debug!(
-                            "Transfer {:?} does not exist, but not reported as cancelled. Reporting now.",
-                            transfer.key()
-                        );
-                        on_transfer_failure(transfer, self.name(), "transfer task does not exist");
-                    }
+                    log::debug!(
+                        "Transfer {:?} does not exist, but not reported as cancelled. Reporting now.",
+                        transfer.key()
+                    );
+                    on_transfer_failure(transfer, self.name(), "transfer task does not exist");
                 }
-                Some(true) => {
+                Some(TaskResult::Running) => (),
+                Some(TaskResult::Finished) => {
                     log::debug!(
                         "Transfer {:?} is finished successfully, but not reported. Reporting now.",
                         transfer.key()
                     );
                     on_transfer_success(transfer, self.name());
                 }
-                Some(false) => {
+                Some(TaskResult::Failed) => {
                     log::debug!(
                         "Transfer {:?} is failed, but not reported as failed. Reporting now.",
                         transfer.key()
@@ -624,7 +626,10 @@ impl Collection {
             for shard in shards_holder.all_shards() {
                 shards_telemetry.push(shard.get_telemetry_data(detail).await)
             }
-            (shards_telemetry, shards_holder.get_shard_transfer_info())
+            (
+                shards_telemetry,
+                shards_holder.get_shard_transfer_info(&*self.transfer_tasks.lock().await),
+            )
         };
 
         CollectionTelemetry {

commit 37fdc56729362393074d8c431a4d36c1839f7593
Author: Alexandru Cihodaru <40807189+AlexandruCihodaru@users.noreply.github.com>
Date:   Wed Feb 28 21:04:12 2024 +0200

    Snapshot store (#3643)
    
    * [snapshots]: Define trait for snapshots managemet
    
    * Define a trait with methods for managing snapshots after their
      creation. The goal of this trait is to allow implementation of
      snapshot management logic for different storage backends without major
      changes in the code.
    
    Signed-off-by: Alexandru Cihodaru <alexandru.cihodaru@gmail.com>
    
    * [snapshots]: Move snapshot deletion logic
    
    * Move the snapshot deletion logic into the implementation of
      SnapshotStorage for LocalFileSystemConfig.
    * Replace snapshot deletion logic with calls to the SnapshotStorage
      implementation.
    
    Signed-off-by: Alexandru Cihodaru <alexandru.cihodaru@gmail.com>
    
    * [snapshots]: Move snapshot list logic
    
    * Move the snapshot listing logic into the implementation of
      SnapshotStorage for LocalFileSystemConfig.
    
    Signed-off-by: Alexandru Cihodaru <alexandru.cihodaru@gmail.com>
    
    * [snapshots]: Move snapshot creation logic
    
    * Move the snapshot creation logic into the implementation of
      SnapshotStorage for LocalFileSystemConfig.
    
    Signed-off-by: Alexandru Cihodaru <alexandru.cihodaru@gmail.com>
    
    * review fixes
    
    * fmt
    
    * refactor full storage snapshot
    
    * create directory on get_stored_file + make sure temp archive file is removed
    
    ---------
    
    Signed-off-by: Alexandru Cihodaru <alexandru.cihodaru@gmail.com>
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index f00d1deb6..78125777d 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -652,6 +652,10 @@ impl Collection {
     pub fn request_shard_transfer(&self, shard_transfer: ShardTransfer) {
         self.request_shard_transfer_cb.deref()(shard_transfer)
     }
+
+    pub fn shared_storage_config(&self) -> Arc<SharedStorageConfig> {
+        self.shared_storage_config.clone()
+    }
 }
 
 struct CollectionVersion;

commit 0368ce57342d8fc8e7506542bf17aadc0e02fe16
Author: Tim Visée <tim+github@visee.me>
Date:   Tue Mar 19 17:08:49 2024 +0100

    Use WAL delta transfer by default for shard recovery for 1.9 (#3800)
    
    * Move peer metadata type around
    
    * Expose peer metadata in channel service
    
    * Use WAL delta transfer by default for recovery, if all nodes are 1.8+
    
    * Add check for missing metadata, assume versionless if we have less
    
    * Use user configured shard transfer method, fall back to WAL delta/stream
    
    * Minor improvements
    
    * Update variable name

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 78125777d..a6f6cf83f 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -37,7 +37,7 @@ use crate::shards::shard::{PeerId, ShardId};
 use crate::shards::shard_holder::{shard_not_found_error, LockedShardHolder, ShardHolder};
 use crate::shards::transfer::helpers::check_transfer_conflicts_strict;
 use crate::shards::transfer::transfer_tasks_pool::{TaskResult, TransferTasksPool};
-use crate::shards::transfer::ShardTransfer;
+use crate::shards::transfer::{ShardTransfer, ShardTransferMethod};
 use crate::shards::{replica_set, CollectionId};
 use crate::telemetry::CollectionTelemetry;
 
@@ -563,6 +563,22 @@ impl Collection {
                 continue;
             }
 
+            // Select shard transfer method, prefer user configured method or choose one now
+            // If all peers are 1.8+, we try WAL delta transfer, otherwise we use the default method
+            let shard_transfer_method = self
+                .shared_storage_config
+                .default_shard_transfer_method
+                .unwrap_or_else(|| {
+                    let all_support_wal_delta = self
+                        .channel_service
+                        .all_peers_at_version(Version::new(1, 8, 0));
+                    if all_support_wal_delta {
+                        ShardTransferMethod::WalDelta
+                    } else {
+                        ShardTransferMethod::default()
+                    }
+                });
+
             // Try to find a replica to transfer from
             for replica_id in replica_set.active_remote_shards().await {
                 let transfer = ShardTransfer {
@@ -571,11 +587,7 @@ impl Collection {
                     shard_id,
                     sync: true,
                     // For automatic shard transfers, always select some default method from this point on
-                    method: Some(
-                        self.shared_storage_config
-                            .default_shard_transfer_method
-                            .unwrap_or_default(),
-                    ),
+                    method: Some(shard_transfer_method),
                 };
 
                 if check_transfer_conflicts_strict(&transfer, transfers.iter()).is_some() {

commit 2c741a216f5ad7eec1761ff6122a3cd943e3e342
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Tue Mar 26 13:59:12 2024 +0100

    Cancel shard transfers when shard is deleted (#3784)
    
    * Allow calling `finish_shard_transfer` with already locked `shards_holder`
    
    * Cancel transfer tasks and remove transfers when removing shard replica
    
    * Cleanup `handle_replica_changes`
    
    * Refactor `abort_shard_transfer` similarly to `finish_shard_transfer`
    
    * Restructure `Collection::abort_shard_transfer`
    
    - TL;DR: carefully check preconditions, when changing local state
    - Do not set replica state to `Dead`, if replica does not exist
    - Do not remove replica, if shard transfer does not exist
    - Only revert proxy shard if transfer exists
    
    * Tiny fixes
    
    * When dropping a shard, only cancel its transfers if all nodes are 1.9+
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index a6f6cf83f..49622ba3a 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -54,7 +54,6 @@ pub struct Collection {
     channel_service: ChannelService,
     transfer_tasks: Mutex<TransferTasksPool>,
     request_shard_transfer_cb: RequestShardTransfer,
-    #[allow(dead_code)] //Might be useful in case of repartition implementation
     notify_peer_failure_cb: ChangePeerState,
     abort_shard_transfer_cb: replica_set::AbortShardTransfer,
     init_time: Duration,
@@ -376,7 +375,7 @@ impl Collection {
             // Terminate transfer if source or target replicas are now dead
             let related_transfers = shard_holder.get_related_transfers(&shard_id, &peer_id);
             for transfer in related_transfers {
-                self._abort_shard_transfer(transfer.key(), &shard_holder)
+                self.abort_shard_transfer(transfer.key(), Some(&shard_holder))
                     .await?;
             }
         }

commit 22c3ee286e9390cff9d060f9158c9751f0b64bd4
Author: Kenshin Tanaka <70839560+kemkemG0@users.noreply.github.com>
Date:   Fri May 10 23:54:23 2024 +0900

    Implement S3 snapshot manager (#4150)
    
    * Add SnapshotsStorageConfig enum(Local or S3) and deserialize implementation
    
    * [refactor]  use snapshots_config instead of s3_config
    
    * update config
    
    * add AWS official`aws-sdk-s3`
    
    * implement store_file() WITHOUT error handling
    
    * implement list_snapshots
    
    * implement delete_snapshot
    
    * run `cargo +nightly fmt`
    
    * delete println
    
    * implement get_stored_file
    
    * Add error handlings
    
    * Refactor AWS S3 configuration and error handling
    
    * fix bugs
    
    * create an empty test file
    
    * fix `alias_test.rs` for StorageConfig type
    
    * tempolary delete some test and try s3 test
    
    * Update integration-tests.yml to use snap instead of apt-get for installing yq
    
    * Update integration-tests.yml to use sudo when installing yq
    
    * add sudo
    
    * make (full/non-full) snapshots downloadable
    
    * debug
    
    * small fix
    
    * Add S3 endpoint URL configuration option
    
    * fix
    
    * fix
    
    * debug
    
    * fix endpoint
    
    * update to http://127.0.0.1:9000/
    
    * update
    
    * fix
    
    * fix `#[get("/collections/{collection}/shards/{shard}/snapshots/{snapshot}")]` for s3
    
    * put original tests back
    
    * refactor
    
    * small fix (delete println & echo)
    
    * use object_store and refactor
    
    * create snapshot_storage_ops and implement
    
    * Refactor get_appropriate_chunk_size function to adjust chunk size based on service limits and file size
    
    * cargo +nightly fmt --all
    
    * make it more abstract
    
    * Refactor SnapshotsStorageConfig deserialization in SnapShotsConfig
    
    * small update
    
    * small fix
    
    * Update dependencies in Cargo.lock
    
    * Update minio image to satantime/minio-server
    
    * Refactor snapshot storage paths in snapshots_manager.rs and snapshot_storage_ops.rs
    
    * Fix issue with downloaded file size not matching expected size in download_snapshot function
    
    * add flush
    
    * Use Streaming instead of donloading once
    
    * apply `cargo +nightly fmt --all`
    
    * Fix issue with opening file in SnapshotStream::LocalFS variant
    
    * Fix error handling in SnapshotStream::LocalFS variant
    
    * Add integration test for Shard Snapshot API with S3 storage (#7)

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 49622ba3a..56c45d81b 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -667,6 +667,14 @@ impl Collection {
     pub fn shared_storage_config(&self) -> Arc<SharedStorageConfig> {
         self.shared_storage_config.clone()
     }
+
+    pub fn snapshots_path(&self) -> &Path {
+        &self.snapshots_path
+    }
+
+    pub fn shards_holder(&self) -> Arc<LockedShardHolder> {
+        self.shards_holder.clone()
+    }
 }
 
 struct CollectionVersion;

commit 89b4be0241e6240890156c0cec149408f88f7f17
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Mon May 13 13:31:03 2024 +0200

    Add basic resharding types (#4216)

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 56c45d81b..076e9e859 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -1,6 +1,7 @@
 mod collection_ops;
 pub mod payload_index_schema;
 mod point_ops;
+mod resharding;
 mod search;
 mod shard_transfer;
 mod sharding_keys;
@@ -41,13 +42,17 @@ use crate::shards::transfer::{ShardTransfer, ShardTransferMethod};
 use crate::shards::{replica_set, CollectionId};
 use crate::telemetry::CollectionTelemetry;
 
+const RESHARDING_STATE_FILE: &str = "resharding_state.json";
+
 /// Collection's data is split into several shards.
+#[allow(dead_code)]
 pub struct Collection {
     pub(crate) id: CollectionId,
     pub(crate) shards_holder: Arc<LockedShardHolder>,
     pub(crate) collection_config: Arc<RwLock<CollectionConfig>>,
     pub(crate) shared_storage_config: Arc<SharedStorageConfig>,
     pub(crate) payload_index_schema: SaveOnDisk<PayloadIndexSchema>,
+    resharding_state: SaveOnDisk<Option<resharding::State>>,
     this_peer_id: PeerId,
     path: PathBuf,
     snapshots_path: PathBuf,
@@ -131,6 +136,7 @@ impl Collection {
         collection_config.save(path)?;
 
         let payload_index_schema = Self::load_payload_index_schema(path)?;
+        let resharding_state = Self::load_resharding_state(path)?;
 
         Ok(Self {
             id: name.clone(),
@@ -138,6 +144,7 @@ impl Collection {
             collection_config: shared_collection_config,
             payload_index_schema,
             shared_storage_config,
+            resharding_state,
             this_peer_id,
             path: path.to_owned(),
             snapshots_path: snapshots_path.to_owned(),
@@ -229,12 +236,16 @@ impl Collection {
         let payload_index_schema = Self::load_payload_index_schema(path)
             .expect("Can't load or initialize payload index schema");
 
+        let resharding_state = Self::load_resharding_state(path)
+            .expect("Can't load or initialize resharding progress");
+
         Self {
             id: collection_id.clone(),
             shards_holder: locked_shard_holder,
             collection_config: shared_collection_config,
             payload_index_schema,
             shared_storage_config,
+            resharding_state,
             this_peer_id,
             path: path.to_owned(),
             snapshots_path: snapshots_path.to_owned(),
@@ -252,6 +263,18 @@ impl Collection {
         }
     }
 
+    fn resharding_state_file(collection_path: &Path) -> PathBuf {
+        collection_path.join(RESHARDING_STATE_FILE)
+    }
+
+    fn load_resharding_state(
+        collection_path: &Path,
+    ) -> CollectionResult<SaveOnDisk<Option<resharding::State>>> {
+        let resharding_state_file = Self::resharding_state_file(collection_path);
+        let resharding_state = SaveOnDisk::load_or_init(resharding_state_file)?;
+        Ok(resharding_state)
+    }
+
     /// Check if stored version have consequent version.
     /// If major version is different, then it is not compatible.
     /// If the difference in consecutive versions is greater than 1 in patch,

commit 1d724579dfd6ed5adeda31429bab5821cab5af30
Author: xzfc <5121426+xzfc@users.noreply.github.com>
Date:   Thu May 16 06:47:47 2024 +0000

    InvertedIndexImmutableRam and index migrations (#4220)
    
    * Move StorageVersion from segment crate to common/io
    
    * Refine StorageVersion API
    
    * Move methods from SparseVectorDataConfig to enum SparseIndexType
    
    * Introduce InvertedIndexImmutableRam
    
    * Add migrate
    
    * Don't migrate

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 076e9e859..828d9803e 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -16,7 +16,7 @@ use std::time::Duration;
 
 use common::cpu::CpuBudget;
 use common::types::TelemetryDetail;
-use segment::common::version::StorageVersion;
+use io::storage_version::StorageVersion;
 use segment::types::ShardKey;
 use semver::Version;
 use tokio::runtime::Handle;
@@ -180,12 +180,9 @@ impl Collection {
         let start_time = std::time::Instant::now();
         let stored_version = CollectionVersion::load(path)
             .expect("Can't read collection version")
-            .parse()
-            .expect("Failed to parse stored collection version as semver");
+            .expect("Collection version is not found");
 
-        let app_version: Version = CollectionVersion::current()
-            .parse()
-            .expect("Failed to parse current collection version as semver");
+        let app_version = CollectionVersion::current();
 
         if stored_version > app_version {
             panic!("Collection version is greater than application version");
@@ -703,7 +700,7 @@ impl Collection {
 struct CollectionVersion;
 
 impl StorageVersion for CollectionVersion {
-    fn current() -> String {
-        env!("CARGO_PKG_VERSION").to_string()
+    fn current_raw() -> &'static str {
+        env!("CARGO_PKG_VERSION")
     }
 }

commit 572bc7d51e20dd0eb40d5cb0563e1087073a566a
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Fri May 17 16:58:32 2024 +0200

    Add `ReshardingOperation::Start` consensus message (#4238)
    
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 828d9803e..f9a876ff1 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -22,6 +22,7 @@ use semver::Version;
 use tokio::runtime::Handle;
 use tokio::sync::{Mutex, RwLock, RwLockWriteGuard};
 
+use self::resharding::ReshardingState;
 use crate::collection::payload_index_schema::PayloadIndexSchema;
 use crate::collection_state::{ShardInfo, State};
 use crate::common::is_ready::IsReady;
@@ -52,7 +53,7 @@ pub struct Collection {
     pub(crate) collection_config: Arc<RwLock<CollectionConfig>>,
     pub(crate) shared_storage_config: Arc<SharedStorageConfig>,
     pub(crate) payload_index_schema: SaveOnDisk<PayloadIndexSchema>,
-    resharding_state: SaveOnDisk<Option<resharding::State>>,
+    resharding_state: SaveOnDisk<Option<ReshardingState>>,
     this_peer_id: PeerId,
     path: PathBuf,
     snapshots_path: PathBuf,
@@ -101,7 +102,7 @@ impl Collection {
     ) -> Result<Self, CollectionError> {
         let start_time = std::time::Instant::now();
 
-        let mut shard_holder = ShardHolder::new(path)?;
+        let mut shard_holder = ShardHolder::new(path, None)?;
 
         let shared_collection_config = Arc::new(RwLock::new(collection_config.clone()));
         for (shard_id, mut peers) in shard_distribution.shards {
@@ -208,7 +209,14 @@ impl Collection {
         });
         collection_config.validate_and_warn();
 
-        let mut shard_holder = ShardHolder::new(path).expect("Can not create shard holder");
+        let resharding_state = Self::load_resharding_state(path)
+            .expect("Can't load or initialize resharding progress");
+
+        let mut shard_holder = ShardHolder::new(
+            path,
+            resharding_state.read().as_ref().map(|state| state.shard_id),
+        )
+        .expect("Can not create shard holder");
 
         let shared_collection_config = Arc::new(RwLock::new(collection_config.clone()));
 
@@ -233,9 +241,6 @@ impl Collection {
         let payload_index_schema = Self::load_payload_index_schema(path)
             .expect("Can't load or initialize payload index schema");
 
-        let resharding_state = Self::load_resharding_state(path)
-            .expect("Can't load or initialize resharding progress");
-
         Self {
             id: collection_id.clone(),
             shards_holder: locked_shard_holder,
@@ -266,7 +271,7 @@ impl Collection {
 
     fn load_resharding_state(
         collection_path: &Path,
-    ) -> CollectionResult<SaveOnDisk<Option<resharding::State>>> {
+    ) -> CollectionResult<SaveOnDisk<Option<ReshardingState>>> {
         let resharding_state_file = Self::resharding_state_file(collection_path);
         let resharding_state = SaveOnDisk::load_or_init(resharding_state_file)?;
         Ok(resharding_state)
@@ -650,6 +655,49 @@ impl Collection {
         Ok(())
     }
 
+    pub async fn start_resharding(
+        &self,
+        peer_id: PeerId,
+        shard_id: ShardId,
+        shard_key: Option<ShardKey>,
+    ) -> CollectionResult<()> {
+        // TODO(resharding): Improve error handling?
+
+        let mut shard_holder = self.shards_holder.write().await;
+
+        if self.resharding_state.read().is_some() {
+            return Err(CollectionError::bad_request(format!(
+                "resharding of collection {} is already in progress",
+                self.id
+            )));
+        }
+
+        if shard_holder.get_shard(&shard_id).is_some() {
+            return Err(CollectionError::bad_shard_selection(format!(
+                "shard {shard_id} already exists in collection {}",
+                self.id
+            )));
+        }
+
+        let replica_set = self
+            .create_replica_set(shard_id, &[peer_id], Some(ReplicaState::Resharding))
+            .await?;
+
+        shard_holder.start_resharding(shard_id, replica_set, shard_key.clone())?;
+
+        self.resharding_state.write(|state| {
+            debug_assert!(
+                state.is_none(),
+                "resharding of collection {} is already in progress",
+                self.id
+            );
+
+            *state = Some(ReshardingState::new(peer_id, shard_id, shard_key));
+        })?;
+
+        Ok(())
+    }
+
     pub async fn get_telemetry_data(&self, detail: TelemetryDetail) -> CollectionTelemetry {
         let (shards_telemetry, transfers) = {
             let mut shards_telemetry = Vec::new();

commit 67614614235069e8fdb7c2fad569a2db3bcb5d7e
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Fri May 17 21:17:09 2024 +0200

    Add `StartResharding` operation to `update_collection_cluster` REST API (#4239)

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index f9a876ff1..1c9f5377a 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -1,7 +1,7 @@
 mod collection_ops;
 pub mod payload_index_schema;
 mod point_ops;
-mod resharding;
+pub mod resharding;
 mod search;
 mod shard_transfer;
 mod sharding_keys;
@@ -471,6 +471,7 @@ impl Collection {
                     (*shard_id, shard_info)
                 })
                 .collect(),
+            resharding: self.resharding_state.read().clone(),
             transfers,
             shards_key_mapping: shards_holder.get_shard_key_to_ids_mapping(),
             payload_index_schema: self.payload_index_schema.read().clone(),

commit 159b9f3002560c14945d727eedf650c4d80fd287
Author: Tim Visée <tim+github@visee.me>
Date:   Wed May 22 21:52:11 2024 +0200

    Fix missing format macro in recovery point function (#4302)

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 1c9f5377a..c9251666b 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -433,7 +433,7 @@ impl Collection {
         let shard = shard_holder_read.get_shard(&shard_id);
         let Some(replica_set) = shard else {
             return Err(CollectionError::NotFound {
-                what: "Shard {shard_id}".into(),
+                what: format!("Shard {shard_id}"),
             });
         };
 

commit fff19dd999cb4f2e978f7cc3b09df5ce182820b3
Author: Tim Visée <tim+github@visee.me>
Date:   Thu May 23 09:22:12 2024 +0200

    Add shard transfer method for resharding (#4217)
    
    * Add stubs for new shard transfer method used in resharding
    
    * Set correct shard replica set state when doing resharding transfer
    
    * Only transfer migrated points on resharding transfer
    
    * Use new resharding type, more convenient moved points check
    
    * Allow to specify different target shard for shard transfers
    
    * Add strict validation to target shard field, only different shard for resharding
    
    * Simplify if-condition
    
    * While doing resharding transfer, take the hashring only once
    
    * Hide resharding transfer method by default
    
    * On resharding transfer validation, the shards must have the same key
    
    * Restructure shard transfer validation logic
    
    * Use correct shard key in resharding transfer
    
    * Update shard transfer validation documentation
    
    * Share transfer batch size constant
    
    * Update lib/collection/src/hash_ring.rs
    
    Co-authored-by: Roman Titov <ffuugoo@users.noreply.github.com>
    
    * Use correct shard IDs when driving shard transfer
    
    ---------
    
    Co-authored-by: Roman Titov <ffuugoo@users.noreply.github.com>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index c9251666b..9f4d11651 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -610,6 +610,7 @@ impl Collection {
                     from: replica_id,
                     to: *this_peer_id,
                     shard_id,
+                    to_shard_id: None,
                     sync: true,
                     // For automatic shard transfers, always select some default method from this point on
                     method: Some(shard_transfer_method),

commit 95c7c877711b6342d7f9044a96422908c3378412
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Tue May 28 10:36:41 2024 +0200

    Add `ReshardingOperation::Abort` consensus message (#4290)
    
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 9f4d11651..ede8e56d5 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -657,6 +657,10 @@ impl Collection {
         Ok(())
     }
 
+    pub fn resharding_state(&self) -> Option<ReshardingState> {
+        self.resharding_state.read().clone()
+    }
+
     pub async fn start_resharding(
         &self,
         peer_id: PeerId,
@@ -667,9 +671,9 @@ impl Collection {
 
         let mut shard_holder = self.shards_holder.write().await;
 
-        if self.resharding_state.read().is_some() {
+        if let Some(state) = self.resharding_state.read().deref() {
             return Err(CollectionError::bad_request(format!(
-                "resharding of collection {} is already in progress",
+                "resharding of collection {} is already in progress: {state:?}",
                 self.id
             )));
         }
@@ -690,7 +694,7 @@ impl Collection {
         self.resharding_state.write(|state| {
             debug_assert!(
                 state.is_none(),
-                "resharding of collection {} is already in progress",
+                "resharding of collection {} is already in progress: {state:?}",
                 self.id
             );
 
@@ -700,6 +704,58 @@ impl Collection {
         Ok(())
     }
 
+    pub async fn abort_resharding(
+        &self,
+        peer_id: PeerId,
+        shard_id: ShardId,
+        shard_key: Option<ShardKey>,
+    ) -> CollectionResult<()> {
+        let mut shard_holder = self.shards_holder.write().await;
+
+        let is_in_progress = if let Some(state) = self.resharding_state.read().deref() {
+            let is_in_progress = state.peer_id == peer_id
+                && state.shard_id == shard_id
+                && state.shard_key == shard_key;
+
+            if !is_in_progress {
+                return Err(CollectionError::bad_request(format!(
+                    "resharding of collection {} is not in progress",
+                    self.id,
+                )));
+            }
+
+            is_in_progress
+        } else {
+            log::warn!(
+                "aborting resharding of collection {} ({peer_id}/{shard_id}/{shard_key:?}),\
+                 but resharding is not in progress",
+                self.id,
+            );
+
+            false
+        };
+
+        if shard_holder.get_shard(&shard_id).is_none() {
+            log::warn!(
+                "aborting resharding of collection {} ({peer_id}/{shard_id}/{shard_key:?}), \
+                 but shard {shard_id} does not exist in collection",
+                self.id,
+            );
+        }
+
+        // TODO(resharding): Contextualize errors? 🤔
+        shard_holder
+            .abort_resharding(shard_id, peer_id, shard_key.clone(), is_in_progress)
+            .await?;
+
+        // TODO(resharding): Contextualize errors? 🤔
+        self.resharding_state.write(|state| {
+            *state = None;
+        })?;
+
+        Ok(())
+    }
+
     pub async fn get_telemetry_data(&self, detail: TelemetryDetail) -> CollectionTelemetry {
         let (shards_telemetry, transfers) = {
             let mut shards_telemetry = Vec::new();

commit 5c0ba10c92375125a47e1f318e6701d1a5b0686a
Author: Tim Visée <tim+github@visee.me>
Date:   Wed May 29 10:25:36 2024 +0200

    Move resharding state into shard holder (#4346)
    
    * Move resharding state into shard holder
    
    * Update shard state inside shard holder
    
    * Add resharding key struct to combine peer id, shard id and shard key

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index ede8e56d5..0b6894f25 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -35,6 +35,7 @@ use crate::shards::collection_shard_distribution::CollectionShardDistribution;
 use crate::shards::local_shard::clock_map::RecoveryPoint;
 use crate::shards::replica_set::ReplicaState::{Active, Dead, Initializing, Listener};
 use crate::shards::replica_set::{ChangePeerState, ReplicaState, ShardReplicaSet};
+use crate::shards::resharding::ReshardingKey;
 use crate::shards::shard::{PeerId, ShardId};
 use crate::shards::shard_holder::{shard_not_found_error, LockedShardHolder, ShardHolder};
 use crate::shards::transfer::helpers::check_transfer_conflicts_strict;
@@ -43,8 +44,6 @@ use crate::shards::transfer::{ShardTransfer, ShardTransferMethod};
 use crate::shards::{replica_set, CollectionId};
 use crate::telemetry::CollectionTelemetry;
 
-const RESHARDING_STATE_FILE: &str = "resharding_state.json";
-
 /// Collection's data is split into several shards.
 #[allow(dead_code)]
 pub struct Collection {
@@ -53,7 +52,6 @@ pub struct Collection {
     pub(crate) collection_config: Arc<RwLock<CollectionConfig>>,
     pub(crate) shared_storage_config: Arc<SharedStorageConfig>,
     pub(crate) payload_index_schema: SaveOnDisk<PayloadIndexSchema>,
-    resharding_state: SaveOnDisk<Option<ReshardingState>>,
     this_peer_id: PeerId,
     path: PathBuf,
     snapshots_path: PathBuf,
@@ -102,7 +100,7 @@ impl Collection {
     ) -> Result<Self, CollectionError> {
         let start_time = std::time::Instant::now();
 
-        let mut shard_holder = ShardHolder::new(path, None)?;
+        let mut shard_holder = ShardHolder::new(path)?;
 
         let shared_collection_config = Arc::new(RwLock::new(collection_config.clone()));
         for (shard_id, mut peers) in shard_distribution.shards {
@@ -137,7 +135,6 @@ impl Collection {
         collection_config.save(path)?;
 
         let payload_index_schema = Self::load_payload_index_schema(path)?;
-        let resharding_state = Self::load_resharding_state(path)?;
 
         Ok(Self {
             id: name.clone(),
@@ -145,7 +142,6 @@ impl Collection {
             collection_config: shared_collection_config,
             payload_index_schema,
             shared_storage_config,
-            resharding_state,
             this_peer_id,
             path: path.to_owned(),
             snapshots_path: snapshots_path.to_owned(),
@@ -209,14 +205,7 @@ impl Collection {
         });
         collection_config.validate_and_warn();
 
-        let resharding_state = Self::load_resharding_state(path)
-            .expect("Can't load or initialize resharding progress");
-
-        let mut shard_holder = ShardHolder::new(
-            path,
-            resharding_state.read().as_ref().map(|state| state.shard_id),
-        )
-        .expect("Can not create shard holder");
+        let mut shard_holder = ShardHolder::new(path).expect("Can not create shard holder");
 
         let shared_collection_config = Arc::new(RwLock::new(collection_config.clone()));
 
@@ -247,7 +236,6 @@ impl Collection {
             collection_config: shared_collection_config,
             payload_index_schema,
             shared_storage_config,
-            resharding_state,
             this_peer_id,
             path: path.to_owned(),
             snapshots_path: snapshots_path.to_owned(),
@@ -265,18 +253,6 @@ impl Collection {
         }
     }
 
-    fn resharding_state_file(collection_path: &Path) -> PathBuf {
-        collection_path.join(RESHARDING_STATE_FILE)
-    }
-
-    fn load_resharding_state(
-        collection_path: &Path,
-    ) -> CollectionResult<SaveOnDisk<Option<ReshardingState>>> {
-        let resharding_state_file = Self::resharding_state_file(collection_path);
-        let resharding_state = SaveOnDisk::load_or_init(resharding_state_file)?;
-        Ok(resharding_state)
-    }
-
     /// Check if stored version have consequent version.
     /// If major version is different, then it is not compatible.
     /// If the difference in consecutive versions is greater than 1 in patch,
@@ -460,6 +436,7 @@ impl Collection {
     pub async fn state(&self) -> State {
         let shards_holder = self.shards_holder.read().await;
         let transfers = shards_holder.shard_transfers.read().clone();
+        let resharding = shards_holder.resharding_state.read().clone();
         State {
             config: self.collection_config.read().await.clone(),
             shards: shards_holder
@@ -471,7 +448,7 @@ impl Collection {
                     (*shard_id, shard_info)
                 })
                 .collect(),
-            resharding: self.resharding_state.read().clone(),
+            resharding,
             transfers,
             shards_key_mapping: shards_holder.get_shard_key_to_ids_mapping(),
             payload_index_schema: self.payload_index_schema.read().clone(),
@@ -657,65 +634,58 @@ impl Collection {
         Ok(())
     }
 
-    pub fn resharding_state(&self) -> Option<ReshardingState> {
-        self.resharding_state.read().clone()
+    pub async fn resharding_state(&self) -> Option<ReshardingState> {
+        self.shards_holder
+            .read()
+            .await
+            .resharding_state
+            .read()
+            .clone()
     }
 
-    pub async fn start_resharding(
-        &self,
-        peer_id: PeerId,
-        shard_id: ShardId,
-        shard_key: Option<ShardKey>,
-    ) -> CollectionResult<()> {
+    pub async fn start_resharding(&self, reshard: ReshardingKey) -> CollectionResult<()> {
         // TODO(resharding): Improve error handling?
 
         let mut shard_holder = self.shards_holder.write().await;
 
-        if let Some(state) = self.resharding_state.read().deref() {
+        if let Some(state) = shard_holder.resharding_state.read().deref() {
             return Err(CollectionError::bad_request(format!(
                 "resharding of collection {} is already in progress: {state:?}",
-                self.id
+                self.id,
             )));
         }
 
-        if shard_holder.get_shard(&shard_id).is_some() {
+        if shard_holder.get_shard(&reshard.shard_id).is_some() {
             return Err(CollectionError::bad_shard_selection(format!(
-                "shard {shard_id} already exists in collection {}",
-                self.id
+                "shard {} already exists in collection {}",
+                reshard.shard_id, self.id,
             )));
         }
 
         let replica_set = self
-            .create_replica_set(shard_id, &[peer_id], Some(ReplicaState::Resharding))
+            .create_replica_set(
+                reshard.shard_id,
+                &[reshard.peer_id],
+                Some(ReplicaState::Resharding),
+            )
             .await?;
 
-        shard_holder.start_resharding(shard_id, replica_set, shard_key.clone())?;
-
-        self.resharding_state.write(|state| {
-            debug_assert!(
-                state.is_none(),
-                "resharding of collection {} is already in progress: {state:?}",
-                self.id
-            );
-
-            *state = Some(ReshardingState::new(peer_id, shard_id, shard_key));
-        })?;
+        shard_holder.start_resharding(reshard, replica_set)?;
 
         Ok(())
     }
 
-    pub async fn abort_resharding(
-        &self,
-        peer_id: PeerId,
-        shard_id: ShardId,
-        shard_key: Option<ShardKey>,
-    ) -> CollectionResult<()> {
+    pub async fn abort_resharding(&self, reshard: ReshardingKey) -> CollectionResult<()> {
+        let ReshardingKey {
+            peer_id,
+            shard_id,
+            ref shard_key,
+        } = reshard;
+
         let mut shard_holder = self.shards_holder.write().await;
 
-        let is_in_progress = if let Some(state) = self.resharding_state.read().deref() {
-            let is_in_progress = state.peer_id == peer_id
-                && state.shard_id == shard_id
-                && state.shard_key == shard_key;
+        let is_in_progress = if let Some(state) = shard_holder.resharding_state.read().deref() {
+            let is_in_progress = state.key() == reshard;
 
             if !is_in_progress {
                 return Err(CollectionError::bad_request(format!(
@@ -735,7 +705,7 @@ impl Collection {
             false
         };
 
-        if shard_holder.get_shard(&shard_id).is_none() {
+        if shard_holder.get_shard(&reshard.shard_id).is_none() {
             log::warn!(
                 "aborting resharding of collection {} ({peer_id}/{shard_id}/{shard_key:?}), \
                  but shard {shard_id} does not exist in collection",
@@ -745,14 +715,9 @@ impl Collection {
 
         // TODO(resharding): Contextualize errors? 🤔
         shard_holder
-            .abort_resharding(shard_id, peer_id, shard_key.clone(), is_in_progress)
+            .abort_resharding(reshard, is_in_progress)
             .await?;
 
-        // TODO(resharding): Contextualize errors? 🤔
-        self.resharding_state.write(|state| {
-            *state = None;
-        })?;
-
         Ok(())
     }
 

commit c36568d8e4bf660a3749b1b4787edab10b609d27
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Wed May 29 13:13:05 2024 +0200

    Resharding code cleanup (#4349)
    
    * Move `ReshardingState` from `collection/resharding` to `shards/resharding`
    
    * Sort `ReshardingKey` derives
    
    * Add `ReshardingState::matches` method
    
    * Implement `fmt::Display` for `ReshardingKey`
    
    * Misc resharding cleanup

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 0b6894f25..539e6e1a6 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -1,7 +1,6 @@
 mod collection_ops;
 pub mod payload_index_schema;
 mod point_ops;
-pub mod resharding;
 mod search;
 mod shard_transfer;
 mod sharding_keys;
@@ -22,7 +21,6 @@ use semver::Version;
 use tokio::runtime::Handle;
 use tokio::sync::{Mutex, RwLock, RwLockWriteGuard};
 
-use self::resharding::ReshardingState;
 use crate::collection::payload_index_schema::PayloadIndexSchema;
 use crate::collection_state::{ShardInfo, State};
 use crate::common::is_ready::IsReady;
@@ -35,7 +33,7 @@ use crate::shards::collection_shard_distribution::CollectionShardDistribution;
 use crate::shards::local_shard::clock_map::RecoveryPoint;
 use crate::shards::replica_set::ReplicaState::{Active, Dead, Initializing, Listener};
 use crate::shards::replica_set::{ChangePeerState, ReplicaState, ShardReplicaSet};
-use crate::shards::resharding::ReshardingKey;
+use crate::shards::resharding::{ReshardingKey, ReshardingState};
 use crate::shards::shard::{PeerId, ShardId};
 use crate::shards::shard_holder::{shard_not_found_error, LockedShardHolder, ShardHolder};
 use crate::shards::transfer::helpers::check_transfer_conflicts_strict;
@@ -676,16 +674,10 @@ impl Collection {
     }
 
     pub async fn abort_resharding(&self, reshard: ReshardingKey) -> CollectionResult<()> {
-        let ReshardingKey {
-            peer_id,
-            shard_id,
-            ref shard_key,
-        } = reshard;
-
         let mut shard_holder = self.shards_holder.write().await;
 
         let is_in_progress = if let Some(state) = shard_holder.resharding_state.read().deref() {
-            let is_in_progress = state.key() == reshard;
+            let is_in_progress = state.matches(&reshard);
 
             if !is_in_progress {
                 return Err(CollectionError::bad_request(format!(
@@ -697,7 +689,7 @@ impl Collection {
             is_in_progress
         } else {
             log::warn!(
-                "aborting resharding of collection {} ({peer_id}/{shard_id}/{shard_key:?}),\
+                "aborting resharding of collection {} ({reshard}),\
                  but resharding is not in progress",
                 self.id,
             );
@@ -707,9 +699,10 @@ impl Collection {
 
         if shard_holder.get_shard(&reshard.shard_id).is_none() {
             log::warn!(
-                "aborting resharding of collection {} ({peer_id}/{shard_id}/{shard_key:?}), \
-                 but shard {shard_id} does not exist in collection",
+                "aborting resharding of collection {} ({reshard}), \
+                 but shard {} does not exist in collection",
                 self.id,
+                reshard.shard_id
             );
         }
 

commit cd6fa6948474010bdb81f82c402a33d80ecf00ee
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Wed May 29 16:58:08 2024 +0200

    Refactor and cleanup resharding consensus operations (#4351)
    
    * Refactor and cleanup resharding consensus operations
    
    * Minor tweaks and cleanup
    
    * Remove `collection/resharding.rs`

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 539e6e1a6..0e86f395d 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -642,23 +642,9 @@ impl Collection {
     }
 
     pub async fn start_resharding(&self, reshard: ReshardingKey) -> CollectionResult<()> {
-        // TODO(resharding): Improve error handling?
-
         let mut shard_holder = self.shards_holder.write().await;
 
-        if let Some(state) = shard_holder.resharding_state.read().deref() {
-            return Err(CollectionError::bad_request(format!(
-                "resharding of collection {} is already in progress: {state:?}",
-                self.id,
-            )));
-        }
-
-        if shard_holder.get_shard(&reshard.shard_id).is_some() {
-            return Err(CollectionError::bad_shard_selection(format!(
-                "shard {} already exists in collection {}",
-                reshard.shard_id, self.id,
-            )));
-        }
+        shard_holder.check_start_resharding(&reshard)?;
 
         let replica_set = self
             .create_replica_set(
@@ -668,50 +654,17 @@ impl Collection {
             )
             .await?;
 
-        shard_holder.start_resharding(reshard, replica_set)?;
+        shard_holder.start_resharding_unchecked(reshard, replica_set)?;
 
         Ok(())
     }
 
     pub async fn abort_resharding(&self, reshard: ReshardingKey) -> CollectionResult<()> {
-        let mut shard_holder = self.shards_holder.write().await;
-
-        let is_in_progress = if let Some(state) = shard_holder.resharding_state.read().deref() {
-            let is_in_progress = state.matches(&reshard);
-
-            if !is_in_progress {
-                return Err(CollectionError::bad_request(format!(
-                    "resharding of collection {} is not in progress",
-                    self.id,
-                )));
-            }
-
-            is_in_progress
-        } else {
-            log::warn!(
-                "aborting resharding of collection {} ({reshard}),\
-                 but resharding is not in progress",
-                self.id,
-            );
-
-            false
-        };
-
-        if shard_holder.get_shard(&reshard.shard_id).is_none() {
-            log::warn!(
-                "aborting resharding of collection {} ({reshard}), \
-                 but shard {} does not exist in collection",
-                self.id,
-                reshard.shard_id
-            );
-        }
-
-        // TODO(resharding): Contextualize errors? 🤔
-        shard_holder
-            .abort_resharding(reshard, is_in_progress)
-            .await?;
-
-        Ok(())
+        self.shards_holder
+            .write()
+            .await
+            .abort_resharding(reshard)
+            .await
     }
 
     pub async fn get_telemetry_data(&self, detail: TelemetryDetail) -> CollectionTelemetry {

commit 504ee0f630e9e768c1e605e04c893082bd4c18c7
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Thu May 30 16:30:39 2024 +0200

    Send updates to the new shard during resharding (#4256)

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 0e86f395d..425d168be 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -333,16 +333,15 @@ impl Collection {
             replica_set.peer_state(&peer_id),
         );
 
+        let current_state = replica_set.peer_state(&peer_id);
+
         // Validation:
         // 0. Check that `from_state` matches current state
 
-        if from_state.is_some() {
-            let current_state = replica_set.peer_state(&peer_id);
-            if current_state != from_state {
-                return Err(CollectionError::bad_input(format!(
-                    "Replica {peer_id} of shard {shard_id} has state {current_state:?}, but expected {from_state:?}"
-                )));
-            }
+        if from_state.is_some() && current_state != from_state {
+            return Err(CollectionError::bad_input(format!(
+                "Replica {peer_id} of shard {shard_id} has state {current_state:?}, but expected {from_state:?}"
+            )));
         }
 
         // 1. Do not deactivate the last active replica
@@ -366,11 +365,34 @@ impl Collection {
             }
         }
 
+        // TODO(resharding): 🤔
+        if current_state == Some(ReplicaState::Resharding) && state == ReplicaState::Dead {
+            let shard_key = shard_holder
+                .get_shard_id_to_key_mapping()
+                .get(&shard_id)
+                .cloned();
+
+            drop(shard_holder);
+
+            self.abort_resharding(ReshardingKey {
+                peer_id,
+                shard_id,
+                shard_key,
+            })
+            .await?;
+
+            // TODO(resharding): Abort all resharding transfers!?
+
+            return Ok(());
+        }
+
         replica_set
             .ensure_replica_with_state(&peer_id, state)
             .await?;
 
         if state == ReplicaState::Dead {
+            // TODO(resharding): Abort all resharding transfers!?
+
             // Terminate transfer if source or target replicas are now dead
             let related_transfers = shard_holder.get_related_transfers(&shard_id, &peer_id);
             for transfer in related_transfers {

commit c9b1e50d7215f99b9f15a0936e0305cc4b17a38c
Author: Tim Visée <tim+github@visee.me>
Date:   Thu May 30 16:37:10 2024 +0200

    Initial stubs for resharding driver (#4355)
    
    * Initial stubs for resharding driver
    
    * Add basic steps in resharding driver
    
    * Link Notion document
    
    * Clean up start function
    
    * Describe difference between reshard key and task
    
    * Report resharding abort/finish to consensus
    
    * Add driver methods for remaining stages
    
    * Fix typo
    
    * Only start driver on the peer that is responsible for driving it
    
    * Remove ReshardTask, repurpose ReshardKey for that

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 425d168be..0da3b2ceb 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -1,6 +1,7 @@
 mod collection_ops;
 pub mod payload_index_schema;
 mod point_ops;
+mod resharding;
 mod search;
 mod shard_transfer;
 mod sharding_keys;
@@ -33,7 +34,7 @@ use crate::shards::collection_shard_distribution::CollectionShardDistribution;
 use crate::shards::local_shard::clock_map::RecoveryPoint;
 use crate::shards::replica_set::ReplicaState::{Active, Dead, Initializing, Listener};
 use crate::shards::replica_set::{ChangePeerState, ReplicaState, ShardReplicaSet};
-use crate::shards::resharding::{ReshardingKey, ReshardingState};
+use crate::shards::resharding::tasks_pool::ReshardTasksPool;
 use crate::shards::shard::{PeerId, ShardId};
 use crate::shards::shard_holder::{shard_not_found_error, LockedShardHolder, ShardHolder};
 use crate::shards::transfer::helpers::check_transfer_conflicts_strict;
@@ -55,6 +56,7 @@ pub struct Collection {
     snapshots_path: PathBuf,
     channel_service: ChannelService,
     transfer_tasks: Mutex<TransferTasksPool>,
+    reshard_tasks: Mutex<ReshardTasksPool>,
     request_shard_transfer_cb: RequestShardTransfer,
     notify_peer_failure_cb: ChangePeerState,
     abort_shard_transfer_cb: replica_set::AbortShardTransfer,
@@ -145,6 +147,7 @@ impl Collection {
             snapshots_path: snapshots_path.to_owned(),
             channel_service,
             transfer_tasks: Mutex::new(TransferTasksPool::new(name.clone())),
+            reshard_tasks: Mutex::new(ReshardTasksPool::new(name)),
             request_shard_transfer_cb: request_shard_transfer.clone(),
             notify_peer_failure_cb: on_replica_failure.clone(),
             abort_shard_transfer_cb: abort_shard_transfer,
@@ -239,6 +242,7 @@ impl Collection {
             snapshots_path: snapshots_path.to_owned(),
             channel_service,
             transfer_tasks: Mutex::new(TransferTasksPool::new(collection_id.clone())),
+            reshard_tasks: Mutex::new(ReshardTasksPool::new(collection_id)),
             request_shard_transfer_cb: request_shard_transfer.clone(),
             notify_peer_failure_cb: on_replica_failure,
             abort_shard_transfer_cb: abort_shard_transfer,
@@ -654,41 +658,6 @@ impl Collection {
         Ok(())
     }
 
-    pub async fn resharding_state(&self) -> Option<ReshardingState> {
-        self.shards_holder
-            .read()
-            .await
-            .resharding_state
-            .read()
-            .clone()
-    }
-
-    pub async fn start_resharding(&self, reshard: ReshardingKey) -> CollectionResult<()> {
-        let mut shard_holder = self.shards_holder.write().await;
-
-        shard_holder.check_start_resharding(&reshard)?;
-
-        let replica_set = self
-            .create_replica_set(
-                reshard.shard_id,
-                &[reshard.peer_id],
-                Some(ReplicaState::Resharding),
-            )
-            .await?;
-
-        shard_holder.start_resharding_unchecked(reshard, replica_set)?;
-
-        Ok(())
-    }
-
-    pub async fn abort_resharding(&self, reshard: ReshardingKey) -> CollectionResult<()> {
-        self.shards_holder
-            .write()
-            .await
-            .abort_resharding(reshard)
-            .await
-    }
-
     pub async fn get_telemetry_data(&self, detail: TelemetryDetail) -> CollectionTelemetry {
         let (shards_telemetry, transfers) = {
             let mut shards_telemetry = Vec::new();

commit 841ae05c487a54fac26dedf6608eb057e0d3d5b6
Author: Tim Visée <tim+github@visee.me>
Date:   Thu May 30 17:04:19 2024 +0200

    Fix compilation error on dev caused by cross merge (#4358)

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 0da3b2ceb..521bd2c9e 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -35,6 +35,7 @@ use crate::shards::local_shard::clock_map::RecoveryPoint;
 use crate::shards::replica_set::ReplicaState::{Active, Dead, Initializing, Listener};
 use crate::shards::replica_set::{ChangePeerState, ReplicaState, ShardReplicaSet};
 use crate::shards::resharding::tasks_pool::ReshardTasksPool;
+use crate::shards::resharding::ReshardKey;
 use crate::shards::shard::{PeerId, ShardId};
 use crate::shards::shard_holder::{shard_not_found_error, LockedShardHolder, ShardHolder};
 use crate::shards::transfer::helpers::check_transfer_conflicts_strict;
@@ -378,7 +379,7 @@ impl Collection {
 
             drop(shard_holder);
 
-            self.abort_resharding(ReshardingKey {
+            self.abort_resharding(ReshardKey {
                 peer_id,
                 shard_id,
                 shard_key,

commit f7a7e6ff128b7134e0cdf9804494981a7880ccee
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Fri May 31 09:02:44 2024 +0200

    Add optimizer_overwrite config option (#4317)
    
    * add config option
    
    * add optimizers_config to optimizer calls
    
    * also add for tests
    
    * add to build_optimizers
    
    * rename function parameter

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 521bd2c9e..4d5cea400 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -26,8 +26,10 @@ use crate::collection::payload_index_schema::PayloadIndexSchema;
 use crate::collection_state::{ShardInfo, State};
 use crate::common::is_ready::IsReady;
 use crate::config::CollectionConfig;
+use crate::operations::config_diff::{DiffConfig, OptimizersConfigDiff};
 use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{CollectionError, CollectionResult, NodeType};
+use crate::optimizers_builder::OptimizersConfig;
 use crate::save_on_disk::SaveOnDisk;
 use crate::shards::channel_service::ChannelService;
 use crate::shards::collection_shard_distribution::CollectionShardDistribution;
@@ -52,6 +54,7 @@ pub struct Collection {
     pub(crate) collection_config: Arc<RwLock<CollectionConfig>>,
     pub(crate) shared_storage_config: Arc<SharedStorageConfig>,
     pub(crate) payload_index_schema: SaveOnDisk<PayloadIndexSchema>,
+    optimizers_overwrite: Option<OptimizersConfigDiff>,
     this_peer_id: PeerId,
     path: PathBuf,
     snapshots_path: PathBuf,
@@ -98,6 +101,7 @@ impl Collection {
         search_runtime: Option<Handle>,
         update_runtime: Option<Handle>,
         optimizer_cpu_budget: CpuBudget,
+        optimizers_overwrite: Option<OptimizersConfigDiff>,
     ) -> Result<Self, CollectionError> {
         let start_time = std::time::Instant::now();
 
@@ -107,6 +111,12 @@ impl Collection {
         for (shard_id, mut peers) in shard_distribution.shards {
             let is_local = peers.remove(&this_peer_id);
 
+            let mut effective_optimizers_config = collection_config.optimizer_config.clone();
+            if let Some(optimizers_overwrite) = optimizers_overwrite.clone() {
+                effective_optimizers_config =
+                    optimizers_overwrite.update(&effective_optimizers_config)?;
+            }
+
             let replica_set = ShardReplicaSet::build(
                 shard_id,
                 name.clone(),
@@ -117,6 +127,7 @@ impl Collection {
                 abort_shard_transfer.clone(),
                 path,
                 shared_collection_config.clone(),
+                effective_optimizers_config,
                 shared_storage_config.clone(),
                 channel_service.clone(),
                 update_runtime.clone().unwrap_or_else(Handle::current),
@@ -141,6 +152,7 @@ impl Collection {
             id: name.clone(),
             shards_holder: locked_shard_holder,
             collection_config: shared_collection_config,
+            optimizers_overwrite,
             payload_index_schema,
             shared_storage_config,
             this_peer_id,
@@ -175,6 +187,7 @@ impl Collection {
         search_runtime: Option<Handle>,
         update_runtime: Option<Handle>,
         optimizer_cpu_budget: CpuBudget,
+        optimizers_overwrite: Option<OptimizersConfigDiff>,
     ) -> Self {
         let start_time = std::time::Instant::now();
         let stored_version = CollectionVersion::load(path)
@@ -209,6 +222,14 @@ impl Collection {
 
         let mut shard_holder = ShardHolder::new(path).expect("Can not create shard holder");
 
+        let mut effective_optimizers_config = collection_config.optimizer_config.clone();
+
+        if let Some(optimizers_overwrite) = optimizers_overwrite.clone() {
+            effective_optimizers_config = optimizers_overwrite
+                .update(&effective_optimizers_config)
+                .expect("Can not apply optimizer overwrite");
+        }
+
         let shared_collection_config = Arc::new(RwLock::new(collection_config.clone()));
 
         shard_holder
@@ -216,6 +237,7 @@ impl Collection {
                 path,
                 &collection_id,
                 shared_collection_config.clone(),
+                effective_optimizers_config,
                 shared_storage_config.clone(),
                 channel_service.clone(),
                 on_replica_failure.clone(),
@@ -236,6 +258,7 @@ impl Collection {
             id: collection_id.clone(),
             shards_holder: locked_shard_holder,
             collection_config: shared_collection_config,
+            optimizers_overwrite,
             payload_index_schema,
             shared_storage_config,
             this_peer_id,
@@ -681,6 +704,16 @@ impl Collection {
         }
     }
 
+    pub async fn effective_optimizers_config(&self) -> CollectionResult<OptimizersConfig> {
+        let config = self.collection_config.read().await;
+
+        if let Some(optimizers_overwrite) = self.optimizers_overwrite.clone() {
+            Ok(optimizers_overwrite.update(&config.optimizer_config)?)
+        } else {
+            Ok(config.optimizer_config.clone())
+        }
+    }
+
     pub async fn lock_updates(&self) -> RwLockWriteGuard<()> {
         self.updates_lock.write().await
     }

commit 02e89fe7ae9b6b163bd7c944718b934f685e5baf
Author: Luis Cossío <luis.cossio@qdrant.com>
Date:   Fri May 31 08:56:06 2024 -0400

    universal-query: Impl of `query_internal` in collection (#4331)
    
    * move ScoredPointTies to segment, make inner by reference
    
    * `query_internal` implementation
    
    * remove empty utils mod
    
    * use `then_with`
    
    * Improve readability, remove duplicated code
    
    * refactoring suggestions
    
    * don't collect eagerly
    
    * remove unused import
    
    * dont panic on empty transpose input
    
    ---------
    
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 4d5cea400..b0140a7d7 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -1,6 +1,7 @@
 mod collection_ops;
 pub mod payload_index_schema;
 mod point_ops;
+pub mod query;
 mod resharding;
 mod search;
 mod shard_transfer;

commit 7a3f489655fecd58c78c1f61b21990f39a4c7683
Author: Tim Visée <tim+github@visee.me>
Date:   Tue Jun 18 17:51:13 2024 +0200

    Properly integrate replica set state switching in resharding transfers (#4480)
    
    * Add serde/validate attributes to resharding operations, matching others
    
    * Fix broken comment
    
    * Add debug message for resharding driver entering stages
    
    * Fix shard transfer start setting state of wrong replica for resharding
    
    * Remove obsolete clones
    
    * Add target shard ID to shard key, add relevant gRPC types
    
    * Move target shard ID below source shard ID field
    
    * Rename collection_name to collection_id
    
    * Reformat
    
    * Transferring point batches must merge points in case of resharding
    
    * In resharding state, sync list of peers on start
    
    * Add logic for setting replica set state through consensus dispatcher
    
    * Properly start resharding transfer
    
    * Properly finish resharding transfers, set shard state correctly
    
    * Fix shard transfer initialisation with different target shard
    
    * Fix shard state handling with resharding on all nodes on transfer start
    
    * Don't reset locally disabled state if only existing shard is resharding
    
    * Add important TODOs
    
    * Update OpenAPI and gRPC specification
    
    * Elaborate on some logic in code with comments
    
    * Use user configured shard transfer method for replication
    
    * Add debug assert, on transfer start we should not replace existing shard
    
    * On transfer start, be aware of different sender/receiver local states
    
    This fixes transfers where we might not have a replica on all nodes
    
    * Fix shard transfer not setting cutoff point on target shard
    
    * While resharding, migrate shards in numerical order
    
    * On shard transfer initialisation, ensure transfer targets given shard

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index b0140a7d7..eac8fbcd3 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -543,7 +543,7 @@ impl Collection {
                 None => {
                     log::debug!(
                         "Transfer {:?} does not exist, but not reported as cancelled. Reporting now.",
-                        transfer.key()
+                        transfer.key(),
                     );
                     on_transfer_failure(transfer, self.name(), "transfer task does not exist");
                 }
@@ -551,14 +551,14 @@ impl Collection {
                 Some(TaskResult::Finished) => {
                     log::debug!(
                         "Transfer {:?} is finished successfully, but not reported. Reporting now.",
-                        transfer.key()
+                        transfer.key(),
                     );
                     on_transfer_success(transfer, self.name());
                 }
                 Some(TaskResult::Failed) => {
                     log::debug!(
                         "Transfer {:?} is failed, but not reported as failed. Reporting now.",
-                        transfer.key()
+                        transfer.key(),
                     );
                     on_transfer_failure(transfer, self.name(), "transfer failed");
                 }

commit 63b2801e4fe25fea190e5a4069d6a1d3702a4661
Author: Tim Visée <tim+github@visee.me>
Date:   Fri Jun 21 20:01:05 2024 +0200

    Fix new appendable segments not having payload indices (#4523)
    
    * Propagate payload index schema down to shard replica set + update handler
    
    * Configure payload indices when creating new appendable segment
    
    * When loading segments, make sure applied payload indices match config
    
    * Add test to assert creating new segments with payload index
    
    * Fix unit test because the collection payload schema wasn't updated
    
    * Add test for updating payload index configuration on segment load
    
    * Update test documentation
    
    * Also create payload indices in temporary snapshot segment
    
    * do not delete extra payload index from segments
    
    * do not delete extra payload index from segments
    
    * fix test
    
    ---------
    
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index eac8fbcd3..0c698b577 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -54,7 +54,7 @@ pub struct Collection {
     pub(crate) shards_holder: Arc<LockedShardHolder>,
     pub(crate) collection_config: Arc<RwLock<CollectionConfig>>,
     pub(crate) shared_storage_config: Arc<SharedStorageConfig>,
-    pub(crate) payload_index_schema: SaveOnDisk<PayloadIndexSchema>,
+    payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,
     optimizers_overwrite: Option<OptimizersConfigDiff>,
     this_peer_id: PeerId,
     path: PathBuf,
@@ -108,6 +108,8 @@ impl Collection {
 
         let mut shard_holder = ShardHolder::new(path)?;
 
+        let payload_index_schema = Arc::new(Self::load_payload_index_schema(path)?);
+
         let shared_collection_config = Arc::new(RwLock::new(collection_config.clone()));
         for (shard_id, mut peers) in shard_distribution.shards {
             let is_local = peers.remove(&this_peer_id);
@@ -130,6 +132,7 @@ impl Collection {
                 shared_collection_config.clone(),
                 effective_optimizers_config,
                 shared_storage_config.clone(),
+                payload_index_schema.clone(),
                 channel_service.clone(),
                 update_runtime.clone().unwrap_or_else(Handle::current),
                 search_runtime.clone().unwrap_or_else(Handle::current),
@@ -147,8 +150,6 @@ impl Collection {
         CollectionVersion::save(path)?;
         collection_config.save(path)?;
 
-        let payload_index_schema = Self::load_payload_index_schema(path)?;
-
         Ok(Self {
             id: name.clone(),
             shards_holder: locked_shard_holder,
@@ -233,6 +234,11 @@ impl Collection {
 
         let shared_collection_config = Arc::new(RwLock::new(collection_config.clone()));
 
+        let payload_index_schema = Arc::new(
+            Self::load_payload_index_schema(path)
+                .expect("Can't load or initialize payload index schema"),
+        );
+
         shard_holder
             .load_shards(
                 path,
@@ -240,6 +246,7 @@ impl Collection {
                 shared_collection_config.clone(),
                 effective_optimizers_config,
                 shared_storage_config.clone(),
+                payload_index_schema.clone(),
                 channel_service.clone(),
                 on_replica_failure.clone(),
                 abort_shard_transfer.clone(),
@@ -252,9 +259,6 @@ impl Collection {
 
         let locked_shard_holder = Arc::new(LockedShardHolder::new(shard_holder));
 
-        let payload_index_schema = Self::load_payload_index_schema(path)
-            .expect("Can't load or initialize payload index schema");
-
         Self {
             id: collection_id.clone(),
             shards_holder: locked_shard_holder,

commit a06d20fb58a70f369c3a3b40178b726a291e6423
Author: xzfc <5121426+xzfc@users.noreply.github.com>
Date:   Mon Jul 8 07:51:59 2024 +0000

    Remove dead code (#4623)

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 0c698b577..9aea10dee 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -731,10 +731,6 @@ impl Collection {
         self.request_shard_transfer_cb.deref()(shard_transfer)
     }
 
-    pub fn shared_storage_config(&self) -> Arc<SharedStorageConfig> {
-        self.shared_storage_config.clone()
-    }
-
     pub fn snapshots_path(&self) -> &Path {
         &self.snapshots_path
     }

commit 16e436489a5b873d65532e09bc4e46af61142386
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Fri Jul 26 16:00:03 2024 +0200

    Abort resharding when relevant consensus events happen (#4699)
    
    Co-authored-by: Tim Visée <tim+github@visee.me>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 9aea10dee..355f403b4 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -398,7 +398,25 @@ impl Collection {
             }
         }
 
-        // TODO(resharding): 🤔
+        // Abort resharding, if resharding shard is marked as `Dead`.
+        //
+        // This branch should only be triggered, if resharding is currently at
+        // `ReshardStage::MigratingPoints` stage, because resharding shard should be marked as
+        // `Active` when all resharding transfers are successfully completed, and so the check
+        // *right above* this one should be triggered.
+        //
+        // So, if resharding reached `ReshardingStage::ReadHashRingCommitted`, this branch *won't*
+        // be triggered, and in this case, resharding *won't* be cancelled. Though, the update
+        // request should *fail* with "failed to update all replicas of a shard" error.
+        //
+        // If resharding reached `ReshardingStage::WriteHashRingCommitted`, and this branch is
+        // triggered *somehow*, then `Collection::abort_resharding` call should return an error,
+        // so no special handling is needed for `ReshardingStage::WriteHashRingCommitted`.
+        //
+        // TODO(resharding):
+        //
+        // Abort resharding, if resharding shard is (being) marked as `Dead` and resharding is at
+        // `ReshardingStage::ReadHashRingCommitted` stage!? 🤔
         if current_state == Some(ReplicaState::Resharding) && state == ReplicaState::Dead {
             let shard_key = shard_holder
                 .get_shard_id_to_key_mapping()
@@ -407,11 +425,14 @@ impl Collection {
 
             drop(shard_holder);
 
-            self.abort_resharding(ReshardKey {
-                peer_id,
-                shard_id,
-                shard_key,
-            })
+            self.abort_resharding(
+                ReshardKey {
+                    peer_id,
+                    shard_id,
+                    shard_key,
+                },
+                false,
+            )
             .await?;
 
             // TODO(resharding): Abort all resharding transfers!?
@@ -509,6 +530,22 @@ impl Collection {
     }
 
     pub async fn remove_shards_at_peer(&self, peer_id: PeerId) -> CollectionResult<()> {
+        // Abort resharding, if shards are removed from peer driving resharding
+        // (which *usually* means the *peer* is being removed from consensus)
+        let resharding_state = self
+            .resharding_state()
+            .await
+            .filter(|state| state.peer_id == peer_id);
+
+        if let Some(state) = resharding_state {
+            if let Err(err) = self.abort_resharding(state.key(), true).await {
+                log::error!(
+                    "Failed to abort resharding {} while removing peer {peer_id}: {err}",
+                    state.key(),
+                );
+            }
+        }
+
         self.shards_holder
             .read()
             .await

commit 67217002fef66d45617f85ed60cbb7dac1c8412c
Author: Tim Visée <tim+github@visee.me>
Date:   Mon Jul 29 13:38:09 2024 +0200

    Implement resharding down (#4697)
    
    Co-authored-by: Roman Titov <ffuugoo@users.noreply.github.com>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 355f403b4..2ca76e991 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -27,6 +27,7 @@ use crate::collection::payload_index_schema::PayloadIndexSchema;
 use crate::collection_state::{ShardInfo, State};
 use crate::common::is_ready::IsReady;
 use crate::config::CollectionConfig;
+use crate::operations::cluster_ops::ReshardingDirection;
 use crate::operations::config_diff::{DiffConfig, OptimizersConfigDiff};
 use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{CollectionError, CollectionResult, NodeType};
@@ -427,6 +428,8 @@ impl Collection {
 
             self.abort_resharding(
                 ReshardKey {
+                    // Always up when setting resharding replica set state
+                    direction: ReshardingDirection::Up,
                     peer_id,
                     shard_id,
                     shard_key,

commit 9818b5bc3f3c32097ccdbf387d5a56c2b06dfc33
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Fri Aug 2 21:48:25 2024 +0200

    Distance matrix API internals (#4793)
    
    * Distance matrix API internals
    
    * minor code review
    
    * use new Query sample instead of manual samplig + retrieve
    
    * do not crash if the point does not contain the specific vector
    
    * Use iterative approach in the absence of HasVector condition
    
    * add tests

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 2ca76e991..74ba1171d 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -1,4 +1,5 @@
 mod collection_ops;
+pub mod distance_matrix;
 pub mod payload_index_schema;
 mod point_ops;
 pub mod query;

commit 867a26d25999986a4d3c05d1b64eec20082e3bbc
Author: Luis Cossío <luis.cossio@qdrant.com>
Date:   Tue Aug 6 14:45:21 2024 -0400

    Facet in Table of Content (#4817)
    
    * implement facets in ToC
    
    - implement CheckableCollectionOperation for FacetRequest
    - implement facet in Collection
    
    * complete facet of internal service + From<StorageError> refactor
    
    - finalize connecting internal service to table of content
    - make error_to_status a From implementation, refactor accordingly

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 74ba1171d..debfd48db 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -1,5 +1,6 @@
 mod collection_ops;
 pub mod distance_matrix;
+mod facet;
 pub mod payload_index_schema;
 mod point_ops;
 pub mod query;

commit 29f7861d31e4f3d2c02b70c4fe4662cca995fa84
Author: Tim Visée <tim+github@visee.me>
Date:   Tue Aug 27 14:45:52 2024 +0200

    Expose resharding operations for collection in telemetry (#4938)
    
    * Expose resharding operations for collection in telemetry
    
    * Update OpenAPI spec

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index debfd48db..c87b14f7c 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -730,7 +730,7 @@ impl Collection {
     }
 
     pub async fn get_telemetry_data(&self, detail: TelemetryDetail) -> CollectionTelemetry {
-        let (shards_telemetry, transfers) = {
+        let (shards_telemetry, transfers, resharding) = {
             let mut shards_telemetry = Vec::new();
             let shards_holder = self.shards_holder.read().await;
             for shard in shards_holder.all_shards() {
@@ -739,6 +739,7 @@ impl Collection {
             (
                 shards_telemetry,
                 shards_holder.get_shard_transfer_info(&*self.transfer_tasks.lock().await),
+                shards_holder.get_resharding_operations_info(&*self.reshard_tasks.lock().await),
             )
         };
 
@@ -748,6 +749,7 @@ impl Collection {
             config: self.collection_config.read().await.clone(),
             shards: shards_telemetry,
             transfers,
+            resharding,
         }
     }
 

commit 23c46791a8c5b9f77b0c2bff5e967a7119d3c106
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Sat Aug 31 09:38:17 2024 +0200

    Check that peer still exist, when applying set shard replica set operation (#4985)
    
    * Check that peer still exist, when applying set shard replica set operation
    
    * Check that peer *or* replica exist, when applying set shard replica set operation

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index c87b14f7c..03f909d4b 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -372,16 +372,33 @@ impl Collection {
         let current_state = replica_set.peer_state(&peer_id);
 
         // Validation:
-        // 0. Check that `from_state` matches current state
+        //
+        // 1. Check that peer exists in the cluster (peer might *not* exist, if it was removed from
+        //    the cluster right before `SetShardReplicaSet` was proposed)
+        let peer_exists = self
+            .channel_service
+            .id_to_address
+            .read()
+            .contains_key(&peer_id);
+
+        let replica_exists = replica_set.peer_state(&peer_id).is_some();
+
+        if !peer_exists && !replica_exists {
+            return Err(CollectionError::bad_input(format!(
+                "Can't set replica {peer_id}:{shard_id} state to {state:?}, \
+                 because replica {peer_id}:{shard_id} does not exist \
+                 and peer {peer_id} is not part of the cluster"
+            )));
+        }
 
+        // 2. Check that `from_state` matches current state
         if from_state.is_some() && current_state != from_state {
             return Err(CollectionError::bad_input(format!(
                 "Replica {peer_id} of shard {shard_id} has state {current_state:?}, but expected {from_state:?}"
             )));
         }
 
-        // 1. Do not deactivate the last active replica
-
+        // 3. Do not deactivate the last active replica
         if state != ReplicaState::Active {
             let active_replicas: HashSet<_> = replica_set
                 .peers()

commit c672c316229739827cb535c475b864908b25831f
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Fri Sep 6 12:04:49 2024 +0200

    Remove unecessary clippy allow deadcode (#5033)

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 03f909d4b..b118046bf 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -51,7 +51,6 @@ use crate::shards::{replica_set, CollectionId};
 use crate::telemetry::CollectionTelemetry;
 
 /// Collection's data is split into several shards.
-#[allow(dead_code)]
 pub struct Collection {
     pub(crate) id: CollectionId,
     pub(crate) shards_holder: Arc<LockedShardHolder>,

commit 4f4ea491ce14044e36152af95515e4fa0011fea7
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Fri Sep 6 17:39:09 2024 +0200

    Abort resharding when aborting resharding shard transfer (#5020)
    
    * Abort resharding when aborting resharding shard transfer
    
    * fixup! Abort resharding when aborting resharding shard transfer
    
    Fix typo 🙄

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index b118046bf..08f791fdc 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -431,11 +431,6 @@ impl Collection {
         // If resharding reached `ReshardingStage::WriteHashRingCommitted`, and this branch is
         // triggered *somehow*, then `Collection::abort_resharding` call should return an error,
         // so no special handling is needed for `ReshardingStage::WriteHashRingCommitted`.
-        //
-        // TODO(resharding):
-        //
-        // Abort resharding, if resharding shard is (being) marked as `Dead` and resharding is at
-        // `ReshardingStage::ReadHashRingCommitted` stage!? 🤔
         if current_state == Some(ReplicaState::Resharding) && state == ReplicaState::Dead {
             let shard_key = shard_holder
                 .get_shard_id_to_key_mapping()
@@ -470,9 +465,12 @@ impl Collection {
 
             // Terminate transfer if source or target replicas are now dead
             let related_transfers = shard_holder.get_related_transfers(&shard_id, &peer_id);
+
+            // `abort_shard_transfer` locks `shard_holder`!
+            drop(shard_holder);
+
             for transfer in related_transfers {
-                self.abort_shard_transfer(transfer.key(), Some(&shard_holder))
-                    .await?;
+                self.abort_shard_transfer(transfer.key()).await?;
             }
         }
 

commit 9ab060b565b7101dcdc19a5e0dbed6b04b4cd55e
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Wed Sep 11 11:11:05 2024 +0200

    Split operations into `update_all`/`update_existing` during resharding (#4928)
    
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 08f791fdc..4fc837bf8 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -419,18 +419,17 @@ impl Collection {
 
         // Abort resharding, if resharding shard is marked as `Dead`.
         //
-        // This branch should only be triggered, if resharding is currently at
-        // `ReshardStage::MigratingPoints` stage, because resharding shard should be marked as
-        // `Active` when all resharding transfers are successfully completed, and so the check
-        // *right above* this one should be triggered.
+        // This branch should only be triggered, if resharding is currently at `MigratingPoints`
+        // stage, because target shard should be marked as `Active`, when all resharding transfers
+        // are successfully completed, and so the check *right above* this one would be triggered.
         //
-        // So, if resharding reached `ReshardingStage::ReadHashRingCommitted`, this branch *won't*
-        // be triggered, and in this case, resharding *won't* be cancelled. Though, the update
-        // request should *fail* with "failed to update all replicas of a shard" error.
+        // So, if resharding reached `ReadHashRingCommitted`, this branch *won't* be triggered,
+        // and resharding *won't* be cancelled. The update request should *fail* with "failed to
+        // update all replicas of a shard" error.
         //
-        // If resharding reached `ReshardingStage::WriteHashRingCommitted`, and this branch is
-        // triggered *somehow*, then `Collection::abort_resharding` call should return an error,
-        // so no special handling is needed for `ReshardingStage::WriteHashRingCommitted`.
+        // If resharding reached `ReadHashRingCommitted`, and this branch is triggered *somehow*,
+        // then `Collection::abort_resharding` call should return an error, so no special handling
+        // is needed.
         if current_state == Some(ReplicaState::Resharding) && state == ReplicaState::Dead {
             let shard_key = shard_holder
                 .get_shard_id_to_key_mapping()

commit 1589d119fcb383e9bb401efdcd98b9330ffe322d
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Wed Sep 11 20:12:16 2024 +0200

    make resharding_operations optional, so OpenAPI is properly generated (#5060)
    
    * make resharding_operations optional, so OpenAPI is properly generated
    
    * Return None early
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 4fc837bf8..895834ffd 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -752,7 +752,9 @@ impl Collection {
             (
                 shards_telemetry,
                 shards_holder.get_shard_transfer_info(&*self.transfer_tasks.lock().await),
-                shards_holder.get_resharding_operations_info(&*self.reshard_tasks.lock().await),
+                shards_holder
+                    .get_resharding_operations_info(&*self.reshard_tasks.lock().await)
+                    .unwrap_or_default(),
             )
         };
 

commit f61deaf44731e37e311ad615144cae6e72d879b5
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Mon Sep 16 14:37:33 2024 +0200

    abort shard transer on delete of replica + check that we dont delete last active (#5079)

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 895834ffd..b15e165fa 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -11,7 +11,7 @@ mod sharding_keys;
 mod snapshots;
 mod state_management;
 
-use std::collections::{HashMap, HashSet};
+use std::collections::HashMap;
 use std::ops::Deref;
 use std::path::{Path, PathBuf};
 use std::sync::Arc;
@@ -398,23 +398,10 @@ impl Collection {
         }
 
         // 3. Do not deactivate the last active replica
-        if state != ReplicaState::Active {
-            let active_replicas: HashSet<_> = replica_set
-                .peers()
-                .into_iter()
-                .filter_map(|(peer, state)| {
-                    if state == ReplicaState::Active {
-                        Some(peer)
-                    } else {
-                        None
-                    }
-                })
-                .collect();
-            if active_replicas.len() == 1 && active_replicas.contains(&peer_id) {
-                return Err(CollectionError::bad_input(format!(
-                    "Cannot deactivate the last active replica {peer_id} of shard {shard_id}"
-                )));
-            }
+        if state != ReplicaState::Active && replica_set.is_last_active_replica(peer_id) {
+            return Err(CollectionError::bad_input(format!(
+                "Cannot deactivate the last active replica {peer_id} of shard {shard_id}"
+            )));
         }
 
         // Abort resharding, if resharding shard is marked as `Dead`.
@@ -469,7 +456,7 @@ impl Collection {
             drop(shard_holder);
 
             for transfer in related_transfers {
-                self.abort_shard_transfer(transfer.key()).await?;
+                self.abort_shard_transfer(transfer.key(), None).await?;
             }
         }
 

commit 428e09d49b8fcd943427c5d397686ed08cd08337
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Tue Sep 24 17:52:03 2024 +0200

    Trigger optimizers when uploading snapshots (#5140)
    
    * Trigger optimizers when uploading snapshots
    
    * Trigger optimizers through new method when optimizer config is updated
    
    * review remarks
    
    * Add comment on why ignoring channel send errors is fine
    
    * Remove obsolete return value omitting
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index b15e165fa..75cd3ba99 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -784,6 +784,10 @@ impl Collection {
     pub fn shards_holder(&self) -> Arc<LockedShardHolder> {
         self.shards_holder.clone()
     }
+
+    pub async fn trigger_optimizers(&self) {
+        self.shards_holder.read().await.trigger_optimizers().await;
+    }
 }
 
 struct CollectionVersion;

commit 1128ac8ff8b91ff052d85bd63974930bbbfed210
Author: Tim Visée <tim+github@visee.me>
Date:   Thu Oct 31 15:44:49 2024 +0100

    Prefer owned versus referenced usage of PeerId/ShardId, they are Copy (#5344)

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 75cd3ba99..04d2a4cf3 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -329,7 +329,7 @@ impl Collection {
     }
 
     pub async fn contains_shard(&self, shard_id: ShardId) -> bool {
-        self.shards_holder.read().await.contains_shard(&shard_id)
+        self.shards_holder.read().await.contains_shard(shard_id)
     }
 
     pub async fn wait_local_shard_replica_state(
@@ -340,7 +340,7 @@ impl Collection {
     ) -> CollectionResult<()> {
         let shard_holder_read = self.shards_holder.read().await;
 
-        let shard = shard_holder_read.get_shard(&shard_id);
+        let shard = shard_holder_read.get_shard(shard_id);
         let Some(replica_set) = shard else {
             return Err(CollectionError::NotFound {
                 what: "Shard {shard_id}".into(),
@@ -359,16 +359,16 @@ impl Collection {
     ) -> CollectionResult<()> {
         let shard_holder = self.shards_holder.read().await;
         let replica_set = shard_holder
-            .get_shard(&shard_id)
+            .get_shard(shard_id)
             .ok_or_else(|| shard_not_found_error(shard_id))?;
 
         log::debug!(
             "Changing shard {}:{shard_id} replica state from {:?} to {state:?}",
             self.id,
-            replica_set.peer_state(&peer_id),
+            replica_set.peer_state(peer_id),
         );
 
-        let current_state = replica_set.peer_state(&peer_id);
+        let current_state = replica_set.peer_state(peer_id);
 
         // Validation:
         //
@@ -380,7 +380,7 @@ impl Collection {
             .read()
             .contains_key(&peer_id);
 
-        let replica_exists = replica_set.peer_state(&peer_id).is_some();
+        let replica_exists = replica_set.peer_state(peer_id).is_some();
 
         if !peer_exists && !replica_exists {
             return Err(CollectionError::bad_input(format!(
@@ -443,14 +443,14 @@ impl Collection {
         }
 
         replica_set
-            .ensure_replica_with_state(&peer_id, state)
+            .ensure_replica_with_state(peer_id, state)
             .await?;
 
         if state == ReplicaState::Dead {
             // TODO(resharding): Abort all resharding transfers!?
 
             // Terminate transfer if source or target replicas are now dead
-            let related_transfers = shard_holder.get_related_transfers(&shard_id, &peer_id);
+            let related_transfers = shard_holder.get_related_transfers(shard_id, peer_id);
 
             // `abort_shard_transfer` locks `shard_holder`!
             drop(shard_holder);
@@ -485,7 +485,7 @@ impl Collection {
     pub async fn shard_recovery_point(&self, shard_id: ShardId) -> CollectionResult<RecoveryPoint> {
         let shard_holder_read = self.shards_holder.read().await;
 
-        let shard = shard_holder_read.get_shard(&shard_id);
+        let shard = shard_holder_read.get_shard(shard_id);
         let Some(replica_set) = shard else {
             return Err(CollectionError::NotFound {
                 what: format!("Shard {shard_id}"),
@@ -502,7 +502,7 @@ impl Collection {
     ) -> CollectionResult<()> {
         let shard_holder_read = self.shards_holder.read().await;
 
-        let shard = shard_holder_read.get_shard(&shard_id);
+        let shard = shard_holder_read.get_shard(shard_id);
         let Some(replica_set) = shard else {
             return Err(CollectionError::NotFound {
                 what: "Shard {shard_id}".into(),
@@ -524,7 +524,7 @@ impl Collection {
                     let shard_info = ShardInfo {
                         replicas: replicas.peers(),
                     };
-                    (*shard_id, shard_info)
+                    (shard_id, shard_info)
                 })
                 .collect(),
             resharding,
@@ -579,7 +579,7 @@ impl Collection {
         }
 
         // Check for un-reported finished transfers
-        let outgoing_transfers = shard_holder.get_outgoing_transfers(&self.this_peer_id);
+        let outgoing_transfers = shard_holder.get_outgoing_transfers(self.this_peer_id);
         let tasks_lock = self.transfer_tasks.lock().await;
         for transfer in outgoing_transfers {
             match tasks_lock
@@ -619,29 +619,29 @@ impl Collection {
 
         // Check for proper replica states
         for replica_set in shard_holder.all_shards() {
-            let this_peer_id = &replica_set.this_peer_id();
+            let this_peer_id = replica_set.this_peer_id();
             let shard_id = replica_set.shard_id;
 
             let peers = replica_set.peers();
-            let this_peer_state = peers.get(this_peer_id).copied();
+            let this_peer_state = peers.get(&this_peer_id).copied();
             let is_last_active = peers.values().filter(|state| **state == Active).count() == 1;
 
             if this_peer_state == Some(Initializing) {
                 // It is possible, that collection creation didn't report
                 // Try to activate shard, as the collection clearly exists
-                on_finish_init(*this_peer_id, shard_id);
+                on_finish_init(this_peer_id, shard_id);
                 continue;
             }
 
             if self.shared_storage_config.node_type == NodeType::Listener {
                 if this_peer_state == Some(Active) && !is_last_active {
                     // Convert active node from active to listener
-                    on_convert_to_listener(*this_peer_id, shard_id);
+                    on_convert_to_listener(this_peer_id, shard_id);
                     continue;
                 }
             } else if this_peer_state == Some(Listener) {
                 // Convert listener node to active
-                on_convert_from_listener(*this_peer_id, shard_id);
+                on_convert_from_listener(this_peer_id, shard_id);
                 continue;
             }
 
@@ -654,7 +654,7 @@ impl Collection {
 
             // Respect shard transfer limit, consider already proposed transfers in our counts
             let (mut incoming, outgoing) = shard_holder.count_shard_transfer_io(this_peer_id);
-            incoming += proposed.get(this_peer_id).copied().unwrap_or(0);
+            incoming += proposed.get(&this_peer_id).copied().unwrap_or(0);
             if self.check_auto_shard_transfer_limit(incoming, outgoing) {
                 log::trace!("Postponing automatic shard {shard_id} transfer to stay below limit on this node (incoming: {incoming}, outgoing: {outgoing})");
                 continue;
@@ -680,7 +680,7 @@ impl Collection {
             for replica_id in replica_set.active_remote_shards().await {
                 let transfer = ShardTransfer {
                     from: replica_id,
-                    to: *this_peer_id,
+                    to: this_peer_id,
                     shard_id,
                     to_shard_id: None,
                     sync: true,
@@ -693,7 +693,7 @@ impl Collection {
                 }
 
                 // Respect shard transfer limit, consider already proposed transfers in our counts
-                let (incoming, mut outgoing) = shard_holder.count_shard_transfer_io(&replica_id);
+                let (incoming, mut outgoing) = shard_holder.count_shard_transfer_io(replica_id);
                 outgoing += proposed.get(&replica_id).copied().unwrap_or(0);
                 if self.check_auto_shard_transfer_limit(incoming, outgoing) {
                     log::trace!("Postponing automatic shard {shard_id} transfer to stay below limit on peer {replica_id} (incoming: {incoming}, outgoing: {outgoing})");

commit 7daf1aa21b2fbc76372b10ec017671e5e9af0ec3
Author: Tim Visée <tim+github@visee.me>
Date:   Fri Nov 1 20:19:16 2024 +0100

    Fix killing replicas too earily on state switches (#5343)
    
    * Send current replica state with disable replica proposals
    
    * Don't be strict about from state if in shard transfer related state
    
    * Link to PR, update formatting
    
    * Fix typo
    
    * Fix test compilation failures
    
    * Provide peer state when deactivating leader, remove unnecessary TODO
    
    * ReplicaState is Copy

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 04d2a4cf3..69618345a 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -39,7 +39,9 @@ use crate::shards::channel_service::ChannelService;
 use crate::shards::collection_shard_distribution::CollectionShardDistribution;
 use crate::shards::local_shard::clock_map::RecoveryPoint;
 use crate::shards::replica_set::ReplicaState::{Active, Dead, Initializing, Listener};
-use crate::shards::replica_set::{ChangePeerState, ReplicaState, ShardReplicaSet};
+use crate::shards::replica_set::{
+    ChangePeerFromState, ChangePeerState, ReplicaState, ShardReplicaSet,
+};
 use crate::shards::resharding::tasks_pool::ReshardTasksPool;
 use crate::shards::resharding::ReshardKey;
 use crate::shards::shard::{PeerId, ShardId};
@@ -65,7 +67,7 @@ pub struct Collection {
     transfer_tasks: Mutex<TransferTasksPool>,
     reshard_tasks: Mutex<ReshardTasksPool>,
     request_shard_transfer_cb: RequestShardTransfer,
-    notify_peer_failure_cb: ChangePeerState,
+    notify_peer_failure_cb: ChangePeerFromState,
     abort_shard_transfer_cb: replica_set::AbortShardTransfer,
     init_time: Duration,
     // One-way boolean flag that is set to true when the collection is fully initialized
@@ -98,7 +100,7 @@ impl Collection {
         shared_storage_config: Arc<SharedStorageConfig>,
         shard_distribution: CollectionShardDistribution,
         channel_service: ChannelService,
-        on_replica_failure: ChangePeerState,
+        on_replica_failure: ChangePeerFromState,
         request_shard_transfer: RequestShardTransfer,
         abort_shard_transfer: replica_set::AbortShardTransfer,
         search_runtime: Option<Handle>,
@@ -185,7 +187,7 @@ impl Collection {
         snapshots_path: &Path,
         shared_storage_config: Arc<SharedStorageConfig>,
         channel_service: ChannelService,
-        on_replica_failure: replica_set::ChangePeerState,
+        on_replica_failure: replica_set::ChangePeerFromState,
         request_shard_transfer: RequestShardTransfer,
         abort_shard_transfer: replica_set::AbortShardTransfer,
         search_runtime: Option<Handle>,

commit 4240e71859b86195c03d84ac363f9699b7bc0317
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Fri Nov 8 10:10:44 2024 +0100

    No useless async (#5401)
    
    * Remove unecessary async/await
    
    * clippy aftermath

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 69618345a..e31296e66 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -679,7 +679,7 @@ impl Collection {
                 });
 
             // Try to find a replica to transfer from
-            for replica_id in replica_set.active_remote_shards().await {
+            for replica_id in replica_set.active_remote_shards() {
                 let transfer = ShardTransfer {
                     from: replica_id,
                     to: this_peer_id,

commit 98633cbd3fdd01ee3c486a3573ff27dc180e3b6d
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Mon Nov 11 10:26:17 2024 +0100

    Use references for less cloning when possible (#5409)

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index e31296e66..a6527c2f3 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -670,7 +670,7 @@ impl Collection {
                 .unwrap_or_else(|| {
                     let all_support_wal_delta = self
                         .channel_service
-                        .all_peers_at_version(Version::new(1, 8, 0));
+                        .all_peers_at_version(&Version::new(1, 8, 0));
                     if all_support_wal_delta {
                         ShardTransferMethod::WalDelta
                     } else {

commit bf2685b87c795388b935562677a476b3f5a5ae29
Author: Tim Visée <tim+github@visee.me>
Date:   Wed Nov 13 16:33:24 2024 +0100

    Fix incorrectly aborting resharding when marking replica dead (#5443)
    
    * When marking resharding replica as dead, abort resharding from state
    
    * Make resharding abort precondition more strict
    
    Don't abort resharding operations that are not in progress.

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index a6527c2f3..8c030550b 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -29,7 +29,6 @@ use crate::collection::payload_index_schema::PayloadIndexSchema;
 use crate::collection_state::{ShardInfo, State};
 use crate::common::is_ready::IsReady;
 use crate::config::CollectionConfig;
-use crate::operations::cluster_ops::ReshardingDirection;
 use crate::operations::config_diff::{DiffConfig, OptimizersConfigDiff};
 use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{CollectionError, CollectionResult, NodeType};
@@ -43,7 +42,6 @@ use crate::shards::replica_set::{
     ChangePeerFromState, ChangePeerState, ReplicaState, ShardReplicaSet,
 };
 use crate::shards::resharding::tasks_pool::ReshardTasksPool;
-use crate::shards::resharding::ReshardKey;
 use crate::shards::shard::{PeerId, ShardId};
 use crate::shards::shard_holder::{shard_not_found_error, LockedShardHolder, ShardHolder};
 use crate::shards::transfer::helpers::check_transfer_conflicts_strict;
@@ -420,26 +418,16 @@ impl Collection {
         // then `Collection::abort_resharding` call should return an error, so no special handling
         // is needed.
         if current_state == Some(ReplicaState::Resharding) && state == ReplicaState::Dead {
-            let shard_key = shard_holder
-                .get_shard_id_to_key_mapping()
-                .get(&shard_id)
-                .cloned();
-
             drop(shard_holder);
 
-            self.abort_resharding(
-                ReshardKey {
-                    // Always up when setting resharding replica set state
-                    direction: ReshardingDirection::Up,
-                    peer_id,
-                    shard_id,
-                    shard_key,
-                },
-                false,
-            )
-            .await?;
+            let resharding_state = self
+                .resharding_state()
+                .await
+                .filter(|state| state.peer_id == peer_id);
 
-            // TODO(resharding): Abort all resharding transfers!?
+            if let Some(state) = resharding_state {
+                self.abort_resharding(state.key(), false).await?;
+            }
 
             return Ok(());
         }

commit 9e06d68661402bb2df271134bab5d9aeda995048
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Sat Nov 16 01:03:50 2024 +0700

    Add UUID to collection config (#5378)
    
    * Add UUID to collection...
    
    ...and recreate collection, when applying Raft snapshot, if UUID of collection is different
    
    * fixup! Add UUID to collection...
    
    Remove UUID field from gRPC and exclude it from OpenAPI spec 🤡
    
    * fixup! fixup! Add UUID to collection...
    
    Always generate collection UUID 🤦‍♀️
    
    * Raft snapshot recreate collection no expose UUID (#5452)
    
    * separate colleciton config structure from API
    
    * fmt
    
    * Update lib/collection/src/operations/types.rs
    
    Co-authored-by: Tim Visée <tim+github@visee.me>
    
    ---------
    
    Co-authored-by: Tim Visée <tim+github@visee.me>
    
    ---------
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>
    Co-authored-by: Tim Visée <tim+github@visee.me>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 8c030550b..afe278a33 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -28,7 +28,7 @@ use tokio::sync::{Mutex, RwLock, RwLockWriteGuard};
 use crate::collection::payload_index_schema::PayloadIndexSchema;
 use crate::collection_state::{ShardInfo, State};
 use crate::common::is_ready::IsReady;
-use crate::config::CollectionConfig;
+use crate::config::CollectionConfigInternal;
 use crate::operations::config_diff::{DiffConfig, OptimizersConfigDiff};
 use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{CollectionError, CollectionResult, NodeType};
@@ -54,7 +54,7 @@ use crate::telemetry::CollectionTelemetry;
 pub struct Collection {
     pub(crate) id: CollectionId,
     pub(crate) shards_holder: Arc<LockedShardHolder>,
-    pub(crate) collection_config: Arc<RwLock<CollectionConfig>>,
+    pub(crate) collection_config: Arc<RwLock<CollectionConfigInternal>>,
     pub(crate) shared_storage_config: Arc<SharedStorageConfig>,
     payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,
     optimizers_overwrite: Option<OptimizersConfigDiff>,
@@ -94,7 +94,7 @@ impl Collection {
         this_peer_id: PeerId,
         path: &Path,
         snapshots_path: &Path,
-        collection_config: &CollectionConfig,
+        collection_config: &CollectionConfigInternal,
         shared_storage_config: Arc<SharedStorageConfig>,
         shard_distribution: CollectionShardDistribution,
         channel_service: ChannelService,
@@ -215,7 +215,7 @@ impl Collection {
             }
         }
 
-        let collection_config = CollectionConfig::load(path).unwrap_or_else(|err| {
+        let collection_config = CollectionConfigInternal::load(path).unwrap_or_else(|err| {
             panic!(
                 "Can't read collection config due to {}\nat {}",
                 err,
@@ -313,6 +313,10 @@ impl Collection {
         self.id.clone()
     }
 
+    pub async fn uuid(&self) -> Option<uuid::Uuid> {
+        self.collection_config.read().await.uuid
+    }
+
     pub async fn get_shard_keys(&self) -> Vec<ShardKey> {
         self.shards_holder
             .read()

commit ed05a46889583d907af996d2559ff89cb81a72ef
Author: Tim Visée <tim+github@visee.me>
Date:   Wed Nov 27 14:57:37 2024 +0100

    Expose shard keys in telemetry (#5522)
    
    * Add shard key to replica set
    
    * Expose shard key in replica set telemetry
    
    * Update OpenAPI specification
    
    * Fix two spelling errors

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index afe278a33..b524ddb0c 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -122,8 +122,10 @@ impl Collection {
                     optimizers_overwrite.update(&effective_optimizers_config)?;
             }
 
+            let shard_key = None;
             let replica_set = ShardReplicaSet::build(
                 shard_id,
+                shard_key.clone(),
                 name.clone(),
                 this_peer_id,
                 is_local,
@@ -143,7 +145,7 @@ impl Collection {
             )
             .await?;
 
-            shard_holder.add_shard(shard_id, replica_set, None)?;
+            shard_holder.add_shard(shard_id, replica_set, shard_key)?;
         }
 
         let locked_shard_holder = Arc::new(LockedShardHolder::new(shard_holder));

commit 0702854477ae7b23f3f50d94ea9a4cac167bd612
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Thu Dec 5 17:28:09 2024 +0100

    Strict mode max collection vector size (#5501)
    
    * Strict mode config: Max collection size
    
    * api specs
    
    * Add tests + set/update payload check
    
    * Improve function names and add comments
    
    * rename config to separate vectors and payload
    
    * fix tests
    
    * Adjust configs docs
    
    * add benchmark
    
    * improve performance by caching shard info
    
    * add bench for size_info() and fix tests
    
    * Also limit the batch-size for vector updates (#5508)
    
    * Also limit the batch-size for vector updates
    
    * clippy
    
    * add lost commit
    
    * Load cache on collection initialization
    
    * add unit type to parameter name
    
    * fix renaming in test
    
    * clearer error message
    
    * fix test
    
    * review remarks
    
    * remove unused function for now
    
    ---------
    
    Co-authored-by: Arnaud Gourlay <arnaud.gourlay@gmail.com>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index b524ddb0c..751d99d5c 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -28,6 +28,7 @@ use tokio::sync::{Mutex, RwLock, RwLockWriteGuard};
 use crate::collection::payload_index_schema::PayloadIndexSchema;
 use crate::collection_state::{ShardInfo, State};
 use crate::common::is_ready::IsReady;
+use crate::common::local_data_stats::{LocalDataStats, LocalDataStatsCache};
 use crate::config::CollectionConfigInternal;
 use crate::operations::config_diff::{DiffConfig, OptimizersConfigDiff};
 use crate::operations::shared_storage_config::SharedStorageConfig;
@@ -80,6 +81,8 @@ pub struct Collection {
     // Search runtime handle.
     search_runtime: Handle,
     optimizer_cpu_budget: CpuBudget,
+    // Cached stats over all local shards used in strict mode, may be outdated
+    local_stats_cache: LocalDataStatsCache,
 }
 
 pub type RequestShardTransfer = Arc<dyn Fn(ShardTransfer) + Send + Sync>;
@@ -150,6 +153,10 @@ impl Collection {
 
         let locked_shard_holder = Arc::new(LockedShardHolder::new(shard_holder));
 
+        let local_stats_cache = LocalDataStatsCache::new_with_values(
+            Self::calculate_segment_stats(&locked_shard_holder).await,
+        );
+
         // Once the config is persisted - the collection is considered to be successfully created.
         CollectionVersion::save(path)?;
         collection_config.save(path)?;
@@ -176,6 +183,7 @@ impl Collection {
             update_runtime: update_runtime.unwrap_or_else(Handle::current),
             search_runtime: search_runtime.unwrap_or_else(Handle::current),
             optimizer_cpu_budget,
+            local_stats_cache,
         })
     }
 
@@ -263,6 +271,10 @@ impl Collection {
 
         let locked_shard_holder = Arc::new(LockedShardHolder::new(shard_holder));
 
+        let local_stats_cache = LocalDataStatsCache::new_with_values(
+            Self::calculate_segment_stats(&locked_shard_holder).await,
+        );
+
         Self {
             id: collection_id.clone(),
             shards_holder: locked_shard_holder,
@@ -285,6 +297,7 @@ impl Collection {
             update_runtime: update_runtime.unwrap_or_else(Handle::current),
             search_runtime: search_runtime.unwrap_or_else(Handle::current),
             optimizer_cpu_budget,
+            local_stats_cache,
         }
     }
 
@@ -784,6 +797,31 @@ impl Collection {
     pub async fn trigger_optimizers(&self) {
         self.shards_holder.read().await.trigger_optimizers().await;
     }
+
+    async fn calculate_segment_stats(shards_holder: &Arc<RwLock<ShardHolder>>) -> LocalDataStats {
+        let shard_lock = shards_holder.read().await;
+        shard_lock.calculate_local_segments_stats().await
+    }
+
+    /// Checks and performs a cache update for local data statistics if needed.
+    /// Returns `Some(..)` with the new values if a cache update has been performed and `None` otherwise.
+    async fn check_and_update_local_size_stats(&self) -> Option<LocalDataStats> {
+        if self.local_stats_cache.check_need_update_and_increment() {
+            let new_stats = Self::calculate_segment_stats(&self.shards_holder).await;
+            self.local_stats_cache.update(new_stats);
+            return Some(new_stats);
+        }
+
+        None
+    }
+
+    /// Returns the estimated local vector storage size for this collection, cached and auto-updated.
+    pub async fn estimated_local_vector_storage_size(&self) -> usize {
+        if let Some(shard_stats) = self.check_and_update_local_size_stats().await {
+            return shard_stats.vector_storage_size;
+        }
+        self.local_stats_cache.get_vector_storage()
+    }
 }
 
 struct CollectionVersion;

commit 947e79e054977921b0167474bdf08e1b7d67be12
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Tue Dec 17 17:33:46 2024 +0100

    Strict mode for payload storage (#5588)
    
    * Strict mode for payload storage
    
    * Don't increment counter multiple times per request
    
    * Add (loose) integration tests for payload storage limit
    
    * Minor improvements
    
    * minor renaming
    
    * Update lib/api/src/grpc/proto/collections.proto
    
    Co-authored-by: Tim Visée <tim+github@visee.me>
    
    ---------
    
    Co-authored-by: jojii <jojii@gmx.net>
    Co-authored-by: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
    Co-authored-by: Tim Visée <tim+github@visee.me>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 751d99d5c..20f26b3f3 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -28,7 +28,7 @@ use tokio::sync::{Mutex, RwLock, RwLockWriteGuard};
 use crate::collection::payload_index_schema::PayloadIndexSchema;
 use crate::collection_state::{ShardInfo, State};
 use crate::common::is_ready::IsReady;
-use crate::common::local_data_stats::{LocalDataStats, LocalDataStatsCache};
+use crate::common::local_data_stats::{LocalDataAtomicStats, LocalDataStats, LocalDataStatsCache};
 use crate::config::CollectionConfigInternal;
 use crate::operations::config_diff::{DiffConfig, OptimizersConfigDiff};
 use crate::operations::shared_storage_config::SharedStorageConfig;
@@ -154,7 +154,7 @@ impl Collection {
         let locked_shard_holder = Arc::new(LockedShardHolder::new(shard_holder));
 
         let local_stats_cache = LocalDataStatsCache::new_with_values(
-            Self::calculate_segment_stats(&locked_shard_holder).await,
+            Self::calculate_local_shards_stats(&locked_shard_holder).await,
         );
 
         // Once the config is persisted - the collection is considered to be successfully created.
@@ -272,7 +272,7 @@ impl Collection {
         let locked_shard_holder = Arc::new(LockedShardHolder::new(shard_holder));
 
         let local_stats_cache = LocalDataStatsCache::new_with_values(
-            Self::calculate_segment_stats(&locked_shard_holder).await,
+            Self::calculate_local_shards_stats(&locked_shard_holder).await,
         );
 
         Self {
@@ -798,29 +798,19 @@ impl Collection {
         self.shards_holder.read().await.trigger_optimizers().await;
     }
 
-    async fn calculate_segment_stats(shards_holder: &Arc<RwLock<ShardHolder>>) -> LocalDataStats {
+    async fn calculate_local_shards_stats(
+        shards_holder: &Arc<RwLock<ShardHolder>>,
+    ) -> LocalDataStats {
         let shard_lock = shards_holder.read().await;
-        shard_lock.calculate_local_segments_stats().await
+        shard_lock.calculate_local_shards_stats().await
     }
 
-    /// Checks and performs a cache update for local data statistics if needed.
-    /// Returns `Some(..)` with the new values if a cache update has been performed and `None` otherwise.
-    async fn check_and_update_local_size_stats(&self) -> Option<LocalDataStats> {
-        if self.local_stats_cache.check_need_update_and_increment() {
-            let new_stats = Self::calculate_segment_stats(&self.shards_holder).await;
-            self.local_stats_cache.update(new_stats);
-            return Some(new_stats);
-        }
-
-        None
-    }
-
-    /// Returns the estimated local vector storage size for this collection, cached and auto-updated.
-    pub async fn estimated_local_vector_storage_size(&self) -> usize {
-        if let Some(shard_stats) = self.check_and_update_local_size_stats().await {
-            return shard_stats.vector_storage_size;
-        }
-        self.local_stats_cache.get_vector_storage()
+    /// Returns estimations of local shards statistics. This values are cached and might be not 100% up to date.
+    /// The cache gets updated every 32 calls.
+    pub async fn local_stats_estimations(&self) -> &LocalDataAtomicStats {
+        self.local_stats_cache
+            .get_or_update_cache(|| Self::calculate_local_shards_stats(&self.shards_holder))
+            .await
     }
 }
 

commit bf4cae877836bd631a4543f696e7e32c84eff856
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Thu Dec 19 16:38:30 2024 +0100

    [Strict Mode] Max collection size in distributed setup (#5592)
    
    * Strict Mode: distributed checking of max collection size
    
    * add size projections in distributed mode
    
    * Add consensus tests
    
    * New Test: All nodes in cluster
    
    * fix tests
    
    * Update lib/collection/src/collection/mod.rs
    
    Co-authored-by: Arnaud Gourlay <arnaud.gourlay@gmail.com>
    
    * increase upsert delay
    
    * add TODO for resharding
    
    * wait for strict mode config to be applied on second node
    
    * remove delays
    
    * Also wait for strict mode in other test
    
    * clearify strict mode config option
    
    ---------
    
    Co-authored-by: Arnaud Gourlay <arnaud.gourlay@gmail.com>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 20f26b3f3..cba0b1afc 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -27,8 +27,10 @@ use tokio::sync::{Mutex, RwLock, RwLockWriteGuard};
 
 use crate::collection::payload_index_schema::PayloadIndexSchema;
 use crate::collection_state::{ShardInfo, State};
+use crate::common::collection_size_stats::{
+    CollectionSizeAtomicStats, CollectionSizeStats, CollectionSizeStatsCache,
+};
 use crate::common::is_ready::IsReady;
-use crate::common::local_data_stats::{LocalDataAtomicStats, LocalDataStats, LocalDataStatsCache};
 use crate::config::CollectionConfigInternal;
 use crate::operations::config_diff::{DiffConfig, OptimizersConfigDiff};
 use crate::operations::shared_storage_config::SharedStorageConfig;
@@ -81,8 +83,8 @@ pub struct Collection {
     // Search runtime handle.
     search_runtime: Handle,
     optimizer_cpu_budget: CpuBudget,
-    // Cached stats over all local shards used in strict mode, may be outdated
-    local_stats_cache: LocalDataStatsCache,
+    // Cached statistics of collection size, may be outdated.
+    collection_stats_cache: CollectionSizeStatsCache,
 }
 
 pub type RequestShardTransfer = Arc<dyn Fn(ShardTransfer) + Send + Sync>;
@@ -153,8 +155,8 @@ impl Collection {
 
         let locked_shard_holder = Arc::new(LockedShardHolder::new(shard_holder));
 
-        let local_stats_cache = LocalDataStatsCache::new_with_values(
-            Self::calculate_local_shards_stats(&locked_shard_holder).await,
+        let collection_stats_cache = CollectionSizeStatsCache::new_with_values(
+            Self::estimate_collection_size_stats(&locked_shard_holder).await,
         );
 
         // Once the config is persisted - the collection is considered to be successfully created.
@@ -183,7 +185,7 @@ impl Collection {
             update_runtime: update_runtime.unwrap_or_else(Handle::current),
             search_runtime: search_runtime.unwrap_or_else(Handle::current),
             optimizer_cpu_budget,
-            local_stats_cache,
+            collection_stats_cache,
         })
     }
 
@@ -271,8 +273,8 @@ impl Collection {
 
         let locked_shard_holder = Arc::new(LockedShardHolder::new(shard_holder));
 
-        let local_stats_cache = LocalDataStatsCache::new_with_values(
-            Self::calculate_local_shards_stats(&locked_shard_holder).await,
+        let collection_stats_cache = CollectionSizeStatsCache::new_with_values(
+            Self::estimate_collection_size_stats(&locked_shard_holder).await,
         );
 
         Self {
@@ -297,7 +299,7 @@ impl Collection {
             update_runtime: update_runtime.unwrap_or_else(Handle::current),
             search_runtime: search_runtime.unwrap_or_else(Handle::current),
             optimizer_cpu_budget,
-            local_stats_cache,
+            collection_stats_cache,
         }
     }
 
@@ -798,18 +800,18 @@ impl Collection {
         self.shards_holder.read().await.trigger_optimizers().await;
     }
 
-    async fn calculate_local_shards_stats(
+    async fn estimate_collection_size_stats(
         shards_holder: &Arc<RwLock<ShardHolder>>,
-    ) -> LocalDataStats {
+    ) -> Option<CollectionSizeStats> {
         let shard_lock = shards_holder.read().await;
-        shard_lock.calculate_local_shards_stats().await
+        shard_lock.estimate_collection_size_stats().await
     }
 
-    /// Returns estimations of local shards statistics. This values are cached and might be not 100% up to date.
+    /// Returns estimations of collection sizes. This values are cached and might be not 100% up to date.
     /// The cache gets updated every 32 calls.
-    pub async fn local_stats_estimations(&self) -> &LocalDataAtomicStats {
-        self.local_stats_cache
-            .get_or_update_cache(|| Self::calculate_local_shards_stats(&self.shards_holder))
+    pub(crate) async fn estimated_collection_stats(&self) -> Option<&CollectionSizeAtomicStats> {
+        self.collection_stats_cache
+            .get_or_update_cache(|| Self::estimate_collection_size_stats(&self.shards_holder))
             .await
     }
 }

commit 05e7c48f7646d5efef47c2d62f32034d03be3a1e
Author: Tim Visée <tim+github@visee.me>
Date:   Mon Dec 23 14:06:52 2024 +0100

    Add task to clean local shard in the background (#5677)
    
    * First naive implementation of local shard clean task
    
    * Add wait and timeout parameters to clean shard API
    
    * Cancel shard cleaning and mark as dirty when changing hash rings
    
    * Expose shards undergoing cleaning in telemetry
    
    * Properly cancel shard clean task, add drop guard to enforce
    
    * Ensure we have a local shard to clean
    
    * Replace existing local shard cleanup API
    
    * Refactor how we create, manage and await clean tasks
    
    * Invalidate only affected shards when committing read hash ring
    
    * Expose all local shard clean task statuses in telemetry
    
    * Properly invalidate shards when aborting resharding
    
    * Add review remark comments
    
    * Log clean task errors, extract task in dedicated function
    
    * Join task when invalidating so we wait for completion
    
    * Invalidate shard clean tasks in a batch to more efficiently join them
    
    * Expose clean progress with number of deleted points
    
    * Annotate cancel safety
    
    * Fix comment
    
    * When calling clean endpoint, ensure we have the specified local shard
    
    * Invalidate shard cleaning when a local shard is being dropped
    
    * Prevent anonymous type in shard clean status telemetry
    
    * Cancel shard clean task directly by dropping future, fix possible deadlock
    
    Before this change trying to invalidate shard cleaning tasks could
    deadlock. The actual task only had two cancel points inside a read lock
    on the shard holder. The shard holder is constantly released and
    relocked.
    
    In some of the places we can trigger invalidation already hold a shard
    holder write lock. The task itself would have to grab a read lock in
    order to reach the cancel point, in which case the locks would be
    fighting each other.
    
    Because invalidation also joins the task and waits for it to abort this
    could get stuck forever.
    
    Now we don't have cancellation points anymore and simply drop the whole
    future in case of cancellation. That'll prevent it getting stuck.

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index cba0b1afc..82f6b5a48 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -1,3 +1,4 @@
+mod clean;
 mod collection_ops;
 pub mod distance_matrix;
 mod facet;
@@ -17,6 +18,7 @@ use std::path::{Path, PathBuf};
 use std::sync::Arc;
 use std::time::Duration;
 
+use clean::ShardCleanTasks;
 use common::cpu::CpuBudget;
 use common::types::TelemetryDetail;
 use io::storage_version::StorageVersion;
@@ -85,6 +87,8 @@ pub struct Collection {
     optimizer_cpu_budget: CpuBudget,
     // Cached statistics of collection size, may be outdated.
     collection_stats_cache: CollectionSizeStatsCache,
+    // Background tasks to clean shards
+    shard_clean_tasks: ShardCleanTasks,
 }
 
 pub type RequestShardTransfer = Arc<dyn Fn(ShardTransfer) + Send + Sync>;
@@ -186,6 +190,7 @@ impl Collection {
             search_runtime: search_runtime.unwrap_or_else(Handle::current),
             optimizer_cpu_budget,
             collection_stats_cache,
+            shard_clean_tasks: Default::default(),
         })
     }
 
@@ -300,6 +305,7 @@ impl Collection {
             search_runtime: search_runtime.unwrap_or_else(Handle::current),
             optimizer_cpu_budget,
             collection_stats_cache,
+            shard_clean_tasks: Default::default(),
         }
     }
 
@@ -763,6 +769,7 @@ impl Collection {
             shards: shards_telemetry,
             transfers,
             resharding,
+            shard_clean_tasks: self.clean_local_shards_statuses(),
         }
     }
 

commit 69b7e0e4735c4f1cf4012caac553e2b8f2453c00
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Tue Dec 24 13:58:32 2024 +0100

    Abort scale-down resharding on update error (#5670)

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 82f6b5a48..3d937d3dc 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -381,7 +381,7 @@ impl Collection {
         &self,
         shard_id: ShardId,
         peer_id: PeerId,
-        state: ReplicaState,
+        new_state: ReplicaState,
         from_state: Option<ReplicaState>,
     ) -> CollectionResult<()> {
         let shard_holder = self.shards_holder.read().await;
@@ -390,7 +390,7 @@ impl Collection {
             .ok_or_else(|| shard_not_found_error(shard_id))?;
 
         log::debug!(
-            "Changing shard {}:{shard_id} replica state from {:?} to {state:?}",
+            "Changing shard {}:{shard_id} replica state from {:?} to {new_state:?}",
             self.id,
             replica_set.peer_state(peer_id),
         );
@@ -411,7 +411,7 @@ impl Collection {
 
         if !peer_exists && !replica_exists {
             return Err(CollectionError::bad_input(format!(
-                "Can't set replica {peer_id}:{shard_id} state to {state:?}, \
+                "Can't set replica {peer_id}:{shard_id} state to {new_state:?}, \
                  because replica {peer_id}:{shard_id} does not exist \
                  and peer {peer_id} is not part of the cluster"
             )));
@@ -425,7 +425,9 @@ impl Collection {
         }
 
         // 3. Do not deactivate the last active replica
-        if state != ReplicaState::Active && replica_set.is_last_active_replica(peer_id) {
+        //
+        // `is_last_active_replica` counts both `Active` and `ReshardingScaleDown` replicas!
+        if replica_set.is_last_active_replica(peer_id) && !new_state.is_active() {
             return Err(CollectionError::bad_input(format!(
                 "Cannot deactivate the last active replica {peer_id} of shard {shard_id}"
             )));
@@ -444,7 +446,12 @@ impl Collection {
         // If resharding reached `ReadHashRingCommitted`, and this branch is triggered *somehow*,
         // then `Collection::abort_resharding` call should return an error, so no special handling
         // is needed.
-        if current_state == Some(ReplicaState::Resharding) && state == ReplicaState::Dead {
+        let is_resharding = matches!(
+            current_state,
+            Some(ReplicaState::Resharding | ReplicaState::ReshardingScaleDown)
+        );
+
+        if is_resharding && new_state == ReplicaState::Dead {
             drop(shard_holder);
 
             let resharding_state = self
@@ -460,10 +467,10 @@ impl Collection {
         }
 
         replica_set
-            .ensure_replica_with_state(peer_id, state)
+            .ensure_replica_with_state(peer_id, new_state)
             .await?;
 
-        if state == ReplicaState::Dead {
+        if new_state == ReplicaState::Dead {
             // TODO(resharding): Abort all resharding transfers!?
 
             // Terminate transfer if source or target replicas are now dead
@@ -477,21 +484,27 @@ impl Collection {
             }
         }
 
+        // If not initialized yet, we need to check if it was initialized by this call
         if !self.is_initialized.check_ready() {
-            // If not initialized yet, we need to check if it was initialized by this call
             let state = self.state().await;
-            let mut is_fully_active = true;
+
+            let mut is_ready = true;
+
             for (_shard_id, shard_info) in state.shards {
-                if shard_info
-                    .replicas
-                    .into_iter()
-                    .any(|(_peer_id, state)| state != ReplicaState::Active)
-                {
-                    is_fully_active = false;
+                let all_replicas_active = shard_info.replicas.into_iter().all(|(_, state)| {
+                    matches!(
+                        state,
+                        ReplicaState::Active | ReplicaState::ReshardingScaleDown
+                    )
+                });
+
+                if !all_replicas_active {
+                    is_ready = false;
                     break;
                 }
             }
-            if is_fully_active {
+
+            if is_ready {
                 self.is_initialized.make_ready();
             }
         }
@@ -641,7 +654,6 @@ impl Collection {
 
             let peers = replica_set.peers();
             let this_peer_state = peers.get(&this_peer_id).copied();
-            let is_last_active = peers.values().filter(|state| **state == Active).count() == 1;
 
             if this_peer_state == Some(Initializing) {
                 // It is possible, that collection creation didn't report
@@ -651,6 +663,10 @@ impl Collection {
             }
 
             if self.shared_storage_config.node_type == NodeType::Listener {
+                // We probably should not switch node type during resharding, so we only check for `Active`,
+                // but not `ReshardingScaleDown` replica state here...
+                let is_last_active = peers.values().filter(|&&state| state == Active).count() == 1;
+
                 if this_peer_state == Some(Active) && !is_last_active {
                     // Convert active node from active to listener
                     on_convert_to_listener(this_peer_id, shard_id);
@@ -694,6 +710,8 @@ impl Collection {
                 });
 
             // Try to find a replica to transfer from
+            //
+            // `active_remote_shards` includes `Active` and `ReshardingScaleDown` replicas!
             for replica_id in replica_set.active_remote_shards() {
                 let transfer = ShardTransfer {
                     from: replica_id,

commit 10c6dcebb7dbc23c4b04081bed92bcab9b9eb3b9
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Mon Jan 13 14:46:42 2025 +0100

    Remove resharding driver (#5782)
    
    * Remove resharding driver
    
    * Trigger CI 🙄
    
    * fixup! Remove resharding driver
    
    Update OpenAPI spec

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 3d937d3dc..5c8b44185 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -46,7 +46,6 @@ use crate::shards::replica_set::ReplicaState::{Active, Dead, Initializing, Liste
 use crate::shards::replica_set::{
     ChangePeerFromState, ChangePeerState, ReplicaState, ShardReplicaSet,
 };
-use crate::shards::resharding::tasks_pool::ReshardTasksPool;
 use crate::shards::shard::{PeerId, ShardId};
 use crate::shards::shard_holder::{shard_not_found_error, LockedShardHolder, ShardHolder};
 use crate::shards::transfer::helpers::check_transfer_conflicts_strict;
@@ -68,7 +67,6 @@ pub struct Collection {
     snapshots_path: PathBuf,
     channel_service: ChannelService,
     transfer_tasks: Mutex<TransferTasksPool>,
-    reshard_tasks: Mutex<ReshardTasksPool>,
     request_shard_transfer_cb: RequestShardTransfer,
     notify_peer_failure_cb: ChangePeerFromState,
     abort_shard_transfer_cb: replica_set::AbortShardTransfer,
@@ -179,7 +177,6 @@ impl Collection {
             snapshots_path: snapshots_path.to_owned(),
             channel_service,
             transfer_tasks: Mutex::new(TransferTasksPool::new(name.clone())),
-            reshard_tasks: Mutex::new(ReshardTasksPool::new(name)),
             request_shard_transfer_cb: request_shard_transfer.clone(),
             notify_peer_failure_cb: on_replica_failure.clone(),
             abort_shard_transfer_cb: abort_shard_transfer,
@@ -294,7 +291,6 @@ impl Collection {
             snapshots_path: snapshots_path.to_owned(),
             channel_service,
             transfer_tasks: Mutex::new(TransferTasksPool::new(collection_id.clone())),
-            reshard_tasks: Mutex::new(ReshardTasksPool::new(collection_id)),
             request_shard_transfer_cb: request_shard_transfer.clone(),
             notify_peer_failure_cb: on_replica_failure,
             abort_shard_transfer_cb: abort_shard_transfer,
@@ -775,7 +771,7 @@ impl Collection {
                 shards_telemetry,
                 shards_holder.get_shard_transfer_info(&*self.transfer_tasks.lock().await),
                 shards_holder
-                    .get_resharding_operations_info(&*self.reshard_tasks.lock().await)
+                    .get_resharding_operations_info()
                     .unwrap_or_default(),
             )
         };

commit 798f14cafb91e546ed19fc44f3dec0e68ab43f40
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Tue Feb 18 10:41:28 2025 +0100

    Fix Clippy 1.85 (#6011)
    
    * Fix Clippy 1.85
    
    * fix false positive

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 5c8b44185..a371902ef 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -366,7 +366,7 @@ impl Collection {
         let shard = shard_holder_read.get_shard(shard_id);
         let Some(replica_set) = shard else {
             return Err(CollectionError::NotFound {
-                what: "Shard {shard_id}".into(),
+                what: format!("Shard {shard_id}"),
             });
         };
 
@@ -531,7 +531,7 @@ impl Collection {
         let shard = shard_holder_read.get_shard(shard_id);
         let Some(replica_set) = shard else {
             return Err(CollectionError::NotFound {
-                what: "Shard {shard_id}".into(),
+                what: format!("Shard {shard_id}"),
             });
         };
 

commit caed5729e5b7ff3db9dcb4531a4af0929b186682
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Thu Feb 20 09:05:00 2025 +0100

    IO resource usage permit (#6015)
    
    * rename cpu_budget -> resource_budget
    
    * clippy
    
    * add io budget to resources
    
    * fmt
    
    * move budget structures into a separate file
    
    * add extend permit function
    
    * dont extend existing permit
    
    * switch from IO to CPU permit
    
    * do not release resource before aquiring an extension
    
    * fmt
    
    * Review remarks
    
    * Improve resource permit number assertion
    
    * Make resource permit replace_with only acquire extra needed permits
    
    * Remove obsolete drop implementation
    
    * allocate IO budget same as CPU
    
    * review fixes
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index a371902ef..e6677b10f 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -19,7 +19,7 @@ use std::sync::Arc;
 use std::time::Duration;
 
 use clean::ShardCleanTasks;
-use common::cpu::CpuBudget;
+use common::budget::ResourceBudget;
 use common::types::TelemetryDetail;
 use io::storage_version::StorageVersion;
 use segment::types::ShardKey;
@@ -82,7 +82,7 @@ pub struct Collection {
     update_runtime: Handle,
     // Search runtime handle.
     search_runtime: Handle,
-    optimizer_cpu_budget: CpuBudget,
+    optimizer_resource_budget: ResourceBudget,
     // Cached statistics of collection size, may be outdated.
     collection_stats_cache: CollectionSizeStatsCache,
     // Background tasks to clean shards
@@ -110,7 +110,7 @@ impl Collection {
         abort_shard_transfer: replica_set::AbortShardTransfer,
         search_runtime: Option<Handle>,
         update_runtime: Option<Handle>,
-        optimizer_cpu_budget: CpuBudget,
+        optimizer_resource_budget: ResourceBudget,
         optimizers_overwrite: Option<OptimizersConfigDiff>,
     ) -> Result<Self, CollectionError> {
         let start_time = std::time::Instant::now();
@@ -147,7 +147,7 @@ impl Collection {
                 channel_service.clone(),
                 update_runtime.clone().unwrap_or_else(Handle::current),
                 search_runtime.clone().unwrap_or_else(Handle::current),
-                optimizer_cpu_budget.clone(),
+                optimizer_resource_budget.clone(),
                 None,
             )
             .await?;
@@ -185,7 +185,7 @@ impl Collection {
             updates_lock: Default::default(),
             update_runtime: update_runtime.unwrap_or_else(Handle::current),
             search_runtime: search_runtime.unwrap_or_else(Handle::current),
-            optimizer_cpu_budget,
+            optimizer_resource_budget,
             collection_stats_cache,
             shard_clean_tasks: Default::default(),
         })
@@ -204,7 +204,7 @@ impl Collection {
         abort_shard_transfer: replica_set::AbortShardTransfer,
         search_runtime: Option<Handle>,
         update_runtime: Option<Handle>,
-        optimizer_cpu_budget: CpuBudget,
+        optimizer_resource_budget: ResourceBudget,
         optimizers_overwrite: Option<OptimizersConfigDiff>,
     ) -> Self {
         let start_time = std::time::Instant::now();
@@ -269,7 +269,7 @@ impl Collection {
                 this_peer_id,
                 update_runtime.clone().unwrap_or_else(Handle::current),
                 search_runtime.clone().unwrap_or_else(Handle::current),
-                optimizer_cpu_budget.clone(),
+                optimizer_resource_budget.clone(),
             )
             .await;
 
@@ -299,7 +299,7 @@ impl Collection {
             updates_lock: Default::default(),
             update_runtime: update_runtime.unwrap_or_else(Handle::current),
             search_runtime: search_runtime.unwrap_or_else(Handle::current),
-            optimizer_cpu_budget,
+            optimizer_resource_budget,
             collection_stats_cache,
             shard_clean_tasks: Default::default(),
         }

commit 8ad2b34265448ec01b89d4093de5fbb1a86dcd4d
Author: Tim Visée <tim+github@visee.me>
Date:   Tue Feb 25 11:21:25 2025 +0100

    Bump Rust edition to 2024 (#6042)
    
    * Bump Rust edition to 2024
    
    * gen is a reserved keyword now
    
    * Remove ref mut on references
    
    * Mark extern C as unsafe
    
    * Wrap unsafe function bodies in unsafe block
    
    * Geo hash implements Copy, don't reference but pass by value instead
    
    * Replace secluded self import with parent
    
    * Update execute_cluster_read_operation with new match semantics
    
    * Fix lifetime issue
    
    * Replace map_or with is_none_or
    
    * set_var is unsafe now
    
    * Reformat

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index e6677b10f..58534ccaf 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -47,11 +47,11 @@ use crate::shards::replica_set::{
     ChangePeerFromState, ChangePeerState, ReplicaState, ShardReplicaSet,
 };
 use crate::shards::shard::{PeerId, ShardId};
-use crate::shards::shard_holder::{shard_not_found_error, LockedShardHolder, ShardHolder};
+use crate::shards::shard_holder::{LockedShardHolder, ShardHolder, shard_not_found_error};
 use crate::shards::transfer::helpers::check_transfer_conflicts_strict;
 use crate::shards::transfer::transfer_tasks_pool::{TaskResult, TransferTasksPool};
 use crate::shards::transfer::{ShardTransfer, ShardTransferMethod};
-use crate::shards::{replica_set, CollectionId};
+use crate::shards::{CollectionId, replica_set};
 use crate::telemetry::CollectionTelemetry;
 
 /// Collection's data is split into several shards.
@@ -225,7 +225,9 @@ impl Collection {
                     .unwrap_or_else(|err| panic!("Can't save collection version {err}"));
             } else {
                 log::error!("Cannot upgrade version {stored_version} to {app_version}.");
-                panic!("Cannot upgrade version {stored_version} to {app_version}. Try to use older version of Qdrant first.");
+                panic!(
+                    "Cannot upgrade version {stored_version} to {app_version}. Try to use older version of Qdrant first.",
+                );
             }
         }
 
@@ -685,7 +687,9 @@ impl Collection {
             let (mut incoming, outgoing) = shard_holder.count_shard_transfer_io(this_peer_id);
             incoming += proposed.get(&this_peer_id).copied().unwrap_or(0);
             if self.check_auto_shard_transfer_limit(incoming, outgoing) {
-                log::trace!("Postponing automatic shard {shard_id} transfer to stay below limit on this node (incoming: {incoming}, outgoing: {outgoing})");
+                log::trace!(
+                    "Postponing automatic shard {shard_id} transfer to stay below limit on this node (incoming: {incoming}, outgoing: {outgoing})",
+                );
                 continue;
             }
 
@@ -727,7 +731,9 @@ impl Collection {
                 let (incoming, mut outgoing) = shard_holder.count_shard_transfer_io(replica_id);
                 outgoing += proposed.get(&replica_id).copied().unwrap_or(0);
                 if self.check_auto_shard_transfer_limit(incoming, outgoing) {
-                    log::trace!("Postponing automatic shard {shard_id} transfer to stay below limit on peer {replica_id} (incoming: {incoming}, outgoing: {outgoing})");
+                    log::trace!(
+                        "Postponing automatic shard {shard_id} transfer to stay below limit on peer {replica_id} (incoming: {incoming}, outgoing: {outgoing})",
+                    );
                     continue;
                 }
 

commit af5b5bedd776ec4b2c43a61d57f592f57f552d5f
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Thu Mar 6 15:36:22 2025 +0100

    Separate StrictModeConfig output structure (#6114)
    
    * Separate StrictModeConfig output structure
    
    * happy regen
    
    * add strict sparse config
    
    * introduce CollectionConfigTelemetry
    
    * upd openapi
    
    ---------
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 58534ccaf..ffd5db357 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -52,7 +52,7 @@ use crate::shards::transfer::helpers::check_transfer_conflicts_strict;
 use crate::shards::transfer::transfer_tasks_pool::{TaskResult, TransferTasksPool};
 use crate::shards::transfer::{ShardTransfer, ShardTransferMethod};
 use crate::shards::{CollectionId, replica_set};
-use crate::telemetry::CollectionTelemetry;
+use crate::telemetry::{CollectionConfigTelemetry, CollectionTelemetry};
 
 /// Collection's data is split into several shards.
 pub struct Collection {
@@ -785,7 +785,7 @@ impl Collection {
         CollectionTelemetry {
             id: self.name(),
             init_time_ms: self.init_time.as_millis() as u64,
-            config: self.collection_config.read().await.clone(),
+            config: CollectionConfigTelemetry::from(self.collection_config.read().await.clone()),
             shards: shards_telemetry,
             transfers,
             resharding,

commit 12492c7bda5597142f053ce5a8baeeb12075ff89
Author: Tim Visée <tim+github@visee.me>
Date:   Thu Mar 20 15:12:12 2025 +0100

    Fix broken shard key mapping in consensus snapshot (#6209)
    
    * Use safe shard key wrapper type in consensus snapshot state
    
    * Add shard ID iterator
    
    * Produce net wrapped shard key mapping type directly
    
    * Add useful comments
    
    * Fix typo

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index ffd5db357..d94e4605f 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -344,8 +344,6 @@ impl Collection {
             .await
             .get_shard_key_to_ids_mapping()
             .keys()
-            .cloned()
-            .collect()
     }
 
     /// Return a list of local shards, present on this peer

commit c41a65a3dea52d12cee8f3cf0fa14cb9020625b9
Author: Tim Visée <tim+github@visee.me>
Date:   Fri Mar 21 09:33:42 2025 +0100

    Fix consensus snapshot not applying shard key mappings (#6212)
    
    * Propagate shard key mapping wrapper deeper
    
    * Also bump reverse shard mapping when applying shard keys from snapshot
    
    * Also apply new shard key mappings to existing replicas
    
    * Apply shard key to replica set through apply_state directly
    
    * Set both new shard key mappings through utility function
    
    * Set shard key mappings directly during directory creation
    
    * Rename get_key to key
    
    * Update comment

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index d94e4605f..ef81141a9 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -47,6 +47,7 @@ use crate::shards::replica_set::{
     ChangePeerFromState, ChangePeerState, ReplicaState, ShardReplicaSet,
 };
 use crate::shards::shard::{PeerId, ShardId};
+use crate::shards::shard_holder::shard_mapping::ShardKeyMappingWrapper;
 use crate::shards::shard_holder::{LockedShardHolder, ShardHolder, shard_not_found_error};
 use crate::shards::transfer::helpers::check_transfer_conflicts_strict;
 use crate::shards::transfer::transfer_tasks_pool::{TaskResult, TransferTasksPool};
@@ -104,6 +105,7 @@ impl Collection {
         collection_config: &CollectionConfigInternal,
         shared_storage_config: Arc<SharedStorageConfig>,
         shard_distribution: CollectionShardDistribution,
+        shard_key_mapping: Option<ShardKeyMappingWrapper>,
         channel_service: ChannelService,
         on_replica_failure: ChangePeerFromState,
         request_shard_transfer: RequestShardTransfer,
@@ -116,6 +118,7 @@ impl Collection {
         let start_time = std::time::Instant::now();
 
         let mut shard_holder = ShardHolder::new(path)?;
+        shard_holder.set_shard_key_mappings(shard_key_mapping.clone().unwrap_or_default())?;
 
         let payload_index_schema = Arc::new(Self::load_payload_index_schema(path)?);
 
@@ -129,7 +132,9 @@ impl Collection {
                     optimizers_overwrite.update(&effective_optimizers_config)?;
             }
 
-            let shard_key = None;
+            let shard_key = shard_key_mapping
+                .as_ref()
+                .and_then(|mapping| mapping.key(shard_id));
             let replica_set = ShardReplicaSet::build(
                 shard_id,
                 shard_key.clone(),

commit c4ec15db9aacfaf2728293105ed55ab5955db4ab
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Mon Mar 24 14:09:24 2025 +0100

    Simplify `ShardKeyMapping` implementation (#6220)
    
    * Simplify `ShardKeyMapping` impl
    
    * Automatically switch to new mappings format, if `Number` shard key is used
    
    * Apply TODOs
    
    * Add comment describing the issue, leave instructions for migration
    
    * Cleanup
    
    Remove unnecessary assignment
    
    * Cleanup
    
    Remove misleading comment
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index ef81141a9..2d95f079b 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -47,7 +47,7 @@ use crate::shards::replica_set::{
     ChangePeerFromState, ChangePeerState, ReplicaState, ShardReplicaSet,
 };
 use crate::shards::shard::{PeerId, ShardId};
-use crate::shards::shard_holder::shard_mapping::ShardKeyMappingWrapper;
+use crate::shards::shard_holder::shard_mapping::ShardKeyMapping;
 use crate::shards::shard_holder::{LockedShardHolder, ShardHolder, shard_not_found_error};
 use crate::shards::transfer::helpers::check_transfer_conflicts_strict;
 use crate::shards::transfer::transfer_tasks_pool::{TaskResult, TransferTasksPool};
@@ -105,7 +105,7 @@ impl Collection {
         collection_config: &CollectionConfigInternal,
         shared_storage_config: Arc<SharedStorageConfig>,
         shard_distribution: CollectionShardDistribution,
-        shard_key_mapping: Option<ShardKeyMappingWrapper>,
+        shard_key_mapping: Option<ShardKeyMapping>,
         channel_service: ChannelService,
         on_replica_failure: ChangePeerFromState,
         request_shard_transfer: RequestShardTransfer,
@@ -134,7 +134,7 @@ impl Collection {
 
             let shard_key = shard_key_mapping
                 .as_ref()
-                .and_then(|mapping| mapping.key(shard_id));
+                .and_then(|mapping| mapping.shard_key(shard_id));
             let replica_set = ShardReplicaSet::build(
                 shard_id,
                 shard_key.clone(),
@@ -349,6 +349,8 @@ impl Collection {
             .await
             .get_shard_key_to_ids_mapping()
             .keys()
+            .cloned()
+            .collect()
     }
 
     /// Return a list of local shards, present on this peer

commit b5e5f97bf997b0844806c9cf6fed02e39f3ced5d
Author: Kumar Shivendu <kshivendu1@gmail.com>
Date:   Wed Apr 16 22:19:35 2025 +0530

    Abort resharding if any Resharding replica is to be marked dead (#6364)
    
    * Abort resharding if any Resharding replica is to be marked dead
    
    * Abort resharding before marking shard as Dead
    
    * add comments
    
    * Abort resharding after marking as dead (#6394)
    
    * Force delete points when aborting resharding, even if replica is dead
    
    * Abort resharding after marking replica as dead
    
    * Only abort resharding if we mark related replica as dead
    
    * Just rely on shard replica state
    
    * Propagate shard transfer error right away
    
    ---------
    
    Co-authored-by: Tim Visée <tim+github@visee.me>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 2d95f079b..cc4d70e7f 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -436,55 +436,49 @@ impl Collection {
             )));
         }
 
-        // Abort resharding, if resharding shard is marked as `Dead`.
-        //
-        // This branch should only be triggered, if resharding is currently at `MigratingPoints`
-        // stage, because target shard should be marked as `Active`, when all resharding transfers
-        // are successfully completed, and so the check *right above* this one would be triggered.
-        //
-        // So, if resharding reached `ReadHashRingCommitted`, this branch *won't* be triggered,
-        // and resharding *won't* be cancelled. The update request should *fail* with "failed to
-        // update all replicas of a shard" error.
-        //
-        // If resharding reached `ReadHashRingCommitted`, and this branch is triggered *somehow*,
-        // then `Collection::abort_resharding` call should return an error, so no special handling
-        // is needed.
-        let is_resharding = matches!(
-            current_state,
-            Some(ReplicaState::Resharding | ReplicaState::ReshardingScaleDown)
-        );
-
-        if is_resharding && new_state == ReplicaState::Dead {
-            drop(shard_holder);
-
-            let resharding_state = self
-                .resharding_state()
-                .await
-                .filter(|state| state.peer_id == peer_id);
-
-            if let Some(state) = resharding_state {
-                self.abort_resharding(state.key(), false).await?;
-            }
-
-            return Ok(());
-        }
-
+        // Update replica status
         replica_set
             .ensure_replica_with_state(peer_id, new_state)
             .await?;
 
         if new_state == ReplicaState::Dead {
-            // TODO(resharding): Abort all resharding transfers!?
-
-            // Terminate transfer if source or target replicas are now dead
+            let resharding_state = shard_holder.resharding_state.read().clone();
             let related_transfers = shard_holder.get_related_transfers(shard_id, peer_id);
 
-            // `abort_shard_transfer` locks `shard_holder`!
+            // Functions below lock `shard_holder`!
             drop(shard_holder);
 
+            let mut abort_resharding_result = CollectionResult::Ok(());
+
+            // Abort resharding, if resharding shard is marked as `Dead`.
+            //
+            // This branch should only be triggered, if resharding is currently at `MigratingPoints`
+            // stage, because target shard should be marked as `Active`, when all resharding transfers
+            // are successfully completed, and so the check *right above* this one would be triggered.
+            //
+            // So, if resharding reached `ReadHashRingCommitted`, this branch *won't* be triggered,
+            // and resharding *won't* be cancelled. The update request should *fail* with "failed to
+            // update all replicas of a shard" error.
+            //
+            // If resharding reached `ReadHashRingCommitted`, and this branch is triggered *somehow*,
+            // then `Collection::abort_resharding` call should return an error, so no special handling
+            // is needed.
+            let is_resharding = current_state
+                .as_ref()
+                .is_some_and(ReplicaState::is_resharding);
+            if is_resharding {
+                if let Some(state) = resharding_state {
+                    abort_resharding_result = self.abort_resharding(state.key(), false).await;
+                }
+            }
+
+            // Terminate transfer if source or target replicas are now dead
             for transfer in related_transfers {
                 self.abort_shard_transfer(transfer.key(), None).await?;
             }
+
+            // Propagate resharding errors now
+            abort_resharding_result?;
         }
 
         // If not initialized yet, we need to check if it was initialized by this call

commit cd1287a2b9be9b3372beb165421297076c6ab11b
Author: Kumar Shivendu <kshivendu1@gmail.com>
Date:   Thu Apr 17 01:22:24 2025 +0530

    Recover dirty shards using other replicas when marked Dead (#6293)
    
    * Test behaviour of Qdrant with shard initializing flag
    
    * Corrupt shard directory and let Qdrant panic like prod
    
    * Wait for shard transfer
    
    * Restore dirty shards using other replicas
    
    * remove unused code
    
    * Request transfer only if replica is dead or dirty
    
    * fmt
    
    * remove comment
    
    * fix clippy
    
    * Delete shard initializing flag after initializing empty local shard
    
    * Expect test to recover shard in existing test
    
    * Review suggestions
    
    * Run tests for longer
    
    * Simplify tests
    
    * Use 2k points
    
    * condition for point_count
    
    * Add comment
    
    * fix flaky tests
    
    * fix flaky tests
    
    * handle edge case
    
    * Include Active in expected states list
    
    * Introduce is_recovery
    
    * simplify tests
    
    * get rid of is_dirty bool in DummyShard
    
    * add missing negation in condition
    
    * fix condition
    
    * final fix for transfer condition
    
    * Don't auto recover if in recovery mode, simplify state checking
    
    * minor comment improvements
    
    * tests scenario where node is killed after deleting shard initializing flag
    
    * Fix failing CI
    
    * Only automatically recover dead replicas
    
    * Mark replica as dead to recover dummy shard
    
    * fix failing test
    
    * Sleep one second after killing peer, give time to release WAL lock
    
    * Prevent waiting for peer to come online indefinitely
    
    * update comment
    
    * minor typo
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index cc4d70e7f..83a59da81 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -675,8 +675,15 @@ impl Collection {
                 continue;
             }
 
-            if this_peer_state != Some(Dead) || replica_set.is_dummy().await {
-                continue; // All good
+            // Don't automatically recover replicas if started in recovery mode
+            if self.shared_storage_config.recovery_mode.is_some() {
+                continue;
+            }
+
+            // Don't recover replicas if not dead
+            let is_dead = this_peer_state == Some(Dead);
+            if !is_dead {
+                continue;
             }
 
             // Try to find dead replicas with no active transfers

commit bbea80e7d4f675d455053c10e751cc0f45df45e6
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Thu Apr 17 23:32:10 2025 +0200

    Telemetry improvements (#6390)
    
    * allow shard states in anonymize telemetry (with hashed peer ids)
    
    * introduce level3 and level4 for telemetry, where level4 = everything, incl. segments info
    
    * upd openapi
    
    * fix tests
    
    * expose vector count & size stats on shard level to avoid reading of segments
    
    * fix spelling
    
    * upd schema
    
    * fix tests
    
    * Use unwrap_or_default
    
    * [#6390] skip serializing shard details in Level2 (#6398)
    
    * skip serializing shard details in Level2
    
    * upd openapi
    
    ---------
    
    Co-authored-by: generall <andrey@vasnetsov.com>
    
    ---------
    
    Co-authored-by: Tim Visée <tim+github@visee.me>
    Co-authored-by: Luis Cossío <luis.cossio@qdrant.com>

diff --git a/lib/collection/src/collection/mod.rs b/lib/collection/src/collection/mod.rs
index 83a59da81..c2a819d75 100644
--- a/lib/collection/src/collection/mod.rs
+++ b/lib/collection/src/collection/mod.rs
@@ -20,7 +20,7 @@ use std::time::Duration;
 
 use clean::ShardCleanTasks;
 use common::budget::ResourceBudget;
-use common::types::TelemetryDetail;
+use common::types::{DetailsLevel, TelemetryDetail};
 use io::storage_version::StorageVersion;
 use segment::types::ShardKey;
 use semver::Version;
@@ -36,7 +36,7 @@ use crate::common::is_ready::IsReady;
 use crate::config::CollectionConfigInternal;
 use crate::operations::config_diff::{DiffConfig, OptimizersConfigDiff};
 use crate::operations::shared_storage_config::SharedStorageConfig;
-use crate::operations::types::{CollectionError, CollectionResult, NodeType};
+use crate::operations::types::{CollectionError, CollectionResult, NodeType, OptimizersStatus};
 use crate::optimizers_builder::OptimizersConfig;
 use crate::save_on_disk::SaveOnDisk;
 use crate::shards::channel_service::ChannelService;
@@ -53,7 +53,9 @@ use crate::shards::transfer::helpers::check_transfer_conflicts_strict;
 use crate::shards::transfer::transfer_tasks_pool::{TaskResult, TransferTasksPool};
 use crate::shards::transfer::{ShardTransfer, ShardTransferMethod};
 use crate::shards::{CollectionId, replica_set};
-use crate::telemetry::{CollectionConfigTelemetry, CollectionTelemetry};
+use crate::telemetry::{
+    CollectionConfigTelemetry, CollectionTelemetry, CollectionsAggregatedTelemetry,
+};
 
 /// Collection's data is split into several shards.
 pub struct Collection {
@@ -774,20 +776,28 @@ impl Collection {
 
     pub async fn get_telemetry_data(&self, detail: TelemetryDetail) -> CollectionTelemetry {
         let (shards_telemetry, transfers, resharding) = {
-            let mut shards_telemetry = Vec::new();
-            let shards_holder = self.shards_holder.read().await;
-            for shard in shards_holder.all_shards() {
-                shards_telemetry.push(shard.get_telemetry_data(detail).await)
+            if detail.level >= DetailsLevel::Level3 {
+                let shards_holder = self.shards_holder.read().await;
+                let mut shards_telemetry = Vec::new();
+                for shard in shards_holder.all_shards() {
+                    shards_telemetry.push(shard.get_telemetry_data(detail).await)
+                }
+                (
+                    Some(shards_telemetry),
+                    Some(shards_holder.get_shard_transfer_info(&*self.transfer_tasks.lock().await)),
+                    Some(
+                        shards_holder
+                            .get_resharding_operations_info()
+                            .unwrap_or_default(),
+                    ),
+                )
+            } else {
+                (None, None, None)
             }
-            (
-                shards_telemetry,
-                shards_holder.get_shard_transfer_info(&*self.transfer_tasks.lock().await),
-                shards_holder
-                    .get_resharding_operations_info()
-                    .unwrap_or_default(),
-            )
         };
 
+        let shard_clean_tasks = self.clean_local_shards_statuses();
+
         CollectionTelemetry {
             id: self.name(),
             init_time_ms: self.init_time.as_millis() as u64,
@@ -795,7 +805,36 @@ impl Collection {
             shards: shards_telemetry,
             transfers,
             resharding,
-            shard_clean_tasks: self.clean_local_shards_statuses(),
+            shard_clean_tasks: (!shard_clean_tasks.is_empty()).then_some(shard_clean_tasks),
+        }
+    }
+
+    pub async fn get_aggregated_telemetry_data(&self) -> CollectionsAggregatedTelemetry {
+        let shards_holder = self.shards_holder.read().await;
+
+        let mut shard_optimization_statuses = Vec::new();
+        let mut vectors = 0;
+
+        for shard in shards_holder.all_shards() {
+            let shard_optimization_status = shard
+                .get_optimization_status()
+                .await
+                .unwrap_or(OptimizersStatus::Ok);
+
+            shard_optimization_statuses.push(shard_optimization_status);
+
+            vectors += shard.get_size_stats().await.num_vectors;
+        }
+
+        let optimizers_status = shard_optimization_statuses
+            .into_iter()
+            .max()
+            .unwrap_or(OptimizersStatus::Ok);
+
+        CollectionsAggregatedTelemetry {
+            vectors,
+            optimizers_status,
+            params: self.collection_config.read().await.params.clone(),
         }
     }
 

