# Instructions

You are being benchmarked. You will see the output of a git log command, and from that must infer the current state of a file. Think carefully, as you must output the exact state of the file to earn full marks.

**Important:** Your goal is to reproduce the file's content *exactly* as it exists at the final commit, even if the code appears broken, buggy, or contains obvious errors. Do **not** try to "fix" the code. Attempting to correct issues will result in a poor score, as this benchmark evaluates your ability to reproduce the precise state of the file based on its history.

# Required Response Format

Wrap the content of the file in triple backticks (```). Any text outside the final closing backticks will be ignored. End your response after outputting the closing backticks.

# Example Response

```python
#!/usr/bin/env python
print('Hello, world!')
```

# File History

> git log -p --cc --topo-order --reverse -- lib/collection/src/update_handler.rs

commit 93e0fb5c2c8f85f232bef82f48ab2b80c43f76cc
Author: Konstantin <kgrech@users.noreply.github.com>
Date:   Sat Jul 3 12:12:21 2021 +0100

    [CLIPPY] Fix the last portion of rules and enable CI check (#53)
    
    * [CLIPPY] Fixed the warning for references of the user defined types
    
    * [CLIPPY] Fix module naming issue
    
    * [CLIPPY] Fix the last set of warnings and enable clippy check during CI
    
    * Moved cargo fmt and cargo clippy into it's own action

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
new file mode 100644
index 000000000..1533c1265
--- /dev/null
+++ b/lib/collection/src/update_handler.rs
@@ -0,0 +1,135 @@
+use crate::operations::types::CollectionResult;
+use crate::operations::CollectionUpdateOperations;
+use crate::segment_manager::holders::segment_holder::LockedSegmentHolder;
+use crate::segment_manager::optimizers::segment_optimizer::SegmentOptimizer;
+use crate::wal::SerdeWal;
+use crossbeam_channel::Receiver;
+use log::debug;
+use parking_lot::Mutex;
+use segment::types::SeqNumberType;
+use std::sync::Arc;
+use tokio::runtime::Handle;
+use tokio::task::JoinHandle;
+use tokio::time::{Duration, Instant};
+
+pub type Optimizer = dyn SegmentOptimizer + Sync + Send;
+
+pub enum UpdateSignal {
+    /// Info that operation with
+    Operation(SeqNumberType),
+    /// Stop all optimizers and listening
+    Stop,
+    /// Empty signal used to trigger optimizers
+    Nop,
+}
+
+pub struct UpdateHandler {
+    pub optimizers: Arc<Vec<Box<Optimizer>>>,
+    pub flush_timeout_sec: u64,
+    segments: LockedSegmentHolder,
+    receiver: Receiver<UpdateSignal>,
+    worker: Option<JoinHandle<()>>,
+    runtime_handle: Handle,
+    wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
+}
+
+impl UpdateHandler {
+    pub fn new(
+        optimizers: Arc<Vec<Box<Optimizer>>>,
+        receiver: Receiver<UpdateSignal>,
+        runtime_handle: Handle,
+        segments: LockedSegmentHolder,
+        wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
+        flush_timeout_sec: u64,
+    ) -> UpdateHandler {
+        let mut handler = UpdateHandler {
+            optimizers,
+            segments,
+            receiver,
+            worker: None,
+            runtime_handle,
+            wal,
+            flush_timeout_sec,
+        };
+        handler.run_worker();
+        handler
+    }
+
+    pub fn run_worker(&mut self) {
+        self.worker = Some(self.runtime_handle.spawn(Self::worker_fn(
+            self.optimizers.clone(),
+            self.receiver.clone(),
+            self.segments.clone(),
+            self.wal.clone(),
+            self.flush_timeout_sec,
+        )));
+    }
+
+    /// Gracefully wait before all optimizations stop
+    /// If some optimization is in progress - it will be finished before shutdown.
+    /// Blocking function.
+    pub fn wait_worker_stops(&mut self) -> CollectionResult<()> {
+        let res = match &mut self.worker {
+            None => (),
+            Some(handle) => self.runtime_handle.block_on(handle)?,
+        };
+
+        self.worker = None;
+        Ok(res)
+    }
+
+    fn process_optimization(optimizers: Arc<Vec<Box<Optimizer>>>, segments: LockedSegmentHolder) {
+        for optimizer in optimizers.iter() {
+            let mut nonoptimal_segment_ids = optimizer.check_condition(segments.clone());
+            while !nonoptimal_segment_ids.is_empty() {
+                debug!(
+                    "Start optimization on segments: {:?}",
+                    nonoptimal_segment_ids
+                );
+                // If optimization fails, it could not be reported to anywhere except for console.
+                // So the only recovery here is to stop optimization and await for restart
+                optimizer
+                    .optimize(segments.clone(), nonoptimal_segment_ids)
+                    .unwrap();
+                nonoptimal_segment_ids = optimizer.check_condition(segments.clone());
+            }
+        }
+    }
+
+    async fn worker_fn(
+        optimizers: Arc<Vec<Box<Optimizer>>>,
+        receiver: Receiver<UpdateSignal>,
+        segments: LockedSegmentHolder,
+        wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
+        flush_timeout_sec: u64,
+    ) {
+        let flush_timeout = Duration::from_secs(flush_timeout_sec);
+        let mut last_flushed = Instant::now();
+        loop {
+            let recv_res = receiver.recv();
+            match recv_res {
+                Ok(signal) => {
+                    match signal {
+                        UpdateSignal::Nop => {
+                            Self::process_optimization(optimizers.clone(), segments.clone());
+                        }
+                        UpdateSignal::Operation(operation_id) => {
+                            debug!("Performing update operation: {}", operation_id);
+                            Self::process_optimization(optimizers.clone(), segments.clone());
+
+                            let elapsed = last_flushed.elapsed();
+                            if elapsed > flush_timeout {
+                                debug!("Performing flushing: {}", operation_id);
+                                last_flushed = Instant::now();
+                                let flushed_operation = segments.read().flush_all().unwrap();
+                                wal.lock().ack(flushed_operation).unwrap();
+                            }
+                        }
+                        UpdateSignal::Stop => break, // Stop gracefully
+                    }
+                }
+                Err(_) => break, // Transmitter was destroyed
+            }
+        }
+    }
+}

commit 12e25089cb1669a3d67c6d5ce9e68fc47d10cb6f
Author: Konstantin <kgrech@users.noreply.github.com>
Date:   Sun Jul 4 23:20:16 2021 +0100

    Actix update (#55)
    
    * Updated actix to 4.0.0-beta.8
    
    * Refactored search, scroll, update and collection operation APIs to be async

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 1533c1265..30c5e9147 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -68,14 +68,12 @@ impl UpdateHandler {
     /// Gracefully wait before all optimizations stop
     /// If some optimization is in progress - it will be finished before shutdown.
     /// Blocking function.
-    pub fn wait_worker_stops(&mut self) -> CollectionResult<()> {
-        let res = match &mut self.worker {
-            None => (),
-            Some(handle) => self.runtime_handle.block_on(handle)?,
-        };
-
-        self.worker = None;
-        Ok(res)
+    pub async fn wait_worker_stops(&mut self) -> CollectionResult<()> {
+        let maybe_handle = self.worker.take();
+        if let Some(handle) = maybe_handle {
+            handle.await?;
+        }
+        Ok(())
     }
 
     fn process_optimization(optimizers: Arc<Vec<Box<Optimizer>>>, segments: LockedSegmentHolder) {

commit 53ddce350e994ab14062f0fef53d556116df3616
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Mon Jul 5 00:43:23 2021 +0200

    Revert "Actix update (#55)" (#56)
    
    This reverts commit 12e25089cb1669a3d67c6d5ce9e68fc47d10cb6f.

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 30c5e9147..1533c1265 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -68,12 +68,14 @@ impl UpdateHandler {
     /// Gracefully wait before all optimizations stop
     /// If some optimization is in progress - it will be finished before shutdown.
     /// Blocking function.
-    pub async fn wait_worker_stops(&mut self) -> CollectionResult<()> {
-        let maybe_handle = self.worker.take();
-        if let Some(handle) = maybe_handle {
-            handle.await?;
-        }
-        Ok(())
+    pub fn wait_worker_stops(&mut self) -> CollectionResult<()> {
+        let res = match &mut self.worker {
+            None => (),
+            Some(handle) => self.runtime_handle.block_on(handle)?,
+        };
+
+        self.worker = None;
+        Ok(res)
     }
 
     fn process_optimization(optimizers: Arc<Vec<Box<Optimizer>>>, segments: LockedSegmentHolder) {

commit 910c3b9f8a9ddf42cb9077b794fc5b6a9fa40484
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Mon Jul 5 23:38:00 2021 +0200

    Revert "Revert "Actix update (#55)" (#56)" (#57)
    
    This reverts commit 53ddce350e994ab14062f0fef53d556116df3616.

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 1533c1265..30c5e9147 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -68,14 +68,12 @@ impl UpdateHandler {
     /// Gracefully wait before all optimizations stop
     /// If some optimization is in progress - it will be finished before shutdown.
     /// Blocking function.
-    pub fn wait_worker_stops(&mut self) -> CollectionResult<()> {
-        let res = match &mut self.worker {
-            None => (),
-            Some(handle) => self.runtime_handle.block_on(handle)?,
-        };
-
-        self.worker = None;
-        Ok(res)
+    pub async fn wait_worker_stops(&mut self) -> CollectionResult<()> {
+        let maybe_handle = self.worker.take();
+        if let Some(handle) = maybe_handle {
+            handle.await?;
+        }
+        Ok(())
     }
 
     fn process_optimization(optimizers: Arc<Vec<Box<Optimizer>>>, segments: LockedSegmentHolder) {

commit 446d0c29f70f1154025e644b154adbd270007290
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Sun Aug 15 23:26:01 2021 +0200

    Deadlock fix (#91)
    
    * refactor: segment managers -> collection managers
    
    * fix segments holder deadlock
    
    * apply cargo fmt
    
    * fix cargo clippy
    
    * replace sequential segment locking with multiple try_lock attempts to prevent deadlocks

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 30c5e9147..60c32a397 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -1,7 +1,7 @@
+use crate::collection_manager::holders::segment_holder::LockedSegmentHolder;
+use crate::collection_manager::optimizers::segment_optimizer::SegmentOptimizer;
 use crate::operations::types::CollectionResult;
 use crate::operations::CollectionUpdateOperations;
-use crate::segment_manager::holders::segment_holder::LockedSegmentHolder;
-use crate::segment_manager::optimizers::segment_optimizer::SegmentOptimizer;
 use crate::wal::SerdeWal;
 use crossbeam_channel::Receiver;
 use log::debug;
@@ -112,7 +112,6 @@ impl UpdateHandler {
                             Self::process_optimization(optimizers.clone(), segments.clone());
                         }
                         UpdateSignal::Operation(operation_id) => {
-                            debug!("Performing update operation: {}", operation_id);
                             Self::process_optimization(optimizers.clone(), segments.clone());
 
                             let elapsed = last_flushed.elapsed();

commit bf3d8c25753188b4ca5e69a13c7f26e3c383f05b
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Sun Oct 24 18:10:39 2021 +0200

    data consistency fixes and updates (#112)
    
    * update segment version after completed update only
    
    * more stable updates: check pre-existing points on update, fail recovery, WAL proper ack. check_unprocessed_points WIP
    
    * switch to async channel
    
    * perform update operations in a separate thread (#111)
    
    * perform update operations in a separate thread
    
    * ordered sending update signal
    
    * locate a segment merging versioning bug
    
    * rename id_mapper -> id_tracker
    
    * per-record versioning
    
    * clippy fixes
    
    * cargo fmt
    
    * rm limit of open files
    
    * fail recovery test
    
    * cargo fmt
    
    * wait for worker stops befor dropping the runtime

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 60c32a397..2d05d1d71 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -1,12 +1,14 @@
+use crate::collection_manager::collection_updater::CollectionUpdater;
 use crate::collection_manager::holders::segment_holder::LockedSegmentHolder;
 use crate::collection_manager::optimizers::segment_optimizer::SegmentOptimizer;
 use crate::operations::types::CollectionResult;
 use crate::operations::CollectionUpdateOperations;
 use crate::wal::SerdeWal;
-use crossbeam_channel::Receiver;
-use log::debug;
+use async_channel::{Receiver, Sender};
+use log::{debug, info};
 use parking_lot::Mutex;
 use segment::types::SeqNumberType;
+use std::cmp::min;
 use std::sync::Arc;
 use tokio::runtime::Handle;
 use tokio::task::JoinHandle;
@@ -14,8 +16,26 @@ use tokio::time::{Duration, Instant};
 
 pub type Optimizer = dyn SegmentOptimizer + Sync + Send;
 
+pub struct OperationData {
+    /// Sequential number of the operation
+    pub op_num: SeqNumberType,
+    /// Operation
+    pub operation: CollectionUpdateOperations,
+    /// Callback notification channel
+    pub sender: Option<Sender<CollectionResult<usize>>>,
+}
+
 pub enum UpdateSignal {
-    /// Info that operation with
+    /// Requested operation to perform
+    Operation(OperationData),
+    /// Stop all optimizers and listening
+    Stop,
+    /// Empty signal used to trigger optimizers
+    Nop,
+}
+
+pub enum OptimizerSignal {
+    /// Sequential number of the operation
     Operation(SeqNumberType),
     /// Stop all optimizers and listening
     Stop,
@@ -27,8 +47,9 @@ pub struct UpdateHandler {
     pub optimizers: Arc<Vec<Box<Optimizer>>>,
     pub flush_timeout_sec: u64,
     segments: LockedSegmentHolder,
-    receiver: Receiver<UpdateSignal>,
-    worker: Option<JoinHandle<()>>,
+    update_receiver: Receiver<UpdateSignal>,
+    update_worker: Option<JoinHandle<()>>,
+    optimizer_worker: Option<JoinHandle<()>>,
     runtime_handle: Handle,
     wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
 }
@@ -36,7 +57,7 @@ pub struct UpdateHandler {
 impl UpdateHandler {
     pub fn new(
         optimizers: Arc<Vec<Box<Optimizer>>>,
-        receiver: Receiver<UpdateSignal>,
+        update_receiver: Receiver<UpdateSignal>,
         runtime_handle: Handle,
         segments: LockedSegmentHolder,
         wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
@@ -45,37 +66,68 @@ impl UpdateHandler {
         let mut handler = UpdateHandler {
             optimizers,
             segments,
-            receiver,
-            worker: None,
+            update_receiver,
+            update_worker: None,
+            optimizer_worker: None,
             runtime_handle,
             wal,
             flush_timeout_sec,
         };
-        handler.run_worker();
+        handler.run_workers();
         handler
     }
 
-    pub fn run_worker(&mut self) {
-        self.worker = Some(self.runtime_handle.spawn(Self::worker_fn(
+    pub fn run_workers(&mut self) {
+        let (tx, rx) = async_channel::unbounded();
+        self.optimizer_worker = Some(self.runtime_handle.spawn(Self::optimization_worker_fn(
             self.optimizers.clone(),
-            self.receiver.clone(),
+            rx,
             self.segments.clone(),
             self.wal.clone(),
             self.flush_timeout_sec,
         )));
+        self.update_worker = Some(self.runtime_handle.spawn(Self::update_worker_fn(
+            self.update_receiver.clone(),
+            tx,
+            self.segments.clone(),
+        )));
     }
 
     /// Gracefully wait before all optimizations stop
     /// If some optimization is in progress - it will be finished before shutdown.
     /// Blocking function.
-    pub async fn wait_worker_stops(&mut self) -> CollectionResult<()> {
-        let maybe_handle = self.worker.take();
+    pub async fn wait_workers_stops(&mut self) -> CollectionResult<()> {
+        let maybe_handle = self.update_worker.take();
+        if let Some(handle) = maybe_handle {
+            handle.await?;
+        }
+        let maybe_handle = self.optimizer_worker.take();
         if let Some(handle) = maybe_handle {
             handle.await?;
         }
         Ok(())
     }
 
+    /// Checks if there are any failed operations.
+    /// If so - attempts to re-apply all failed operations.
+    fn try_recover(
+        segments: LockedSegmentHolder,
+        wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
+    ) -> CollectionResult<usize> {
+        // Try to re-apply everything starting from the first failed operation
+        let first_failed_operation_option = segments.read().failed_operation.iter().cloned().min();
+        match first_failed_operation_option {
+            None => {}
+            Some(first_failed_op) => {
+                let wal_lock = wal.lock();
+                for (op_num, operation) in wal_lock.read(first_failed_op) {
+                    CollectionUpdater::update(&segments, op_num, operation)?;
+                }
+            }
+        };
+        Ok(0)
+    }
+
     fn process_optimization(optimizers: Arc<Vec<Box<Optimizer>>>, segments: LockedSegmentHolder) {
         for optimizer in optimizers.iter() {
             let mut nonoptimal_segment_ids = optimizer.check_condition(segments.clone());
@@ -94,9 +146,9 @@ impl UpdateHandler {
         }
     }
 
-    async fn worker_fn(
+    async fn optimization_worker_fn(
         optimizers: Arc<Vec<Box<Optimizer>>>,
-        receiver: Receiver<UpdateSignal>,
+        receiver: Receiver<OptimizerSignal>,
         segments: LockedSegmentHolder,
         wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
         flush_timeout_sec: u64,
@@ -104,29 +156,96 @@ impl UpdateHandler {
         let flush_timeout = Duration::from_secs(flush_timeout_sec);
         let mut last_flushed = Instant::now();
         loop {
-            let recv_res = receiver.recv();
+            let recv_res = receiver.recv().await;
             match recv_res {
                 Ok(signal) => {
                     match signal {
-                        UpdateSignal::Nop => {
+                        OptimizerSignal::Nop => {
+                            if Self::try_recover(segments.clone(), wal.clone()).is_err() {
+                                continue;
+                            }
                             Self::process_optimization(optimizers.clone(), segments.clone());
                         }
-                        UpdateSignal::Operation(operation_id) => {
+                        OptimizerSignal::Operation(operation_id) => {
+                            if Self::try_recover(segments.clone(), wal.clone()).is_err() {
+                                continue;
+                            }
                             Self::process_optimization(optimizers.clone(), segments.clone());
 
                             let elapsed = last_flushed.elapsed();
                             if elapsed > flush_timeout {
                                 debug!("Performing flushing: {}", operation_id);
                                 last_flushed = Instant::now();
-                                let flushed_operation = segments.read().flush_all().unwrap();
-                                wal.lock().ack(flushed_operation).unwrap();
+                                let confirmed_version = {
+                                    let read_segments = segments.read();
+                                    let flushed_version = read_segments.flush_all().unwrap();
+                                    match read_segments.failed_operation.iter().cloned().min() {
+                                        None => flushed_version,
+                                        Some(failed_operation) => {
+                                            min(failed_operation, flushed_version)
+                                        }
+                                    }
+                                };
+                                wal.lock().ack(confirmed_version).unwrap();
                             }
                         }
-                        UpdateSignal::Stop => break, // Stop gracefully
+                        OptimizerSignal::Stop => break, // Stop gracefully
                     }
                 }
                 Err(_) => break, // Transmitter was destroyed
             }
         }
     }
+
+    async fn update_worker_fn(
+        receiver: Receiver<UpdateSignal>,
+        optimize_sender: Sender<OptimizerSignal>,
+        segments: LockedSegmentHolder,
+    ) {
+        loop {
+            let recv_res = receiver.recv().await;
+            match recv_res {
+                Ok(signal) => {
+                    match signal {
+                        UpdateSignal::Operation(OperationData {
+                            op_num,
+                            operation,
+                            sender,
+                        }) => {
+                            let res = match CollectionUpdater::update(&segments, op_num, operation)
+                            {
+                                Ok(update_res) => optimize_sender
+                                    .send(OptimizerSignal::Operation(op_num))
+                                    .await
+                                    .and(Ok(update_res))
+                                    .map_err(|send_err| send_err.into()),
+                                Err(err) => Err(err),
+                            };
+
+                            if let Some(feedback) = sender {
+                                feedback.send(res).await.unwrap_or_else(|_| {
+                                    info!("Can't report operation {} result. Assume already not required", op_num);
+                                });
+                            };
+                        }
+                        UpdateSignal::Stop => {
+                            optimize_sender.send(OptimizerSignal::Stop).await.unwrap_or_else(|_| {
+                                debug!("Optimizer already stopped")
+                            });
+                            break;
+                        }
+                        UpdateSignal::Nop => optimize_sender.send(OptimizerSignal::Nop).await.unwrap_or_else(|_| {
+                            info!("Can't notify optimizers, assume process is dead. Restart is required");
+                        }),
+                    }
+                }
+                Err(_) => {
+                    optimize_sender.send(OptimizerSignal::Stop).await.unwrap_or_else(|_| {
+                        debug!("Optimizer already stopped")
+                    });
+                    break;
+                } // Transmitter was destroyed
+            }
+        }
+    }
 }

commit 97b227048513143e555353d346a7f4560db9854e
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Mon Nov 29 09:39:22 2021 +0100

    Rustdoc and README for internal entities and processes (#123)
    
    * extend comments for strorage crate
    
    * update comments and readme for collection crate
    
    * apply cargo fmt
    
    * fix tests
    
    * apply fmt

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 2d05d1d71..209f2da34 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -16,6 +16,7 @@ use tokio::time::{Duration, Instant};
 
 pub type Optimizer = dyn SegmentOptimizer + Sync + Send;
 
+/// Information, required to perform operation and notify regarding the result
 pub struct OperationData {
     /// Sequential number of the operation
     pub op_num: SeqNumberType,
@@ -25,6 +26,7 @@ pub struct OperationData {
     pub sender: Option<Sender<CollectionResult<usize>>>,
 }
 
+/// Signal, used to inform Updater process
 pub enum UpdateSignal {
     /// Requested operation to perform
     Operation(OperationData),
@@ -34,6 +36,7 @@ pub enum UpdateSignal {
     Nop,
 }
 
+/// Signal, used to inform Optimization process
 pub enum OptimizerSignal {
     /// Sequential number of the operation
     Operation(SeqNumberType),
@@ -43,14 +46,21 @@ pub enum OptimizerSignal {
     Nop,
 }
 
+/// Structure, which holds object, required for processing updates of the collection
 pub struct UpdateHandler {
+    /// List of used optimizers
     pub optimizers: Arc<Vec<Box<Optimizer>>>,
+    /// How frequent can we flush data
     pub flush_timeout_sec: u64,
     segments: LockedSegmentHolder,
+    /// Channel receiver, which is listened by the updater process
     update_receiver: Receiver<UpdateSignal>,
+    /// Process, that listens updates signals and perform updates
     update_worker: Option<JoinHandle<()>>,
+    /// Process, that listens for post-update signals and performs optimization
     optimizer_worker: Option<JoinHandle<()>>,
     runtime_handle: Handle,
+    /// WAL, required for operations
     wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
 }
 

commit eb786ab64ba01b31eabaa6ae2f59c86321f0398e
Author: Anton V <94402218+anveq@users.noreply.github.com>
Date:   Thu Dec 9 20:18:20 2021 +0300

    Multithread optimizer (#134)
    
    * run optimizers on tokio thread pool for cpu-bound tasks
    
    * [WIP] move check condition to another thread
    
    * [WIP] optimizer iter live not long enough
    
    * [WIP] change Box to Arc in optimizers vector
    
    * add blocking handles management
    
    * cargo fmt apply
    
    * Update lib/collection/src/update_handler.rs
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>
    
    * [WIP] optimizer iter live not long enough
    
    * [WIP] change Box to Arc in optimizers vector
    
    * add blocking handles management
    
    * fix code review issues
    
    * use CollectionConfig available cpu value
    
    * apply updated fmt
    
    * [WIP] move count of optimization threads to OptimizersConfig
    
    * optimization options
    
    * fix formatting
    
    * fix proto for optimizer config
    
    * fmt
    
    * Update config/config.yaml
    
    related task: https://github.com/qdrant/qdrant/issues/30
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 209f2da34..5829a7edb 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -49,7 +49,7 @@ pub enum OptimizerSignal {
 /// Structure, which holds object, required for processing updates of the collection
 pub struct UpdateHandler {
     /// List of used optimizers
-    pub optimizers: Arc<Vec<Box<Optimizer>>>,
+    pub optimizers: Arc<Vec<Arc<Optimizer>>>,
     /// How frequent can we flush data
     pub flush_timeout_sec: u64,
     segments: LockedSegmentHolder,
@@ -62,11 +62,12 @@ pub struct UpdateHandler {
     runtime_handle: Handle,
     /// WAL, required for operations
     wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
+    optimization_handles: Arc<Mutex<Vec<JoinHandle<()>>>>,
 }
 
 impl UpdateHandler {
     pub fn new(
-        optimizers: Arc<Vec<Box<Optimizer>>>,
+        optimizers: Arc<Vec<Arc<Optimizer>>>,
         update_receiver: Receiver<UpdateSignal>,
         runtime_handle: Handle,
         segments: LockedSegmentHolder,
@@ -82,6 +83,7 @@ impl UpdateHandler {
             runtime_handle,
             wal,
             flush_timeout_sec,
+            optimization_handles: Arc::new(Mutex::new(vec![])),
         };
         handler.run_workers();
         handler
@@ -95,6 +97,7 @@ impl UpdateHandler {
             self.segments.clone(),
             self.wal.clone(),
             self.flush_timeout_sec,
+            self.optimization_handles.clone(),
         )));
         self.update_worker = Some(self.runtime_handle.spawn(Self::update_worker_fn(
             self.update_receiver.clone(),
@@ -107,6 +110,9 @@ impl UpdateHandler {
     /// If some optimization is in progress - it will be finished before shutdown.
     /// Blocking function.
     pub async fn wait_workers_stops(&mut self) -> CollectionResult<()> {
+        for handle in self.optimization_handles.lock().iter() {
+            handle.abort();
+        }
         let maybe_handle = self.update_worker.take();
         if let Some(handle) = maybe_handle {
             handle.await?;
@@ -138,30 +144,36 @@ impl UpdateHandler {
         Ok(0)
     }
 
-    fn process_optimization(optimizers: Arc<Vec<Box<Optimizer>>>, segments: LockedSegmentHolder) {
+    fn process_optimization(
+        optimizers: Arc<Vec<Arc<Optimizer>>>,
+        segments: LockedSegmentHolder,
+    ) -> Vec<JoinHandle<()>> {
+        let mut handles = vec![];
         for optimizer in optimizers.iter() {
-            let mut nonoptimal_segment_ids = optimizer.check_condition(segments.clone());
-            while !nonoptimal_segment_ids.is_empty() {
-                debug!(
-                    "Start optimization on segments: {:?}",
-                    nonoptimal_segment_ids
-                );
-                // If optimization fails, it could not be reported to anywhere except for console.
-                // So the only recovery here is to stop optimization and await for restart
-                optimizer
-                    .optimize(segments.clone(), nonoptimal_segment_ids)
-                    .unwrap();
-                nonoptimal_segment_ids = optimizer.check_condition(segments.clone());
+            loop {
+                let nonoptimal_segment_ids = optimizer.check_condition(segments.clone());
+                if nonoptimal_segment_ids.is_empty() {
+                    break;
+                } else {
+                    let optim = optimizer.clone();
+                    let segs = segments.clone();
+                    let nsi = nonoptimal_segment_ids.clone();
+                    handles.push(tokio::task::spawn_blocking(move || {
+                        optim.as_ref().optimize(segs, nsi).unwrap();
+                    }));
+                }
             }
         }
+        handles
     }
 
     async fn optimization_worker_fn(
-        optimizers: Arc<Vec<Box<Optimizer>>>,
+        optimizers: Arc<Vec<Arc<Optimizer>>>,
         receiver: Receiver<OptimizerSignal>,
         segments: LockedSegmentHolder,
         wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
         flush_timeout_sec: u64,
+        blocking_handles: Arc<Mutex<Vec<JoinHandle<()>>>>,
     ) {
         let flush_timeout = Duration::from_secs(flush_timeout_sec);
         let mut last_flushed = Instant::now();
@@ -174,13 +186,23 @@ impl UpdateHandler {
                             if Self::try_recover(segments.clone(), wal.clone()).is_err() {
                                 continue;
                             }
-                            Self::process_optimization(optimizers.clone(), segments.clone());
+                            let mut handles = blocking_handles.lock();
+                            handles.append(&mut Self::process_optimization(
+                                optimizers.clone(),
+                                segments.clone(),
+                            ));
                         }
                         OptimizerSignal::Operation(operation_id) => {
                             if Self::try_recover(segments.clone(), wal.clone()).is_err() {
                                 continue;
                             }
-                            Self::process_optimization(optimizers.clone(), segments.clone());
+                            {
+                                let mut handles = blocking_handles.lock();
+                                handles.append(&mut Self::process_optimization(
+                                    optimizers.clone(),
+                                    segments.clone(),
+                                ));
+                            }
 
                             let elapsed = last_flushed.elapsed();
                             if elapsed > flush_timeout {

commit 3cc7e5f7aafe46d04c5592bbc48450bd3012830c
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Mon Dec 13 11:44:51 2021 +0100

    refactor: replace parking_lot with tokio mutex for WAL (#140)
    
    * refactor: replace parking_lot with tokio mutex for WAL
    
    * cargo fmt

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 5829a7edb..9fc23c48e 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -6,11 +6,11 @@ use crate::operations::CollectionUpdateOperations;
 use crate::wal::SerdeWal;
 use async_channel::{Receiver, Sender};
 use log::{debug, info};
-use parking_lot::Mutex;
 use segment::types::SeqNumberType;
 use std::cmp::min;
 use std::sync::Arc;
 use tokio::runtime::Handle;
+use tokio::sync::Mutex;
 use tokio::task::JoinHandle;
 use tokio::time::{Duration, Instant};
 
@@ -110,7 +110,7 @@ impl UpdateHandler {
     /// If some optimization is in progress - it will be finished before shutdown.
     /// Blocking function.
     pub async fn wait_workers_stops(&mut self) -> CollectionResult<()> {
-        for handle in self.optimization_handles.lock().iter() {
+        for handle in self.optimization_handles.lock().await.iter() {
             handle.abort();
         }
         let maybe_handle = self.update_worker.take();
@@ -126,7 +126,7 @@ impl UpdateHandler {
 
     /// Checks if there are any failed operations.
     /// If so - attempts to re-apply all failed operations.
-    fn try_recover(
+    async fn try_recover(
         segments: LockedSegmentHolder,
         wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
     ) -> CollectionResult<usize> {
@@ -135,7 +135,7 @@ impl UpdateHandler {
         match first_failed_operation_option {
             None => {}
             Some(first_failed_op) => {
-                let wal_lock = wal.lock();
+                let wal_lock = wal.lock().await;
                 for (op_num, operation) in wal_lock.read(first_failed_op) {
                     CollectionUpdater::update(&segments, op_num, operation)?;
                 }
@@ -183,21 +183,27 @@ impl UpdateHandler {
                 Ok(signal) => {
                     match signal {
                         OptimizerSignal::Nop => {
-                            if Self::try_recover(segments.clone(), wal.clone()).is_err() {
+                            if Self::try_recover(segments.clone(), wal.clone())
+                                .await
+                                .is_err()
+                            {
                                 continue;
                             }
-                            let mut handles = blocking_handles.lock();
+                            let mut handles = blocking_handles.lock().await;
                             handles.append(&mut Self::process_optimization(
                                 optimizers.clone(),
                                 segments.clone(),
                             ));
                         }
                         OptimizerSignal::Operation(operation_id) => {
-                            if Self::try_recover(segments.clone(), wal.clone()).is_err() {
+                            if Self::try_recover(segments.clone(), wal.clone())
+                                .await
+                                .is_err()
+                            {
                                 continue;
                             }
                             {
-                                let mut handles = blocking_handles.lock();
+                                let mut handles = blocking_handles.lock().await;
                                 handles.append(&mut Self::process_optimization(
                                     optimizers.clone(),
                                     segments.clone(),
@@ -218,7 +224,7 @@ impl UpdateHandler {
                                         }
                                     }
                                 };
-                                wal.lock().ack(confirmed_version).unwrap();
+                                wal.lock().await.ack(confirmed_version).unwrap();
                             }
                         }
                         OptimizerSignal::Stop => break, // Stop gracefully

commit 0d18625ebd4a3e3c2c7ca7a19403ebfd5f979aef
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Mon Jan 17 17:48:59 2022 +0100

    Multiprocessing optimization fix (#155)
    
    * test to detect a problem
    
    * add checks for already scheduled and currently optimizing segments
    
    * fmt

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 9fc23c48e..6dbcb20f7 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -8,6 +8,7 @@ use async_channel::{Receiver, Sender};
 use log::{debug, info};
 use segment::types::SeqNumberType;
 use std::cmp::min;
+use std::collections::HashSet;
 use std::sync::Arc;
 use tokio::runtime::Handle;
 use tokio::sync::Mutex;
@@ -62,7 +63,7 @@ pub struct UpdateHandler {
     runtime_handle: Handle,
     /// WAL, required for operations
     wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
-    optimization_handles: Arc<Mutex<Vec<JoinHandle<()>>>>,
+    optimization_handles: Arc<Mutex<Vec<JoinHandle<bool>>>>,
 }
 
 impl UpdateHandler {
@@ -144,22 +145,28 @@ impl UpdateHandler {
         Ok(0)
     }
 
-    fn process_optimization(
+    pub(crate) fn process_optimization(
         optimizers: Arc<Vec<Arc<Optimizer>>>,
         segments: LockedSegmentHolder,
-    ) -> Vec<JoinHandle<()>> {
+    ) -> Vec<JoinHandle<bool>> {
+        let mut scheduled_segment_ids: HashSet<_> = Default::default();
         let mut handles = vec![];
         for optimizer in optimizers.iter() {
             loop {
-                let nonoptimal_segment_ids = optimizer.check_condition(segments.clone());
+                let nonoptimal_segment_ids =
+                    optimizer.check_condition(segments.clone(), &scheduled_segment_ids);
                 if nonoptimal_segment_ids.is_empty() {
                     break;
                 } else {
                     let optim = optimizer.clone();
                     let segs = segments.clone();
                     let nsi = nonoptimal_segment_ids.clone();
+                    for sid in &nsi {
+                        scheduled_segment_ids.insert(*sid);
+                    }
+
                     handles.push(tokio::task::spawn_blocking(move || {
-                        optim.as_ref().optimize(segs, nsi).unwrap();
+                        optim.as_ref().optimize(segs, nsi).unwrap()
                     }));
                 }
             }
@@ -173,7 +180,7 @@ impl UpdateHandler {
         segments: LockedSegmentHolder,
         wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
         flush_timeout_sec: u64,
-        blocking_handles: Arc<Mutex<Vec<JoinHandle<()>>>>,
+        blocking_handles: Arc<Mutex<Vec<JoinHandle<bool>>>>,
     ) {
         let flush_timeout = Duration::from_secs(flush_timeout_sec);
         let mut last_flushed = Instant::now();

commit d7b80e0337d834be41deb4e550c88795ff0104ad
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Tue Jan 18 13:25:00 2022 +0100

    [WIP] Clear optimization handles 30 (#156)
    
    * test to detect a problem
    
    * add checks for already scheduled and currently optimizing segments
    
    * fmt
    
    * use stoppable tasks for optimization
    
    * lock fix attempt
    
    * fmt
    
    * fix clippy

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 6dbcb20f7..165717e78 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -1,10 +1,12 @@
 use crate::collection_manager::collection_updater::CollectionUpdater;
 use crate::collection_manager::holders::segment_holder::LockedSegmentHolder;
 use crate::collection_manager::optimizers::segment_optimizer::SegmentOptimizer;
+use crate::common::stoppable_task::{spawn_stoppable, StoppableTaskHandle};
 use crate::operations::types::CollectionResult;
 use crate::operations::CollectionUpdateOperations;
 use crate::wal::SerdeWal;
 use async_channel::{Receiver, Sender};
+use itertools::Itertools;
 use log::{debug, info};
 use segment::types::SeqNumberType;
 use std::cmp::min;
@@ -63,7 +65,7 @@ pub struct UpdateHandler {
     runtime_handle: Handle,
     /// WAL, required for operations
     wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
-    optimization_handles: Arc<Mutex<Vec<JoinHandle<bool>>>>,
+    optimization_handles: Arc<Mutex<Vec<StoppableTaskHandle<bool>>>>,
 }
 
 impl UpdateHandler {
@@ -109,11 +111,7 @@ impl UpdateHandler {
 
     /// Gracefully wait before all optimizations stop
     /// If some optimization is in progress - it will be finished before shutdown.
-    /// Blocking function.
     pub async fn wait_workers_stops(&mut self) -> CollectionResult<()> {
-        for handle in self.optimization_handles.lock().await.iter() {
-            handle.abort();
-        }
         let maybe_handle = self.update_worker.take();
         if let Some(handle) = maybe_handle {
             handle.await?;
@@ -122,6 +120,15 @@ impl UpdateHandler {
         if let Some(handle) = maybe_handle {
             handle.await?;
         }
+
+        let mut opt_handles_guard = self.optimization_handles.lock().await;
+        let opt_handles = std::mem::take(&mut *opt_handles_guard);
+        let stopping_handles = opt_handles.into_iter().map(|h| h.stop()).collect_vec();
+
+        for res in stopping_handles {
+            res.await?;
+        }
+
         Ok(())
     }
 
@@ -145,10 +152,13 @@ impl UpdateHandler {
         Ok(0)
     }
 
-    pub(crate) fn process_optimization(
+    /// Checks conditions for all optimizers until there is no suggested segment
+    /// Starts a task for each optimization
+    /// Returns handles for started tasks
+    pub(crate) fn launch_optimization(
         optimizers: Arc<Vec<Arc<Optimizer>>>,
         segments: LockedSegmentHolder,
-    ) -> Vec<JoinHandle<bool>> {
+    ) -> Vec<StoppableTaskHandle<bool>> {
         let mut scheduled_segment_ids: HashSet<_> = Default::default();
         let mut handles = vec![];
         for optimizer in optimizers.iter() {
@@ -165,7 +175,7 @@ impl UpdateHandler {
                         scheduled_segment_ids.insert(*sid);
                     }
 
-                    handles.push(tokio::task::spawn_blocking(move || {
+                    handles.push(spawn_stoppable(move |_stopped| {
                         optim.as_ref().optimize(segs, nsi).unwrap()
                     }));
                 }
@@ -174,13 +184,24 @@ impl UpdateHandler {
         handles
     }
 
+    pub(crate) async fn process_optimization(
+        optimizers: Arc<Vec<Arc<Optimizer>>>,
+        segments: LockedSegmentHolder,
+        optimization_handles: Arc<Mutex<Vec<StoppableTaskHandle<bool>>>>,
+    ) {
+        let mut new_handles = Self::launch_optimization(optimizers.clone(), segments.clone());
+        let mut handles = optimization_handles.lock().await;
+        handles.append(&mut new_handles);
+        handles.retain(|h| !h.is_finished())
+    }
+
     async fn optimization_worker_fn(
         optimizers: Arc<Vec<Arc<Optimizer>>>,
         receiver: Receiver<OptimizerSignal>,
         segments: LockedSegmentHolder,
         wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
         flush_timeout_sec: u64,
-        blocking_handles: Arc<Mutex<Vec<JoinHandle<bool>>>>,
+        optimization_handles: Arc<Mutex<Vec<StoppableTaskHandle<bool>>>>,
     ) {
         let flush_timeout = Duration::from_secs(flush_timeout_sec);
         let mut last_flushed = Instant::now();
@@ -196,11 +217,12 @@ impl UpdateHandler {
                             {
                                 continue;
                             }
-                            let mut handles = blocking_handles.lock().await;
-                            handles.append(&mut Self::process_optimization(
+                            Self::process_optimization(
                                 optimizers.clone(),
                                 segments.clone(),
-                            ));
+                                optimization_handles.clone(),
+                            )
+                            .await;
                         }
                         OptimizerSignal::Operation(operation_id) => {
                             if Self::try_recover(segments.clone(), wal.clone())
@@ -209,13 +231,12 @@ impl UpdateHandler {
                             {
                                 continue;
                             }
-                            {
-                                let mut handles = blocking_handles.lock().await;
-                                handles.append(&mut Self::process_optimization(
-                                    optimizers.clone(),
-                                    segments.clone(),
-                                ));
-                            }
+                            Self::process_optimization(
+                                optimizers.clone(),
+                                segments.clone(),
+                                optimization_handles.clone(),
+                            )
+                            .await;
 
                             let elapsed = last_flushed.elapsed();
                             if elapsed > flush_timeout {

commit 0f91c9a5e29ef9065c79a20e0ace25be898beff8
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Tue Jan 18 15:06:42 2022 +0100

    [WIP] Force optimization stop #31 (#161)
    
    * implement checking stop-flag in the optimization routine
    
    * wip: optimization cancel test
    
    * force optimization stop during the construction of vector index
    
    * fix clippy

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 165717e78..aa8b25fff 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -2,7 +2,7 @@ use crate::collection_manager::collection_updater::CollectionUpdater;
 use crate::collection_manager::holders::segment_holder::LockedSegmentHolder;
 use crate::collection_manager::optimizers::segment_optimizer::SegmentOptimizer;
 use crate::common::stoppable_task::{spawn_stoppable, StoppableTaskHandle};
-use crate::operations::types::CollectionResult;
+use crate::operations::types::{CollectionError, CollectionResult};
 use crate::operations::CollectionUpdateOperations;
 use crate::wal::SerdeWal;
 use async_channel::{Receiver, Sender};
@@ -175,8 +175,23 @@ impl UpdateHandler {
                         scheduled_segment_ids.insert(*sid);
                     }
 
-                    handles.push(spawn_stoppable(move |_stopped| {
-                        optim.as_ref().optimize(segs, nsi).unwrap()
+                    handles.push(spawn_stoppable(move |stopped| {
+                        match optim.as_ref().optimize(segs, nsi, stopped) {
+                            Ok(result) => result,
+                            Err(error) => match error {
+                                CollectionError::Cancelled { description } => {
+                                    log::info!("Optimization cancelled - {}", description);
+                                    false
+                                }
+                                _ => {
+                                    // Error of the optimization can not be handled by API user
+                                    // It is only possible to fix after full restart,
+                                    // so the best available action here is to stop whole
+                                    // optimization thread and log the error
+                                    panic!("Optimization error: {}", error);
+                                }
+                            },
+                        }
                     }));
                 }
             }

commit 2912042fb67f2ffd74088e6212cff061e30fcb42
Author: Egor Ivkov <e.o.ivkov@gmail.com>
Date:   Tue Feb 1 20:37:05 2022 +0300

    Use tokio::sync channels instead of async-channel (#273)

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index aa8b25fff..0a081a6ec 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -5,7 +5,6 @@ use crate::common::stoppable_task::{spawn_stoppable, StoppableTaskHandle};
 use crate::operations::types::{CollectionError, CollectionResult};
 use crate::operations::CollectionUpdateOperations;
 use crate::wal::SerdeWal;
-use async_channel::{Receiver, Sender};
 use itertools::Itertools;
 use log::{debug, info};
 use segment::types::SeqNumberType;
@@ -13,7 +12,10 @@ use std::cmp::min;
 use std::collections::HashSet;
 use std::sync::Arc;
 use tokio::runtime::Handle;
-use tokio::sync::Mutex;
+use tokio::sync::{
+    mpsc::{self, UnboundedReceiver, UnboundedSender},
+    oneshot, Mutex,
+};
 use tokio::task::JoinHandle;
 use tokio::time::{Duration, Instant};
 
@@ -26,7 +28,7 @@ pub struct OperationData {
     /// Operation
     pub operation: CollectionUpdateOperations,
     /// Callback notification channel
-    pub sender: Option<Sender<CollectionResult<usize>>>,
+    pub sender: Option<oneshot::Sender<CollectionResult<usize>>>,
 }
 
 /// Signal, used to inform Updater process
@@ -57,7 +59,7 @@ pub struct UpdateHandler {
     pub flush_timeout_sec: u64,
     segments: LockedSegmentHolder,
     /// Channel receiver, which is listened by the updater process
-    update_receiver: Receiver<UpdateSignal>,
+    update_receiver: Option<UnboundedReceiver<UpdateSignal>>,
     /// Process, that listens updates signals and perform updates
     update_worker: Option<JoinHandle<()>>,
     /// Process, that listens for post-update signals and performs optimization
@@ -71,7 +73,7 @@ pub struct UpdateHandler {
 impl UpdateHandler {
     pub fn new(
         optimizers: Arc<Vec<Arc<Optimizer>>>,
-        update_receiver: Receiver<UpdateSignal>,
+        update_receiver: UnboundedReceiver<UpdateSignal>,
         runtime_handle: Handle,
         segments: LockedSegmentHolder,
         wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
@@ -80,7 +82,7 @@ impl UpdateHandler {
         let mut handler = UpdateHandler {
             optimizers,
             segments,
-            update_receiver,
+            update_receiver: Some(update_receiver),
             update_worker: None,
             optimizer_worker: None,
             runtime_handle,
@@ -93,7 +95,7 @@ impl UpdateHandler {
     }
 
     pub fn run_workers(&mut self) {
-        let (tx, rx) = async_channel::unbounded();
+        let (tx, rx) = mpsc::unbounded_channel();
         self.optimizer_worker = Some(self.runtime_handle.spawn(Self::optimization_worker_fn(
             self.optimizers.clone(),
             rx,
@@ -103,7 +105,7 @@ impl UpdateHandler {
             self.optimization_handles.clone(),
         )));
         self.update_worker = Some(self.runtime_handle.spawn(Self::update_worker_fn(
-            self.update_receiver.clone(),
+            self.update_receiver.take().expect("Unreachable."),
             tx,
             self.segments.clone(),
         )));
@@ -212,7 +214,7 @@ impl UpdateHandler {
 
     async fn optimization_worker_fn(
         optimizers: Arc<Vec<Arc<Optimizer>>>,
-        receiver: Receiver<OptimizerSignal>,
+        mut receiver: UnboundedReceiver<OptimizerSignal>,
         segments: LockedSegmentHolder,
         wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
         flush_timeout_sec: u64,
@@ -220,113 +222,105 @@ impl UpdateHandler {
     ) {
         let flush_timeout = Duration::from_secs(flush_timeout_sec);
         let mut last_flushed = Instant::now();
-        loop {
-            let recv_res = receiver.recv().await;
-            match recv_res {
-                Ok(signal) => {
-                    match signal {
-                        OptimizerSignal::Nop => {
-                            if Self::try_recover(segments.clone(), wal.clone())
-                                .await
-                                .is_err()
-                            {
-                                continue;
-                            }
-                            Self::process_optimization(
-                                optimizers.clone(),
-                                segments.clone(),
-                                optimization_handles.clone(),
-                            )
-                            .await;
-                        }
-                        OptimizerSignal::Operation(operation_id) => {
-                            if Self::try_recover(segments.clone(), wal.clone())
-                                .await
-                                .is_err()
-                            {
-                                continue;
-                            }
-                            Self::process_optimization(
-                                optimizers.clone(),
-                                segments.clone(),
-                                optimization_handles.clone(),
-                            )
-                            .await;
+        while let Some(signal) = receiver.recv().await {
+            match signal {
+                OptimizerSignal::Nop => {
+                    if Self::try_recover(segments.clone(), wal.clone())
+                        .await
+                        .is_err()
+                    {
+                        continue;
+                    }
+                    Self::process_optimization(
+                        optimizers.clone(),
+                        segments.clone(),
+                        optimization_handles.clone(),
+                    )
+                    .await;
+                }
+                OptimizerSignal::Operation(operation_id) => {
+                    if Self::try_recover(segments.clone(), wal.clone())
+                        .await
+                        .is_err()
+                    {
+                        continue;
+                    }
+                    Self::process_optimization(
+                        optimizers.clone(),
+                        segments.clone(),
+                        optimization_handles.clone(),
+                    )
+                    .await;
 
-                            let elapsed = last_flushed.elapsed();
-                            if elapsed > flush_timeout {
-                                debug!("Performing flushing: {}", operation_id);
-                                last_flushed = Instant::now();
-                                let confirmed_version = {
-                                    let read_segments = segments.read();
-                                    let flushed_version = read_segments.flush_all().unwrap();
-                                    match read_segments.failed_operation.iter().cloned().min() {
-                                        None => flushed_version,
-                                        Some(failed_operation) => {
-                                            min(failed_operation, flushed_version)
-                                        }
-                                    }
-                                };
-                                wal.lock().await.ack(confirmed_version).unwrap();
+                    let elapsed = last_flushed.elapsed();
+                    if elapsed > flush_timeout {
+                        debug!("Performing flushing: {}", operation_id);
+                        last_flushed = Instant::now();
+                        let confirmed_version = {
+                            let read_segments = segments.read();
+                            let flushed_version = read_segments.flush_all().unwrap();
+                            match read_segments.failed_operation.iter().cloned().min() {
+                                None => flushed_version,
+                                Some(failed_operation) => min(failed_operation, flushed_version),
                             }
-                        }
-                        OptimizerSignal::Stop => break, // Stop gracefully
+                        };
+                        wal.lock().await.ack(confirmed_version).unwrap();
                     }
                 }
-                Err(_) => break, // Transmitter was destroyed
+                OptimizerSignal::Stop => break, // Stop gracefully
             }
         }
     }
 
     async fn update_worker_fn(
-        receiver: Receiver<UpdateSignal>,
-        optimize_sender: Sender<OptimizerSignal>,
+        mut receiver: UnboundedReceiver<UpdateSignal>,
+        optimize_sender: UnboundedSender<OptimizerSignal>,
         segments: LockedSegmentHolder,
     ) {
-        loop {
-            let recv_res = receiver.recv().await;
-            match recv_res {
-                Ok(signal) => {
-                    match signal {
-                        UpdateSignal::Operation(OperationData {
-                            op_num,
-                            operation,
-                            sender,
-                        }) => {
-                            let res = match CollectionUpdater::update(&segments, op_num, operation)
-                            {
-                                Ok(update_res) => optimize_sender
-                                    .send(OptimizerSignal::Operation(op_num))
-                                    .await
-                                    .and(Ok(update_res))
-                                    .map_err(|send_err| send_err.into()),
-                                Err(err) => Err(err),
-                            };
+        while let Some(signal) = receiver.recv().await {
+            match signal {
+                UpdateSignal::Operation(OperationData {
+                    op_num,
+                    operation,
+                    sender,
+                }) => {
+                    let res = match CollectionUpdater::update(&segments, op_num, operation) {
+                        Ok(update_res) => optimize_sender
+                            .send(OptimizerSignal::Operation(op_num))
+                            .and(Ok(update_res))
+                            .map_err(|send_err| send_err.into()),
+                        Err(err) => Err(err),
+                    };
 
-                            if let Some(feedback) = sender {
-                                feedback.send(res).await.unwrap_or_else(|_| {
-                                    info!("Can't report operation {} result. Assume already not required", op_num);
-                                });
-                            };
-                        }
-                        UpdateSignal::Stop => {
-                            optimize_sender.send(OptimizerSignal::Stop).await.unwrap_or_else(|_| {
-                                debug!("Optimizer already stopped")
-                            });
-                            break;
-                        }
-                        UpdateSignal::Nop => optimize_sender.send(OptimizerSignal::Nop).await.unwrap_or_else(|_| {
-                            info!("Can't notify optimizers, assume process is dead. Restart is required");
-                        }),
-                    }
+                    if let Some(feedback) = sender {
+                        feedback.send(res).unwrap_or_else(|_| {
+                            info!(
+                                "Can't report operation {} result. Assume already not required",
+                                op_num
+                            );
+                        });
+                    };
                 }
-                Err(_) => {
-                    optimize_sender.send(OptimizerSignal::Stop).await.unwrap_or_else(|_| {
-                        debug!("Optimizer already stopped")
-                    });
+                UpdateSignal::Stop => {
+                    optimize_sender
+                        .send(OptimizerSignal::Stop)
+                        .unwrap_or_else(|_| debug!("Optimizer already stopped"));
                     break;
-                } // Transmitter was destroyed
+                }
+                UpdateSignal::Nop => {
+                    optimize_sender
+                        .send(OptimizerSignal::Nop)
+                        .unwrap_or_else(|_| {
+                            info!(
+                            "Can't notify optimizers, assume process is dead. Restart is required"
+                        );
+                        })
+                }
             }
         }
+        // Transmitter was destroyed
+        optimize_sender
+            .send(OptimizerSignal::Stop)
+            .unwrap_or_else(|_| debug!("Optimizer already stopped"));
     }
 }

commit 0b9463a6749fd9f80452f25c0266d8fbc3c9263b
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Tue Feb 15 10:21:46 2022 +0100

    reproduce with integration test and fix #309
    
    * reproduce with integration test
    
    * create new update channel each time we spin up a new update worker

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 0a081a6ec..d17ae540d 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -22,6 +22,7 @@ use tokio::time::{Duration, Instant};
 pub type Optimizer = dyn SegmentOptimizer + Sync + Send;
 
 /// Information, required to perform operation and notify regarding the result
+#[derive(Debug)]
 pub struct OperationData {
     /// Sequential number of the operation
     pub op_num: SeqNumberType,
@@ -32,6 +33,7 @@ pub struct OperationData {
 }
 
 /// Signal, used to inform Updater process
+#[derive(Debug)]
 pub enum UpdateSignal {
     /// Requested operation to perform
     Operation(OperationData),
@@ -58,8 +60,6 @@ pub struct UpdateHandler {
     /// How frequent can we flush data
     pub flush_timeout_sec: u64,
     segments: LockedSegmentHolder,
-    /// Channel receiver, which is listened by the updater process
-    update_receiver: Option<UnboundedReceiver<UpdateSignal>>,
     /// Process, that listens updates signals and perform updates
     update_worker: Option<JoinHandle<()>>,
     /// Process, that listens for post-update signals and performs optimization
@@ -73,28 +73,24 @@ pub struct UpdateHandler {
 impl UpdateHandler {
     pub fn new(
         optimizers: Arc<Vec<Arc<Optimizer>>>,
-        update_receiver: UnboundedReceiver<UpdateSignal>,
         runtime_handle: Handle,
         segments: LockedSegmentHolder,
         wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
         flush_timeout_sec: u64,
     ) -> UpdateHandler {
-        let mut handler = UpdateHandler {
+        UpdateHandler {
             optimizers,
             segments,
-            update_receiver: Some(update_receiver),
             update_worker: None,
             optimizer_worker: None,
             runtime_handle,
             wal,
             flush_timeout_sec,
             optimization_handles: Arc::new(Mutex::new(vec![])),
-        };
-        handler.run_workers();
-        handler
+        }
     }
 
-    pub fn run_workers(&mut self) {
+    pub fn run_workers(&mut self, update_receiver: UnboundedReceiver<UpdateSignal>) {
         let (tx, rx) = mpsc::unbounded_channel();
         self.optimizer_worker = Some(self.runtime_handle.spawn(Self::optimization_worker_fn(
             self.optimizers.clone(),
@@ -105,7 +101,7 @@ impl UpdateHandler {
             self.optimization_handles.clone(),
         )));
         self.update_worker = Some(self.runtime_handle.spawn(Self::update_worker_fn(
-            self.update_receiver.take().expect("Unreachable."),
+            update_receiver,
             tx,
             self.segments.clone(),
         )));

commit e45379e4384062e92ee1c9be82c250047464c9ef
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Wed Feb 16 09:59:11 2022 +0100

    Better optimizer error reporting + small bug fixes (#316)
    
    * optimizer error reporting, decouple data removing, optimizator fix
    
    * fmt
    
    * fmt + clippy
    
    * update openapi

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index d17ae540d..a09be3596 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -94,6 +94,7 @@ impl UpdateHandler {
         let (tx, rx) = mpsc::unbounded_channel();
         self.optimizer_worker = Some(self.runtime_handle.spawn(Self::optimization_worker_fn(
             self.optimizers.clone(),
+            tx.clone(),
             rx,
             self.segments.clone(),
             self.wal.clone(),
@@ -153,10 +154,16 @@ impl UpdateHandler {
     /// Checks conditions for all optimizers until there is no suggested segment
     /// Starts a task for each optimization
     /// Returns handles for started tasks
-    pub(crate) fn launch_optimization(
+    pub(crate) fn launch_optimization<F>(
         optimizers: Arc<Vec<Arc<Optimizer>>>,
         segments: LockedSegmentHolder,
-    ) -> Vec<StoppableTaskHandle<bool>> {
+        callback: F,
+    ) -> Vec<StoppableTaskHandle<bool>>
+    where
+        F: FnOnce(bool),
+        F: Send + 'static,
+        F: Clone,
+    {
         let mut scheduled_segment_ids: HashSet<_> = Default::default();
         let mut handles = vec![];
         for optimizer in optimizers.iter() {
@@ -172,20 +179,33 @@ impl UpdateHandler {
                     for sid in &nsi {
                         scheduled_segment_ids.insert(*sid);
                     }
+                    let callback_cloned = callback.clone();
 
                     handles.push(spawn_stoppable(move |stopped| {
-                        match optim.as_ref().optimize(segs, nsi, stopped) {
-                            Ok(result) => result,
+                        match optim.as_ref().optimize(segs.clone(), nsi, stopped) {
+                            Ok(result) => {
+                                callback_cloned(result); // Perform some actions when optimization if finished
+                                result
+                            }
                             Err(error) => match error {
                                 CollectionError::Cancelled { description } => {
                                     log::info!("Optimization cancelled - {}", description);
                                     false
                                 }
                                 _ => {
+                                    {
+                                        let mut segments_write = segs.write();
+                                        if segments_write.optimizer_errors.is_none() {
+                                            // Save only the first error
+                                            // If is more likely to be the real cause of all further problems
+                                            segments_write.optimizer_errors = Some(error.clone());
+                                        }
+                                    }
                                     // Error of the optimization can not be handled by API user
                                     // It is only possible to fix after full restart,
                                     // so the best available action here is to stop whole
                                     // optimization thread and log the error
+                                    log::error!("Optimization error: {}", error);
                                     panic!("Optimization error: {}", error);
                                 }
                             },
@@ -201,8 +221,18 @@ impl UpdateHandler {
         optimizers: Arc<Vec<Arc<Optimizer>>>,
         segments: LockedSegmentHolder,
         optimization_handles: Arc<Mutex<Vec<StoppableTaskHandle<bool>>>>,
+        sender: UnboundedSender<OptimizerSignal>,
     ) {
-        let mut new_handles = Self::launch_optimization(optimizers.clone(), segments.clone());
+        let mut new_handles = Self::launch_optimization(
+            optimizers.clone(),
+            segments.clone(),
+            move |_optimization_result| {
+                // After optimization is finished, we still need to check if there are
+                // some further optimizations possible.
+                // If receiver is already dead - we do not care.
+                let _ = sender.send(OptimizerSignal::Nop);
+            },
+        );
         let mut handles = optimization_handles.lock().await;
         handles.append(&mut new_handles);
         handles.retain(|h| !h.is_finished())
@@ -210,6 +240,7 @@ impl UpdateHandler {
 
     async fn optimization_worker_fn(
         optimizers: Arc<Vec<Arc<Optimizer>>>,
+        sender: UnboundedSender<OptimizerSignal>,
         mut receiver: UnboundedReceiver<OptimizerSignal>,
         segments: LockedSegmentHolder,
         wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
@@ -231,6 +262,7 @@ impl UpdateHandler {
                         optimizers.clone(),
                         segments.clone(),
                         optimization_handles.clone(),
+                        sender.clone(),
                     )
                     .await;
                 }
@@ -245,6 +277,7 @@ impl UpdateHandler {
                         optimizers.clone(),
                         segments.clone(),
                         optimization_handles.clone(),
+                        sender.clone(),
                     )
                     .await;
 

commit b07aa392a634571cb3b222bf06aef43b529eba4f
Author: Egor Ivkov <e.o.ivkov@gmail.com>
Date:   Wed Feb 23 16:52:33 2022 +0300

    Periodic flushing (#332)
    
    * Periodic flushing
    
    * Save flush error

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index a09be3596..1e93fcf6d 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -6,7 +6,8 @@ use crate::operations::types::{CollectionError, CollectionResult};
 use crate::operations::CollectionUpdateOperations;
 use crate::wal::SerdeWal;
 use itertools::Itertools;
-use log::{debug, info};
+use log::{debug, error, info, trace, warn};
+use segment::entry::entry_point::OperationResult;
 use segment::types::SeqNumberType;
 use std::cmp::min;
 use std::collections::HashSet;
@@ -17,7 +18,7 @@ use tokio::sync::{
     oneshot, Mutex,
 };
 use tokio::task::JoinHandle;
-use tokio::time::{Duration, Instant};
+use tokio::time::Duration;
 
 pub type Optimizer = dyn SegmentOptimizer + Sync + Send;
 
@@ -58,12 +59,16 @@ pub struct UpdateHandler {
     /// List of used optimizers
     pub optimizers: Arc<Vec<Arc<Optimizer>>>,
     /// How frequent can we flush data
-    pub flush_timeout_sec: u64,
+    pub flush_interval_sec: u64,
     segments: LockedSegmentHolder,
     /// Process, that listens updates signals and perform updates
     update_worker: Option<JoinHandle<()>>,
     /// Process, that listens for post-update signals and performs optimization
     optimizer_worker: Option<JoinHandle<()>>,
+    /// Process that periodically flushes segments and tries to truncate wal
+    flush_worker: Option<JoinHandle<()>>,
+    /// Sender to stop flush worker
+    flush_stop: Option<oneshot::Sender<()>>,
     runtime_handle: Handle,
     /// WAL, required for operations
     wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
@@ -76,16 +81,18 @@ impl UpdateHandler {
         runtime_handle: Handle,
         segments: LockedSegmentHolder,
         wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
-        flush_timeout_sec: u64,
+        flush_interval_sec: u64,
     ) -> UpdateHandler {
         UpdateHandler {
             optimizers,
             segments,
             update_worker: None,
             optimizer_worker: None,
+            flush_worker: None,
+            flush_stop: None,
             runtime_handle,
             wal,
-            flush_timeout_sec,
+            flush_interval_sec,
             optimization_handles: Arc::new(Mutex::new(vec![])),
         }
     }
@@ -98,7 +105,6 @@ impl UpdateHandler {
             rx,
             self.segments.clone(),
             self.wal.clone(),
-            self.flush_timeout_sec,
             self.optimization_handles.clone(),
         )));
         self.update_worker = Some(self.runtime_handle.spawn(Self::update_worker_fn(
@@ -106,6 +112,22 @@ impl UpdateHandler {
             tx,
             self.segments.clone(),
         )));
+        let (flush_tx, flush_rx) = oneshot::channel();
+        self.flush_worker = Some(self.runtime_handle.spawn(Self::flush_worker(
+            self.segments.clone(),
+            self.wal.clone(),
+            self.flush_interval_sec,
+            flush_rx,
+        )));
+        self.flush_stop = Some(flush_tx);
+    }
+
+    pub fn stop_flush_worker(&mut self) {
+        if let Some(flush_stop) = self.flush_stop.take() {
+            if let Err(()) = flush_stop.send(()) {
+                warn!("Failed to stop flush worker as it is already stopped.");
+            }
+        }
     }
 
     /// Gracefully wait before all optimizations stop
@@ -119,6 +141,10 @@ impl UpdateHandler {
         if let Some(handle) = maybe_handle {
             handle.await?;
         }
+        let maybe_handle = self.flush_worker.take();
+        if let Some(handle) = maybe_handle {
+            handle.await?;
+        }
 
         let mut opt_handles_guard = self.optimization_handles.lock().await;
         let opt_handles = std::mem::take(&mut *opt_handles_guard);
@@ -193,14 +219,10 @@ impl UpdateHandler {
                                     false
                                 }
                                 _ => {
-                                    {
-                                        let mut segments_write = segs.write();
-                                        if segments_write.optimizer_errors.is_none() {
-                                            // Save only the first error
-                                            // If is more likely to be the real cause of all further problems
-                                            segments_write.optimizer_errors = Some(error.clone());
-                                        }
-                                    }
+                                    // Save only the first error
+                                    // If is more likely to be the real cause of all further problems
+                                    segs.write().report_optimizer_error(error.clone());
+
                                     // Error of the optimization can not be handled by API user
                                     // It is only possible to fix after full restart,
                                     // so the best available action here is to stop whole
@@ -244,29 +266,11 @@ impl UpdateHandler {
         mut receiver: UnboundedReceiver<OptimizerSignal>,
         segments: LockedSegmentHolder,
         wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
-        flush_timeout_sec: u64,
         optimization_handles: Arc<Mutex<Vec<StoppableTaskHandle<bool>>>>,
     ) {
-        let flush_timeout = Duration::from_secs(flush_timeout_sec);
-        let mut last_flushed = Instant::now();
         while let Some(signal) = receiver.recv().await {
             match signal {
-                OptimizerSignal::Nop => {
-                    if Self::try_recover(segments.clone(), wal.clone())
-                        .await
-                        .is_err()
-                    {
-                        continue;
-                    }
-                    Self::process_optimization(
-                        optimizers.clone(),
-                        segments.clone(),
-                        optimization_handles.clone(),
-                        sender.clone(),
-                    )
-                    .await;
-                }
-                OptimizerSignal::Operation(operation_id) => {
+                OptimizerSignal::Nop | OptimizerSignal::Operation(_) => {
                     if Self::try_recover(segments.clone(), wal.clone())
                         .await
                         .is_err()
@@ -280,21 +284,6 @@ impl UpdateHandler {
                         sender.clone(),
                     )
                     .await;
-
-                    let elapsed = last_flushed.elapsed();
-                    if elapsed > flush_timeout {
-                        debug!("Performing flushing: {}", operation_id);
-                        last_flushed = Instant::now();
-                        let confirmed_version = {
-                            let read_segments = segments.read();
-                            let flushed_version = read_segments.flush_all().unwrap();
-                            match read_segments.failed_operation.iter().cloned().min() {
-                                None => flushed_version,
-                                Some(failed_operation) => min(failed_operation, flushed_version),
-                            }
-                        };
-                        wal.lock().await.ack(confirmed_version).unwrap();
-                    }
                 }
                 OptimizerSignal::Stop => break, // Stop gracefully
             }
@@ -352,4 +341,50 @@ impl UpdateHandler {
             .send(OptimizerSignal::Stop)
             .unwrap_or_else(|_| debug!("Optimizer already stopped"));
     }
+
+    async fn flush_worker(
+        segments: LockedSegmentHolder,
+        wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
+        flush_interval_sec: u64,
+        mut stop_receiver: oneshot::Receiver<()>,
+    ) {
+        loop {
+            // Stop flush worker on signal or if sender was dropped
+            // Even if timer did not finish
+            tokio::select! {
+                _ = tokio::time::sleep(Duration::from_secs(flush_interval_sec)) => {},
+                _ = &mut stop_receiver => {
+                    debug!("Stopping flush worker.");
+                    return;
+                }
+            };
+
+            trace!("Attempting flushing");
+            let confirmed_version = Self::flush_segments(segments.clone());
+            let confirmed_version = match confirmed_version {
+                Ok(version) => version,
+                Err(err) => {
+                    error!("Failed to flush: {err}");
+                    segments.write().report_optimizer_error(err);
+                    continue;
+                }
+            };
+            if let Err(err) = wal.lock().await.ack(confirmed_version) {
+                segments.write().report_optimizer_error(err);
+            }
+        }
+    }
+
+    /// Returns confirmed version after flush of all segements
+    ///
+    /// # Errors
+    /// Returns an error on flush failure
+    fn flush_segments(segments: LockedSegmentHolder) -> OperationResult<u64> {
+        let read_segments = segments.read();
+        let flushed_version = read_segments.flush_all()?;
+        Ok(match read_segments.failed_operation.iter().cloned().min() {
+            None => flushed_version,
+            Some(failed_operation) => min(failed_operation, flushed_version),
+        })
+    }
 }

commit c82f11956bd99290ca003ea1322e0b3fa578c3e2
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Tue Jun 14 10:31:02 2022 +0200

    Better logging (#681)
    
    * disable info level for non-essential logging
    
    * welcome banner
    
    * fmt + clippy
    
    * return debug log level in dev mode

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 1e93fcf6d..69a33c41f 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -215,7 +215,7 @@ impl UpdateHandler {
                             }
                             Err(error) => match error {
                                 CollectionError::Cancelled { description } => {
-                                    log::info!("Optimization cancelled - {}", description);
+                                    log::debug!("Optimization cancelled - {}", description);
                                     false
                                 }
                                 _ => {

commit f612460f88e0ea3bdc849598829fdfe174760b99
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Mon Jul 4 12:35:15 2022 +0200

    merge optimizer should try to take all possible segments, not just 3 (#775)
    
    * merge optimizer should try to take all possible segments, not just 3
    
    * prevent creating optimizing tasks if it reaches max available threads

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 69a33c41f..e4ba6fde0 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -73,6 +73,7 @@ pub struct UpdateHandler {
     /// WAL, required for operations
     wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
     optimization_handles: Arc<Mutex<Vec<StoppableTaskHandle<bool>>>>,
+    max_optimization_threads: usize,
 }
 
 impl UpdateHandler {
@@ -82,6 +83,7 @@ impl UpdateHandler {
         segments: LockedSegmentHolder,
         wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
         flush_interval_sec: u64,
+        max_optimization_threads: usize,
     ) -> UpdateHandler {
         UpdateHandler {
             optimizers,
@@ -94,6 +96,7 @@ impl UpdateHandler {
             wal,
             flush_interval_sec,
             optimization_handles: Arc::new(Mutex::new(vec![])),
+            max_optimization_threads,
         }
     }
 
@@ -106,6 +109,7 @@ impl UpdateHandler {
             self.segments.clone(),
             self.wal.clone(),
             self.optimization_handles.clone(),
+            self.max_optimization_threads,
         )));
         self.update_worker = Some(self.runtime_handle.spawn(Self::update_worker_fn(
             update_receiver,
@@ -267,10 +271,31 @@ impl UpdateHandler {
         segments: LockedSegmentHolder,
         wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
         optimization_handles: Arc<Mutex<Vec<StoppableTaskHandle<bool>>>>,
+        max_handles: usize,
     ) {
         while let Some(signal) = receiver.recv().await {
             match signal {
-                OptimizerSignal::Nop | OptimizerSignal::Operation(_) => {
+                OptimizerSignal::Nop => {
+                    // We skip the check for number of optimization handles here
+                    // Because `Nop` usually means that we need to force the optimization
+                    if Self::try_recover(segments.clone(), wal.clone())
+                        .await
+                        .is_err()
+                    {
+                        continue;
+                    }
+                    Self::process_optimization(
+                        optimizers.clone(),
+                        segments.clone(),
+                        optimization_handles.clone(),
+                        sender.clone(),
+                    )
+                    .await;
+                }
+                OptimizerSignal::Operation(_) => {
+                    if optimization_handles.lock().await.len() >= max_handles {
+                        continue;
+                    }
                     if Self::try_recover(segments.clone(), wal.clone())
                         .await
                         .is_err()

commit 4fc0ba16eaf147b29c35fe28df9b80cf26279781
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Thu Jul 7 17:57:20 2022 +0200

    Upd shard proxy (#788)
    
    * add reinit_changelog to the proxy shard
    
    * fmt
    
    * Update lib/collection/src/shard/proxy_shard.rs
    
    Co-authored-by: Arnaud Gourlay <arnaud.gourlay@gmail.com>
    
    Co-authored-by: Arnaud Gourlay <arnaud.gourlay@gmail.com>

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index e4ba6fde0..6ca1fa46f 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -42,6 +42,8 @@ pub enum UpdateSignal {
     Stop,
     /// Empty signal used to trigger optimizers
     Nop,
+    /// Ensures that previous updates are applied
+    Plunger(oneshot::Sender<()>),
 }
 
 /// Signal, used to inform Optimization process
@@ -359,6 +361,11 @@ impl UpdateHandler {
                         );
                         })
                 }
+                UpdateSignal::Plunger(callback_sender) => {
+                    callback_sender.send(()).unwrap_or_else(|_| {
+                        debug!("Can't notify sender, assume nobody is waiting anymore");
+                    });
+                }
             }
         }
         // Transmitter was destroyed

commit ff236410df6558ceb060c2915472de69af81ccd0
Author: Ivan Pleshkov <pleshkov.ivan@gmail.com>
Date:   Mon Jul 11 18:29:34 2022 +0400

    Graph layers remove code duplications (#796)
    
    * remove add link points
    
    * remove level factor
    
    * remove use heuristic
    
    * remove code duplications in graph layers
    
    * are you happy fmt
    
    * restore unit tests
    
    * remove obsolete comment

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 6ca1fa46f..b7bc6b7e3 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -47,6 +47,7 @@ pub enum UpdateSignal {
 }
 
 /// Signal, used to inform Optimization process
+#[derive(PartialEq, Clone, Copy)]
 pub enum OptimizerSignal {
     /// Sequential number of the operation
     Operation(SeqNumberType),
@@ -277,27 +278,14 @@ impl UpdateHandler {
     ) {
         while let Some(signal) = receiver.recv().await {
             match signal {
-                OptimizerSignal::Nop => {
-                    // We skip the check for number of optimization handles here
-                    // Because `Nop` usually means that we need to force the optimization
-                    if Self::try_recover(segments.clone(), wal.clone())
-                        .await
-                        .is_err()
+                OptimizerSignal::Nop | OptimizerSignal::Operation(_) => {
+                    if signal != OptimizerSignal::Nop
+                        && optimization_handles.lock().await.len() >= max_handles
                     {
                         continue;
                     }
-                    Self::process_optimization(
-                        optimizers.clone(),
-                        segments.clone(),
-                        optimization_handles.clone(),
-                        sender.clone(),
-                    )
-                    .await;
-                }
-                OptimizerSignal::Operation(_) => {
-                    if optimization_handles.lock().await.len() >= max_handles {
-                        continue;
-                    }
+                    // We skip the check for number of optimization handles here
+                    // Because `Nop` usually means that we need to force the optimization
                     if Self::try_recover(segments.clone(), wal.clone())
                         .await
                         .is_err()

commit 026bd040b001f1c66e16fc911322f1f182d1cf0f
Author: Egor Ivkov <e.o.ivkov@gmail.com>
Date:   Fri Jul 15 15:42:25 2022 +0300

    Add import formatting rules (#820)
    
    * Add import formatting rules
    
    * Review fix: update rusty hook

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index b7bc6b7e3..016cc2b48 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -1,25 +1,25 @@
-use crate::collection_manager::collection_updater::CollectionUpdater;
-use crate::collection_manager::holders::segment_holder::LockedSegmentHolder;
-use crate::collection_manager::optimizers::segment_optimizer::SegmentOptimizer;
-use crate::common::stoppable_task::{spawn_stoppable, StoppableTaskHandle};
-use crate::operations::types::{CollectionError, CollectionResult};
-use crate::operations::CollectionUpdateOperations;
-use crate::wal::SerdeWal;
+use std::cmp::min;
+use std::collections::HashSet;
+use std::sync::Arc;
+
 use itertools::Itertools;
 use log::{debug, error, info, trace, warn};
 use segment::entry::entry_point::OperationResult;
 use segment::types::SeqNumberType;
-use std::cmp::min;
-use std::collections::HashSet;
-use std::sync::Arc;
 use tokio::runtime::Handle;
-use tokio::sync::{
-    mpsc::{self, UnboundedReceiver, UnboundedSender},
-    oneshot, Mutex,
-};
+use tokio::sync::mpsc::{self, UnboundedReceiver, UnboundedSender};
+use tokio::sync::{oneshot, Mutex};
 use tokio::task::JoinHandle;
 use tokio::time::Duration;
 
+use crate::collection_manager::collection_updater::CollectionUpdater;
+use crate::collection_manager::holders::segment_holder::LockedSegmentHolder;
+use crate::collection_manager::optimizers::segment_optimizer::SegmentOptimizer;
+use crate::common::stoppable_task::{spawn_stoppable, StoppableTaskHandle};
+use crate::operations::types::{CollectionError, CollectionResult};
+use crate::operations::CollectionUpdateOperations;
+use crate::wal::SerdeWal;
+
 pub type Optimizer = dyn SegmentOptimizer + Sync + Send;
 
 /// Information, required to perform operation and notify regarding the result

commit 38c8097fc8a6a843df73025b21e3fe71257bb2fc
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Fri Aug 12 09:38:31 2022 +0200

    Clippy next (#941)
    
    * Clippy derive_partial_eq_without_eq
    
    * Clippy  explicit_auto_deref
    
    * Clippy single_match
    
    * Clippy manual_find_map
    
    * Clippy unnecessary_to_owned
    
    * Clippy derive_partial_eq_without_eq
    
    * Clippy get_first

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 016cc2b48..e788db097 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -47,7 +47,7 @@ pub enum UpdateSignal {
 }
 
 /// Signal, used to inform Optimization process
-#[derive(PartialEq, Clone, Copy)]
+#[derive(PartialEq, Eq, Clone, Copy)]
 pub enum OptimizerSignal {
     /// Sequential number of the operation
     Operation(SeqNumberType),

commit f357bd5d9bc8cdc05915111419894d4f25512d83
Author: Ivan Pleshkov <pleshkov.ivan@gmail.com>
Date:   Mon Aug 15 13:47:52 2022 +0400

    Allow to flush segment in separate thread (#927)
    
    * allow to flush segment in separate thread
    
    * flush as separate function (#928)
    
    * flush as separate function
    
    * review suggestion
    
    * reduce locks during vector scoring
    
    * fmt
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>
    
    * don't run background flush twice
    
    * Update lib/segment/src/segment.rs
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>
    
    * increase flush interval
    
    * Update lib/segment/src/segment.rs
    
    Co-authored-by: Arnaud Gourlay <arnaud.gourlay@gmail.com>
    
    * are you happy fmt
    
    * test background flush
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>
    Co-authored-by: Arnaud Gourlay <arnaud.gourlay@gmail.com>

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index e788db097..2d700ae24 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -399,9 +399,9 @@ impl UpdateHandler {
     ///
     /// # Errors
     /// Returns an error on flush failure
-    fn flush_segments(segments: LockedSegmentHolder) -> OperationResult<u64> {
+    fn flush_segments(segments: LockedSegmentHolder) -> OperationResult<SeqNumberType> {
         let read_segments = segments.read();
-        let flushed_version = read_segments.flush_all()?;
+        let flushed_version = read_segments.flush_all(false)?;
         Ok(match read_segments.failed_operation.iter().cloned().min() {
             None => flushed_version,
             Some(failed_operation) => min(failed_operation, flushed_version),

commit b6b586c0756461455aea263e197f0d4c80ba8a6e
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Sat Sep 24 01:03:06 2022 +0200

    prevent blocked optimizer state

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 2d700ae24..779e0e543 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -282,6 +282,8 @@ impl UpdateHandler {
                     if signal != OptimizerSignal::Nop
                         && optimization_handles.lock().await.len() >= max_handles
                     {
+                        let mut handles = optimization_handles.lock().await;
+                        handles.retain(|h| !h.is_finished());
                         continue;
                     }
                     // We skip the check for number of optimization handles here

commit 1a295ac3a099c459d7e5b01c056f84c2a22578e6
Author: Ivan Pleshkov <pleshkov.ivan@gmail.com>
Date:   Tue Sep 27 20:03:11 2022 +0400

    Fix upsert freezes on rust client stress test (#1061)
    
    * use parking lot for wal
    
    * fair unlock
    
    * limit update queue
    
    * Revert "limit update queue"
    
    This reverts commit 7df88870f64571ef92c4f99677f166568e2399c2.
    
    * Limited queue (#1062)
    
    * limit update queue size
    
    * fmt
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 779e0e543..e1890ab77 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -4,11 +4,12 @@ use std::sync::Arc;
 
 use itertools::Itertools;
 use log::{debug, error, info, trace, warn};
+use parking_lot::Mutex as ParkingMutex;
 use segment::entry::entry_point::OperationResult;
 use segment::types::SeqNumberType;
 use tokio::runtime::Handle;
-use tokio::sync::mpsc::{self, UnboundedReceiver, UnboundedSender};
-use tokio::sync::{oneshot, Mutex};
+use tokio::sync::mpsc::{self, Receiver, Sender};
+use tokio::sync::{oneshot, Mutex as TokioMutex};
 use tokio::task::JoinHandle;
 use tokio::time::Duration;
 
@@ -20,6 +21,8 @@ use crate::operations::types::{CollectionError, CollectionResult};
 use crate::operations::CollectionUpdateOperations;
 use crate::wal::SerdeWal;
 
+pub const UPDATE_QUEUE_SIZE: usize = 100;
+
 pub type Optimizer = dyn SegmentOptimizer + Sync + Send;
 
 /// Information, required to perform operation and notify regarding the result
@@ -74,8 +77,8 @@ pub struct UpdateHandler {
     flush_stop: Option<oneshot::Sender<()>>,
     runtime_handle: Handle,
     /// WAL, required for operations
-    wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
-    optimization_handles: Arc<Mutex<Vec<StoppableTaskHandle<bool>>>>,
+    wal: Arc<ParkingMutex<SerdeWal<CollectionUpdateOperations>>>,
+    optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
     max_optimization_threads: usize,
 }
 
@@ -84,7 +87,7 @@ impl UpdateHandler {
         optimizers: Arc<Vec<Arc<Optimizer>>>,
         runtime_handle: Handle,
         segments: LockedSegmentHolder,
-        wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
+        wal: Arc<ParkingMutex<SerdeWal<CollectionUpdateOperations>>>,
         flush_interval_sec: u64,
         max_optimization_threads: usize,
     ) -> UpdateHandler {
@@ -98,13 +101,13 @@ impl UpdateHandler {
             runtime_handle,
             wal,
             flush_interval_sec,
-            optimization_handles: Arc::new(Mutex::new(vec![])),
+            optimization_handles: Arc::new(TokioMutex::new(vec![])),
             max_optimization_threads,
         }
     }
 
-    pub fn run_workers(&mut self, update_receiver: UnboundedReceiver<UpdateSignal>) {
-        let (tx, rx) = mpsc::unbounded_channel();
+    pub fn run_workers(&mut self, update_receiver: Receiver<UpdateSignal>) {
+        let (tx, rx) = mpsc::channel(UPDATE_QUEUE_SIZE);
         self.optimizer_worker = Some(self.runtime_handle.spawn(Self::optimization_worker_fn(
             self.optimizers.clone(),
             tx.clone(),
@@ -168,14 +171,14 @@ impl UpdateHandler {
     /// If so - attempts to re-apply all failed operations.
     async fn try_recover(
         segments: LockedSegmentHolder,
-        wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
+        wal: Arc<ParkingMutex<SerdeWal<CollectionUpdateOperations>>>,
     ) -> CollectionResult<usize> {
         // Try to re-apply everything starting from the first failed operation
         let first_failed_operation_option = segments.read().failed_operation.iter().cloned().min();
         match first_failed_operation_option {
             None => {}
             Some(first_failed_op) => {
-                let wal_lock = wal.lock().await;
+                let wal_lock = wal.lock();
                 for (op_num, operation) in wal_lock.read(first_failed_op) {
                     CollectionUpdater::update(&segments, op_num, operation)?;
                 }
@@ -249,8 +252,8 @@ impl UpdateHandler {
     pub(crate) async fn process_optimization(
         optimizers: Arc<Vec<Arc<Optimizer>>>,
         segments: LockedSegmentHolder,
-        optimization_handles: Arc<Mutex<Vec<StoppableTaskHandle<bool>>>>,
-        sender: UnboundedSender<OptimizerSignal>,
+        optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
+        sender: Sender<OptimizerSignal>,
     ) {
         let mut new_handles = Self::launch_optimization(
             optimizers.clone(),
@@ -269,11 +272,11 @@ impl UpdateHandler {
 
     async fn optimization_worker_fn(
         optimizers: Arc<Vec<Arc<Optimizer>>>,
-        sender: UnboundedSender<OptimizerSignal>,
-        mut receiver: UnboundedReceiver<OptimizerSignal>,
+        sender: Sender<OptimizerSignal>,
+        mut receiver: Receiver<OptimizerSignal>,
         segments: LockedSegmentHolder,
-        wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
-        optimization_handles: Arc<Mutex<Vec<StoppableTaskHandle<bool>>>>,
+        wal: Arc<ParkingMutex<SerdeWal<CollectionUpdateOperations>>>,
+        optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
         max_handles: usize,
     ) {
         while let Some(signal) = receiver.recv().await {
@@ -308,8 +311,8 @@ impl UpdateHandler {
     }
 
     async fn update_worker_fn(
-        mut receiver: UnboundedReceiver<UpdateSignal>,
-        optimize_sender: UnboundedSender<OptimizerSignal>,
+        mut receiver: Receiver<UpdateSignal>,
+        optimize_sender: Sender<OptimizerSignal>,
         segments: LockedSegmentHolder,
     ) {
         while let Some(signal) = receiver.recv().await {
@@ -322,6 +325,7 @@ impl UpdateHandler {
                     let res = match CollectionUpdater::update(&segments, op_num, operation) {
                         Ok(update_res) => optimize_sender
                             .send(OptimizerSignal::Operation(op_num))
+                            .await
                             .and(Ok(update_res))
                             .map_err(|send_err| send_err.into()),
                         Err(err) => Err(err),
@@ -339,18 +343,18 @@ impl UpdateHandler {
                 UpdateSignal::Stop => {
                     optimize_sender
                         .send(OptimizerSignal::Stop)
+                        .await
                         .unwrap_or_else(|_| debug!("Optimizer already stopped"));
                     break;
                 }
-                UpdateSignal::Nop => {
-                    optimize_sender
-                        .send(OptimizerSignal::Nop)
-                        .unwrap_or_else(|_| {
-                            info!(
+                UpdateSignal::Nop => optimize_sender
+                    .send(OptimizerSignal::Nop)
+                    .await
+                    .unwrap_or_else(|_| {
+                        info!(
                             "Can't notify optimizers, assume process is dead. Restart is required"
                         );
-                        })
-                }
+                    }),
                 UpdateSignal::Plunger(callback_sender) => {
                     callback_sender.send(()).unwrap_or_else(|_| {
                         debug!("Can't notify sender, assume nobody is waiting anymore");
@@ -361,12 +365,13 @@ impl UpdateHandler {
         // Transmitter was destroyed
         optimize_sender
             .send(OptimizerSignal::Stop)
+            .await
             .unwrap_or_else(|_| debug!("Optimizer already stopped"));
     }
 
     async fn flush_worker(
         segments: LockedSegmentHolder,
-        wal: Arc<Mutex<SerdeWal<CollectionUpdateOperations>>>,
+        wal: Arc<ParkingMutex<SerdeWal<CollectionUpdateOperations>>>,
         flush_interval_sec: u64,
         mut stop_receiver: oneshot::Receiver<()>,
     ) {
@@ -391,7 +396,7 @@ impl UpdateHandler {
                     continue;
                 }
             };
-            if let Err(err) = wal.lock().await.ack(confirmed_version) {
+            if let Err(err) = wal.lock().ack(confirmed_version) {
                 segments.write().report_optimizer_error(err);
             }
         }

commit cde627e364055581091a2c234a636096a6eebea6
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Fri Sep 30 09:31:01 2022 +0200

    send is a future (#1076)

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index e1890ab77..06fe73ced 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -262,7 +262,8 @@ impl UpdateHandler {
                 // After optimization is finished, we still need to check if there are
                 // some further optimizations possible.
                 // If receiver is already dead - we do not care.
-                let _ = sender.send(OptimizerSignal::Nop);
+                // If channel is full - optimization will be triggered by some other signal
+                let _ = sender.try_send(OptimizerSignal::Nop);
             },
         );
         let mut handles = optimization_handles.lock().await;

commit 58bd9bd07a6f1da3057eaa5416091348294b002f
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Mon Dec 19 13:21:00 2022 +0100

    Explicitly flush consensus WAL after (#1282)
    
    * panic on wal flush
    
    * use latest wal
    
    * try different flushing after truncate scheme
    
    * pull latest wal branch rev
    
    * use latest wal with fix

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 06fe73ced..fb2bee0f1 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -403,7 +403,7 @@ impl UpdateHandler {
         }
     }
 
-    /// Returns confirmed version after flush of all segements
+    /// Returns confirmed version after flush of all segments
     ///
     /// # Errors
     /// Returns an error on flush failure

commit bcb52f9aee210d02a10eb250ab3e602d29e17313
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Sun Dec 25 22:36:31 2022 +0100

    Id mapper inconsistency (#1302)
    
    * always flush wal
    
    * always flush wal fix
    
    * always flush wal fmt
    
    * flush wal during background flush
    
    * async wal flush
    
    * use id-tracker internal id for next-id instead of vector storage
    
    * add flush order and recovery comment
    
    fix merge bug
    
    * longer timeout in test

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index fb2bee0f1..0690e5912 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -19,7 +19,7 @@ use crate::collection_manager::optimizers::segment_optimizer::SegmentOptimizer;
 use crate::common::stoppable_task::{spawn_stoppable, StoppableTaskHandle};
 use crate::operations::types::{CollectionError, CollectionResult};
 use crate::operations::CollectionUpdateOperations;
-use crate::wal::SerdeWal;
+use crate::wal::{SerdeWal, WalError};
 
 pub const UPDATE_QUEUE_SIZE: usize = 100;
 
@@ -388,6 +388,19 @@ impl UpdateHandler {
             };
 
             trace!("Attempting flushing");
+            let wal_flash_job = wal.lock().flush_async();
+
+            if let Err(err) = wal_flash_job.join() {
+                error!("Failed to flush wal: {:?}", err);
+                segments
+                    .write()
+                    .report_optimizer_error(WalError::WriteWalError(format!(
+                        "WAL flush error: {:?}",
+                        err
+                    )));
+                continue;
+            }
+
             let confirmed_version = Self::flush_segments(segments.clone());
             let confirmed_version = match confirmed_version {
                 Ok(version) => version,

commit 0e329957b62a39cee7964bf82c8d93e438e4e332
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Fri Jan 20 23:52:45 2023 +0100

    Faster snapshot creation (#1353)
    
    * deduplication after proxy snapshot recovery
    
    * revert changes in apply_points_to_appendable
    
    * clippy fixes
    
    * spawn blocking in snapshot creation

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 0690e5912..dfdddf4dd 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -4,7 +4,6 @@ use std::sync::Arc;
 
 use itertools::Itertools;
 use log::{debug, error, info, trace, warn};
-use parking_lot::Mutex as ParkingMutex;
 use segment::entry::entry_point::OperationResult;
 use segment::types::SeqNumberType;
 use tokio::runtime::Handle;
@@ -19,7 +18,8 @@ use crate::collection_manager::optimizers::segment_optimizer::SegmentOptimizer;
 use crate::common::stoppable_task::{spawn_stoppable, StoppableTaskHandle};
 use crate::operations::types::{CollectionError, CollectionResult};
 use crate::operations::CollectionUpdateOperations;
-use crate::wal::{SerdeWal, WalError};
+use crate::shards::local_shard::LockedWal;
+use crate::wal::WalError;
 
 pub const UPDATE_QUEUE_SIZE: usize = 100;
 
@@ -77,7 +77,7 @@ pub struct UpdateHandler {
     flush_stop: Option<oneshot::Sender<()>>,
     runtime_handle: Handle,
     /// WAL, required for operations
-    wal: Arc<ParkingMutex<SerdeWal<CollectionUpdateOperations>>>,
+    wal: LockedWal,
     optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
     max_optimization_threads: usize,
 }
@@ -87,7 +87,7 @@ impl UpdateHandler {
         optimizers: Arc<Vec<Arc<Optimizer>>>,
         runtime_handle: Handle,
         segments: LockedSegmentHolder,
-        wal: Arc<ParkingMutex<SerdeWal<CollectionUpdateOperations>>>,
+        wal: LockedWal,
         flush_interval_sec: u64,
         max_optimization_threads: usize,
     ) -> UpdateHandler {
@@ -169,10 +169,7 @@ impl UpdateHandler {
 
     /// Checks if there are any failed operations.
     /// If so - attempts to re-apply all failed operations.
-    async fn try_recover(
-        segments: LockedSegmentHolder,
-        wal: Arc<ParkingMutex<SerdeWal<CollectionUpdateOperations>>>,
-    ) -> CollectionResult<usize> {
+    async fn try_recover(segments: LockedSegmentHolder, wal: LockedWal) -> CollectionResult<usize> {
         // Try to re-apply everything starting from the first failed operation
         let first_failed_operation_option = segments.read().failed_operation.iter().cloned().min();
         match first_failed_operation_option {
@@ -276,7 +273,7 @@ impl UpdateHandler {
         sender: Sender<OptimizerSignal>,
         mut receiver: Receiver<OptimizerSignal>,
         segments: LockedSegmentHolder,
-        wal: Arc<ParkingMutex<SerdeWal<CollectionUpdateOperations>>>,
+        wal: LockedWal,
         optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
         max_handles: usize,
     ) {
@@ -372,7 +369,7 @@ impl UpdateHandler {
 
     async fn flush_worker(
         segments: LockedSegmentHolder,
-        wal: Arc<ParkingMutex<SerdeWal<CollectionUpdateOperations>>>,
+        wal: LockedWal,
         flush_interval_sec: u64,
         mut stop_receiver: oneshot::Receiver<()>,
     ) {

commit 66aa2c99cedbdc31648feb0b28cb469d7021bef4
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Thu Jan 26 17:48:52 2023 +0100

    Clippy rust 1.67 (#1406)
    
    * inline format! args
    
    * inline format! args
    
    * explicit lifetime could be elided
    
    * fmt

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index dfdddf4dd..1e6210879 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -235,7 +235,7 @@ impl UpdateHandler {
                                     // so the best available action here is to stop whole
                                     // optimization thread and log the error
                                     log::error!("Optimization error: {}", error);
-                                    panic!("Optimization error: {}", error);
+                                    panic!("Optimization error: {error}");
                                 }
                             },
                         }
@@ -392,8 +392,7 @@ impl UpdateHandler {
                 segments
                     .write()
                     .report_optimizer_error(WalError::WriteWalError(format!(
-                        "WAL flush error: {:?}",
-                        err
+                        "WAL flush error: {err:?}"
                     )));
                 continue;
             }

commit a9f800bdd694d885e506902e1a568c0235e49be1
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Mon Mar 13 10:27:43 2023 +0100

    Add `update_queue_size` field to the `StorageConfig` structure (#1543)
    
    * WIP: Add `update_queue_size` field to the `StorageConfig` structure
    
    * WIP: Add `collection::config::GlobalConfig`... [skip ci]
    
    ...to propagate "global" (i.e., non per-collection) parameters to `collection` crate types.
    
    TODO:
    - fix tests/clippy/formatting (I'm on the move and laptop will die any minute now)
    
    * Fix tests, clippy and formatting
    
    * Remove unnecessary nesting in `collection/tests/alias_tests.rs`
    
    * review
    
    * fmt
    
    * fix review
    
    * fix review
    
    ---------
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 1e6210879..e9e0c78bc 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -16,13 +16,12 @@ use crate::collection_manager::collection_updater::CollectionUpdater;
 use crate::collection_manager::holders::segment_holder::LockedSegmentHolder;
 use crate::collection_manager::optimizers::segment_optimizer::SegmentOptimizer;
 use crate::common::stoppable_task::{spawn_stoppable, StoppableTaskHandle};
+use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{CollectionError, CollectionResult};
 use crate::operations::CollectionUpdateOperations;
 use crate::shards::local_shard::LockedWal;
 use crate::wal::WalError;
 
-pub const UPDATE_QUEUE_SIZE: usize = 100;
-
 pub type Optimizer = dyn SegmentOptimizer + Sync + Send;
 
 /// Information, required to perform operation and notify regarding the result
@@ -62,6 +61,7 @@ pub enum OptimizerSignal {
 
 /// Structure, which holds object, required for processing updates of the collection
 pub struct UpdateHandler {
+    shared_storage_config: Arc<SharedStorageConfig>,
     /// List of used optimizers
     pub optimizers: Arc<Vec<Arc<Optimizer>>>,
     /// How frequent can we flush data
@@ -84,6 +84,7 @@ pub struct UpdateHandler {
 
 impl UpdateHandler {
     pub fn new(
+        shared_storage_config: Arc<SharedStorageConfig>,
         optimizers: Arc<Vec<Arc<Optimizer>>>,
         runtime_handle: Handle,
         segments: LockedSegmentHolder,
@@ -92,6 +93,7 @@ impl UpdateHandler {
         max_optimization_threads: usize,
     ) -> UpdateHandler {
         UpdateHandler {
+            shared_storage_config,
             optimizers,
             segments,
             update_worker: None,
@@ -107,7 +109,7 @@ impl UpdateHandler {
     }
 
     pub fn run_workers(&mut self, update_receiver: Receiver<UpdateSignal>) {
-        let (tx, rx) = mpsc::channel(UPDATE_QUEUE_SIZE);
+        let (tx, rx) = mpsc::channel(self.shared_storage_config.update_queue_size);
         self.optimizer_worker = Some(self.runtime_handle.spawn(Self::optimization_worker_fn(
             self.optimizers.clone(),
             tx.clone(),

commit 45ae3e048b15f10e71b5825a9fc00ee7b7676390
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Tue May 9 18:01:01 2023 +0200

    Dynamic mmap vector storage (#1838)
    
    * wip: chunked mmap
    
    * Fix typo
    
    * insert and get methods
    
    * dynamic bitvec
    
    * clippy
    
    * wip: vector storage
    
    * wip: fmt
    
    * wip: mmap chunks
    
    * wip: mmap problems
    
    * Share transmuted mutable reference over mmap
    
    * option to enable appendable mmap vectors
    
    * fmt
    
    * rename storage status file
    
    * update tests
    
    * fix get deleted value range
    
    * add recovery to vector storage tests
    
    * add flush to tests
    
    * fix transmute from immutable to mutable
    
    * make transmuted pointer private
    
    * remove unused unsafe functions
    
    * force WAL flush if wait=true
    
    * move wal flush into updater thread
    
    * remove flush from update api
    
    * Minimize pub visibility for specialized/dangerous functions
    
    * Allocate vector with predefined capacity
    
    * Inline format parameters
    
    * Assert we have multiple chunks while testing, test is useless otherwise
    
    * Remove unnecessary scope
    
    * Remove unnecessary dereference
    
    * Random bool has 0.5 as standard distribution, use iter::repeat_with
    
    * Replace RemovableMmap::new with Default derive
    
    * Rename len to num_flags
    
    * Use Option replace as it is convention alongside take
    
    * Add FileId enum to replace error prone manual ID rotating
    
    * Use debug_assert_eq where applicable
    
    * Refactor drop and set to replace
    
    * Change default chunk size for chunked mmap vectors to 32MB
    
    This change is made as per GitHub review, because allocating a few
    storages with 128MB would take a significant amount of time and storage.
    
    See: https://github.com/qdrant/qdrant/pull/1838#discussion_r1187215475
    
    * Replace for-loops with iterators
    
    * Draft: add typed mmap to improve code safety (#1860)
    
    * Add typed mmap
    
    * Replace some crude mmap usages with typed mmap
    
    * Use typed mmap for deleted flags
    
    * Simplify dynamic mmap flags a lot with new typed mmap, remove flags option
    
    * Reformat
    
    * Remove old mmap functions that are now unused
    
    * Reimplement mmap locking for mmap_vectors
    
    * Add MmapBitSlice tests
    
    * Replace MmapChunk with new typed mmap
    
    * Update docs
    
    * Clean-up
    
    * Disable alignment assertions on Windows for now
    
    * Rename mmap lock to mlock to prevent confusion with lockable types
    
    * one more small test
    
    * Some review fixes
    
    * Add aliasing note
    
    * Add basic error handling in typed mmap constructors
    
    * Use typed mmap error handling throughout project
    
    * Move mmap type module to common
    
    * Fix transmute functions being unsound
    
    See https://github.com/qdrant/qdrant/pull/1860#discussion_r1188593854
    
    ---------
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>
    Co-authored-by: Tim Vise <tim+github@visee.me>

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index e9e0c78bc..b48be2681 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -31,6 +31,8 @@ pub struct OperationData {
     pub op_num: SeqNumberType,
     /// Operation
     pub operation: CollectionUpdateOperations,
+    /// If operation was requested to wait for result
+    pub wait: bool,
     /// Callback notification channel
     pub sender: Option<oneshot::Sender<CollectionResult<usize>>>,
 }
@@ -122,6 +124,7 @@ impl UpdateHandler {
         self.update_worker = Some(self.runtime_handle.spawn(Self::update_worker_fn(
             update_receiver,
             tx,
+            self.wal.clone(),
             self.segments.clone(),
         )));
         let (flush_tx, flush_rx) = oneshot::channel();
@@ -313,6 +316,7 @@ impl UpdateHandler {
     async fn update_worker_fn(
         mut receiver: Receiver<UpdateSignal>,
         optimize_sender: Sender<OptimizerSignal>,
+        wal: LockedWal,
         segments: LockedSegmentHolder,
     ) {
         while let Some(signal) = receiver.recv().await {
@@ -321,8 +325,23 @@ impl UpdateHandler {
                     op_num,
                     operation,
                     sender,
+                    wait,
                 }) => {
-                    let res = match CollectionUpdater::update(&segments, op_num, operation) {
+                    let flush_res = if wait {
+                        wal.lock().flush().map_err(|err| {
+                            CollectionError::service_error(format!(
+                                "Can't flush WAL before operation {} - {}",
+                                op_num, err
+                            ))
+                        })
+                    } else {
+                        Ok(())
+                    };
+
+                    let operation_result = flush_res
+                        .and_then(|_| CollectionUpdater::update(&segments, op_num, operation));
+
+                    let res = match operation_result {
                         Ok(update_res) => optimize_sender
                             .send(OptimizerSignal::Operation(op_num))
                             .await

commit 6b06c048e25c3a63980c7bfc0ca70e16ff5cf692
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Fri Jul 7 16:06:26 2023 +0200

    Fix optimizers stop lock (#2222)
    
    Co-authored-by: Tim Vise <tim+github@visee.me>
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index b48be2681..05ef0d16c 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -163,7 +163,10 @@ impl UpdateHandler {
 
         let mut opt_handles_guard = self.optimization_handles.lock().await;
         let opt_handles = std::mem::take(&mut *opt_handles_guard);
-        let stopping_handles = opt_handles.into_iter().map(|h| h.stop()).collect_vec();
+        let stopping_handles = opt_handles
+            .into_iter()
+            .filter_map(|h| h.stop())
+            .collect_vec();
 
         for res in stopping_handles {
             res.await?;
@@ -308,7 +311,8 @@ impl UpdateHandler {
                     )
                     .await;
                 }
-                OptimizerSignal::Stop => break, // Stop gracefully
+
+                OptimizerSignal::Stop => break,
             }
         }
     }

commit 34f654568bf2847ddc1485735b160cd3a7c77547
Author: Tim Vise <tim+github@visee.me>
Date:   Mon Aug 28 09:14:37 2023 +0200

    Report optimizer status and history in telemetry (#2475)
    
    * Add name to optimizers
    
    * Track optimizer status in update handler
    
    * Remove unused optimizer telemetry implementation
    
    * Report tracked optimizer status in local shard telemetry
    
    * Keep just the last 16 optimizer trackers and non successful ones
    
    * Also eventually truncate cancelled optimizer statuses
    
    * Fix codespell
    
    * Assert basic optimizer log state in unit test
    
    * Remove repetitive suffix from optimizer names
    
    * Loosen requirements for optimizer status test to prevent flakyness

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 05ef0d16c..1a756e299 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -4,6 +4,7 @@ use std::sync::Arc;
 
 use itertools::Itertools;
 use log::{debug, error, info, trace, warn};
+use parking_lot::Mutex;
 use segment::entry::entry_point::OperationResult;
 use segment::types::SeqNumberType;
 use tokio::runtime::Handle;
@@ -15,6 +16,7 @@ use tokio::time::Duration;
 use crate::collection_manager::collection_updater::CollectionUpdater;
 use crate::collection_manager::holders::segment_holder::LockedSegmentHolder;
 use crate::collection_manager::optimizers::segment_optimizer::SegmentOptimizer;
+use crate::collection_manager::optimizers::{Tracker, TrackerLog, TrackerStatus};
 use crate::common::stoppable_task::{spawn_stoppable, StoppableTaskHandle};
 use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{CollectionError, CollectionResult};
@@ -66,6 +68,8 @@ pub struct UpdateHandler {
     shared_storage_config: Arc<SharedStorageConfig>,
     /// List of used optimizers
     pub optimizers: Arc<Vec<Arc<Optimizer>>>,
+    /// Log of optimizer statuses
+    optimizers_log: Arc<Mutex<TrackerLog>>,
     /// How frequent can we flush data
     pub flush_interval_sec: u64,
     segments: LockedSegmentHolder,
@@ -85,9 +89,11 @@ pub struct UpdateHandler {
 }
 
 impl UpdateHandler {
+    #[allow(clippy::too_many_arguments)]
     pub fn new(
         shared_storage_config: Arc<SharedStorageConfig>,
         optimizers: Arc<Vec<Arc<Optimizer>>>,
+        optimizers_log: Arc<Mutex<TrackerLog>>,
         runtime_handle: Handle,
         segments: LockedSegmentHolder,
         wal: LockedWal,
@@ -100,6 +106,7 @@ impl UpdateHandler {
             segments,
             update_worker: None,
             optimizer_worker: None,
+            optimizers_log,
             flush_worker: None,
             flush_stop: None,
             runtime_handle,
@@ -119,6 +126,7 @@ impl UpdateHandler {
             self.segments.clone(),
             self.wal.clone(),
             self.optimization_handles.clone(),
+            self.optimizers_log.clone(),
             self.max_optimization_threads,
         )));
         self.update_worker = Some(self.runtime_handle.spawn(Self::update_worker_fn(
@@ -197,6 +205,7 @@ impl UpdateHandler {
     /// Returns handles for started tasks
     pub(crate) fn launch_optimization<F>(
         optimizers: Arc<Vec<Arc<Optimizer>>>,
+        optimizers_log: Arc<Mutex<TrackerLog>>,
         segments: LockedSegmentHolder,
         callback: F,
     ) -> Vec<StoppableTaskHandle<bool>>
@@ -215,22 +224,31 @@ impl UpdateHandler {
                     break;
                 } else {
                     let optim = optimizer.clone();
+                    let optimizers_log = optimizers_log.clone();
                     let segs = segments.clone();
                     let nsi = nonoptimal_segment_ids.clone();
                     for sid in &nsi {
                         scheduled_segment_ids.insert(*sid);
                     }
-                    let callback_cloned = callback.clone();
+                    let callback = callback.clone();
 
                     handles.push(spawn_stoppable(move |stopped| {
+                        // Track optimizer status
+                        let tracker = Tracker::start(optim.as_ref().name(), nsi.clone());
+                        let tracker_handle = tracker.handle();
+                        optimizers_log.lock().register(tracker);
+
+                        // Optimize and handle result
                         match optim.as_ref().optimize(segs.clone(), nsi, stopped) {
                             Ok(result) => {
-                                callback_cloned(result); // Perform some actions when optimization if finished
+                                tracker_handle.update(TrackerStatus::Done);
+                                callback(result); // Perform some actions when optimization if finished
                                 result
                             }
                             Err(error) => match error {
                                 CollectionError::Cancelled { description } => {
                                     log::debug!("Optimization cancelled - {}", description);
+                                    tracker_handle.update(TrackerStatus::Cancelled(description));
                                     false
                                 }
                                 _ => {
@@ -243,6 +261,9 @@ impl UpdateHandler {
                                     // so the best available action here is to stop whole
                                     // optimization thread and log the error
                                     log::error!("Optimization error: {}", error);
+
+                                    tracker_handle.update(TrackerStatus::Error(error.to_string()));
+
                                     panic!("Optimization error: {error}");
                                 }
                             },
@@ -258,10 +279,12 @@ impl UpdateHandler {
         optimizers: Arc<Vec<Arc<Optimizer>>>,
         segments: LockedSegmentHolder,
         optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
+        optimizers_log: Arc<Mutex<TrackerLog>>,
         sender: Sender<OptimizerSignal>,
     ) {
         let mut new_handles = Self::launch_optimization(
             optimizers.clone(),
+            optimizers_log,
             segments.clone(),
             move |_optimization_result| {
                 // After optimization is finished, we still need to check if there are
@@ -276,6 +299,7 @@ impl UpdateHandler {
         handles.retain(|h| !h.is_finished())
     }
 
+    #[allow(clippy::too_many_arguments)]
     async fn optimization_worker_fn(
         optimizers: Arc<Vec<Arc<Optimizer>>>,
         sender: Sender<OptimizerSignal>,
@@ -283,6 +307,7 @@ impl UpdateHandler {
         segments: LockedSegmentHolder,
         wal: LockedWal,
         optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
+        optimizers_log: Arc<Mutex<TrackerLog>>,
         max_handles: usize,
     ) {
         while let Some(signal) = receiver.recv().await {
@@ -307,6 +332,7 @@ impl UpdateHandler {
                         optimizers.clone(),
                         segments.clone(),
                         optimization_handles.clone(),
+                        optimizers_log.clone(),
                         sender.clone(),
                     )
                     .await;

commit c56b5dcc0a58f48907e3018b9ae0430848630abd
Author: Tim Vise <tim+github@visee.me>
Date:   Tue Aug 29 16:58:20 2023 +0200

    Add shard queue proxy (#2421)
    
    * Add queue proxy shard type
    
    This is similar to the forward proxy shard, but instead of forwarding
    operations to a remote it queues them. These queued operations can be
    transferred to a remote at a later time.
    
    * Add queue proxy methods for shard to replica set
    
    * Allow to specify max ack WAL version to prevent truncating too early
    
    * Prevent truncating WAL messages a queue proxy shard still relies on
    
    * Fix off by one error, don't batch last item that is already sent
    
    * Minor improvements
    
    * Use forward_update rather than update
    
    * Fix allowing forward proxy into queue proxy for same peer

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 1a756e299..2c79e3e09 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -84,6 +84,10 @@ pub struct UpdateHandler {
     runtime_handle: Handle,
     /// WAL, required for operations
     wal: LockedWal,
+    /// Maximum version to acknowledge to WAL to prevent truncating too early
+    /// This is used when another part still relies on part of the WAL, such as the queue proxy
+    /// shard.
+    pub(super) max_ack_version: Arc<TokioMutex<Option<u64>>>,
     optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
     max_optimization_threads: usize,
 }
@@ -111,6 +115,7 @@ impl UpdateHandler {
             flush_stop: None,
             runtime_handle,
             wal,
+            max_ack_version: Default::default(),
             flush_interval_sec,
             optimization_handles: Arc::new(TokioMutex::new(vec![])),
             max_optimization_threads,
@@ -139,6 +144,7 @@ impl UpdateHandler {
         self.flush_worker = Some(self.runtime_handle.spawn(Self::flush_worker(
             self.segments.clone(),
             self.wal.clone(),
+            self.max_ack_version.clone(),
             self.flush_interval_sec,
             flush_rx,
         )));
@@ -421,6 +427,7 @@ impl UpdateHandler {
     async fn flush_worker(
         segments: LockedSegmentHolder,
         wal: LockedWal,
+        max_ack: Arc<TokioMutex<Option<u64>>>,
         flush_interval_sec: u64,
         mut stop_receiver: oneshot::Receiver<()>,
     ) {
@@ -457,7 +464,20 @@ impl UpdateHandler {
                     continue;
                 }
             };
-            if let Err(err) = wal.lock().ack(confirmed_version) {
+
+            // Acknowledge confirmed version in WAL, but don't exceed specified maximum
+            // This is to prevent truncating WAL entries that may still be used by other things
+            // such as the queue proxy shard.
+            let max_ack_version = match *max_ack.lock().await {
+                Some(max_id) => {
+                    if confirmed_version > max_id {
+                        log::trace!("Acknowledging message {max_id} in WAL, {confirmed_version} is already confirmed but max_ack_version is set");
+                    }
+                    confirmed_version.min(max_id)
+                }
+                None => confirmed_version,
+            };
+            if let Err(err) = wal.lock().ack(max_ack_version) {
                 segments.write().report_optimizer_error(err);
             }
         }

commit 569b7896c79bfc0a6fbb8676c64866659157de4d
Author: Tim Vise <tim+github@visee.me>
Date:   Wed Aug 30 10:13:17 2023 +0200

    Catch optimizer panics (#2485)
    
    * Add panic handler to stoppable task
    
    * Define panic handler for optimizer task, report panic to segment holder
    
    * Report panic payload message when optimizer panics
    
    * Add function to handle and clean finished optimizer tasks
    
    * Rewrite optimizer worker, periodically clean finished optimizer tasks
    
    * Minor improvements and codespell fixes
    
    * Add simple test to ensure panic callback is called
    
    * Explicitly handle elapsed timeout errors in optimizer handle handling

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 2c79e3e09..7e7bf5250 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -11,19 +11,27 @@ use tokio::runtime::Handle;
 use tokio::sync::mpsc::{self, Receiver, Sender};
 use tokio::sync::{oneshot, Mutex as TokioMutex};
 use tokio::task::JoinHandle;
-use tokio::time::Duration;
+use tokio::time::error::Elapsed;
+use tokio::time::{timeout, Duration};
 
 use crate::collection_manager::collection_updater::CollectionUpdater;
 use crate::collection_manager::holders::segment_holder::LockedSegmentHolder;
 use crate::collection_manager::optimizers::segment_optimizer::SegmentOptimizer;
 use crate::collection_manager::optimizers::{Tracker, TrackerLog, TrackerStatus};
-use crate::common::stoppable_task::{spawn_stoppable, StoppableTaskHandle};
+use crate::common::stoppable_task::{
+    panic_payload_into_string, spawn_stoppable, StoppableTaskHandle,
+};
 use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{CollectionError, CollectionResult};
 use crate::operations::CollectionUpdateOperations;
 use crate::shards::local_shard::LockedWal;
 use crate::wal::WalError;
 
+/// Interval at which the optimizer worker cleans up old optimization handles
+///
+/// The longer the duration, the longer it  takes for panicked tasks to be reported.
+const OPTIMIZER_CLEANUP_INTERVAL: Duration = Duration::from_secs(5);
+
 pub type Optimizer = dyn SegmentOptimizer + Sync + Send;
 
 /// Information, required to perform operation and notify regarding the result
@@ -228,54 +236,73 @@ impl UpdateHandler {
                     optimizer.check_condition(segments.clone(), &scheduled_segment_ids);
                 if nonoptimal_segment_ids.is_empty() {
                     break;
-                } else {
-                    let optim = optimizer.clone();
-                    let optimizers_log = optimizers_log.clone();
-                    let segs = segments.clone();
-                    let nsi = nonoptimal_segment_ids.clone();
-                    for sid in &nsi {
-                        scheduled_segment_ids.insert(*sid);
-                    }
-                    let callback = callback.clone();
-
-                    handles.push(spawn_stoppable(move |stopped| {
-                        // Track optimizer status
-                        let tracker = Tracker::start(optim.as_ref().name(), nsi.clone());
-                        let tracker_handle = tracker.handle();
-                        optimizers_log.lock().register(tracker);
-
-                        // Optimize and handle result
-                        match optim.as_ref().optimize(segs.clone(), nsi, stopped) {
-                            Ok(result) => {
-                                tracker_handle.update(TrackerStatus::Done);
-                                callback(result); // Perform some actions when optimization if finished
-                                result
-                            }
-                            Err(error) => match error {
-                                CollectionError::Cancelled { description } => {
-                                    log::debug!("Optimization cancelled - {}", description);
-                                    tracker_handle.update(TrackerStatus::Cancelled(description));
-                                    false
-                                }
-                                _ => {
-                                    // Save only the first error
-                                    // If is more likely to be the real cause of all further problems
-                                    segs.write().report_optimizer_error(error.clone());
-
-                                    // Error of the optimization can not be handled by API user
-                                    // It is only possible to fix after full restart,
-                                    // so the best available action here is to stop whole
-                                    // optimization thread and log the error
-                                    log::error!("Optimization error: {}", error);
+                }
 
-                                    tracker_handle.update(TrackerStatus::Error(error.to_string()));
+                let optimizer = optimizer.clone();
+                let optimizers_log = optimizers_log.clone();
+                let segments = segments.clone();
+                let nsi = nonoptimal_segment_ids.clone();
+                scheduled_segment_ids.extend(&nsi);
+                let callback = callback.clone();
 
-                                    panic!("Optimization error: {error}");
+                let handle = spawn_stoppable(
+                    // Stoppable task
+                    {
+                        let segments = segments.clone();
+                        move |stopped| {
+                            // Track optimizer status
+                            let tracker = Tracker::start(optimizer.as_ref().name(), nsi.clone());
+                            let tracker_handle = tracker.handle();
+                            optimizers_log.lock().register(tracker);
+
+                            // Optimize and handle result
+                            match optimizer.as_ref().optimize(segments.clone(), nsi, stopped) {
+                                // Perform some actions when optimization if finished
+                                Ok(result) => {
+                                    tracker_handle.update(TrackerStatus::Done);
+                                    callback(result);
+                                    result
                                 }
-                            },
+                                // Handle and report errors
+                                Err(error) => match error {
+                                    CollectionError::Cancelled { description } => {
+                                        log::debug!("Optimization cancelled - {}", description);
+                                        tracker_handle
+                                            .update(TrackerStatus::Cancelled(description));
+                                        false
+                                    }
+                                    _ => {
+                                        segments.write().report_optimizer_error(error.clone());
+
+                                        // Error of the optimization can not be handled by API user
+                                        // It is only possible to fix after full restart,
+                                        // so the best available action here is to stop whole
+                                        // optimization thread and log the error
+                                        log::error!("Optimization error: {}", error);
+
+                                        tracker_handle
+                                            .update(TrackerStatus::Error(error.to_string()));
+
+                                        panic!("Optimization error: {error}");
+                                    }
+                                },
+                            }
                         }
-                    }));
-                }
+                    },
+                    // Panic handler
+                    Some(Box::new(move |panic_payload| {
+                        let panic_msg = panic_payload_into_string(panic_payload);
+                        log::warn!(
+                            "Optimization task panicked, collection may be in unstable state: {panic_msg}"
+                        );
+                        segments
+                            .write()
+                            .report_optimizer_error(CollectionError::service_error(format!(
+                                "Optimization task panicked: {panic_msg}"
+                            )));
+                    })),
+                );
+                handles.push(handle);
             }
         }
         handles
@@ -302,7 +329,33 @@ impl UpdateHandler {
         );
         let mut handles = optimization_handles.lock().await;
         handles.append(&mut new_handles);
-        handles.retain(|h| !h.is_finished())
+    }
+
+    /// Cleanup finalized optimization task handles
+    ///
+    /// This finds and removes completed tasks from our list of optimization handles.
+    /// It also propagates any panics (and unknown errors) so we properly handle them if desired.
+    ///
+    /// It is essential to call this every once in a while for handling panics in time.
+    async fn cleanup_optimization_handles(
+        optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
+    ) {
+        // Remove finished handles
+        let finished_handles: Vec<_> = {
+            let mut handles = optimization_handles.lock().await;
+            (0..handles.len())
+                .filter(|i| handles[*i].is_finished())
+                .collect::<Vec<_>>()
+                .into_iter()
+                .rev()
+                .map(|i| handles.remove(i))
+                .collect()
+        };
+
+        // Finalize all finished handles to propagate panics
+        for handle in finished_handles {
+            handle.join_and_handle_panic().await;
+        }
     }
 
     #[allow(clippy::too_many_arguments)]
@@ -316,18 +369,27 @@ impl UpdateHandler {
         optimizers_log: Arc<Mutex<TrackerLog>>,
         max_handles: usize,
     ) {
-        while let Some(signal) = receiver.recv().await {
-            match signal {
-                OptimizerSignal::Nop | OptimizerSignal::Operation(_) => {
+        loop {
+            let receiver = timeout(OPTIMIZER_CLEANUP_INTERVAL, receiver.recv());
+            let result = receiver.await;
+
+            // Always clean up on any signal
+            Self::cleanup_optimization_handles(optimization_handles.clone()).await;
+
+            match result {
+                // Channel closed or stop signal
+                Ok(None | Some(OptimizerSignal::Stop)) => break,
+                // Clean up interval
+                Err(Elapsed { .. }) => continue,
+                // Optimizer signal
+                Ok(Some(signal @ (OptimizerSignal::Nop | OptimizerSignal::Operation(_)))) => {
+                    // If not forcing with Nop, wait on next signal if we have too many handles
                     if signal != OptimizerSignal::Nop
                         && optimization_handles.lock().await.len() >= max_handles
                     {
-                        let mut handles = optimization_handles.lock().await;
-                        handles.retain(|h| !h.is_finished());
                         continue;
                     }
-                    // We skip the check for number of optimization handles here
-                    // Because `Nop` usually means that we need to force the optimization
+
                     if Self::try_recover(segments.clone(), wal.clone())
                         .await
                         .is_err()
@@ -343,8 +405,6 @@ impl UpdateHandler {
                     )
                     .await;
                 }
-
-                OptimizerSignal::Stop => break,
             }
         }
     }

commit 4f983e495db72336b2311dc2abe95a11eab8c620
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Fri Sep 29 16:23:24 2023 +0200

    Promote operation error to dedicated file (#2736)

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 7e7bf5250..477662f3e 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -5,7 +5,7 @@ use std::sync::Arc;
 use itertools::Itertools;
 use log::{debug, error, info, trace, warn};
 use parking_lot::Mutex;
-use segment::entry::entry_point::OperationResult;
+use segment::common::operation_error::OperationResult;
 use segment::types::SeqNumberType;
 use tokio::runtime::Handle;
 use tokio::sync::mpsc::{self, Receiver, Sender};
@@ -266,7 +266,7 @@ impl UpdateHandler {
                                 // Handle and report errors
                                 Err(error) => match error {
                                     CollectionError::Cancelled { description } => {
-                                        log::debug!("Optimization cancelled - {}", description);
+                                        debug!("Optimization cancelled - {}", description);
                                         tracker_handle
                                             .update(TrackerStatus::Cancelled(description));
                                         false
@@ -292,7 +292,7 @@ impl UpdateHandler {
                     // Panic handler
                     Some(Box::new(move |panic_payload| {
                         let panic_msg = panic_payload_into_string(panic_payload);
-                        log::warn!(
+                        warn!(
                             "Optimization task panicked, collection may be in unstable state: {panic_msg}"
                         );
                         segments
@@ -531,7 +531,7 @@ impl UpdateHandler {
             let max_ack_version = match *max_ack.lock().await {
                 Some(max_id) => {
                     if confirmed_version > max_id {
-                        log::trace!("Acknowledging message {max_id} in WAL, {confirmed_version} is already confirmed but max_ack_version is set");
+                        trace!("Acknowledging message {max_id} in WAL, {confirmed_version} is already confirmed but max_ack_version is set");
                     }
                     confirmed_version.min(max_id)
                 }

commit 4885b1e70d6600f6718726dc86d4221df74c319f
Author: Tim Vise <tim+github@visee.me>
Date:   Wed Nov 8 11:51:33 2023 +0100

    Transform `max_ack_version` from mutex into atomic (#2951)
    
    * Change queue proxy max ack version type from mutex into atomic
    
    * Remove need for async locks in `set_max_ack_version`
    
    * Remove async marker from methods not needing it anymore

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 477662f3e..72aa5d06e 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -1,5 +1,6 @@
 use std::cmp::min;
 use std::collections::HashSet;
+use std::sync::atomic::AtomicU64;
 use std::sync::Arc;
 
 use itertools::Itertools;
@@ -95,7 +96,8 @@ pub struct UpdateHandler {
     /// Maximum version to acknowledge to WAL to prevent truncating too early
     /// This is used when another part still relies on part of the WAL, such as the queue proxy
     /// shard.
-    pub(super) max_ack_version: Arc<TokioMutex<Option<u64>>>,
+    /// Defaults to `u64::MAX` to allow acknowledging all confirmed versions.
+    pub(super) max_ack_version: Arc<AtomicU64>,
     optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
     max_optimization_threads: usize,
 }
@@ -123,7 +125,7 @@ impl UpdateHandler {
             flush_stop: None,
             runtime_handle,
             wal,
-            max_ack_version: Default::default(),
+            max_ack_version: Arc::new(u64::MAX.into()),
             flush_interval_sec,
             optimization_handles: Arc::new(TokioMutex::new(vec![])),
             max_optimization_threads,
@@ -487,7 +489,7 @@ impl UpdateHandler {
     async fn flush_worker(
         segments: LockedSegmentHolder,
         wal: LockedWal,
-        max_ack: Arc<TokioMutex<Option<u64>>>,
+        max_ack: Arc<AtomicU64>,
         flush_interval_sec: u64,
         mut stop_receiver: oneshot::Receiver<()>,
     ) {
@@ -528,16 +530,14 @@ impl UpdateHandler {
             // Acknowledge confirmed version in WAL, but don't exceed specified maximum
             // This is to prevent truncating WAL entries that may still be used by other things
             // such as the queue proxy shard.
-            let max_ack_version = match *max_ack.lock().await {
-                Some(max_id) => {
-                    if confirmed_version > max_id {
-                        trace!("Acknowledging message {max_id} in WAL, {confirmed_version} is already confirmed but max_ack_version is set");
-                    }
-                    confirmed_version.min(max_id)
-                }
-                None => confirmed_version,
-            };
-            if let Err(err) = wal.lock().ack(max_ack_version) {
+            // Default maximum ack version is `u64::MAX` to allow acknowledging all confirmed.
+            let max_ack = max_ack.load(std::sync::atomic::Ordering::Relaxed);
+            if confirmed_version > max_ack {
+                trace!("Acknowledging message {max_ack} in WAL, {confirmed_version} is already confirmed but max_ack_version is set");
+            }
+            let ack = confirmed_version.min(max_ack);
+
+            if let Err(err) = wal.lock().ack(ack) {
                 segments.write().report_optimizer_error(err);
             }
         }

commit cb46a73609aaa79d2d8bb1b6389778740d3f1935
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Tue Dec 5 17:59:46 2023 +0100

    Extend `/readyz` with shards readiness check (#3053, #3084)
    
    Co-authored-by: generall <andrey@vasnetsov.com>
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 72aa5d06e..5d7a8a058 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -3,6 +3,7 @@ use std::collections::HashSet;
 use std::sync::atomic::AtomicU64;
 use std::sync::Arc;
 
+use common::panic;
 use itertools::Itertools;
 use log::{debug, error, info, trace, warn};
 use parking_lot::Mutex;
@@ -19,9 +20,7 @@ use crate::collection_manager::collection_updater::CollectionUpdater;
 use crate::collection_manager::holders::segment_holder::LockedSegmentHolder;
 use crate::collection_manager::optimizers::segment_optimizer::SegmentOptimizer;
 use crate::collection_manager::optimizers::{Tracker, TrackerLog, TrackerStatus};
-use crate::common::stoppable_task::{
-    panic_payload_into_string, spawn_stoppable, StoppableTaskHandle,
-};
+use crate::common::stoppable_task::{spawn_stoppable, StoppableTaskHandle};
 use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{CollectionError, CollectionResult};
 use crate::operations::CollectionUpdateOperations;
@@ -293,14 +292,18 @@ impl UpdateHandler {
                     },
                     // Panic handler
                     Some(Box::new(move |panic_payload| {
-                        let panic_msg = panic_payload_into_string(panic_payload);
+                        let message = panic::downcast_str(&panic_payload).unwrap_or("");
+                        let separator = if !message.is_empty() { ": " } else { "" };
+
                         warn!(
-                            "Optimization task panicked, collection may be in unstable state: {panic_msg}"
+                            "Optimization task panicked, collection may be in unstable state\
+                             {separator}{message}"
                         );
+
                         segments
                             .write()
                             .report_optimizer_error(CollectionError::service_error(format!(
-                                "Optimization task panicked: {panic_msg}"
+                                "Optimization task panicked{separator}{message}"
                             )));
                     })),
                 );

commit 680574347f3b3dd6f604f452b80734a8c6f2f7c6
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Mon Dec 25 14:26:21 2023 +0100

    Fix clippy 1.75 (#3270)

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 5d7a8a058..b8e2e3f2f 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -505,7 +505,7 @@ impl UpdateHandler {
                     debug!("Stopping flush worker.");
                     return;
                 }
-            };
+            }
 
             trace!("Attempting flushing");
             let wal_flash_job = wal.lock().flush_async();

commit 19514265330ac9a1049b9439517deb104a5a19ed
Author: Tim Vise <tim+github@visee.me>
Date:   Wed Jan 31 11:56:34 2024 +0100

    Dynamic CPU saturation internals (#3364)
    
    * Move CPU count function to common, fix wrong CPU count in visited list
    
    * Change default number of rayon threads to 8
    
    * Use CPU budget and CPU permits for optimizer tasks to limit utilization
    
    * Respect configured thread limits, use new sane defaults in config
    
    * Fix spelling issues
    
    * Fix test compilation error
    
    * Improve breaking if there is no CPU budget
    
    * Block optimizations until CPU budget, fix potentially getting stuck
    
    Our optimization worker now blocks until CPU budget is available to
    perform the task.
    
    Fix potential issue where optimization worker could get stuck. This
    would happen if no optimization task is started because there's no
    available CPU budget. This ensures the worker is woken up again to
    retry.
    
    * Utilize n-1 CPUs with optimization tasks
    
    * Better handle situations where CPU budget is drained
    
    * Dynamically scale rayon CPU count based on CPU size
    
    * Fix incorrect default for max_indexing_threads conversion
    
    * Respect max_indexing_threads for collection
    
    * Make max_indexing_threads optional, use none to set no limit
    
    * Update property documentation and comments
    
    * Property max_optimization_threads is per shard, not per collection
    
    * If we reached shard optimization limit, skip further checks
    
    * Add remaining TODOs
    
    * Fix spelling mistake
    
    * Align gRPC comment blocks
    
    * Fix compilation errors since last rebase
    
    * Make tests aware of CPU budget
    
    * Use new CPU budget calculation function everywhere
    
    * Make CPU budget configurable in settings, move static budget to common
    
    * Do not use static CPU budget, instance it and pass it through
    
    * Update CPU budget description
    
    * Move heuristic into defaults
    
    * Fix spelling issues
    
    * Move cpu_budget property to a better place
    
    * Move some things around
    
    * Minor review improvements
    
    * Use range match statement for CPU count heuristics
    
    * Systems with 1 or 2 CPUs do not keep cores unallocated by default
    
    * Fix compilation errors since last rebase
    
    * Update lib/segment/src/types.rs
    
    Co-authored-by: Luis Cosso <luis.cossio@qdrant.com>
    
    * Update lib/storage/src/content_manager/toc/transfer.rs
    
    Co-authored-by: Luis Cosso <luis.cossio@qdrant.com>
    
    * Rename cpu_budget to optimizer_cpu_budget
    
    * Update OpenAPI specification
    
    * Require at least half of the desired CPUs for optimizers
    
    This prevents running optimizations with just one CPU, which could be
    very slow.
    
    * Don't use wildcard in CPU heuristic match statements
    
    * Rename cpu_budget setting to optimizer_cpu_budget
    
    * Update CPU budget comments
    
    * Spell acquire correctly
    
    * Change if-else into match
    
    Co-authored-by: Luis Cosso <luis.cossio@qdrant.com>
    
    * Rename max_rayon_threads to num_rayon_threads, add explanation
    
    * Explain limit in update handler
    
    * Remove numbers for automatic selection of indexing threads
    
    * Inline max_workers variable
    
    * Remove CPU budget from ShardTransferConsensus trait, it is in collection
    
    * small allow(dead_code) => cfg(test)
    
    * Remove now obsolete lazy_static
    
    * Fix incorrect CPU calculation in CPU saturation test
    
    * Make waiting for CPU budget async, don't block current thread
    
    * Prevent deadlock on optimizer signal channel
    
    Do not block the optimization worker task anymore to wait for CPU budget
    to be available. That prevents our optimizer signal channel from being
    drained, blocking incoming updates because the cannot send another
    optimizer signal. Now, prevent blocking this task all together and
    retrigger the optimizers separately when CPU budget is available again.
    
    * Fix incorrect CPU calculation in optimization cancel test
    
    * Rename CPU budget wait function to notify
    
    * Detach API changes from CPU saturation internals
    
    This allows us to merge into a patch version of Qdrant. We can
    reintroduce the API changes in the upcoming minor release to make all of
    it fully functional.
    
    ---------
    
    Co-authored-by: Luis Cosso <luis.cossio@qdrant.com>
    Co-authored-by: Luis Cosso <luis.cossio@outlook.com>

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index b8e2e3f2f..0be5b2f41 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -3,16 +3,18 @@ use std::collections::HashSet;
 use std::sync::atomic::AtomicU64;
 use std::sync::Arc;
 
+use common::cpu::CpuBudget;
 use common::panic;
 use itertools::Itertools;
 use log::{debug, error, info, trace, warn};
 use parking_lot::Mutex;
 use segment::common::operation_error::OperationResult;
+use segment::index::hnsw_index::num_rayon_threads;
 use segment::types::SeqNumberType;
 use tokio::runtime::Handle;
 use tokio::sync::mpsc::{self, Receiver, Sender};
 use tokio::sync::{oneshot, Mutex as TokioMutex};
-use tokio::task::JoinHandle;
+use tokio::task::{self, JoinHandle};
 use tokio::time::error::Elapsed;
 use tokio::time::{timeout, Duration};
 
@@ -78,6 +80,9 @@ pub struct UpdateHandler {
     pub optimizers: Arc<Vec<Arc<Optimizer>>>,
     /// Log of optimizer statuses
     optimizers_log: Arc<Mutex<TrackerLog>>,
+    /// Global CPU budget in number of cores for all optimization tasks.
+    /// Assigns CPU permits to tasks to limit overall resource utilization.
+    optimizer_cpu_budget: CpuBudget,
     /// How frequent can we flush data
     pub flush_interval_sec: u64,
     segments: LockedSegmentHolder,
@@ -98,7 +103,8 @@ pub struct UpdateHandler {
     /// Defaults to `u64::MAX` to allow acknowledging all confirmed versions.
     pub(super) max_ack_version: Arc<AtomicU64>,
     optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
-    max_optimization_threads: usize,
+    /// Maximum number of concurrent optimization jobs in this update handler.
+    max_optimization_threads: Option<usize>,
 }
 
 impl UpdateHandler {
@@ -107,11 +113,12 @@ impl UpdateHandler {
         shared_storage_config: Arc<SharedStorageConfig>,
         optimizers: Arc<Vec<Arc<Optimizer>>>,
         optimizers_log: Arc<Mutex<TrackerLog>>,
+        optimizer_cpu_budget: CpuBudget,
         runtime_handle: Handle,
         segments: LockedSegmentHolder,
         wal: LockedWal,
         flush_interval_sec: u64,
-        max_optimization_threads: usize,
+        max_optimization_threads: Option<usize>,
     ) -> UpdateHandler {
         UpdateHandler {
             shared_storage_config,
@@ -120,6 +127,7 @@ impl UpdateHandler {
             update_worker: None,
             optimizer_worker: None,
             optimizers_log,
+            optimizer_cpu_budget,
             flush_worker: None,
             flush_stop: None,
             runtime_handle,
@@ -141,6 +149,7 @@ impl UpdateHandler {
             self.wal.clone(),
             self.optimization_handles.clone(),
             self.optimizers_log.clone(),
+            self.optimizer_cpu_budget.clone(),
             self.max_optimization_threads,
         )));
         self.update_worker = Some(self.runtime_handle.spawn(Self::update_worker_fn(
@@ -221,24 +230,55 @@ impl UpdateHandler {
     pub(crate) fn launch_optimization<F>(
         optimizers: Arc<Vec<Arc<Optimizer>>>,
         optimizers_log: Arc<Mutex<TrackerLog>>,
+        optimizer_cpu_budget: &CpuBudget,
         segments: LockedSegmentHolder,
         callback: F,
+        limit: Option<usize>,
     ) -> Vec<StoppableTaskHandle<bool>>
     where
-        F: FnOnce(bool),
-        F: Send + 'static,
-        F: Clone,
+        F: FnOnce(bool) + Send + Clone + 'static,
     {
-        let mut scheduled_segment_ids: HashSet<_> = Default::default();
+        let mut scheduled_segment_ids = HashSet::<_>::default();
         let mut handles = vec![];
-        for optimizer in optimizers.iter() {
+        'outer: for optimizer in optimizers.iter() {
             loop {
+                // Return early if we reached the optimization job limit
+                if limit.map(|extra| handles.len() >= extra).unwrap_or(false) {
+                    log::trace!("Reached optimization job limit, postponing other optimizations");
+                    break 'outer;
+                }
+
                 let nonoptimal_segment_ids =
                     optimizer.check_condition(segments.clone(), &scheduled_segment_ids);
                 if nonoptimal_segment_ids.is_empty() {
                     break;
                 }
 
+                // Determine how many CPUs we prefer for optimization task, acquire permit for it
+                let max_indexing_threads = optimizer.hnsw_config().max_indexing_threads;
+                let desired_cpus = num_rayon_threads(max_indexing_threads);
+                let permit = match optimizer_cpu_budget.try_acquire(desired_cpus) {
+                    Some(permit) => permit,
+                    // If there is no CPU budget, break outer loop and return early
+                    // If we have no handles (no optimizations) trigger callback so that we wake up
+                    // our optimization worker to try again later, otherwise it could get stuck
+                    None => {
+                        log::trace!(
+                            "No available CPU permit for {} optimizer, postponing",
+                            optimizer.name(),
+                        );
+                        if handles.is_empty() {
+                            callback(false);
+                        }
+                        break 'outer;
+                    }
+                };
+                log::trace!(
+                    "Acquired {} CPU permit for {} optimizer",
+                    permit.num_cpus,
+                    optimizer.name(),
+                );
+
                 let optimizer = optimizer.clone();
                 let optimizers_log = optimizers_log.clone();
                 let segments = segments.clone();
@@ -257,7 +297,12 @@ impl UpdateHandler {
                             optimizers_log.lock().register(tracker);
 
                             // Optimize and handle result
-                            match optimizer.as_ref().optimize(segments.clone(), nsi, stopped) {
+                            match optimizer.as_ref().optimize(
+                                segments.clone(),
+                                nsi,
+                                permit,
+                                stopped,
+                            ) {
                                 // Perform some actions when optimization if finished
                                 Ok(result) => {
                                     tracker_handle.update(TrackerStatus::Done);
@@ -310,6 +355,7 @@ impl UpdateHandler {
                 handles.push(handle);
             }
         }
+
         handles
     }
 
@@ -318,11 +364,14 @@ impl UpdateHandler {
         segments: LockedSegmentHolder,
         optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
         optimizers_log: Arc<Mutex<TrackerLog>>,
+        optimizer_cpu_budget: &CpuBudget,
         sender: Sender<OptimizerSignal>,
+        limit: usize,
     ) {
         let mut new_handles = Self::launch_optimization(
             optimizers.clone(),
             optimizers_log,
+            optimizer_cpu_budget,
             segments.clone(),
             move |_optimization_result| {
                 // After optimization is finished, we still need to check if there are
@@ -331,6 +380,7 @@ impl UpdateHandler {
                 // If channel is full - optimization will be triggered by some other signal
                 let _ = sender.try_send(OptimizerSignal::Nop);
             },
+            Some(limit),
         );
         let mut handles = optimization_handles.lock().await;
         handles.append(&mut new_handles);
@@ -372,8 +422,18 @@ impl UpdateHandler {
         wal: LockedWal,
         optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
         optimizers_log: Arc<Mutex<TrackerLog>>,
-        max_handles: usize,
+        optimizer_cpu_budget: CpuBudget,
+        max_handles: Option<usize>,
     ) {
+        let max_handles = max_handles.unwrap_or(usize::MAX);
+        let max_indexing_threads = optimizers
+            .first()
+            .map(|optimizer| optimizer.hnsw_config().max_indexing_threads)
+            .unwrap_or_default();
+
+        // Asynchronous task to trigger optimizers once CPU budget is available again
+        let mut cpu_available_trigger: Option<JoinHandle<()>> = None;
+
         loop {
             let receiver = timeout(OPTIMIZER_CLEANUP_INTERVAL, receiver.recv());
             let result = receiver.await;
@@ -401,12 +461,46 @@ impl UpdateHandler {
                     {
                         continue;
                     }
+
+                    // Continue if we have enough CPU budget available to start an optimization
+                    // Otherwise skip now and start a task to trigger the optimizer again once CPU
+                    // budget becomes available
+                    let desired_cpus = num_rayon_threads(max_indexing_threads);
+                    if !optimizer_cpu_budget.has_budget(desired_cpus) {
+                        let trigger_active = cpu_available_trigger
+                            .as_ref()
+                            .map_or(false, |t| !t.is_finished());
+                        if !trigger_active {
+                            cpu_available_trigger.replace(trigger_optimizers_on_cpu_budget(
+                                optimizer_cpu_budget.clone(),
+                                desired_cpus,
+                                sender.clone(),
+                            ));
+                        }
+                        continue;
+                    }
+
+                    // Determine optimization handle limit based on max handles we allow
+                    // Not related to the CPU budget, but a different limit for the maximum number
+                    // of concurrent concrete optimizations per shard as configured by the user in
+                    // the Qdrant configuration.
+                    // Skip if we reached limit, an ongoing optimization that finishes will trigger this loop again
+                    let limit = max_handles.saturating_sub(optimization_handles.lock().await.len());
+                    if limit == 0 {
+                        log::trace!(
+                            "Skipping optimization check, we reached optimization thread limit"
+                        );
+                        continue;
+                    }
+
                     Self::process_optimization(
                         optimizers.clone(),
                         segments.clone(),
                         optimization_handles.clone(),
                         optimizers_log.clone(),
+                        &optimizer_cpu_budget,
                         sender.clone(),
+                        limit,
                     )
                     .await;
                 }
@@ -559,3 +653,23 @@ impl UpdateHandler {
         })
     }
 }
+
+/// Trigger optimizers when CPU budget is available
+fn trigger_optimizers_on_cpu_budget(
+    optimizer_cpu_budget: CpuBudget,
+    desired_cpus: usize,
+    sender: Sender<OptimizerSignal>,
+) -> JoinHandle<()> {
+    task::spawn(async move {
+        log::trace!("Skipping optimization checks, waiting for CPU budget to be available");
+        optimizer_cpu_budget
+            .notify_on_budget_available(desired_cpus)
+            .await;
+        log::trace!("Continue optimization checks, new CPU budget available");
+
+        // Trigger optimizers with Nop operation
+        sender.send(OptimizerSignal::Nop).await.unwrap_or_else(|_| {
+            log::info!("Can't notify optimizers, assume process is dead. Restart is required")
+        });
+    })
+}

commit 99b750fcfa63444e07105947f7fd4fb241d7b050
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Thu Feb 1 11:44:13 2024 +0100

    Add `clock_tag` field to update operations (#3408)

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 0be5b2f41..61a7beb51 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -217,7 +217,7 @@ impl UpdateHandler {
             Some(first_failed_op) => {
                 let wal_lock = wal.lock();
                 for (op_num, operation) in wal_lock.read(first_failed_op) {
-                    CollectionUpdater::update(&segments, op_num, operation)?;
+                    CollectionUpdater::update(&segments, op_num, operation.operation)?;
                 }
             }
         };

commit 133a1ccddc5330c64e9920157f0e0b324143fce0
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Mon Feb 5 11:02:58 2024 +0100

    Demote missing client for update ack. log (#3523)

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 61a7beb51..1922e7bf7 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -547,7 +547,7 @@ impl UpdateHandler {
 
                     if let Some(feedback) = sender {
                         feedback.send(res).unwrap_or_else(|_| {
-                            info!(
+                            debug!(
                                 "Can't report operation {} result. Assume already not required",
                                 op_num
                             );

commit fd9e8bf311417f8406976ddc94c75841d80108d4
Author: Tim Vise <tim+github@visee.me>
Date:   Wed Feb 7 14:14:16 2024 +0100

    Change `max_ack_version` to be `wal_keep_from` (#3550)
    
    * Change max_ack_version to be wal_keep_from
    
    This shifts the logic by one. The variable now specifies what version to
    keep rather than the last version we can acknowledge.
    
    * Update comment
    
    * Limit ack function visibility to prevent unwanted callers
    
    * Use better name for wal_keep_from parameter

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 1922e7bf7..5062e28da 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -97,11 +97,11 @@ pub struct UpdateHandler {
     runtime_handle: Handle,
     /// WAL, required for operations
     wal: LockedWal,
-    /// Maximum version to acknowledge to WAL to prevent truncating too early
-    /// This is used when another part still relies on part of the WAL, such as the queue proxy
-    /// shard.
+    /// Always keep this WAL version and later and prevent acknowledging/truncating from the WAL.
+    /// This is used when other bits of code still depend on information in the WAL, such as the
+    /// queue proxy shard.
     /// Defaults to `u64::MAX` to allow acknowledging all confirmed versions.
-    pub(super) max_ack_version: Arc<AtomicU64>,
+    pub(super) wal_keep_from: Arc<AtomicU64>,
     optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
     /// Maximum number of concurrent optimization jobs in this update handler.
     max_optimization_threads: Option<usize>,
@@ -132,7 +132,7 @@ impl UpdateHandler {
             flush_stop: None,
             runtime_handle,
             wal,
-            max_ack_version: Arc::new(u64::MAX.into()),
+            wal_keep_from: Arc::new(u64::MAX.into()),
             flush_interval_sec,
             optimization_handles: Arc::new(TokioMutex::new(vec![])),
             max_optimization_threads,
@@ -162,7 +162,7 @@ impl UpdateHandler {
         self.flush_worker = Some(self.runtime_handle.spawn(Self::flush_worker(
             self.segments.clone(),
             self.wal.clone(),
-            self.max_ack_version.clone(),
+            self.wal_keep_from.clone(),
             self.flush_interval_sec,
             flush_rx,
         )));
@@ -586,7 +586,7 @@ impl UpdateHandler {
     async fn flush_worker(
         segments: LockedSegmentHolder,
         wal: LockedWal,
-        max_ack: Arc<AtomicU64>,
+        wal_keep_from: Arc<AtomicU64>,
         flush_interval_sec: u64,
         mut stop_receiver: oneshot::Receiver<()>,
     ) {
@@ -624,16 +624,19 @@ impl UpdateHandler {
                 }
             };
 
-            // Acknowledge confirmed version in WAL, but don't exceed specified maximum
-            // This is to prevent truncating WAL entries that may still be used by other things
+            // Acknowledge confirmed version in WAL, but don't acknowledge the specified
+            // `keep_from` index or higher.
+            // This is to prevent truncating WAL entries that other bits of code still depend on
             // such as the queue proxy shard.
-            // Default maximum ack version is `u64::MAX` to allow acknowledging all confirmed.
-            let max_ack = max_ack.load(std::sync::atomic::Ordering::Relaxed);
-            if confirmed_version > max_ack {
-                trace!("Acknowledging message {max_ack} in WAL, {confirmed_version} is already confirmed but max_ack_version is set");
+            // Default keep_from is `u64::MAX` to allow acknowledging all confirmed.
+            let keep_from = wal_keep_from.load(std::sync::atomic::Ordering::Relaxed);
+
+            // If we should keep the first message, do not acknowledge at all
+            if keep_from == 0 {
+                continue;
             }
-            let ack = confirmed_version.min(max_ack);
 
+            let ack = confirmed_version.min(keep_from.saturating_sub(1));
             if let Err(err) = wal.lock().ack(ack) {
                 segments.write().report_optimizer_error(err);
             }

commit 1df94c53b149a96d433f9185982c9e51e85f0109
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Thu Feb 8 16:00:13 2024 +0100

    Add `ClockMap` type to track versions of update operations in the local shard WAL (#3506)
    
    * Add `ClockMap` type to track versions of update operation in local shard WAL
    
    * `ClockMap::advance_clock_and_correct_tag`: only correct `clock_tag.clock_tick` if it's `0`
    
    * Add `ClockMap::load` and `ClockMap::store`
    
    * WIP: Load/restore `ClockMap` from disk/WAL when loading `LocalShard`
    
    * WIP: Save `ClockMap` to disk when truncating WAL
    
    * Do not apply operation with `clock_tick` that is older than clock value in `ClockMap`
    
    * Handle `ClockMap` properly during shard snapshot operations
    
    * Use "atomic disk write" when saving `ClockMap` to disk
    
    * Use `into` instead of explicit `Atomic*::new`
    
    * Return *local* shard result (instead of *remote* shard result) when updating `ForwardProxyShard`
    
    * Refactor `ClockMap::advance_clock*`
    
    * Refactor `ClockMap::advance_clock*` once again
    
    * Add documentation and `must_use` attributes
    
    * Use `atomicwrites` crate for `ClockMap::store`
    
    * WIP: Add `force` flag to the operations forwarded by forward/queue proxy shards...
    
    ...and add `force` flag handling to the `ClockMap`
    
    * Disable force on forward proxy, always accept old clock values
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 5062e28da..9f2e2222c 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -1,5 +1,6 @@
 use std::cmp::min;
 use std::collections::HashSet;
+use std::path::PathBuf;
 use std::sync::atomic::AtomicU64;
 use std::sync::Arc;
 
@@ -26,6 +27,7 @@ use crate::common::stoppable_task::{spawn_stoppable, StoppableTaskHandle};
 use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{CollectionError, CollectionResult};
 use crate::operations::CollectionUpdateOperations;
+use crate::shards::local_shard::clock_map::ClockMap;
 use crate::shards::local_shard::LockedWal;
 use crate::wal::WalError;
 
@@ -105,6 +107,9 @@ pub struct UpdateHandler {
     optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
     /// Maximum number of concurrent optimization jobs in this update handler.
     max_optimization_threads: Option<usize>,
+
+    clock_map: Arc<TokioMutex<ClockMap>>,
+    clock_map_path: PathBuf,
 }
 
 impl UpdateHandler {
@@ -119,6 +124,8 @@ impl UpdateHandler {
         wal: LockedWal,
         flush_interval_sec: u64,
         max_optimization_threads: Option<usize>,
+        clock_map: Arc<TokioMutex<ClockMap>>,
+        clock_map_path: PathBuf,
     ) -> UpdateHandler {
         UpdateHandler {
             shared_storage_config,
@@ -136,6 +143,8 @@ impl UpdateHandler {
             flush_interval_sec,
             optimization_handles: Arc::new(TokioMutex::new(vec![])),
             max_optimization_threads,
+            clock_map,
+            clock_map_path,
         }
     }
 
@@ -165,6 +174,8 @@ impl UpdateHandler {
             self.wal_keep_from.clone(),
             self.flush_interval_sec,
             flush_rx,
+            self.clock_map.clone(),
+            self.clock_map_path.clone(),
         )));
         self.flush_stop = Some(flush_tx);
     }
@@ -589,6 +600,8 @@ impl UpdateHandler {
         wal_keep_from: Arc<AtomicU64>,
         flush_interval_sec: u64,
         mut stop_receiver: oneshot::Receiver<()>,
+        clock_map: Arc<tokio::sync::Mutex<ClockMap>>,
+        clock_map_path: PathBuf,
     ) {
         loop {
             // Stop flush worker on signal or if sender was dropped
@@ -637,6 +650,11 @@ impl UpdateHandler {
             }
 
             let ack = confirmed_version.min(keep_from.saturating_sub(1));
+
+            if let Err(err) = clock_map.lock().await.store(&clock_map_path) {
+                segments.write().report_optimizer_error(err);
+            }
+
             if let Err(err) = wal.lock().ack(ack) {
                 segments.write().report_optimizer_error(err);
             }

commit f4389e68b068da02fb17b5246d1a82ce133f2aeb
Author: Tim Vise <tim+github@visee.me>
Date:   Tue Feb 13 11:09:43 2024 +0100

    Fix broken serialization of ClockMap (#3590)
    
    * Serialize clock map entries as vector tuple, cannot use struct as map key
    
    * Log warning when storing clock map or acknowledging fails
    
    * Add test for (de)serializing, storing and loading clock map
    
    * Tweak serialization/deserialization format
    
    * fixup! Tweak serialization/deserialization format
    
    * Remove transparent from clock, we flatten it elsewhere
    
    * Tweak `ClockMap` serde tests so that they don't require disk access
    
    * fixup! Tweak `ClockMap` serde tests so that they don't require disk access
    
    * fixup! Tweak `ClockMap` serde tests so that they don't require disk access
    
    Run `cargo check` without `--all --tests` 
    
    ---------
    
    Co-authored-by: Roman Titov <ffuugoo@users.noreply.github.com>

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 9f2e2222c..c1012ee76 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -652,10 +652,12 @@ impl UpdateHandler {
             let ack = confirmed_version.min(keep_from.saturating_sub(1));
 
             if let Err(err) = clock_map.lock().await.store(&clock_map_path) {
+                log::warn!("Failed to store clock map to disk: {err}");
                 segments.write().report_optimizer_error(err);
             }
 
             if let Err(err) = wal.lock().ack(ack) {
+                log::warn!("Failed to acknowledge WAL version: {err}");
                 segments.write().report_optimizer_error(err);
             }
         }

commit 42b80f454346b39c2fba09e38c8c6c8e578080c6
Author: Tim Vise <tim+github@visee.me>
Date:   Wed Feb 14 17:25:49 2024 +0100

    Add WAL type with integrated clock map (#3620)
    
    * Add WAL type with integrated clock map
    
    * Rename clock map field to last clocks
    
    * Move WAL delta logic into separate module

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index c1012ee76..65fe471a3 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -28,8 +28,8 @@ use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{CollectionError, CollectionResult};
 use crate::operations::CollectionUpdateOperations;
 use crate::shards::local_shard::clock_map::ClockMap;
-use crate::shards::local_shard::LockedWal;
 use crate::wal::WalError;
+use crate::wal_delta::LockedWal;
 
 /// Interval at which the optimizer worker cleans up old optimization handles
 ///

commit cac93508e6dc649f8db3376bb3ad0d31cd1c28bf
Author: Tim Vise <tim+github@visee.me>
Date:   Fri Feb 16 13:21:10 2024 +0100

    Add WAL cutoff clock map (#3631)
    
    * Add cutoff clock map to recoverable WAL
    
    * Rename last seen clock map to highest clock map
    
    * Add method to update cutoff clock map in recoverable WAL
    
    * Check cutoff point when resolving WAL delta
    
    * Rename last clocks to highest clocks

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 65fe471a3..f42e12011 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -108,8 +108,10 @@ pub struct UpdateHandler {
     /// Maximum number of concurrent optimization jobs in this update handler.
     max_optimization_threads: Option<usize>,
 
-    clock_map: Arc<TokioMutex<ClockMap>>,
-    clock_map_path: PathBuf,
+    highest_clock_map: Arc<TokioMutex<ClockMap>>,
+    cutoff_clock_map: Arc<TokioMutex<ClockMap>>,
+    highest_clock_map_path: PathBuf,
+    cutoff_clock_map_path: PathBuf,
 }
 
 impl UpdateHandler {
@@ -124,8 +126,10 @@ impl UpdateHandler {
         wal: LockedWal,
         flush_interval_sec: u64,
         max_optimization_threads: Option<usize>,
-        clock_map: Arc<TokioMutex<ClockMap>>,
-        clock_map_path: PathBuf,
+        highest_clock_map: Arc<TokioMutex<ClockMap>>,
+        cutoff_clock_map: Arc<TokioMutex<ClockMap>>,
+        highest_clock_map_path: PathBuf,
+        cutoff_clock_map_path: PathBuf,
     ) -> UpdateHandler {
         UpdateHandler {
             shared_storage_config,
@@ -143,8 +147,10 @@ impl UpdateHandler {
             flush_interval_sec,
             optimization_handles: Arc::new(TokioMutex::new(vec![])),
             max_optimization_threads,
-            clock_map,
-            clock_map_path,
+            highest_clock_map,
+            cutoff_clock_map,
+            highest_clock_map_path,
+            cutoff_clock_map_path,
         }
     }
 
@@ -174,8 +180,10 @@ impl UpdateHandler {
             self.wal_keep_from.clone(),
             self.flush_interval_sec,
             flush_rx,
-            self.clock_map.clone(),
-            self.clock_map_path.clone(),
+            self.highest_clock_map.clone(),
+            self.cutoff_clock_map.clone(),
+            self.highest_clock_map_path.clone(),
+            self.cutoff_clock_map_path.clone(),
         )));
         self.flush_stop = Some(flush_tx);
     }
@@ -594,14 +602,17 @@ impl UpdateHandler {
             .unwrap_or_else(|_| debug!("Optimizer already stopped"));
     }
 
+    #[allow(clippy::too_many_arguments)]
     async fn flush_worker(
         segments: LockedSegmentHolder,
         wal: LockedWal,
         wal_keep_from: Arc<AtomicU64>,
         flush_interval_sec: u64,
         mut stop_receiver: oneshot::Receiver<()>,
-        clock_map: Arc<tokio::sync::Mutex<ClockMap>>,
-        clock_map_path: PathBuf,
+        highest_clock_map: Arc<tokio::sync::Mutex<ClockMap>>,
+        cutoff_clock_map: Arc<tokio::sync::Mutex<ClockMap>>,
+        highest_clock_map_path: PathBuf,
+        cutoff_clock_map_path: PathBuf,
     ) {
         loop {
             // Stop flush worker on signal or if sender was dropped
@@ -651,8 +662,16 @@ impl UpdateHandler {
 
             let ack = confirmed_version.min(keep_from.saturating_sub(1));
 
-            if let Err(err) = clock_map.lock().await.store(&clock_map_path) {
-                log::warn!("Failed to store clock map to disk: {err}");
+            if let Err(err) = cutoff_clock_map.lock().await.store(&cutoff_clock_map_path) {
+                log::warn!("Failed to store cutoff clock map to disk: {err}");
+                segments.write().report_optimizer_error(err);
+            }
+            if let Err(err) = highest_clock_map
+                .lock()
+                .await
+                .store(&highest_clock_map_path)
+            {
+                log::warn!("Failed to store last seen clock map to disk: {err}");
                 segments.write().report_optimizer_error(err);
             }
 

commit 530d24a3452b0f01ed3948acf5a3e4891327a99a
Author: Tim Vise <tim+github@visee.me>
Date:   Thu Feb 22 11:20:45 2024 +0100

    Add struct combining local shard clock maps (#3662)
    
    * Create structure holding shared highest and cutoff clock maps and paths
    
    * Use local visibility
    
    * Rename ShardClocks to LocalShardClocks
    
    * Move shard clock map file IO into dedicated functions
    
    * Fix typo

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index f42e12011..4007761e1 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -1,6 +1,5 @@
 use std::cmp::min;
 use std::collections::HashSet;
-use std::path::PathBuf;
 use std::sync::atomic::AtomicU64;
 use std::sync::Arc;
 
@@ -27,7 +26,7 @@ use crate::common::stoppable_task::{spawn_stoppable, StoppableTaskHandle};
 use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{CollectionError, CollectionResult};
 use crate::operations::CollectionUpdateOperations;
-use crate::shards::local_shard::clock_map::ClockMap;
+use crate::shards::local_shard::LocalShardClocks;
 use crate::wal::WalError;
 use crate::wal_delta::LockedWal;
 
@@ -107,11 +106,8 @@ pub struct UpdateHandler {
     optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
     /// Maximum number of concurrent optimization jobs in this update handler.
     max_optimization_threads: Option<usize>,
-
-    highest_clock_map: Arc<TokioMutex<ClockMap>>,
-    cutoff_clock_map: Arc<TokioMutex<ClockMap>>,
-    highest_clock_map_path: PathBuf,
-    cutoff_clock_map_path: PathBuf,
+    /// Highest and cutoff clocks for the shard WAL.
+    clocks: LocalShardClocks,
 }
 
 impl UpdateHandler {
@@ -126,10 +122,7 @@ impl UpdateHandler {
         wal: LockedWal,
         flush_interval_sec: u64,
         max_optimization_threads: Option<usize>,
-        highest_clock_map: Arc<TokioMutex<ClockMap>>,
-        cutoff_clock_map: Arc<TokioMutex<ClockMap>>,
-        highest_clock_map_path: PathBuf,
-        cutoff_clock_map_path: PathBuf,
+        clocks: LocalShardClocks,
     ) -> UpdateHandler {
         UpdateHandler {
             shared_storage_config,
@@ -147,10 +140,7 @@ impl UpdateHandler {
             flush_interval_sec,
             optimization_handles: Arc::new(TokioMutex::new(vec![])),
             max_optimization_threads,
-            highest_clock_map,
-            cutoff_clock_map,
-            highest_clock_map_path,
-            cutoff_clock_map_path,
+            clocks,
         }
     }
 
@@ -180,10 +170,7 @@ impl UpdateHandler {
             self.wal_keep_from.clone(),
             self.flush_interval_sec,
             flush_rx,
-            self.highest_clock_map.clone(),
-            self.cutoff_clock_map.clone(),
-            self.highest_clock_map_path.clone(),
-            self.cutoff_clock_map_path.clone(),
+            self.clocks.clone(),
         )));
         self.flush_stop = Some(flush_tx);
     }
@@ -609,10 +596,7 @@ impl UpdateHandler {
         wal_keep_from: Arc<AtomicU64>,
         flush_interval_sec: u64,
         mut stop_receiver: oneshot::Receiver<()>,
-        highest_clock_map: Arc<tokio::sync::Mutex<ClockMap>>,
-        cutoff_clock_map: Arc<tokio::sync::Mutex<ClockMap>>,
-        highest_clock_map_path: PathBuf,
-        cutoff_clock_map_path: PathBuf,
+        clocks: LocalShardClocks,
     ) {
         loop {
             // Stop flush worker on signal or if sender was dropped
@@ -662,16 +646,8 @@ impl UpdateHandler {
 
             let ack = confirmed_version.min(keep_from.saturating_sub(1));
 
-            if let Err(err) = cutoff_clock_map.lock().await.store(&cutoff_clock_map_path) {
-                log::warn!("Failed to store cutoff clock map to disk: {err}");
-                segments.write().report_optimizer_error(err);
-            }
-            if let Err(err) = highest_clock_map
-                .lock()
-                .await
-                .store(&highest_clock_map_path)
-            {
-                log::warn!("Failed to store last seen clock map to disk: {err}");
+            if let Err(err) = clocks.store().await {
+                log::warn!("Failed to store clock maps to disk: {err}");
                 segments.write().report_optimizer_error(err);
             }
 

commit 1539087a6dfa968fce556f8b386ea60049fe8025
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Thu Feb 29 12:41:07 2024 +0100

    Diff transfer improvements (#3645)

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 4007761e1..778a8f303 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -1,5 +1,6 @@
 use std::cmp::min;
 use std::collections::HashSet;
+use std::path::PathBuf;
 use std::sync::atomic::AtomicU64;
 use std::sync::Arc;
 
@@ -108,6 +109,7 @@ pub struct UpdateHandler {
     max_optimization_threads: Option<usize>,
     /// Highest and cutoff clocks for the shard WAL.
     clocks: LocalShardClocks,
+    shard_path: PathBuf,
 }
 
 impl UpdateHandler {
@@ -123,6 +125,7 @@ impl UpdateHandler {
         flush_interval_sec: u64,
         max_optimization_threads: Option<usize>,
         clocks: LocalShardClocks,
+        shard_path: PathBuf,
     ) -> UpdateHandler {
         UpdateHandler {
             shared_storage_config,
@@ -141,6 +144,7 @@ impl UpdateHandler {
             optimization_handles: Arc::new(TokioMutex::new(vec![])),
             max_optimization_threads,
             clocks,
+            shard_path,
         }
     }
 
@@ -171,6 +175,7 @@ impl UpdateHandler {
             self.flush_interval_sec,
             flush_rx,
             self.clocks.clone(),
+            self.shard_path.clone(),
         )));
         self.flush_stop = Some(flush_tx);
     }
@@ -597,6 +602,7 @@ impl UpdateHandler {
         flush_interval_sec: u64,
         mut stop_receiver: oneshot::Receiver<()>,
         clocks: LocalShardClocks,
+        shard_path: PathBuf,
     ) {
         loop {
             // Stop flush worker on signal or if sender was dropped
@@ -646,7 +652,7 @@ impl UpdateHandler {
 
             let ack = confirmed_version.min(keep_from.saturating_sub(1));
 
-            if let Err(err) = clocks.store().await {
+            if let Err(err) = clocks.store(&shard_path).await {
                 log::warn!("Failed to store clock maps to disk: {err}");
                 segments.write().report_optimizer_error(err);
             }

commit a6817c54671d824943515ebfc79a40248d94d5b0
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Fri Mar 15 13:59:03 2024 +0100

    Fix optimizations config (#3832)
    
    * fix updating of the max_optimization_threads param
    
    * fix logic of the indexig optimizer
    
    * fmt
    
    * Update lib/collection/src/collection_manager/optimizers/indexing_optimizer.rs [no-ci]
    
    Co-authored-by: Tim Vise <tim+github@visee.me>
    
    * add test
    
    ---------
    
    Co-authored-by: Tim Vise <tim+github@visee.me>

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 778a8f303..d73b98511 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -86,6 +86,7 @@ pub struct UpdateHandler {
     /// Assigns CPU permits to tasks to limit overall resource utilization.
     optimizer_cpu_budget: CpuBudget,
     /// How frequent can we flush data
+    /// This parameter depends on the optimizer config and should be updated accordingly.
     pub flush_interval_sec: u64,
     segments: LockedSegmentHolder,
     /// Process, that listens updates signals and perform updates
@@ -106,7 +107,8 @@ pub struct UpdateHandler {
     pub(super) wal_keep_from: Arc<AtomicU64>,
     optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
     /// Maximum number of concurrent optimization jobs in this update handler.
-    max_optimization_threads: Option<usize>,
+    /// This parameter depends on the optimizer config and should be updated accordingly.
+    pub max_optimization_threads: Option<usize>,
     /// Highest and cutoff clocks for the shard WAL.
     clocks: LocalShardClocks,
     shard_path: PathBuf,

commit ec1e91aebb23d41b94fe11e9b5b01430c471494b
Author: Tim Vise <tim+github@visee.me>
Date:   Fri Mar 15 17:38:49 2024 +0100

    Only flush clock maps to disk if they have changed (#3835)
    
    * Only flush clock maps to disk if they have changed
    
    * When loading from disk, we don't have new changes
    
    * With the serde helper, we don't care about the changed field
    
    * Rename store_changed to store_if_changed
    
    * Fix changed state in unit test
    
    * Only set clock map changed state if advancing a clock tag is accepted

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index d73b98511..386367b78 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -654,7 +654,7 @@ impl UpdateHandler {
 
             let ack = confirmed_version.min(keep_from.saturating_sub(1));
 
-            if let Err(err) = clocks.store(&shard_path).await {
+            if let Err(err) = clocks.store_if_changed(&shard_path).await {
                 log::warn!("Failed to store clock maps to disk: {err}");
                 segments.write().report_optimizer_error(err);
             }

commit 7d5da6b23785150ac164abadc8b08b69d0c8e49e
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Wed Mar 20 13:03:39 2024 +0100

    Remove unecessary Clippy allows (#3879)

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 386367b78..1d7278e00 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -596,7 +596,6 @@ impl UpdateHandler {
             .unwrap_or_else(|_| debug!("Optimizer already stopped"));
     }
 
-    #[allow(clippy::too_many_arguments)]
     async fn flush_worker(
         segments: LockedSegmentHolder,
         wal: LockedWal,

commit 647d7e9dec2fc2a9e6a82014a38663f3836a25fb
Author: Tim Vise <tim+github@visee.me>
Date:   Thu Apr 4 10:04:02 2024 +0200

    Add grey collection status (#3962)
    
    * Add grey color for collection info status
    
    * Restructure locks in local shard info method
    
    * Set collection status to grey if we have pending optimizations
    
    * Update OpenAPI specification and gRPC documentation
    
    * Set optimizer status instead of color for compatibility reasons
    
    * Only set and check for grey status if there are no other statuses

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 1d7278e00..697e944c2 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -372,6 +372,18 @@ impl UpdateHandler {
         handles
     }
 
+    /// Checks conditions for all optimizers and returns whether any is satisfied
+    ///
+    /// In other words, if this returns true we have pending optimizations.
+    pub(crate) fn has_pending_optimizations(&self) -> bool {
+        let excluded_ids = HashSet::<_>::default();
+        self.optimizers.iter().any(|optimizer| {
+            let nonoptimal_segment_ids =
+                optimizer.check_condition(self.segments.clone(), &excluded_ids);
+            !nonoptimal_segment_ids.is_empty()
+        })
+    }
+
     pub(crate) async fn process_optimization(
         optimizers: Arc<Vec<Arc<Optimizer>>>,
         segments: LockedSegmentHolder,

commit 8d0c02b32c85244899d967a8fd687ccac076b42c
Author: Tim Vise <tim+github@visee.me>
Date:   Fri Apr 5 10:41:22 2024 +0200

    Only report pending optimizations status if we never triggered optimizers yet (#3971)
    
    * Do not report pending optimizations if optimizers are waiting on limits
    
    * Simplify, only track whether we ever triggered optimizers

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 697e944c2..824046ed7 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -1,7 +1,7 @@
 use std::cmp::min;
 use std::collections::HashSet;
 use std::path::PathBuf;
-use std::sync::atomic::AtomicU64;
+use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
 use std::sync::Arc;
 
 use common::cpu::CpuBudget;
@@ -112,6 +112,8 @@ pub struct UpdateHandler {
     /// Highest and cutoff clocks for the shard WAL.
     clocks: LocalShardClocks,
     shard_path: PathBuf,
+    /// Whether we have ever triggered optimizers since starting.
+    has_triggered_optimizers: Arc<AtomicBool>,
 }
 
 impl UpdateHandler {
@@ -147,6 +149,7 @@ impl UpdateHandler {
             max_optimization_threads,
             clocks,
             shard_path,
+            has_triggered_optimizers: Default::default(),
         }
     }
 
@@ -162,6 +165,7 @@ impl UpdateHandler {
             self.optimizers_log.clone(),
             self.optimizer_cpu_budget.clone(),
             self.max_optimization_threads,
+            self.has_triggered_optimizers.clone(),
         )));
         self.update_worker = Some(self.runtime_handle.spawn(Self::update_worker_fn(
             update_receiver,
@@ -376,6 +380,11 @@ impl UpdateHandler {
     ///
     /// In other words, if this returns true we have pending optimizations.
     pub(crate) fn has_pending_optimizations(&self) -> bool {
+        // If we did trigger optimizers at least once, we do not consider to be pending
+        if self.has_triggered_optimizers.load(Ordering::Relaxed) {
+            return false;
+        }
+
         let excluded_ids = HashSet::<_>::default();
         self.optimizers.iter().any(|optimizer| {
             let nonoptimal_segment_ids =
@@ -449,6 +458,7 @@ impl UpdateHandler {
         optimizers_log: Arc<Mutex<TrackerLog>>,
         optimizer_cpu_budget: CpuBudget,
         max_handles: Option<usize>,
+        has_triggered_optimizers: Arc<AtomicBool>,
     ) {
         let max_handles = max_handles.unwrap_or(usize::MAX);
         let max_indexing_threads = optimizers
@@ -473,6 +483,8 @@ impl UpdateHandler {
                 Err(Elapsed { .. }) => continue,
                 // Optimizer signal
                 Ok(Some(signal @ (OptimizerSignal::Nop | OptimizerSignal::Operation(_)))) => {
+                    has_triggered_optimizers.store(true, Ordering::Relaxed);
+
                     // If not forcing with Nop, wait on next signal if we have too many handles
                     if signal != OptimizerSignal::Nop
                         && optimization_handles.lock().await.len() >= max_handles

commit 96ca9039aafdb93158111ca7e5f0f696187ce1aa
Author: Kenshin Tanaka <70839560+kemkemG0@users.noreply.github.com>
Date:   Mon May 13 18:36:56 2024 +0900

    Handle Out-Of-Disk gracefully  (#4165)
    
    * tests: Add test on low disk
    
    * Remove redundant assertion
    
    * keep container after failure and print latest logs in console
    
    * Add fs2 crate as a dependency and ensure sufficient disk space in LocalShard operations
    
    * small fix
    
    * Update test
    
    * small fix
    
    * use available_space
    
    * Use `fs2` -> `fs4` and offload sync IO with `tokio::task::spawn_blocking`
    
    * create DiskUsageWathcer
    
    * chore: Remove unnecessary println statement in update_handler.rs
    
    * chore: Fix typo in DiskUsageWatcher struct name
    
    * chore: Refactor DiskUsageWatcher to improve disk usage tracking and update logic
    
    ---------
    
    Co-authored-by: tellet-q <elena.dubrovina@qdrant.com>
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 824046ed7..404100134 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -14,7 +14,7 @@ use segment::index::hnsw_index::num_rayon_threads;
 use segment::types::SeqNumberType;
 use tokio::runtime::Handle;
 use tokio::sync::mpsc::{self, Receiver, Sender};
-use tokio::sync::{oneshot, Mutex as TokioMutex};
+use tokio::sync::{oneshot, Mutex as TokioMutex, RwLock};
 use tokio::task::{self, JoinHandle};
 use tokio::time::error::Elapsed;
 use tokio::time::{timeout, Duration};
@@ -27,6 +27,7 @@ use crate::common::stoppable_task::{spawn_stoppable, StoppableTaskHandle};
 use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{CollectionError, CollectionResult};
 use crate::operations::CollectionUpdateOperations;
+use crate::shards::local_shard::disk_usage_watcher::DiskUsageWatcher;
 use crate::shards::local_shard::LocalShardClocks;
 use crate::wal::WalError;
 use crate::wal_delta::LockedWal;
@@ -114,6 +115,8 @@ pub struct UpdateHandler {
     shard_path: PathBuf,
     /// Whether we have ever triggered optimizers since starting.
     has_triggered_optimizers: Arc<AtomicBool>,
+    // Monitor for if the disk usage is sufficient for write operation
+    disk_usage_watcher: Arc<RwLock<DiskUsageWatcher>>,
 }
 
 impl UpdateHandler {
@@ -130,6 +133,7 @@ impl UpdateHandler {
         max_optimization_threads: Option<usize>,
         clocks: LocalShardClocks,
         shard_path: PathBuf,
+        disk_usage_watcher: Arc<RwLock<DiskUsageWatcher>>,
     ) -> UpdateHandler {
         UpdateHandler {
             shared_storage_config,
@@ -150,6 +154,7 @@ impl UpdateHandler {
             clocks,
             shard_path,
             has_triggered_optimizers: Default::default(),
+            disk_usage_watcher,
         }
     }
 
@@ -182,6 +187,7 @@ impl UpdateHandler {
             flush_rx,
             self.clocks.clone(),
             self.shard_path.clone(),
+            self.disk_usage_watcher.clone(),
         )));
         self.flush_stop = Some(flush_tx);
     }
@@ -620,6 +626,7 @@ impl UpdateHandler {
             .unwrap_or_else(|_| debug!("Optimizer already stopped"));
     }
 
+    #[allow(clippy::too_many_arguments)]
     async fn flush_worker(
         segments: LockedSegmentHolder,
         wal: LockedWal,
@@ -628,6 +635,7 @@ impl UpdateHandler {
         mut stop_receiver: oneshot::Receiver<()>,
         clocks: LocalShardClocks,
         shard_path: PathBuf,
+        disk_usage_watcher: Arc<RwLock<DiskUsageWatcher>>,
     ) {
         loop {
             // Stop flush worker on signal or if sender was dropped
@@ -641,6 +649,11 @@ impl UpdateHandler {
             }
 
             trace!("Attempting flushing");
+            if let Err(err) = disk_usage_watcher.write().await.update_disk_usage().await {
+                error!("Failed to ensure sufficient disk space: {err}");
+                segments.write().report_optimizer_error(err);
+                continue;
+            }
             let wal_flash_job = wal.lock().flush_async();
 
             if let Err(err) = wal_flash_job.join() {

commit 9a70b90cb6adcf5e1c3e9ed8b64eaf71a17785aa
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Mon May 13 21:21:47 2024 +0200

    refactor disk usage checks (#4222)
    
    * refactor disk usage checks
    
    * review fix
    
    * fix logic here and there

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 404100134..3c9629c3b 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -14,7 +14,7 @@ use segment::index::hnsw_index::num_rayon_threads;
 use segment::types::SeqNumberType;
 use tokio::runtime::Handle;
 use tokio::sync::mpsc::{self, Receiver, Sender};
-use tokio::sync::{oneshot, Mutex as TokioMutex, RwLock};
+use tokio::sync::{oneshot, Mutex as TokioMutex};
 use tokio::task::{self, JoinHandle};
 use tokio::time::error::Elapsed;
 use tokio::time::{timeout, Duration};
@@ -27,7 +27,6 @@ use crate::common::stoppable_task::{spawn_stoppable, StoppableTaskHandle};
 use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{CollectionError, CollectionResult};
 use crate::operations::CollectionUpdateOperations;
-use crate::shards::local_shard::disk_usage_watcher::DiskUsageWatcher;
 use crate::shards::local_shard::LocalShardClocks;
 use crate::wal::WalError;
 use crate::wal_delta::LockedWal;
@@ -115,8 +114,6 @@ pub struct UpdateHandler {
     shard_path: PathBuf,
     /// Whether we have ever triggered optimizers since starting.
     has_triggered_optimizers: Arc<AtomicBool>,
-    // Monitor for if the disk usage is sufficient for write operation
-    disk_usage_watcher: Arc<RwLock<DiskUsageWatcher>>,
 }
 
 impl UpdateHandler {
@@ -133,7 +130,6 @@ impl UpdateHandler {
         max_optimization_threads: Option<usize>,
         clocks: LocalShardClocks,
         shard_path: PathBuf,
-        disk_usage_watcher: Arc<RwLock<DiskUsageWatcher>>,
     ) -> UpdateHandler {
         UpdateHandler {
             shared_storage_config,
@@ -154,7 +150,6 @@ impl UpdateHandler {
             clocks,
             shard_path,
             has_triggered_optimizers: Default::default(),
-            disk_usage_watcher,
         }
     }
 
@@ -187,7 +182,6 @@ impl UpdateHandler {
             flush_rx,
             self.clocks.clone(),
             self.shard_path.clone(),
-            self.disk_usage_watcher.clone(),
         )));
         self.flush_stop = Some(flush_tx);
     }
@@ -635,7 +629,6 @@ impl UpdateHandler {
         mut stop_receiver: oneshot::Receiver<()>,
         clocks: LocalShardClocks,
         shard_path: PathBuf,
-        disk_usage_watcher: Arc<RwLock<DiskUsageWatcher>>,
     ) {
         loop {
             // Stop flush worker on signal or if sender was dropped
@@ -649,11 +642,6 @@ impl UpdateHandler {
             }
 
             trace!("Attempting flushing");
-            if let Err(err) = disk_usage_watcher.write().await.update_disk_usage().await {
-                error!("Failed to ensure sufficient disk space: {err}");
-                segments.write().report_optimizer_error(err);
-                continue;
-            }
             let wal_flash_job = wal.lock().flush_async();
 
             if let Err(err) = wal_flash_job.join() {

commit 106002c3034ac9eddc3e4cc3d2027a3f3aaa900f
Author: Tim Vise <tim+github@visee.me>
Date:   Mon Jun 10 18:45:53 2024 +0200

    Ensure we have any segment within capacity, otherwise add new one (#4416)
    
    * Extract logic for creating thresholds config
    
    * Put collection params and threshold config in update handler
    
    * Add function to add a new appendable segment if all are over capacity
    
    * Make new method static, call it before each optimization loop
    
    * Update error message formatting
    
    * Use exact point count in replication consensus test
    
    * Add a test to assert segment creation when all are over capacity
    
    * Suffix optimizer thresholds with _kb to clarify unit
    
    * Move segment capacity check logic, run if optimizers are disabled
    
    * fix: add -> mul
    
    ---------
    
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 3c9629c3b..9c2dec608 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -1,6 +1,6 @@
 use std::cmp::min;
 use std::collections::HashSet;
-use std::path::PathBuf;
+use std::path::{Path, PathBuf};
 use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
 use std::sync::Arc;
 
@@ -21,9 +21,12 @@ use tokio::time::{timeout, Duration};
 
 use crate::collection_manager::collection_updater::CollectionUpdater;
 use crate::collection_manager::holders::segment_holder::LockedSegmentHolder;
-use crate::collection_manager::optimizers::segment_optimizer::SegmentOptimizer;
+use crate::collection_manager::optimizers::segment_optimizer::{
+    OptimizerThresholds, SegmentOptimizer,
+};
 use crate::collection_manager::optimizers::{Tracker, TrackerLog, TrackerStatus};
 use crate::common::stoppable_task::{spawn_stoppable, StoppableTaskHandle};
+use crate::config::CollectionParams;
 use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{CollectionError, CollectionResult};
 use crate::operations::CollectionUpdateOperations;
@@ -257,6 +260,7 @@ impl UpdateHandler {
     {
         let mut scheduled_segment_ids = HashSet::<_>::default();
         let mut handles = vec![];
+
         'outer: for optimizer in optimizers.iter() {
             loop {
                 // Return early if we reached the optimization job limit
@@ -329,7 +333,7 @@ impl UpdateHandler {
                                 // Handle and report errors
                                 Err(error) => match error {
                                     CollectionError::Cancelled { description } => {
-                                        debug!("Optimization cancelled - {}", description);
+                                        debug!("Optimization cancelled - {description}");
                                         tracker_handle
                                             .update(TrackerStatus::Cancelled(description));
                                         false
@@ -341,7 +345,7 @@ impl UpdateHandler {
                                         // It is only possible to fix after full restart,
                                         // so the best available action here is to stop whole
                                         // optimization thread and log the error
-                                        log::error!("Optimization error: {}", error);
+                                        log::error!("Optimization error: {error}");
 
                                         tracker_handle
                                             .update(TrackerStatus::Error(error.to_string()));
@@ -376,6 +380,48 @@ impl UpdateHandler {
         handles
     }
 
+    /// Ensure there is at least one appendable segment with enough capacity
+    ///
+    /// If there is no appendable segment, or all are at or over capacity, a new empty one is
+    /// created.
+    ///
+    /// Capacity is determined based on `optimizers.max_segment_size_kb`.
+    pub(super) fn ensure_appendable_segment_with_capacity(
+        segments: &LockedSegmentHolder,
+        segments_path: &Path,
+        collection_params: &CollectionParams,
+        thresholds_config: &OptimizerThresholds,
+    ) -> OperationResult<()> {
+        let no_segment_with_capacity = {
+            let segments_read = segments.read();
+            segments_read
+                .appendable_segments_ids()
+                .into_iter()
+                .filter_map(|segment_id| segments_read.get(segment_id))
+                .all(|segment| {
+                    let max_vector_size_bytes = segment
+                        .get()
+                        .read()
+                        .max_available_vectors_size_in_bytes()
+                        .unwrap_or_default();
+                    let max_segment_size_bytes = thresholds_config
+                        .max_segment_size_kb
+                        .saturating_mul(segment::common::BYTES_IN_KB);
+
+                    max_vector_size_bytes >= max_segment_size_bytes
+                })
+        };
+
+        if no_segment_with_capacity {
+            log::debug!("Creating new appendable segment, all existing segments are over capacity");
+            segments
+                .write()
+                .create_appendable_segment(segments_path, collection_params)?;
+        }
+
+        Ok(())
+    }
+
     /// Checks conditions for all optimizers and returns whether any is satisfied
     ///
     /// In other words, if this returns true we have pending optimizations.
@@ -485,6 +531,21 @@ impl UpdateHandler {
                 Ok(Some(signal @ (OptimizerSignal::Nop | OptimizerSignal::Operation(_)))) => {
                     has_triggered_optimizers.store(true, Ordering::Relaxed);
 
+                    // Ensure we have at least one appendable segment with enough capacity
+                    // Source required parameters from first optimizer
+                    if let Some(optimizer) = optimizers.first() {
+                        let result = Self::ensure_appendable_segment_with_capacity(
+                            &segments,
+                            optimizer.segments_path(),
+                            &optimizer.collection_params(),
+                            optimizer.threshold_config(),
+                        );
+                        if let Err(err) = result {
+                            log::error!("Failed to ensure there are appendable segments with capacity: {err}");
+                            panic!("Failed to ensure there are appendable segments with capacity: {err}");
+                        }
+                    }
+
                     // If not forcing with Nop, wait on next signal if we have too many handles
                     if signal != OptimizerSignal::Nop
                         && optimization_handles.lock().await.len() >= max_handles

commit c0621cff54983cb422da66f635465d03681db79c
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Wed Jun 12 17:11:09 2024 +0200

    Remove unecessary Clippy allows (#4456)

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 9c2dec608..52bcd432c 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -681,7 +681,6 @@ impl UpdateHandler {
             .unwrap_or_else(|_| debug!("Optimizer already stopped"));
     }
 
-    #[allow(clippy::too_many_arguments)]
     async fn flush_worker(
         segments: LockedSegmentHolder,
         wal: LockedWal,

commit 63b2801e4fe25fea190e5a4069d6a1d3702a4661
Author: Tim Vise <tim+github@visee.me>
Date:   Fri Jun 21 20:01:05 2024 +0200

    Fix new appendable segments not having payload indices (#4523)
    
    * Propagate payload index schema down to shard replica set + update handler
    
    * Configure payload indices when creating new appendable segment
    
    * When loading segments, make sure applied payload indices match config
    
    * Add test to assert creating new segments with payload index
    
    * Fix unit test because the collection payload schema wasn't updated
    
    * Add test for updating payload index configuration on segment load
    
    * Update test documentation
    
    * Also create payload indices in temporary snapshot segment
    
    * do not delete extra payload index from segments
    
    * do not delete extra payload index from segments
    
    * fix test
    
    ---------
    
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 52bcd432c..d15c68607 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -19,6 +19,7 @@ use tokio::task::{self, JoinHandle};
 use tokio::time::error::Elapsed;
 use tokio::time::{timeout, Duration};
 
+use crate::collection::payload_index_schema::PayloadIndexSchema;
 use crate::collection_manager::collection_updater::CollectionUpdater;
 use crate::collection_manager::holders::segment_holder::LockedSegmentHolder;
 use crate::collection_manager::optimizers::segment_optimizer::{
@@ -30,6 +31,7 @@ use crate::config::CollectionParams;
 use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{CollectionError, CollectionResult};
 use crate::operations::CollectionUpdateOperations;
+use crate::save_on_disk::SaveOnDisk;
 use crate::shards::local_shard::LocalShardClocks;
 use crate::wal::WalError;
 use crate::wal_delta::LockedWal;
@@ -81,6 +83,7 @@ pub enum OptimizerSignal {
 /// Structure, which holds object, required for processing updates of the collection
 pub struct UpdateHandler {
     shared_storage_config: Arc<SharedStorageConfig>,
+    payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,
     /// List of used optimizers
     pub optimizers: Arc<Vec<Arc<Optimizer>>>,
     /// Log of optimizer statuses
@@ -123,6 +126,7 @@ impl UpdateHandler {
     #[allow(clippy::too_many_arguments)]
     pub fn new(
         shared_storage_config: Arc<SharedStorageConfig>,
+        payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,
         optimizers: Arc<Vec<Arc<Optimizer>>>,
         optimizers_log: Arc<Mutex<TrackerLog>>,
         optimizer_cpu_budget: CpuBudget,
@@ -136,6 +140,7 @@ impl UpdateHandler {
     ) -> UpdateHandler {
         UpdateHandler {
             shared_storage_config,
+            payload_index_schema,
             optimizers,
             segments,
             update_worker: None,
@@ -169,6 +174,7 @@ impl UpdateHandler {
             self.optimizer_cpu_budget.clone(),
             self.max_optimization_threads,
             self.has_triggered_optimizers.clone(),
+            self.payload_index_schema.clone(),
         )));
         self.update_worker = Some(self.runtime_handle.spawn(Self::update_worker_fn(
             update_receiver,
@@ -391,6 +397,7 @@ impl UpdateHandler {
         segments_path: &Path,
         collection_params: &CollectionParams,
         thresholds_config: &OptimizerThresholds,
+        payload_index_schema: &PayloadIndexSchema,
     ) -> OperationResult<()> {
         let no_segment_with_capacity = {
             let segments_read = segments.read();
@@ -414,9 +421,11 @@ impl UpdateHandler {
 
         if no_segment_with_capacity {
             log::debug!("Creating new appendable segment, all existing segments are over capacity");
-            segments
-                .write()
-                .create_appendable_segment(segments_path, collection_params)?;
+            segments.write().create_appendable_segment(
+                segments_path,
+                collection_params,
+                payload_index_schema,
+            )?;
         }
 
         Ok(())
@@ -505,6 +514,7 @@ impl UpdateHandler {
         optimizer_cpu_budget: CpuBudget,
         max_handles: Option<usize>,
         has_triggered_optimizers: Arc<AtomicBool>,
+        payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,
     ) {
         let max_handles = max_handles.unwrap_or(usize::MAX);
         let max_indexing_threads = optimizers
@@ -539,6 +549,7 @@ impl UpdateHandler {
                             optimizer.segments_path(),
                             &optimizer.collection_params(),
                             optimizer.threshold_config(),
+                            &payload_index_schema.read(),
                         );
                         if let Err(err) = result {
                             log::error!("Failed to ensure there are appendable segments with capacity: {err}");

commit 40830a1729f176a8691022e47119ad5dce2d1a54
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Mon Jul 8 15:58:19 2024 +0200

    Merge pull request #4620
    
    * Add `force` flag to `SegmentEntry::flush` and `ShardHolder::flush_all

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index d15c68607..659d8d167 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -767,7 +767,7 @@ impl UpdateHandler {
     /// Returns an error on flush failure
     fn flush_segments(segments: LockedSegmentHolder) -> OperationResult<SeqNumberType> {
         let read_segments = segments.read();
-        let flushed_version = read_segments.flush_all(false)?;
+        let flushed_version = read_segments.flush_all(false, false)?;
         Ok(match read_segments.failed_operation.iter().cloned().min() {
             None => flushed_version,
             Some(failed_operation) => min(failed_operation, flushed_version),

commit 07c278ad51084c98adf9a7093619ffc5a73f87c9
Author: xzfc <5121426+xzfc@users.noreply.github.com>
Date:   Mon Jul 22 08:19:19 2024 +0000

    Enable some of the pedantic clippy lints (#4715)
    
    * Use workspace lints
    
    * Enable lint: manual_let_else
    
    * Enable lint: enum_glob_use
    
    * Enable lint: filter_map_next
    
    * Enable lint: ref_as_ptr
    
    * Enable lint: ref_option_ref
    
    * Enable lint: manual_is_variant_and
    
    * Enable lint: flat_map_option
    
    * Enable lint: inefficient_to_string
    
    * Enable lint: implicit_clone
    
    * Enable lint: inconsistent_struct_constructor
    
    * Enable lint: unnecessary_wraps
    
    * Enable lint: needless_continue
    
    * Enable lint: unused_self
    
    * Enable lint: from_iter_instead_of_collect
    
    * Enable lint: uninlined_format_args
    
    * Enable lint: doc_link_with_quotes
    
    * Enable lint: needless_raw_string_hashes
    
    * Enable lint: used_underscore_binding
    
    * Enable lint: ptr_as_ptr
    
    * Enable lint: explicit_into_iter_loop
    
    * Enable lint: cast_lossless

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 659d8d167..ec9068cce 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -284,21 +284,18 @@ impl UpdateHandler {
                 // Determine how many CPUs we prefer for optimization task, acquire permit for it
                 let max_indexing_threads = optimizer.hnsw_config().max_indexing_threads;
                 let desired_cpus = num_rayon_threads(max_indexing_threads);
-                let permit = match optimizer_cpu_budget.try_acquire(desired_cpus) {
-                    Some(permit) => permit,
+                let Some(permit) = optimizer_cpu_budget.try_acquire(desired_cpus) else {
                     // If there is no CPU budget, break outer loop and return early
                     // If we have no handles (no optimizations) trigger callback so that we wake up
                     // our optimization worker to try again later, otherwise it could get stuck
-                    None => {
-                        log::trace!(
-                            "No available CPU permit for {} optimizer, postponing",
-                            optimizer.name(),
-                        );
-                        if handles.is_empty() {
-                            callback(false);
-                        }
-                        break 'outer;
+                    log::trace!(
+                        "No available CPU permit for {} optimizer, postponing",
+                        optimizer.name(),
+                    );
+                    if handles.is_empty() {
+                        callback(false);
                     }
+                    break 'outer;
                 };
                 log::trace!(
                     "Acquired {} CPU permit for {} optimizer",
@@ -634,8 +631,7 @@ impl UpdateHandler {
                     let flush_res = if wait {
                         wal.lock().flush().map_err(|err| {
                             CollectionError::service_error(format!(
-                                "Can't flush WAL before operation {} - {}",
-                                op_num, err
+                                "Can't flush WAL before operation {op_num} - {err}"
                             ))
                         })
                     } else {

commit 35682861325ad345058ef3e33e74cba7afba33d3
Author: Kumar Shivendu <kshivendu1@gmail.com>
Date:   Thu Sep 5 13:08:39 2024 +0530

    Introduce grey collection status and expose shard status in telemetry (#4940)
    
    * Expose shard status in telemetry API
    
    * fmt
    
    * Drop segment lock before using async fetching shard status
    
    * Use Self in From implementation for ShardStatus to CollectionStatus mapping
    
    * Improve comments
    
    * Remove redundant clone
    
    * Update openapi specs
    
    * Isolate function for shard status
    
    * Fix compiler error
    
    * Avoid adding dedicated function for shard status
    
    * review fixes
    
    * define missing var
    
    * lint err
    
    * comment
    
    * comment
    
    * refactor
    
    * improve comments
    
    * Improve comments
    
    * fix lint and update openapi specs
    
    * improve comment

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index ec9068cce..a66e6191a 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -431,7 +431,7 @@ impl UpdateHandler {
     /// Checks conditions for all optimizers and returns whether any is satisfied
     ///
     /// In other words, if this returns true we have pending optimizations.
-    pub(crate) fn has_pending_optimizations(&self) -> bool {
+    pub(crate) fn has_non_optimal_segments(&self) -> bool {
         // If we did trigger optimizers at least once, we do not consider to be pending
         if self.has_triggered_optimizers.load(Ordering::Relaxed) {
             return false;

commit ca26a76e25154c60ed249efc59e8a330d2970549
Author: Kumar Shivendu <kshivendu1@gmail.com>
Date:   Fri Sep 6 13:26:56 2024 +0530

    Improve logging (#5027)
    
    * Improve logging
    
    * Update lib/collection/src/collection_manager/optimizers/merge_optimizer.rs
    
    Co-authored-by: Tim Vise <tim+github@visee.me>
    
    * move log to optimizer launch
    
    ---------
    
    Co-authored-by: Tim Vise <tim+github@visee.me>
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index a66e6191a..062f4fb58 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -281,6 +281,8 @@ impl UpdateHandler {
                     break;
                 }
 
+                debug!("Optimizing segments: {:?}", &nonoptimal_segment_ids);
+
                 // Determine how many CPUs we prefer for optimization task, acquire permit for it
                 let max_indexing_threads = optimizer.hnsw_config().max_indexing_threads;
                 let desired_cpus = num_rayon_threads(max_indexing_threads);

commit 8c365f27efca127c9dc95cc731b3ca6fec936611
Author: Kumar Shivendu <kshivendu1@gmail.com>
Date:   Mon Sep 9 14:37:51 2024 +0530

    Fix shard/collection status blinking (#5043)
    
    * Fix shard/collection status blinking
    
    * Improve var names
    
    * improve var names
    
    * Improve function docs

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 062f4fb58..7c5099582 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -430,21 +430,23 @@ impl UpdateHandler {
         Ok(())
     }
 
-    /// Checks conditions for all optimizers and returns whether any is satisfied
+    /// Checks the optimizer conditions.
     ///
-    /// In other words, if this returns true we have pending optimizations.
-    pub(crate) fn has_non_optimal_segments(&self) -> bool {
-        // If we did trigger optimizers at least once, we do not consider to be pending
-        if self.has_triggered_optimizers.load(Ordering::Relaxed) {
-            return false;
-        }
+    /// This function returns a tuple of two booleans:
+    /// - The first indicates if any optimizers have been triggered since startup.
+    /// - The second indicates if there are any pending/suboptimal optimizers.
+    pub(crate) fn check_optimizer_conditions(&self) -> (bool, bool) {
+        // Check if Qdrant triggered any optimizations since starting at all
+        let has_triggered_any_optimizers = self.has_triggered_optimizers.load(Ordering::Relaxed);
 
         let excluded_ids = HashSet::<_>::default();
-        self.optimizers.iter().any(|optimizer| {
+        let has_suboptimal_optimizers = self.optimizers.iter().any(|optimizer| {
             let nonoptimal_segment_ids =
                 optimizer.check_condition(self.segments.clone(), &excluded_ids);
             !nonoptimal_segment_ids.is_empty()
-        })
+        });
+
+        (has_triggered_any_optimizers, has_suboptimal_optimizers)
     }
 
     pub(crate) async fn process_optimization(

commit 70c46bbb6f49739acac3ee7ce55074029a40b5a1
Author: Kumar Shivendu <kshivendu1@gmail.com>
Date:   Tue Sep 10 16:52:38 2024 +0530

    Track number of points optimized and expose in telemetry (#5000)
    
    * Track number of points optimized and expose in telemetry
    
    * refactor
    
    * openapi specs
    
    * remove dbg
    
    * Return num points optimized from optimize() func
    
    * fmt
    
    * fix
    
    * fix type in tests
    
    * Store total points indexed on shard level instead of optimization level
    
    * fmt
    
    * fix test
    
    * trigger ci
    
    * fix openapi schema
    
    * review fixes
    
    * fmt
    
    * improvements and fix test
    
    * review fixes
    
    * use const for indexing optimizer name
    
    * fmt
    
    * return segment id from optimize() func
    
    * review fixes
    
    * fix
    
    * fix
    
    * fik
    
    * minor var name improvement
    
    * Use Option<usize> to return segment id
    
    * Use segment ID type rather than ambiguous usize
    
    * fix test
    
    * avoid intermediate check
    
    * review fixes
    
    * Rename total_indexed_points to total_optimized_points
    
    * Update openapi schema
    
    * optimize() should return number of points in new segment instead of segment id
    
    * add else condition
    
    * take read lock
    
    * fmt
    
    * remove flaky assert
    
    * Count points on new segment without locking
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 7c5099582..d5bc1c7ea 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -1,7 +1,7 @@
 use std::cmp::min;
 use std::collections::HashSet;
 use std::path::{Path, PathBuf};
-use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
+use std::sync::atomic::{AtomicBool, AtomicU64, AtomicUsize, Ordering};
 use std::sync::Arc;
 
 use common::cpu::CpuBudget;
@@ -88,6 +88,8 @@ pub struct UpdateHandler {
     pub optimizers: Arc<Vec<Arc<Optimizer>>>,
     /// Log of optimizer statuses
     optimizers_log: Arc<Mutex<TrackerLog>>,
+    /// Total number of optimized points since last start
+    total_optimized_points: Arc<AtomicUsize>,
     /// Global CPU budget in number of cores for all optimization tasks.
     /// Assigns CPU permits to tasks to limit overall resource utilization.
     optimizer_cpu_budget: CpuBudget,
@@ -129,6 +131,7 @@ impl UpdateHandler {
         payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,
         optimizers: Arc<Vec<Arc<Optimizer>>>,
         optimizers_log: Arc<Mutex<TrackerLog>>,
+        total_optimized_points: Arc<AtomicUsize>,
         optimizer_cpu_budget: CpuBudget,
         runtime_handle: Handle,
         segments: LockedSegmentHolder,
@@ -146,6 +149,7 @@ impl UpdateHandler {
             update_worker: None,
             optimizer_worker: None,
             optimizers_log,
+            total_optimized_points,
             optimizer_cpu_budget,
             flush_worker: None,
             flush_stop: None,
@@ -171,6 +175,7 @@ impl UpdateHandler {
             self.wal.clone(),
             self.optimization_handles.clone(),
             self.optimizers_log.clone(),
+            self.total_optimized_points.clone(),
             self.optimizer_cpu_budget.clone(),
             self.max_optimization_threads,
             self.has_triggered_optimizers.clone(),
@@ -256,6 +261,7 @@ impl UpdateHandler {
     pub(crate) fn launch_optimization<F>(
         optimizers: Arc<Vec<Arc<Optimizer>>>,
         optimizers_log: Arc<Mutex<TrackerLog>>,
+        total_optimized_points: Arc<AtomicUsize>,
         optimizer_cpu_budget: &CpuBudget,
         segments: LockedSegmentHolder,
         callback: F,
@@ -307,6 +313,7 @@ impl UpdateHandler {
 
                 let optimizer = optimizer.clone();
                 let optimizers_log = optimizers_log.clone();
+                let total_optimized_points = total_optimized_points.clone();
                 let segments = segments.clone();
                 let nsi = nonoptimal_segment_ids.clone();
                 scheduled_segment_ids.extend(&nsi);
@@ -330,10 +337,13 @@ impl UpdateHandler {
                                 stopped,
                             ) {
                                 // Perform some actions when optimization if finished
-                                Ok(result) => {
+                                Ok(optimized_points) => {
+                                    let is_optimized = optimized_points > 0;
+                                    total_optimized_points
+                                        .fetch_add(optimized_points, Ordering::Relaxed);
                                     tracker_handle.update(TrackerStatus::Done);
-                                    callback(result);
-                                    result
+                                    callback(is_optimized);
+                                    is_optimized
                                 }
                                 // Handle and report errors
                                 Err(error) => match error {
@@ -449,11 +459,13 @@ impl UpdateHandler {
         (has_triggered_any_optimizers, has_suboptimal_optimizers)
     }
 
+    #[allow(clippy::too_many_arguments)]
     pub(crate) async fn process_optimization(
         optimizers: Arc<Vec<Arc<Optimizer>>>,
         segments: LockedSegmentHolder,
         optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
         optimizers_log: Arc<Mutex<TrackerLog>>,
+        total_optimized_points: Arc<AtomicUsize>,
         optimizer_cpu_budget: &CpuBudget,
         sender: Sender<OptimizerSignal>,
         limit: usize,
@@ -461,6 +473,7 @@ impl UpdateHandler {
         let mut new_handles = Self::launch_optimization(
             optimizers.clone(),
             optimizers_log,
+            total_optimized_points,
             optimizer_cpu_budget,
             segments.clone(),
             move |_optimization_result| {
@@ -512,6 +525,7 @@ impl UpdateHandler {
         wal: LockedWal,
         optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
         optimizers_log: Arc<Mutex<TrackerLog>>,
+        total_optimized_points: Arc<AtomicUsize>,
         optimizer_cpu_budget: CpuBudget,
         max_handles: Option<usize>,
         has_triggered_optimizers: Arc<AtomicBool>,
@@ -608,6 +622,7 @@ impl UpdateHandler {
                         segments.clone(),
                         optimization_handles.clone(),
                         optimizers_log.clone(),
+                        total_optimized_points.clone(),
                         &optimizer_cpu_budget,
                         sender.clone(),
                         limit,

commit 977c3783489dc146489b7045e0a0e4cd3e66e2ab
Author: Tim Vise <tim+github@visee.me>
Date:   Mon Sep 23 10:42:31 2024 +0200

    Fix race condition getting optimizers stuck, leaving collection in yellow state (#5111)
    
    * Return whether any task was cleaned
    
    * Retrigger optimizers after cleanup timeout if we cleared a handle
    
    * Flatten match used as decision logic on whether to run optimizers
    
    * Add comment to elaborate we have a branch to prevent race condition
    
    * Rename force to ignore_max_handles
    
    * Use swap remove as it is a bit more efficient

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index d5bc1c7ea..ad704a528 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -495,9 +495,11 @@ impl UpdateHandler {
     /// It also propagates any panics (and unknown errors) so we properly handle them if desired.
     ///
     /// It is essential to call this every once in a while for handling panics in time.
+    ///
+    /// Returns true if any optimization handle was finished, joined and removed.
     async fn cleanup_optimization_handles(
         optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
-    ) {
+    ) -> bool {
         // Remove finished handles
         let finished_handles: Vec<_> = {
             let mut handles = optimization_handles.lock().await;
@@ -506,14 +508,18 @@ impl UpdateHandler {
                 .collect::<Vec<_>>()
                 .into_iter()
                 .rev()
-                .map(|i| handles.remove(i))
+                .map(|i| handles.swap_remove(i))
                 .collect()
         };
 
+        let finished_any = !finished_handles.is_empty();
+
         // Finalize all finished handles to propagate panics
         for handle in finished_handles {
             handle.join_and_handle_panic().await;
         }
+
+        finished_any
     }
 
     #[allow(clippy::too_many_arguments)]
@@ -541,95 +547,110 @@ impl UpdateHandler {
         let mut cpu_available_trigger: Option<JoinHandle<()>> = None;
 
         loop {
-            let receiver = timeout(OPTIMIZER_CLEANUP_INTERVAL, receiver.recv());
-            let result = receiver.await;
-
-            // Always clean up on any signal
-            Self::cleanup_optimization_handles(optimization_handles.clone()).await;
-
-            match result {
-                // Channel closed or stop signal
-                Ok(None | Some(OptimizerSignal::Stop)) => break,
-                // Clean up interval
+            let result = timeout(OPTIMIZER_CLEANUP_INTERVAL, receiver.recv()).await;
+
+            let cleaned_any =
+                Self::cleanup_optimization_handles(optimization_handles.clone()).await;
+
+            // Either continue below here with the worker, or reloop/break
+            // Decision logic doing one of three things:
+            // 1. run optimizers
+            // 2. reloop and wait for next signal
+            // 3. break here and stop the optimization worker
+            let ignore_max_handles = match result {
+                // Regular optimizer signal: run optimizers: do 1
+                Ok(Some(OptimizerSignal::Operation(_))) => false,
+                // Optimizer signal ignoring max handles: do 1
+                Ok(Some(OptimizerSignal::Nop)) => true,
+                // Hit optimizer cleanup interval, did clean up a task: do 1
+                Err(Elapsed { .. }) if cleaned_any => {
+                    // This branch prevents a race condition where optimizers would get stuck
+                    // If the optimizer cleanup interval was triggered and we did clean any task we
+                    // must run optimizers now. If we don't there may not be any other ongoing
+                    // tasks that'll trigger this for us. If we don't run optimizers here we might
+                    // get stuck into yellow state until a new update operation is received.
+                    // See: <https://github.com/qdrant/qdrant/pull/5111>
+                    log::warn!("Cleaned a optimization handle after timeout, explicitly triggering optimizers");
+                    true
+                }
+                // Hit optimizer cleanup interval, did not clean up a task: do 2
                 Err(Elapsed { .. }) => continue,
-                // Optimizer signal
-                Ok(Some(signal @ (OptimizerSignal::Nop | OptimizerSignal::Operation(_)))) => {
-                    has_triggered_optimizers.store(true, Ordering::Relaxed);
-
-                    // Ensure we have at least one appendable segment with enough capacity
-                    // Source required parameters from first optimizer
-                    if let Some(optimizer) = optimizers.first() {
-                        let result = Self::ensure_appendable_segment_with_capacity(
-                            &segments,
-                            optimizer.segments_path(),
-                            &optimizer.collection_params(),
-                            optimizer.threshold_config(),
-                            &payload_index_schema.read(),
-                        );
-                        if let Err(err) = result {
-                            log::error!("Failed to ensure there are appendable segments with capacity: {err}");
-                            panic!("Failed to ensure there are appendable segments with capacity: {err}");
-                        }
-                    }
-
-                    // If not forcing with Nop, wait on next signal if we have too many handles
-                    if signal != OptimizerSignal::Nop
-                        && optimization_handles.lock().await.len() >= max_handles
-                    {
-                        continue;
-                    }
+                // Channel closed or received stop signal: do 3
+                Ok(None | Some(OptimizerSignal::Stop)) => break,
+            };
 
-                    if Self::try_recover(segments.clone(), wal.clone())
-                        .await
-                        .is_err()
-                    {
-                        continue;
-                    }
+            has_triggered_optimizers.store(true, Ordering::Relaxed);
+
+            // Ensure we have at least one appendable segment with enough capacity
+            // Source required parameters from first optimizer
+            if let Some(optimizer) = optimizers.first() {
+                let result = Self::ensure_appendable_segment_with_capacity(
+                    &segments,
+                    optimizer.segments_path(),
+                    &optimizer.collection_params(),
+                    optimizer.threshold_config(),
+                    &payload_index_schema.read(),
+                );
+                if let Err(err) = result {
+                    log::error!(
+                        "Failed to ensure there are appendable segments with capacity: {err}"
+                    );
+                    panic!("Failed to ensure there are appendable segments with capacity: {err}");
+                }
+            }
 
-                    // Continue if we have enough CPU budget available to start an optimization
-                    // Otherwise skip now and start a task to trigger the optimizer again once CPU
-                    // budget becomes available
-                    let desired_cpus = num_rayon_threads(max_indexing_threads);
-                    if !optimizer_cpu_budget.has_budget(desired_cpus) {
-                        let trigger_active = cpu_available_trigger
-                            .as_ref()
-                            .map_or(false, |t| !t.is_finished());
-                        if !trigger_active {
-                            cpu_available_trigger.replace(trigger_optimizers_on_cpu_budget(
-                                optimizer_cpu_budget.clone(),
-                                desired_cpus,
-                                sender.clone(),
-                            ));
-                        }
-                        continue;
-                    }
+            // If not forcing, wait on next signal if we have too many handles
+            if !ignore_max_handles && optimization_handles.lock().await.len() >= max_handles {
+                continue;
+            }
 
-                    // Determine optimization handle limit based on max handles we allow
-                    // Not related to the CPU budget, but a different limit for the maximum number
-                    // of concurrent concrete optimizations per shard as configured by the user in
-                    // the Qdrant configuration.
-                    // Skip if we reached limit, an ongoing optimization that finishes will trigger this loop again
-                    let limit = max_handles.saturating_sub(optimization_handles.lock().await.len());
-                    if limit == 0 {
-                        log::trace!(
-                            "Skipping optimization check, we reached optimization thread limit"
-                        );
-                        continue;
-                    }
+            if Self::try_recover(segments.clone(), wal.clone())
+                .await
+                .is_err()
+            {
+                continue;
+            }
 
-                    Self::process_optimization(
-                        optimizers.clone(),
-                        segments.clone(),
-                        optimization_handles.clone(),
-                        optimizers_log.clone(),
-                        total_optimized_points.clone(),
-                        &optimizer_cpu_budget,
+            // Continue if we have enough CPU budget available to start an optimization
+            // Otherwise skip now and start a task to trigger the optimizer again once CPU
+            // budget becomes available
+            let desired_cpus = num_rayon_threads(max_indexing_threads);
+            if !optimizer_cpu_budget.has_budget(desired_cpus) {
+                let trigger_active = cpu_available_trigger
+                    .as_ref()
+                    .map_or(false, |t| !t.is_finished());
+                if !trigger_active {
+                    cpu_available_trigger.replace(trigger_optimizers_on_cpu_budget(
+                        optimizer_cpu_budget.clone(),
+                        desired_cpus,
                         sender.clone(),
-                        limit,
-                    )
-                    .await;
+                    ));
                 }
+                continue;
             }
+
+            // Determine optimization handle limit based on max handles we allow
+            // Not related to the CPU budget, but a different limit for the maximum number
+            // of concurrent concrete optimizations per shard as configured by the user in
+            // the Qdrant configuration.
+            // Skip if we reached limit, an ongoing optimization that finishes will trigger this loop again
+            let limit = max_handles.saturating_sub(optimization_handles.lock().await.len());
+            if limit == 0 {
+                log::trace!("Skipping optimization check, we reached optimization thread limit");
+                continue;
+            }
+
+            Self::process_optimization(
+                optimizers.clone(),
+                segments.clone(),
+                optimization_handles.clone(),
+                optimizers_log.clone(),
+                total_optimized_points.clone(),
+                &optimizer_cpu_budget,
+                sender.clone(),
+                limit,
+            )
+            .await;
         }
     }
 

commit 4904e623a5d3ab50a71e556591e6f7051aee7b0b
Author: Tim Vise <tim+github@visee.me>
Date:   Wed Oct 30 17:51:14 2024 +0100

    Tiny refactoring during data race investigation (#5299)
    
    * We can always try to remove without checking first
    
    * Remove redundant condition
    
    * Simplify log message
    
    * Revert simplification
    
    * Remove intermediate write guard

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index ad704a528..39942eb5b 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -692,10 +692,7 @@ impl UpdateHandler {
 
                     if let Some(feedback) = sender {
                         feedback.send(res).unwrap_or_else(|_| {
-                            debug!(
-                                "Can't report operation {} result. Assume already not required",
-                                op_num
-                            );
+                            debug!("Can't report operation {op_num} result. Assume already not required");
                         });
                     };
                 }

commit 4240e71859b86195c03d84ac363f9699b7bc0317
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Fri Nov 8 10:10:44 2024 +0100

    No useless async (#5401)
    
    * Remove unecessary async/await
    
    * clippy aftermath

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 39942eb5b..bd9408688 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -240,7 +240,7 @@ impl UpdateHandler {
 
     /// Checks if there are any failed operations.
     /// If so - attempts to re-apply all failed operations.
-    async fn try_recover(segments: LockedSegmentHolder, wal: LockedWal) -> CollectionResult<usize> {
+    fn try_recover(segments: LockedSegmentHolder, wal: LockedWal) -> CollectionResult<usize> {
         // Try to re-apply everything starting from the first failed operation
         let first_failed_operation_option = segments.read().failed_operation.iter().cloned().min();
         match first_failed_operation_option {
@@ -604,10 +604,7 @@ impl UpdateHandler {
                 continue;
             }
 
-            if Self::try_recover(segments.clone(), wal.clone())
-                .await
-                .is_err()
-            {
+            if Self::try_recover(segments.clone(), wal.clone()).is_err() {
                 continue;
             }
 

commit f416f2b98f08fc7749814b0725f9035459b5c057
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Wed Nov 27 11:24:58 2024 +0100

    Clippy 1.83 (#5513)
    
    * Clippy 1.83
    
    * there is more

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index bd9408688..630c05c12 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -615,7 +615,7 @@ impl UpdateHandler {
             if !optimizer_cpu_budget.has_budget(desired_cpus) {
                 let trigger_active = cpu_available_trigger
                     .as_ref()
-                    .map_or(false, |t| !t.is_finished());
+                    .is_some_and(|t| !t.is_finished());
                 if !trigger_active {
                     cpu_available_trigger.replace(trigger_optimizers_on_cpu_budget(
                         optimizer_cpu_budget.clone(),

commit 38f478ddf7a9d03a1c783c5599f3b6ae33a05195
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Thu Jan 16 14:25:55 2025 +0100

    Measure payload read IO (#5773)
    
    * Measure read io for payload storage
    
    * Add Hardware Counter to update functions
    
    * Fix tests and benches
    
    * Rename (some) *_measured functions back to original

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 630c05c12..7a7e1061d 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -4,6 +4,7 @@ use std::path::{Path, PathBuf};
 use std::sync::atomic::{AtomicBool, AtomicU64, AtomicUsize, Ordering};
 use std::sync::Arc;
 
+use common::counter::hardware_counter::HardwareCounterCell;
 use common::cpu::CpuBudget;
 use common::panic;
 use itertools::Itertools;
@@ -248,7 +249,12 @@ impl UpdateHandler {
             Some(first_failed_op) => {
                 let wal_lock = wal.lock();
                 for (op_num, operation) in wal_lock.read(first_failed_op) {
-                    CollectionUpdater::update(&segments, op_num, operation.operation)?;
+                    CollectionUpdater::update(
+                        &segments,
+                        op_num,
+                        operation.operation,
+                        &HardwareCounterCell::disposable(), // Internal operation, no measurement needed
+                    )?;
                 }
             }
         };
@@ -675,8 +681,11 @@ impl UpdateHandler {
                         Ok(())
                     };
 
-                    let operation_result = flush_res
-                        .and_then(|_| CollectionUpdater::update(&segments, op_num, operation));
+                    let hw_counter = HardwareCounterCell::disposable(); // TODO(io_measurement): implement!!
+
+                    let operation_result = flush_res.and_then(|_| {
+                        CollectionUpdater::update(&segments, op_num, operation, &hw_counter)
+                    });
 
                     let res = match operation_result {
                         Ok(update_res) => optimize_sender

commit caed5729e5b7ff3db9dcb4531a4af0929b186682
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Thu Feb 20 09:05:00 2025 +0100

    IO resource usage permit (#6015)
    
    * rename cpu_budget -> resource_budget
    
    * clippy
    
    * add io budget to resources
    
    * fmt
    
    * move budget structures into a separate file
    
    * add extend permit function
    
    * dont extend existing permit
    
    * switch from IO to CPU permit
    
    * do not release resource before aquiring an extension
    
    * fmt
    
    * Review remarks
    
    * Improve resource permit number assertion
    
    * Make resource permit replace_with only acquire extra needed permits
    
    * Remove obsolete drop implementation
    
    * allocate IO budget same as CPU
    
    * review fixes
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 7a7e1061d..af56b27cb 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -4,8 +4,8 @@ use std::path::{Path, PathBuf};
 use std::sync::atomic::{AtomicBool, AtomicU64, AtomicUsize, Ordering};
 use std::sync::Arc;
 
+use common::budget::ResourceBudget;
 use common::counter::hardware_counter::HardwareCounterCell;
-use common::cpu::CpuBudget;
 use common::panic;
 use itertools::Itertools;
 use log::{debug, error, info, trace, warn};
@@ -93,7 +93,7 @@ pub struct UpdateHandler {
     total_optimized_points: Arc<AtomicUsize>,
     /// Global CPU budget in number of cores for all optimization tasks.
     /// Assigns CPU permits to tasks to limit overall resource utilization.
-    optimizer_cpu_budget: CpuBudget,
+    optimizer_resource_budget: ResourceBudget,
     /// How frequent can we flush data
     /// This parameter depends on the optimizer config and should be updated accordingly.
     pub flush_interval_sec: u64,
@@ -133,7 +133,7 @@ impl UpdateHandler {
         optimizers: Arc<Vec<Arc<Optimizer>>>,
         optimizers_log: Arc<Mutex<TrackerLog>>,
         total_optimized_points: Arc<AtomicUsize>,
-        optimizer_cpu_budget: CpuBudget,
+        optimizer_resource_budget: ResourceBudget,
         runtime_handle: Handle,
         segments: LockedSegmentHolder,
         wal: LockedWal,
@@ -151,7 +151,7 @@ impl UpdateHandler {
             optimizer_worker: None,
             optimizers_log,
             total_optimized_points,
-            optimizer_cpu_budget,
+            optimizer_resource_budget,
             flush_worker: None,
             flush_stop: None,
             runtime_handle,
@@ -177,7 +177,7 @@ impl UpdateHandler {
             self.optimization_handles.clone(),
             self.optimizers_log.clone(),
             self.total_optimized_points.clone(),
-            self.optimizer_cpu_budget.clone(),
+            self.optimizer_resource_budget.clone(),
             self.max_optimization_threads,
             self.has_triggered_optimizers.clone(),
             self.payload_index_schema.clone(),
@@ -268,7 +268,7 @@ impl UpdateHandler {
         optimizers: Arc<Vec<Arc<Optimizer>>>,
         optimizers_log: Arc<Mutex<TrackerLog>>,
         total_optimized_points: Arc<AtomicUsize>,
-        optimizer_cpu_budget: &CpuBudget,
+        optimizer_resource_budget: &ResourceBudget,
         segments: LockedSegmentHolder,
         callback: F,
         limit: Option<usize>,
@@ -296,14 +296,15 @@ impl UpdateHandler {
                 debug!("Optimizing segments: {:?}", &nonoptimal_segment_ids);
 
                 // Determine how many CPUs we prefer for optimization task, acquire permit for it
+                // And use same amount of IO threads as CPUs
                 let max_indexing_threads = optimizer.hnsw_config().max_indexing_threads;
-                let desired_cpus = num_rayon_threads(max_indexing_threads);
-                let Some(permit) = optimizer_cpu_budget.try_acquire(desired_cpus) else {
+                let desired_io = num_rayon_threads(max_indexing_threads);
+                let Some(permit) = optimizer_resource_budget.try_acquire(0, desired_io) else {
                     // If there is no CPU budget, break outer loop and return early
                     // If we have no handles (no optimizations) trigger callback so that we wake up
                     // our optimization worker to try again later, otherwise it could get stuck
                     log::trace!(
-                        "No available CPU permit for {} optimizer, postponing",
+                        "No available IO permit for {} optimizer, postponing",
                         optimizer.name(),
                     );
                     if handles.is_empty() {
@@ -312,8 +313,8 @@ impl UpdateHandler {
                     break 'outer;
                 };
                 log::trace!(
-                    "Acquired {} CPU permit for {} optimizer",
-                    permit.num_cpus,
+                    "Acquired {} IO permit for {} optimizer",
+                    permit.num_io,
                     optimizer.name(),
                 );
 
@@ -328,6 +329,7 @@ impl UpdateHandler {
                 let handle = spawn_stoppable(
                     // Stoppable task
                     {
+                        let resource_budget = optimizer_resource_budget.clone();
                         let segments = segments.clone();
                         move |stopped| {
                             // Track optimizer status
@@ -340,6 +342,7 @@ impl UpdateHandler {
                                 segments.clone(),
                                 nsi,
                                 permit,
+                                resource_budget,
                                 stopped,
                             ) {
                                 // Perform some actions when optimization if finished
@@ -472,7 +475,7 @@ impl UpdateHandler {
         optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
         optimizers_log: Arc<Mutex<TrackerLog>>,
         total_optimized_points: Arc<AtomicUsize>,
-        optimizer_cpu_budget: &CpuBudget,
+        optimizer_resource_budget: &ResourceBudget,
         sender: Sender<OptimizerSignal>,
         limit: usize,
     ) {
@@ -480,7 +483,7 @@ impl UpdateHandler {
             optimizers.clone(),
             optimizers_log,
             total_optimized_points,
-            optimizer_cpu_budget,
+            optimizer_resource_budget,
             segments.clone(),
             move |_optimization_result| {
                 // After optimization is finished, we still need to check if there are
@@ -538,7 +541,7 @@ impl UpdateHandler {
         optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
         optimizers_log: Arc<Mutex<TrackerLog>>,
         total_optimized_points: Arc<AtomicUsize>,
-        optimizer_cpu_budget: CpuBudget,
+        optimizer_resource_budget: ResourceBudget,
         max_handles: Option<usize>,
         has_triggered_optimizers: Arc<AtomicBool>,
         payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,
@@ -550,7 +553,7 @@ impl UpdateHandler {
             .unwrap_or_default();
 
         // Asynchronous task to trigger optimizers once CPU budget is available again
-        let mut cpu_available_trigger: Option<JoinHandle<()>> = None;
+        let mut resource_available_trigger: Option<JoinHandle<()>> = None;
 
         loop {
             let result = timeout(OPTIMIZER_CLEANUP_INTERVAL, receiver.recv()).await;
@@ -614,18 +617,20 @@ impl UpdateHandler {
                 continue;
             }
 
-            // Continue if we have enough CPU budget available to start an optimization
-            // Otherwise skip now and start a task to trigger the optimizer again once CPU
+            // Continue if we have enough resource budget available to start an optimization
+            // Otherwise skip now and start a task to trigger the optimizer again once resource
             // budget becomes available
-            let desired_cpus = num_rayon_threads(max_indexing_threads);
-            if !optimizer_cpu_budget.has_budget(desired_cpus) {
-                let trigger_active = cpu_available_trigger
+            let desired_cpus = 0;
+            let desired_io = num_rayon_threads(max_indexing_threads);
+            if !optimizer_resource_budget.has_budget(desired_cpus, desired_io) {
+                let trigger_active = resource_available_trigger
                     .as_ref()
                     .is_some_and(|t| !t.is_finished());
                 if !trigger_active {
-                    cpu_available_trigger.replace(trigger_optimizers_on_cpu_budget(
-                        optimizer_cpu_budget.clone(),
+                    resource_available_trigger.replace(trigger_optimizers_on_resource_budget(
+                        optimizer_resource_budget.clone(),
                         desired_cpus,
+                        desired_io,
                         sender.clone(),
                     ));
                 }
@@ -649,7 +654,7 @@ impl UpdateHandler {
                 optimization_handles.clone(),
                 optimizers_log.clone(),
                 total_optimized_points.clone(),
-                &optimizer_cpu_budget,
+                &optimizer_resource_budget,
                 sender.clone(),
                 limit,
             )
@@ -815,15 +820,16 @@ impl UpdateHandler {
 }
 
 /// Trigger optimizers when CPU budget is available
-fn trigger_optimizers_on_cpu_budget(
-    optimizer_cpu_budget: CpuBudget,
+fn trigger_optimizers_on_resource_budget(
+    optimizer_resource_budget: ResourceBudget,
     desired_cpus: usize,
+    desired_io: usize,
     sender: Sender<OptimizerSignal>,
 ) -> JoinHandle<()> {
     task::spawn(async move {
         log::trace!("Skipping optimization checks, waiting for CPU budget to be available");
-        optimizer_cpu_budget
-            .notify_on_budget_available(desired_cpus)
+        optimizer_resource_budget
+            .notify_on_budget_available(desired_cpus, desired_io)
             .await;
         log::trace!("Continue optimization checks, new CPU budget available");
 

commit f6d58b46bfb1f89112a9e1d14daa9d568fce5c8b
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Mon Feb 24 11:57:23 2025 +0100

    notify optimizers scheduler of the budget change (#6040)
    
    * notify optimizers scheduler of the budget change
    
    * Zero permit count when releasing CPU or IO
    
    ---------
    
    Co-authored-by: Tim Vise <tim+github@visee.me>

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index af56b27cb..3ad084761 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -274,7 +274,7 @@ impl UpdateHandler {
         limit: Option<usize>,
     ) -> Vec<StoppableTaskHandle<bool>>
     where
-        F: FnOnce(bool) + Send + Clone + 'static,
+        F: Fn(bool) + Send + Clone + Sync + 'static,
     {
         let mut scheduled_segment_ids = HashSet::<_>::default();
         let mut handles = vec![];
@@ -295,12 +295,12 @@ impl UpdateHandler {
 
                 debug!("Optimizing segments: {:?}", &nonoptimal_segment_ids);
 
-                // Determine how many CPUs we prefer for optimization task, acquire permit for it
+                // Determine how many Resources we prefer for optimization task, acquire permit for it
                 // And use same amount of IO threads as CPUs
                 let max_indexing_threads = optimizer.hnsw_config().max_indexing_threads;
                 let desired_io = num_rayon_threads(max_indexing_threads);
-                let Some(permit) = optimizer_resource_budget.try_acquire(0, desired_io) else {
-                    // If there is no CPU budget, break outer loop and return early
+                let Some(mut permit) = optimizer_resource_budget.try_acquire(0, desired_io) else {
+                    // If there is no Resource budget, break outer loop and return early
                     // If we have no handles (no optimizations) trigger callback so that we wake up
                     // our optimization worker to try again later, otherwise it could get stuck
                     log::trace!(
@@ -318,6 +318,13 @@ impl UpdateHandler {
                     optimizer.name(),
                 );
 
+                let permit_callback = callback.clone();
+
+                permit.set_on_release(move || {
+                    // Notify scheduler that resource budget changed
+                    permit_callback(false);
+                });
+
                 let optimizer = optimizer.clone();
                 let optimizers_log = optimizers_log.clone();
                 let total_optimized_points = total_optimized_points.clone();

commit 8ad2b34265448ec01b89d4093de5fbb1a86dcd4d
Author: Tim Vise <tim+github@visee.me>
Date:   Tue Feb 25 11:21:25 2025 +0100

    Bump Rust edition to 2024 (#6042)
    
    * Bump Rust edition to 2024
    
    * gen is a reserved keyword now
    
    * Remove ref mut on references
    
    * Mark extern C as unsafe
    
    * Wrap unsafe function bodies in unsafe block
    
    * Geo hash implements Copy, don't reference but pass by value instead
    
    * Replace secluded self import with parent
    
    * Update execute_cluster_read_operation with new match semantics
    
    * Fix lifetime issue
    
    * Replace map_or with is_none_or
    
    * set_var is unsafe now
    
    * Reformat

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 3ad084761..94ea26402 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -1,8 +1,8 @@
 use std::cmp::min;
 use std::collections::HashSet;
 use std::path::{Path, PathBuf};
-use std::sync::atomic::{AtomicBool, AtomicU64, AtomicUsize, Ordering};
 use std::sync::Arc;
+use std::sync::atomic::{AtomicBool, AtomicU64, AtomicUsize, Ordering};
 
 use common::budget::ResourceBudget;
 use common::counter::hardware_counter::HardwareCounterCell;
@@ -15,10 +15,10 @@ use segment::index::hnsw_index::num_rayon_threads;
 use segment::types::SeqNumberType;
 use tokio::runtime::Handle;
 use tokio::sync::mpsc::{self, Receiver, Sender};
-use tokio::sync::{oneshot, Mutex as TokioMutex};
+use tokio::sync::{Mutex as TokioMutex, oneshot};
 use tokio::task::{self, JoinHandle};
 use tokio::time::error::Elapsed;
-use tokio::time::{timeout, Duration};
+use tokio::time::{Duration, timeout};
 
 use crate::collection::payload_index_schema::PayloadIndexSchema;
 use crate::collection_manager::collection_updater::CollectionUpdater;
@@ -27,11 +27,11 @@ use crate::collection_manager::optimizers::segment_optimizer::{
     OptimizerThresholds, SegmentOptimizer,
 };
 use crate::collection_manager::optimizers::{Tracker, TrackerLog, TrackerStatus};
-use crate::common::stoppable_task::{spawn_stoppable, StoppableTaskHandle};
+use crate::common::stoppable_task::{StoppableTaskHandle, spawn_stoppable};
 use crate::config::CollectionParams;
+use crate::operations::CollectionUpdateOperations;
 use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{CollectionError, CollectionResult};
-use crate::operations::CollectionUpdateOperations;
 use crate::save_on_disk::SaveOnDisk;
 use crate::shards::local_shard::LocalShardClocks;
 use crate::wal::WalError;
@@ -586,7 +586,9 @@ impl UpdateHandler {
                     // tasks that'll trigger this for us. If we don't run optimizers here we might
                     // get stuck into yellow state until a new update operation is received.
                     // See: <https://github.com/qdrant/qdrant/pull/5111>
-                    log::warn!("Cleaned a optimization handle after timeout, explicitly triggering optimizers");
+                    log::warn!(
+                        "Cleaned a optimization handle after timeout, explicitly triggering optimizers",
+                    );
                     true
                 }
                 // Hit optimizer cleanup interval, did not clean up a task: do 2

commit 78f0428f3e23b41cb5702b7aa6caab5564f4af26
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Thu Mar 6 15:03:23 2025 +0100

    Measure hardware IO for update operations (#5922)
    
    * Measure update operations hardware IO
    
    * Add support for distributed setups
    
    * also measure update_local
    
    * Add consensus tests for HW metrics of update operations
    
    * add test for upserting without waiting
    
    * Disable HW usage reporting when not waiting for update API
    
    * Review remarks
    
    * Fix resharding collecting hw measurements
    
    * Fix metric type
    
    * New struct HardwareData for better accumulation
    
    * Ensure we always apply CPU multiplier
    
    * Apply suggestions from code review
    
    * Update src/actix/api/update_api.rs
    
    Co-authored-by: Tim Vise <tim+github@visee.me>
    
    * Fix assert_with_upper_bound_error threshold calculation.
    
    * Clarifying why we don't measure shard cleanup
    
    ---------
    
    Co-authored-by: Tim Vise <tim+github@visee.me>

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 94ea26402..5d1947796 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -5,6 +5,7 @@ use std::sync::Arc;
 use std::sync::atomic::{AtomicBool, AtomicU64, AtomicUsize, Ordering};
 
 use common::budget::ResourceBudget;
+use common::counter::hardware_accumulator::HwMeasurementAcc;
 use common::counter::hardware_counter::HardwareCounterCell;
 use common::panic;
 use itertools::Itertools;
@@ -55,6 +56,7 @@ pub struct OperationData {
     pub wait: bool,
     /// Callback notification channel
     pub sender: Option<oneshot::Sender<CollectionResult<usize>>>,
+    pub hw_measurements: HwMeasurementAcc,
 }
 
 /// Signal, used to inform Updater process
@@ -684,6 +686,7 @@ impl UpdateHandler {
                     operation,
                     sender,
                     wait,
+                    hw_measurements,
                 }) => {
                     let flush_res = if wait {
                         wal.lock().flush().map_err(|err| {
@@ -695,10 +698,13 @@ impl UpdateHandler {
                         Ok(())
                     };
 
-                    let hw_counter = HardwareCounterCell::disposable(); // TODO(io_measurement): implement!!
-
                     let operation_result = flush_res.and_then(|_| {
-                        CollectionUpdater::update(&segments, op_num, operation, &hw_counter)
+                        CollectionUpdater::update(
+                            &segments,
+                            op_num,
+                            operation,
+                            &hw_measurements.get_counter_cell(),
+                        )
                     });
 
                     let res = match operation_result {

commit c9b48bccd153f9742e1a42c1d76561c58e2a0b9a
Author: Kumar Shivendu <kshivendu1@gmail.com>
Date:   Fri Mar 21 13:51:36 2025 +0530

    Snapshot logging improve (#6192)
    
    * Add details to snapshot logging for more details
    
    * Format code and improve log msg
    
    * Remove unused import and improve logs
    
    * Improve terminology for logging
    
    * Minimize noise using logging levels
    
    * Log when moving local shard
    
    * Apply suggestions from code review
    
    Co-authored-by: Roman Titov <ffuugoo@users.noreply.github.com>
    
    * fmt fix
    
    ---------
    
    Co-authored-by: Roman Titov <ffuugoo@users.noreply.github.com>

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 5d1947796..6d1b91f47 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -766,7 +766,7 @@ impl UpdateHandler {
             tokio::select! {
                 _ = tokio::time::sleep(Duration::from_secs(flush_interval_sec)) => {},
                 _ = &mut stop_receiver => {
-                    debug!("Stopping flush worker.");
+                    debug!("Stopping flush worker for shard {}", shard_path.display());
                     return;
                 }
             }

commit f230629fa0e62e069e683cce60e24319ab3cc84b
Author: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>
Date:   Tue Mar 25 10:08:21 2025 +0100

    build(deps): bump log from 0.4.26 to 0.4.27 (#6247)
    
    * build(deps): bump log from 0.4.26 to 0.4.27
    
    Bumps [log](https://github.com/rust-lang/log) from 0.4.26 to 0.4.27.
    - [Release notes](https://github.com/rust-lang/log/releases)
    - [Changelog](https://github.com/rust-lang/log/blob/master/CHANGELOG.md)
    - [Commits](https://github.com/rust-lang/log/compare/0.4.26...0.4.27)
    
    ---
    updated-dependencies:
    - dependency-name: log
      dependency-type: direct:production
      update-type: version-update:semver-patch
    ...
    
    Signed-off-by: dependabot[bot] <support@github.com>
    
    * put variables inside the strings for log macros
    
    * also for pyroscope
    
    ---------
    
    Signed-off-by: dependabot[bot] <support@github.com>
    Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>
    Co-authored-by: Luis Cosso <luis.cossio@outlook.com>

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 6d1b91f47..8c8e200cb 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -775,7 +775,7 @@ impl UpdateHandler {
             let wal_flash_job = wal.lock().flush_async();
 
             if let Err(err) = wal_flash_job.join() {
-                error!("Failed to flush wal: {:?}", err);
+                error!("Failed to flush wal: {err:?}");
                 segments
                     .write()
                     .report_optimizer_error(WalError::WriteWalError(format!(

commit d854b41ac1a346ed1addd42f26e66c97634dc0c7
Author: Tim Vise <tim+github@visee.me>
Date:   Fri Apr 4 12:18:53 2025 +0200

    Migrate WAL mutex from parking lot to tokio (#6307)
    
    * Migrate WAL from sync parking lot to async tokio mutex
    
    * Improve logging
    
    * Migrate tests

diff --git a/lib/collection/src/update_handler.rs b/lib/collection/src/update_handler.rs
index 8c8e200cb..0d4d2d824 100644
--- a/lib/collection/src/update_handler.rs
+++ b/lib/collection/src/update_handler.rs
@@ -243,13 +243,13 @@ impl UpdateHandler {
 
     /// Checks if there are any failed operations.
     /// If so - attempts to re-apply all failed operations.
-    fn try_recover(segments: LockedSegmentHolder, wal: LockedWal) -> CollectionResult<usize> {
+    async fn try_recover(segments: LockedSegmentHolder, wal: LockedWal) -> CollectionResult<usize> {
         // Try to re-apply everything starting from the first failed operation
         let first_failed_operation_option = segments.read().failed_operation.iter().cloned().min();
         match first_failed_operation_option {
             None => {}
             Some(first_failed_op) => {
-                let wal_lock = wal.lock();
+                let wal_lock = wal.lock().await;
                 for (op_num, operation) in wal_lock.read(first_failed_op) {
                     CollectionUpdater::update(
                         &segments,
@@ -624,7 +624,10 @@ impl UpdateHandler {
                 continue;
             }
 
-            if Self::try_recover(segments.clone(), wal.clone()).is_err() {
+            if Self::try_recover(segments.clone(), wal.clone())
+                .await
+                .is_err()
+            {
                 continue;
             }
 
@@ -689,7 +692,7 @@ impl UpdateHandler {
                     hw_measurements,
                 }) => {
                     let flush_res = if wait {
-                        wal.lock().flush().map_err(|err| {
+                        wal.lock().await.flush().map_err(|err| {
                             CollectionError::service_error(format!(
                                 "Can't flush WAL before operation {op_num} - {err}"
                             ))
@@ -772,7 +775,7 @@ impl UpdateHandler {
             }
 
             trace!("Attempting flushing");
-            let wal_flash_job = wal.lock().flush_async();
+            let wal_flash_job = wal.lock().await.flush_async();
 
             if let Err(err) = wal_flash_job.join() {
                 error!("Failed to flush wal: {err:?}");
@@ -813,7 +816,7 @@ impl UpdateHandler {
                 segments.write().report_optimizer_error(err);
             }
 
-            if let Err(err) = wal.lock().ack(ack) {
+            if let Err(err) = wal.lock().await.ack(ack) {
                 log::warn!("Failed to acknowledge WAL version: {err}");
                 segments.write().report_optimizer_error(err);
             }

