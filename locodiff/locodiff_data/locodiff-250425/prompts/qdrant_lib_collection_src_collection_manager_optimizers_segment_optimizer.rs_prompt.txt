# Instructions

You are being benchmarked. You will see the output of a git log command, and from that must infer the current state of a file. Think carefully, as you must output the exact state of the file to earn full marks.

**Important:** Your goal is to reproduce the file's content *exactly* as it exists at the final commit, even if the code appears broken, buggy, or contains obvious errors. Do **not** try to "fix" the code. Attempting to correct issues will result in a poor score, as this benchmark evaluates your ability to reproduce the precise state of the file based on its history.

# Required Response Format

Wrap the content of the file in triple backticks (```). Any text outside the final closing backticks will be ignored. End your response after outputting the closing backticks.

# Example Response

```python
#!/usr/bin/env python
print('Hello, world!')
```

# File History

> git log -p --cc --topo-order --reverse -- lib/collection/src/collection_manager/optimizers/segment_optimizer.rs

commit 446d0c29f70f1154025e644b154adbd270007290
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Sun Aug 15 23:26:01 2021 +0200

    Deadlock fix (#91)
    
    * refactor: segment managers -> collection managers
    
    * fix segments holder deadlock
    
    * apply cargo fmt
    
    * fix cargo clippy
    
    * replace sequential segment locking with multiple try_lock attempts to prevent deadlocks

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
new file mode 100644
index 000000000..5cef9030a
--- /dev/null
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -0,0 +1,238 @@
+use crate::collection_manager::holders::proxy_segment::ProxySegment;
+use crate::collection_manager::holders::segment_holder::{
+    LockedSegment, LockedSegmentHolder, SegmentId,
+};
+use crate::config::CollectionParams;
+use crate::operations::types::CollectionResult;
+use itertools::Itertools;
+use parking_lot::RwLock;
+use segment::entry::entry_point::SegmentEntry;
+use segment::segment::Segment;
+use segment::segment_constructor::segment_builder::SegmentBuilder;
+use segment::segment_constructor::simple_segment_constructor::build_simple_segment;
+use segment::types::{
+    HnswConfig, Indexes, PayloadIndexType, PayloadKeyType, PointIdType, SegmentConfig, StorageType,
+};
+use std::collections::HashSet;
+use std::convert::TryInto;
+use std::path::Path;
+use std::sync::Arc;
+
+#[derive(Debug, Clone)]
+pub struct OptimizerThresholds {
+    pub memmap_threshold: usize,
+    pub indexing_threshold: usize,
+    pub payload_indexing_threshold: usize,
+}
+
+pub trait SegmentOptimizer {
+    /// Get path of the whole collection
+    fn collection_path(&self) -> &Path;
+
+    /// Get temp path, where optimized segments could be temporary stored
+    fn temp_path(&self) -> &Path;
+
+    /// Get basic segment config
+    fn collection_params(&self) -> CollectionParams;
+
+    /// Get HNSW config
+    fn hnsw_config(&self) -> HnswConfig;
+
+    /// Get thresholds configuration for the current optimizer
+    fn threshold_config(&self) -> &OptimizerThresholds;
+
+    /// Checks if segment optimization is required
+    fn check_condition(&self, segments: LockedSegmentHolder) -> Vec<SegmentId>;
+
+    /// Build temp segment
+    fn temp_segment(&self) -> CollectionResult<LockedSegment> {
+        let collection_params = self.collection_params();
+        let config = SegmentConfig {
+            vector_size: collection_params.vector_size,
+            distance: collection_params.distance,
+            index: Indexes::Plain {},
+            payload_index: Some(PayloadIndexType::Plain),
+            storage_type: StorageType::InMemory,
+        };
+        Ok(LockedSegment::new(build_simple_segment(
+            self.collection_path(),
+            config.vector_size,
+            config.distance,
+        )?))
+    }
+
+    /// Build optimized segment
+    fn optimized_segment_builder(
+        &self,
+        optimizing_segments: &[LockedSegment],
+    ) -> CollectionResult<SegmentBuilder> {
+        let total_vectors: usize = optimizing_segments
+            .iter()
+            .map(|s| s.get().read().vectors_count())
+            .sum();
+
+        let have_indexed_fields = optimizing_segments
+            .iter()
+            .any(|s| !s.get().read().get_indexed_fields().is_empty());
+
+        let thresholds = self.threshold_config();
+        let collection_params = self.collection_params();
+
+        let is_indexed = total_vectors >= thresholds.indexing_threshold;
+
+        // Create structure index only if there is something to index
+        let is_payload_indexed =
+            total_vectors >= thresholds.payload_indexing_threshold && have_indexed_fields;
+
+        let is_on_disk = total_vectors >= thresholds.memmap_threshold;
+
+        let optimized_config = SegmentConfig {
+            vector_size: collection_params.vector_size,
+            distance: collection_params.distance,
+            index: if is_indexed {
+                Indexes::Hnsw(self.hnsw_config())
+            } else {
+                Indexes::Plain {}
+            },
+            payload_index: Some(if is_payload_indexed {
+                PayloadIndexType::Struct
+            } else {
+                PayloadIndexType::Plain
+            }),
+            storage_type: if is_on_disk {
+                StorageType::Mmap
+            } else {
+                StorageType::InMemory
+            },
+        };
+
+        Ok(SegmentBuilder::new(
+            self.collection_path(),
+            self.temp_path(),
+            &optimized_config,
+        )?)
+    }
+
+    /// Performs optimization of collections's segments, including:
+    ///     - Segment rebuilding
+    ///     - Segment joining
+    fn optimize(
+        &self,
+        segments: LockedSegmentHolder,
+        ids: Vec<SegmentId>,
+    ) -> CollectionResult<bool> {
+        let tmp_segment = self.temp_segment()?;
+
+        let proxy_deleted_points = Arc::new(RwLock::new(HashSet::<PointIdType>::new()));
+        let proxy_deleted_indexes = Arc::new(RwLock::new(HashSet::<PayloadKeyType>::new()));
+        let proxy_created_indexes = Arc::new(RwLock::new(HashSet::<PayloadKeyType>::new()));
+
+        // Exclusive lock for the segments operations
+        let mut write_segments = segments.write();
+
+        let optimizing_segments: Vec<_> = ids
+            .iter()
+            .cloned()
+            .map(|id| write_segments.get(id))
+            .filter_map(|x| x.cloned())
+            .collect();
+
+        let proxies = optimizing_segments.iter().map(|sg| {
+            ProxySegment::new(
+                sg.clone(),
+                tmp_segment.clone(),
+                proxy_deleted_points.clone(),
+                proxy_deleted_indexes.clone(),
+                proxy_created_indexes.clone(),
+            )
+        });
+
+        let proxy_ids: Vec<_> = proxies
+            .zip(ids.iter().cloned())
+            .map(|(proxy, idx)| write_segments.swap(proxy, &[idx], false).unwrap())
+            .collect();
+
+        // Release segments lock
+        drop(write_segments);
+
+        let mut segment_builder = self.optimized_segment_builder(&optimizing_segments)?;
+
+        // ---- SLOW PART -----
+        for segment in optimizing_segments {
+            match segment {
+                LockedSegment::Original(segment_arc) => {
+                    let segment_guard = segment_arc.read();
+                    segment_builder.update_from(&segment_guard)?;
+                }
+                LockedSegment::Proxy(_) => panic!("Attempt to optimize segment which is already currently under optimization. Should never happen"),
+            }
+        }
+
+        for field in proxy_deleted_indexes.read().iter() {
+            segment_builder.indexed_fields.remove(field);
+        }
+        for field in proxy_created_indexes.read().iter().cloned() {
+            segment_builder.indexed_fields.insert(field);
+        }
+
+        let mut optimized_segment: Segment = segment_builder.try_into()?;
+
+        // Delete points in 2 steps
+        // First step - delete all points with read lock
+        // Second step - delete all the rest points with full write lock
+        let deleted_points_snapshot: HashSet<PointIdType> =
+            proxy_deleted_points.read().iter().cloned().collect();
+
+        for point_id in deleted_points_snapshot.iter().cloned() {
+            optimized_segment
+                .delete_point(optimized_segment.version, point_id)
+                .unwrap();
+        }
+
+        let deleted_indexes = proxy_deleted_indexes.read().iter().cloned().collect_vec();
+        let create_indexes = proxy_created_indexes.read().iter().cloned().collect_vec();
+
+        for delete_field_name in deleted_indexes.iter() {
+            optimized_segment.delete_field_index(optimized_segment.version, delete_field_name)?;
+        }
+
+        for create_field_name in create_indexes.iter() {
+            optimized_segment.create_field_index(optimized_segment.version, create_field_name)?;
+        }
+        // ---- SLOW PART ENDS HERE -----
+
+        {
+            // This block locks all operations with collection. It should be fast
+            let mut write_segments = segments.write();
+            let deleted_points = proxy_deleted_points.read();
+            let points_diff = deleted_points_snapshot.difference(&deleted_points);
+            for point_id in points_diff.into_iter() {
+                optimized_segment
+                    .delete_point(optimized_segment.version, *point_id)
+                    .unwrap();
+            }
+
+            for deleted_field_name in proxy_deleted_indexes.read().iter() {
+                optimized_segment
+                    .delete_field_index(optimized_segment.version, deleted_field_name)?;
+            }
+
+            for created_field_name in proxy_created_indexes.read().iter() {
+                optimized_segment
+                    .create_field_index(optimized_segment.version, created_field_name)?;
+            }
+
+            write_segments.swap(optimized_segment, &proxy_ids, true)?;
+
+            let has_appendable_segments = write_segments.random_appendable_segment().is_some();
+
+            // Append a temp segment to a collection if it is not empty or there is no other appendable segment
+            if tmp_segment.get().read().vectors_count() > 0 || !has_appendable_segments {
+                write_segments.add_locked(tmp_segment);
+            } else {
+                tmp_segment.drop_data()?;
+            }
+        }
+        Ok(true)
+    }
+}

commit bf3d8c25753188b4ca5e69a13c7f26e3c383f05b
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Sun Oct 24 18:10:39 2021 +0200

    data consistency fixes and updates (#112)
    
    * update segment version after completed update only
    
    * more stable updates: check pre-existing points on update, fail recovery, WAL proper ack. check_unprocessed_points WIP
    
    * switch to async channel
    
    * perform update operations in a separate thread (#111)
    
    * perform update operations in a separate thread
    
    * ordered sending update signal
    
    * locate a segment merging versioning bug
    
    * rename id_mapper -> id_tracker
    
    * per-record versioning
    
    * clippy fixes
    
    * cargo fmt
    
    * rm limit of open files
    
    * fail recovery test
    
    * cargo fmt
    
    * wait for worker stops befor dropping the runtime

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 5cef9030a..fa32cbbf9 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -185,7 +185,7 @@ pub trait SegmentOptimizer {
 
         for point_id in deleted_points_snapshot.iter().cloned() {
             optimized_segment
-                .delete_point(optimized_segment.version, point_id)
+                .delete_point(optimized_segment.version(), point_id)
                 .unwrap();
         }
 
@@ -193,11 +193,11 @@ pub trait SegmentOptimizer {
         let create_indexes = proxy_created_indexes.read().iter().cloned().collect_vec();
 
         for delete_field_name in deleted_indexes.iter() {
-            optimized_segment.delete_field_index(optimized_segment.version, delete_field_name)?;
+            optimized_segment.delete_field_index(optimized_segment.version(), delete_field_name)?;
         }
 
         for create_field_name in create_indexes.iter() {
-            optimized_segment.create_field_index(optimized_segment.version, create_field_name)?;
+            optimized_segment.create_field_index(optimized_segment.version(), create_field_name)?;
         }
         // ---- SLOW PART ENDS HERE -----
 
@@ -208,18 +208,18 @@ pub trait SegmentOptimizer {
             let points_diff = deleted_points_snapshot.difference(&deleted_points);
             for point_id in points_diff.into_iter() {
                 optimized_segment
-                    .delete_point(optimized_segment.version, *point_id)
+                    .delete_point(optimized_segment.version(), *point_id)
                     .unwrap();
             }
 
             for deleted_field_name in proxy_deleted_indexes.read().iter() {
                 optimized_segment
-                    .delete_field_index(optimized_segment.version, deleted_field_name)?;
+                    .delete_field_index(optimized_segment.version(), deleted_field_name)?;
             }
 
             for created_field_name in proxy_created_indexes.read().iter() {
                 optimized_segment
-                    .create_field_index(optimized_segment.version, created_field_name)?;
+                    .create_field_index(optimized_segment.version(), created_field_name)?;
             }
 
             write_segments.swap(optimized_segment, &proxy_ids, true)?;

commit c603f0075e9b546afee57522cdbd8ad28c0da27f
Author: Marcin Puc <5671049+tranzystorek-io@users.noreply.github.com>
Date:   Wed Nov 10 21:32:25 2021 +0100

    Add various refactorings (#118)

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index fa32cbbf9..0f65c9c5e 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -183,7 +183,7 @@ pub trait SegmentOptimizer {
         let deleted_points_snapshot: HashSet<PointIdType> =
             proxy_deleted_points.read().iter().cloned().collect();
 
-        for point_id in deleted_points_snapshot.iter().cloned() {
+        for &point_id in &deleted_points_snapshot {
             optimized_segment
                 .delete_point(optimized_segment.version(), point_id)
                 .unwrap();
@@ -192,11 +192,11 @@ pub trait SegmentOptimizer {
         let deleted_indexes = proxy_deleted_indexes.read().iter().cloned().collect_vec();
         let create_indexes = proxy_created_indexes.read().iter().cloned().collect_vec();
 
-        for delete_field_name in deleted_indexes.iter() {
+        for delete_field_name in &deleted_indexes {
             optimized_segment.delete_field_index(optimized_segment.version(), delete_field_name)?;
         }
 
-        for create_field_name in create_indexes.iter() {
+        for create_field_name in &create_indexes {
             optimized_segment.create_field_index(optimized_segment.version(), create_field_name)?;
         }
         // ---- SLOW PART ENDS HERE -----
@@ -206,9 +206,9 @@ pub trait SegmentOptimizer {
             let mut write_segments = segments.write();
             let deleted_points = proxy_deleted_points.read();
             let points_diff = deleted_points_snapshot.difference(&deleted_points);
-            for point_id in points_diff.into_iter() {
+            for &point_id in points_diff {
                 optimized_segment
-                    .delete_point(optimized_segment.version(), *point_id)
+                    .delete_point(optimized_segment.version(), point_id)
                     .unwrap();
             }
 

commit 97b227048513143e555353d346a7f4560db9854e
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Mon Nov 29 09:39:22 2021 +0100

    Rustdoc and README for internal entities and processes (#123)
    
    * extend comments for strorage crate
    
    * update comments and readme for collection crate
    
    * apply cargo fmt
    
    * fix tests
    
    * apply fmt

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 0f65c9c5e..9abdf4853 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -25,6 +25,14 @@ pub struct OptimizerThresholds {
     pub payload_indexing_threshold: usize,
 }
 
+/// SegmentOptimizer - trait implementing common functionality of the optimizers
+///
+/// It provides functions which allow to re-build specified segments into a new, better one.
+/// Process allows read and write (with some tricks) access to the optimized segments.
+///
+/// Process of the optimization is same for all optimizers.
+/// The selection of the candidates for optimization and the configuration
+/// of resulting segment are up to concrete implementations.
 pub trait SegmentOptimizer {
     /// Get path of the whole collection
     fn collection_path(&self) -> &Path;

commit 0d18625ebd4a3e3c2c7ca7a19403ebfd5f979aef
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Mon Jan 17 17:48:59 2022 +0100

    Multiprocessing optimization fix (#155)
    
    * test to detect a problem
    
    * add checks for already scheduled and currently optimizing segments
    
    * fmt

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 9abdf4853..dde36dcb5 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -5,7 +5,7 @@ use crate::collection_manager::holders::segment_holder::{
 use crate::config::CollectionParams;
 use crate::operations::types::CollectionResult;
 use itertools::Itertools;
-use parking_lot::RwLock;
+use parking_lot::{RwLock, RwLockUpgradableReadGuard};
 use segment::entry::entry_point::SegmentEntry;
 use segment::segment::Segment;
 use segment::segment_constructor::segment_builder::SegmentBuilder;
@@ -50,7 +50,11 @@ pub trait SegmentOptimizer {
     fn threshold_config(&self) -> &OptimizerThresholds;
 
     /// Checks if segment optimization is required
-    fn check_condition(&self, segments: LockedSegmentHolder) -> Vec<SegmentId>;
+    fn check_condition(
+        &self,
+        segments: LockedSegmentHolder,
+        excluded_ids: &HashSet<SegmentId>,
+    ) -> Vec<SegmentId>;
 
     /// Build temp segment
     fn temp_segment(&self) -> CollectionResult<LockedSegment> {
@@ -129,22 +133,38 @@ pub trait SegmentOptimizer {
         segments: LockedSegmentHolder,
         ids: Vec<SegmentId>,
     ) -> CollectionResult<bool> {
-        let tmp_segment = self.temp_segment()?;
-
-        let proxy_deleted_points = Arc::new(RwLock::new(HashSet::<PointIdType>::new()));
-        let proxy_deleted_indexes = Arc::new(RwLock::new(HashSet::<PayloadKeyType>::new()));
-        let proxy_created_indexes = Arc::new(RwLock::new(HashSet::<PayloadKeyType>::new()));
-
-        // Exclusive lock for the segments operations
-        let mut write_segments = segments.write();
+        // On the one hand - we want to check consistently if all provided segments are
+        // available for optimization (not already under one) and we want to do it before creating a temp segment
+        // which is an expensive operation. So we can't not unlock `segments` after the check and before the insert.
+        //
+        // On the other hand - we do not want to hold write lock during the segment creation.
+        // Solution in the middle - is a upgradable lock. It ensures consistency after the check and allows to perform read operation.
+        let segment_lock = segments.upgradable_read();
 
         let optimizing_segments: Vec<_> = ids
             .iter()
             .cloned()
-            .map(|id| write_segments.get(id))
+            .map(|id| segment_lock.get(id))
             .filter_map(|x| x.cloned())
             .collect();
 
+        // Check if all segments are not under other optimization or some ids are missing
+        let all_segments_ok = optimizing_segments.len() == ids.len()
+            && optimizing_segments
+                .iter()
+                .all(|s| matches!(s, LockedSegment::Original(_)));
+
+        if !all_segments_ok {
+            // Cancel the optimization
+            return Ok(false);
+        }
+
+        let tmp_segment = self.temp_segment()?;
+
+        let proxy_deleted_points = Arc::new(RwLock::new(HashSet::<PointIdType>::new()));
+        let proxy_deleted_indexes = Arc::new(RwLock::new(HashSet::<PayloadKeyType>::new()));
+        let proxy_created_indexes = Arc::new(RwLock::new(HashSet::<PayloadKeyType>::new()));
+
         let proxies = optimizing_segments.iter().map(|sg| {
             ProxySegment::new(
                 sg.clone(),
@@ -155,13 +175,15 @@ pub trait SegmentOptimizer {
             )
         });
 
-        let proxy_ids: Vec<_> = proxies
-            .zip(ids.iter().cloned())
-            .map(|(proxy, idx)| write_segments.swap(proxy, &[idx], false).unwrap())
-            .collect();
+        let proxy_ids: Vec<_> = {
+            // Exclusive lock for the segments operations
+            let mut write_segments = RwLockUpgradableReadGuard::upgrade(segment_lock);
 
-        // Release segments lock
-        drop(write_segments);
+            proxies
+                .zip(ids.iter().cloned())
+                .map(|(proxy, idx)| write_segments.swap(proxy, &[idx], false).unwrap())
+                .collect()
+        };
 
         let mut segment_builder = self.optimized_segment_builder(&optimizing_segments)?;
 

commit 0f91c9a5e29ef9065c79a20e0ace25be898beff8
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Tue Jan 18 15:06:42 2022 +0100

    [WIP] Force optimization stop #31 (#161)
    
    * implement checking stop-flag in the optimization routine
    
    * wip: optimization cancel test
    
    * force optimization stop during the construction of vector index
    
    * fix clippy

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index dde36dcb5..98f53f4d7 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -3,7 +3,7 @@ use crate::collection_manager::holders::segment_holder::{
     LockedSegment, LockedSegmentHolder, SegmentId,
 };
 use crate::config::CollectionParams;
-use crate::operations::types::CollectionResult;
+use crate::operations::types::{CollectionError, CollectionResult};
 use itertools::Itertools;
 use parking_lot::{RwLock, RwLockUpgradableReadGuard};
 use segment::entry::entry_point::SegmentEntry;
@@ -14,8 +14,8 @@ use segment::types::{
     HnswConfig, Indexes, PayloadIndexType, PayloadKeyType, PointIdType, SegmentConfig, StorageType,
 };
 use std::collections::HashSet;
-use std::convert::TryInto;
 use std::path::Path;
+use std::sync::atomic::{AtomicBool, Ordering};
 use std::sync::Arc;
 
 #[derive(Debug, Clone)]
@@ -125,13 +125,182 @@ pub trait SegmentOptimizer {
         )?)
     }
 
+    /// Restores original segments from proxies
+    ///
+    /// # Arguments
+    ///
+    /// * `segments` - segment holder
+    /// * `proxy_ids` - ids of poxy-wrapped segment to restore
+    ///
+    /// # Result
+    ///
+    /// Original segments are pushed into `segments`, proxies removed.
+    /// Returns IDs on restored segments
+    ///
+    fn unwrap_proxy(
+        &self,
+        segments: &LockedSegmentHolder,
+        proxy_ids: &[SegmentId],
+    ) -> CollectionResult<Vec<SegmentId>> {
+        let mut segments_lock = segments.write();
+        let mut restored_segment_ids = vec![];
+        for &proxy_id in proxy_ids {
+            if let Some(proxy_segment_ref) = segments_lock.get(proxy_id) {
+                let locked_proxy_segment = proxy_segment_ref.clone();
+                match locked_proxy_segment {
+                    LockedSegment::Original(_) => {
+                        /* Already unwrapped. It should not actually be here */
+                        log::warn!("Attempt to unwrap raw segment! Should not happen.")
+                    }
+                    LockedSegment::Proxy(proxy_segment) => {
+                        let wrapped_segment = proxy_segment.read().wrapped_segment.clone();
+                        restored_segment_ids.push(segments_lock.swap(
+                            wrapped_segment,
+                            &[proxy_id],
+                            false,
+                        )?);
+                    }
+                }
+            }
+        }
+        Ok(restored_segment_ids)
+    }
+
+    /// Checks if optimization cancellation is requested.
+    fn check_cancellation(&self, stopped: &AtomicBool) -> CollectionResult<()> {
+        if stopped.load(Ordering::Relaxed) {
+            return Err(CollectionError::Cancelled {
+                description: "optimization cancelled by service".to_string(),
+            });
+        }
+        Ok(())
+    }
+
+    /// Unwraps proxy, adds temp segment into collection and returns a `Cancelled` error.
+    ///
+    /// # Arguments
+    ///
+    /// * `segments` - all registered segments of the collection
+    /// * `proxy_ids` - currently used proxies
+    /// * `temp_segment` - currently used temporary segment
+    ///
+    /// # Result
+    ///
+    /// Rolls back back optimization state.
+    /// All processed changes will still be there, but the collection should be returned
+    /// into state before optimization.
+    ///
+    fn handle_cancellation(
+        &self,
+        segments: &LockedSegmentHolder,
+        proxy_ids: &[SegmentId],
+        temp_segment: &LockedSegment,
+    ) -> CollectionResult<()> {
+        self.unwrap_proxy(segments, proxy_ids)?;
+        if temp_segment.get().read().vectors_count() > 0 {
+            let mut write_segments = segments.write();
+            write_segments.add_locked(temp_segment.clone());
+        }
+        Ok(())
+    }
+
+    /// Function to wrap slow part of optimization. Performs proxy rollback in case of cancellation.
+    /// Warn: this function might be _VERY_ CPU intensive,
+    /// so it is necessary to avoid any locks inside this part of the code
+    ///
+    /// # Arguments
+    ///
+    /// * `optimizing_segments` - Segments to optimize
+    /// * `proxy_deleted_points` - Holds a set of points, deleted while optimization was running
+    /// * `proxy_deleted_indexes` - Holds a set of Indexes, deleted while optimization was running
+    /// * `proxy_created_indexes` - Holds a set of Indexes, created while optimization was running
+    /// * `stopped` - flag to check if optimization was cancelled by external thread
+    ///
+    /// # Result
+    ///
+    /// Constructs optimized segment
+    fn build_new_segment(
+        &self,
+        optimizing_segments: &[LockedSegment],
+        proxy_deleted_points: Arc<RwLock<HashSet<PointIdType>>>,
+        proxy_deleted_indexes: Arc<RwLock<HashSet<PayloadKeyType>>>,
+        proxy_created_indexes: Arc<RwLock<HashSet<PayloadKeyType>>>,
+        stopped: &AtomicBool,
+    ) -> CollectionResult<Segment> {
+        let mut segment_builder = self.optimized_segment_builder(optimizing_segments)?;
+
+        self.check_cancellation(stopped)?;
+
+        for segment in optimizing_segments {
+            match segment {
+                LockedSegment::Original(segment_arc) => {
+                    let segment_guard = segment_arc.read();
+                    segment_builder.update_from(&segment_guard, stopped)?;
+                }
+                LockedSegment::Proxy(_) => panic!("Attempt to optimize segment which is already currently under optimization. Should never happen"),
+            }
+        }
+
+        for field in proxy_deleted_indexes.read().iter() {
+            segment_builder.indexed_fields.remove(field);
+        }
+        for field in proxy_created_indexes.read().iter().cloned() {
+            segment_builder.indexed_fields.insert(field);
+        }
+
+        let mut optimized_segment: Segment = segment_builder.build(stopped)?;
+
+        // Delete points in 2 steps
+        // First step - delete all points with read lock
+        // Second step - delete all the rest points with full write lock
+        //
+        // Use collection copy to prevent long time lock of `proxy_deleted_points`
+        let deleted_points_snapshot: Vec<PointIdType> =
+            proxy_deleted_points.read().iter().cloned().collect();
+
+        for &point_id in &deleted_points_snapshot {
+            optimized_segment
+                .delete_point(optimized_segment.version(), point_id)
+                .unwrap();
+        }
+
+        let deleted_indexes = proxy_deleted_indexes.read().iter().cloned().collect_vec();
+        let create_indexes = proxy_created_indexes.read().iter().cloned().collect_vec();
+
+        for delete_field_name in &deleted_indexes {
+            optimized_segment.delete_field_index(optimized_segment.version(), delete_field_name)?;
+            self.check_cancellation(stopped)?;
+        }
+
+        for create_field_name in &create_indexes {
+            optimized_segment.create_field_index(optimized_segment.version(), create_field_name)?;
+            self.check_cancellation(stopped)?;
+        }
+
+        Ok(optimized_segment)
+    }
+
     /// Performs optimization of collections's segments, including:
     ///     - Segment rebuilding
     ///     - Segment joining
+    ///
+    /// # Arguments
+    ///
+    /// * `segments` - segments holder
+    /// * `ids` - list of segment ids to perform optimization on. All segments will be merged into single one
+    /// * `stopped` - flag for early stopping of the optimization.
+    ///               If appears to be `true` - optimization process should be cancelled, all segments unwrapped
+    ///
+    /// # Result
+    ///
+    /// New optimized segment should be added into `segments`.
+    /// If there were any record changes during the optimization - an additional plain segment will be created.
+    ///
     fn optimize(
         &self,
         segments: LockedSegmentHolder,
         ids: Vec<SegmentId>,
+        stopped: &AtomicBool,
     ) -> CollectionResult<bool> {
         // On the one hand - we want to check consistently if all provided segments are
         // available for optimization (not already under one) and we want to do it before creating a temp segment
@@ -185,57 +354,42 @@ pub trait SegmentOptimizer {
                 .collect()
         };
 
-        let mut segment_builder = self.optimized_segment_builder(&optimizing_segments)?;
-
         // ---- SLOW PART -----
-        for segment in optimizing_segments {
-            match segment {
-                LockedSegment::Original(segment_arc) => {
-                    let segment_guard = segment_arc.read();
-                    segment_builder.update_from(&segment_guard)?;
+        let mut optimized_segment = match self.build_new_segment(
+            &optimizing_segments,
+            proxy_deleted_points.clone(),
+            proxy_deleted_indexes.clone(),
+            proxy_created_indexes.clone(),
+            stopped,
+        ) {
+            Ok(segment) => segment,
+            Err(error) => {
+                if matches!(error, CollectionError::Cancelled { .. }) {
+                    self.handle_cancellation(&segments, &proxy_ids, &tmp_segment)?;
                 }
-                LockedSegment::Proxy(_) => panic!("Attempt to optimize segment which is already currently under optimization. Should never happen"),
+                return Err(error);
             }
-        }
-
-        for field in proxy_deleted_indexes.read().iter() {
-            segment_builder.indexed_fields.remove(field);
-        }
-        for field in proxy_created_indexes.read().iter().cloned() {
-            segment_builder.indexed_fields.insert(field);
-        }
-
-        let mut optimized_segment: Segment = segment_builder.try_into()?;
-
-        // Delete points in 2 steps
-        // First step - delete all points with read lock
-        // Second step - delete all the rest points with full write lock
-        let deleted_points_snapshot: HashSet<PointIdType> =
-            proxy_deleted_points.read().iter().cloned().collect();
-
-        for &point_id in &deleted_points_snapshot {
-            optimized_segment
-                .delete_point(optimized_segment.version(), point_id)
-                .unwrap();
-        }
-
-        let deleted_indexes = proxy_deleted_indexes.read().iter().cloned().collect_vec();
-        let create_indexes = proxy_created_indexes.read().iter().cloned().collect_vec();
+        };
 
-        for delete_field_name in &deleted_indexes {
-            optimized_segment.delete_field_index(optimized_segment.version(), delete_field_name)?;
-        }
+        // Avoid unnecessary point removing in the critical section:
+        // - save already removed points while avoiding long read locks
+        // - exclude already removed points from post-optimization removing
+        let already_remove_points = {
+            let mut all_removed_points: HashSet<_> =
+                proxy_deleted_points.read().iter().cloned().collect();
+            for existing_point in optimized_segment.iter_points() {
+                all_removed_points.remove(&existing_point);
+            }
+            all_removed_points
+        };
 
-        for create_field_name in &create_indexes {
-            optimized_segment.create_field_index(optimized_segment.version(), create_field_name)?;
-        }
         // ---- SLOW PART ENDS HERE -----
 
         {
             // This block locks all operations with collection. It should be fast
             let mut write_segments = segments.write();
             let deleted_points = proxy_deleted_points.read();
-            let points_diff = deleted_points_snapshot.difference(&deleted_points);
+            let points_diff = already_remove_points.difference(&deleted_points);
             for &point_id in points_diff {
                 optimized_segment
                     .delete_point(optimized_segment.version(), point_id)

commit 4483ea0d60bb4cf97df1267de6299556674d83fa
Author: Gabriel Velo <gabriel.velo@gmail.com>
Date:   Wed Feb 9 11:46:01 2022 -0300

    fix: #101 Payload type consistency is not enforced.

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 98f53f4d7..865e183ae 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -1,22 +1,26 @@
-use crate::collection_manager::holders::proxy_segment::ProxySegment;
-use crate::collection_manager::holders::segment_holder::{
-    LockedSegment, LockedSegmentHolder, SegmentId,
-};
-use crate::config::CollectionParams;
-use crate::operations::types::{CollectionError, CollectionResult};
+use std::collections::HashSet;
+use std::path::Path;
+use std::sync::atomic::{AtomicBool, Ordering};
+use std::sync::Arc;
+
 use itertools::Itertools;
 use parking_lot::{RwLock, RwLockUpgradableReadGuard};
+
 use segment::entry::entry_point::SegmentEntry;
+use segment::payload_storage::schema_storage::SchemaStorage;
 use segment::segment::Segment;
 use segment::segment_constructor::segment_builder::SegmentBuilder;
 use segment::segment_constructor::simple_segment_constructor::build_simple_segment;
 use segment::types::{
     HnswConfig, Indexes, PayloadIndexType, PayloadKeyType, PointIdType, SegmentConfig, StorageType,
 };
-use std::collections::HashSet;
-use std::path::Path;
-use std::sync::atomic::{AtomicBool, Ordering};
-use std::sync::Arc;
+
+use crate::collection_manager::holders::proxy_segment::ProxySegment;
+use crate::collection_manager::holders::segment_holder::{
+    LockedSegment, LockedSegmentHolder, SegmentId,
+};
+use crate::config::CollectionParams;
+use crate::operations::types::{CollectionError, CollectionResult};
 
 #[derive(Debug, Clone)]
 pub struct OptimizerThresholds {
@@ -56,6 +60,8 @@ pub trait SegmentOptimizer {
         excluded_ids: &HashSet<SegmentId>,
     ) -> Vec<SegmentId>;
 
+    fn schema_store(&self) -> Arc<SchemaStorage>;
+
     /// Build temp segment
     fn temp_segment(&self) -> CollectionResult<LockedSegment> {
         let collection_params = self.collection_params();
@@ -70,6 +76,7 @@ pub trait SegmentOptimizer {
             self.collection_path(),
             config.vector_size,
             config.distance,
+            self.schema_store(),
         )?))
     }
 
@@ -122,6 +129,7 @@ pub trait SegmentOptimizer {
             self.collection_path(),
             self.temp_path(),
             &optimized_config,
+            self.schema_store(),
         )?)
     }
 

commit e45379e4384062e92ee1c9be82c250047464c9ef
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Wed Feb 16 09:59:11 2022 +0100

    Better optimizer error reporting + small bug fixes (#316)
    
    * optimizer error reporting, decouple data removing, optimizator fix
    
    * fmt
    
    * fmt + clippy
    
    * update openapi

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 865e183ae..67ddb6bcf 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -149,7 +149,7 @@ pub trait SegmentOptimizer {
         &self,
         segments: &LockedSegmentHolder,
         proxy_ids: &[SegmentId],
-    ) -> CollectionResult<Vec<SegmentId>> {
+    ) -> Vec<SegmentId> {
         let mut segments_lock = segments.write();
         let mut restored_segment_ids = vec![];
         for &proxy_id in proxy_ids {
@@ -162,16 +162,14 @@ pub trait SegmentOptimizer {
                     }
                     LockedSegment::Proxy(proxy_segment) => {
                         let wrapped_segment = proxy_segment.read().wrapped_segment.clone();
-                        restored_segment_ids.push(segments_lock.swap(
-                            wrapped_segment,
-                            &[proxy_id],
-                            false,
-                        )?);
+                        let (restored_id, _proxies) =
+                            segments_lock.swap(wrapped_segment, &[proxy_id]);
+                        restored_segment_ids.push(restored_id);
                     }
                 }
             }
         }
-        Ok(restored_segment_ids)
+        restored_segment_ids
     }
 
     /// Checks if optimization cancellation is requested.
@@ -203,13 +201,12 @@ pub trait SegmentOptimizer {
         segments: &LockedSegmentHolder,
         proxy_ids: &[SegmentId],
         temp_segment: &LockedSegment,
-    ) -> CollectionResult<()> {
-        self.unwrap_proxy(segments, proxy_ids)?;
+    ) {
+        self.unwrap_proxy(segments, proxy_ids);
         if temp_segment.get().read().vectors_count() > 0 {
             let mut write_segments = segments.write();
             write_segments.add_locked(temp_segment.clone());
         }
-        Ok(())
     }
 
     /// Function to wrap slow part of optimization. Performs proxy rollback in case of cancellation.
@@ -358,7 +355,7 @@ pub trait SegmentOptimizer {
 
             proxies
                 .zip(ids.iter().cloned())
-                .map(|(proxy, idx)| write_segments.swap(proxy, &[idx], false).unwrap())
+                .map(|(proxy, idx)| write_segments.swap(proxy, &[idx]).0)
                 .collect()
         };
 
@@ -373,7 +370,7 @@ pub trait SegmentOptimizer {
             Ok(segment) => segment,
             Err(error) => {
                 if matches!(error, CollectionError::Cancelled { .. }) {
-                    self.handle_cancellation(&segments, &proxy_ids, &tmp_segment)?;
+                    self.handle_cancellation(&segments, &proxy_ids, &tmp_segment);
                 }
                 return Err(error);
             }
@@ -414,7 +411,7 @@ pub trait SegmentOptimizer {
                     .create_field_index(optimized_segment.version(), created_field_name)?;
             }
 
-            write_segments.swap(optimized_segment, &proxy_ids, true)?;
+            let (_, proxies) = write_segments.swap(optimized_segment, &proxy_ids);
 
             let has_appendable_segments = write_segments.random_appendable_segment().is_some();
 
@@ -424,6 +421,12 @@ pub trait SegmentOptimizer {
             } else {
                 tmp_segment.drop_data()?;
             }
+
+            // Only remove data after we ensure the consistency of the collection.
+            // If remove fails - we will till have operational collection with reported error.
+            for proxy in proxies {
+                proxy.drop_data()?;
+            }
         }
         Ok(true)
     }

commit f69a7b740fb57da8ed887f36afb173a3f3846c66
Author: Gabriel Velo <gabriel.velo@gmail.com>
Date:   Mon Mar 21 07:09:10 2022 -0300

    json as payload (#306)
    
    add json as payload
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 67ddb6bcf..b43ce955f 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -1,4 +1,4 @@
-use std::collections::HashSet;
+use std::collections::{HashMap, HashSet};
 use std::path::Path;
 use std::sync::atomic::{AtomicBool, Ordering};
 use std::sync::Arc;
@@ -7,12 +7,12 @@ use itertools::Itertools;
 use parking_lot::{RwLock, RwLockUpgradableReadGuard};
 
 use segment::entry::entry_point::SegmentEntry;
-use segment::payload_storage::schema_storage::SchemaStorage;
 use segment::segment::Segment;
 use segment::segment_constructor::segment_builder::SegmentBuilder;
 use segment::segment_constructor::simple_segment_constructor::build_simple_segment;
 use segment::types::{
-    HnswConfig, Indexes, PayloadIndexType, PayloadKeyType, PointIdType, SegmentConfig, StorageType,
+    HnswConfig, Indexes, PayloadIndexType, PayloadKeyType, PayloadSchemaType, PointIdType,
+    SegmentConfig, StorageType,
 };
 
 use crate::collection_manager::holders::proxy_segment::ProxySegment;
@@ -60,8 +60,6 @@ pub trait SegmentOptimizer {
         excluded_ids: &HashSet<SegmentId>,
     ) -> Vec<SegmentId>;
 
-    fn schema_store(&self) -> Arc<SchemaStorage>;
-
     /// Build temp segment
     fn temp_segment(&self) -> CollectionResult<LockedSegment> {
         let collection_params = self.collection_params();
@@ -76,7 +74,6 @@ pub trait SegmentOptimizer {
             self.collection_path(),
             config.vector_size,
             config.distance,
-            self.schema_store(),
         )?))
     }
 
@@ -129,7 +126,6 @@ pub trait SegmentOptimizer {
             self.collection_path(),
             self.temp_path(),
             &optimized_config,
-            self.schema_store(),
         )?)
     }
 
@@ -229,7 +225,7 @@ pub trait SegmentOptimizer {
         optimizing_segments: &[LockedSegment],
         proxy_deleted_points: Arc<RwLock<HashSet<PointIdType>>>,
         proxy_deleted_indexes: Arc<RwLock<HashSet<PayloadKeyType>>>,
-        proxy_created_indexes: Arc<RwLock<HashSet<PayloadKeyType>>>,
+        proxy_created_indexes: Arc<RwLock<HashMap<PayloadKeyType, PayloadSchemaType>>>,
         stopped: &AtomicBool,
     ) -> CollectionResult<Segment> {
         let mut segment_builder = self.optimized_segment_builder(optimizing_segments)?;
@@ -249,8 +245,10 @@ pub trait SegmentOptimizer {
         for field in proxy_deleted_indexes.read().iter() {
             segment_builder.indexed_fields.remove(field);
         }
-        for field in proxy_created_indexes.read().iter().cloned() {
-            segment_builder.indexed_fields.insert(field);
+        for (field, schema_type) in proxy_created_indexes.read().iter() {
+            segment_builder
+                .indexed_fields
+                .insert(field.to_owned(), schema_type.to_owned());
         }
 
         let mut optimized_segment: Segment = segment_builder.build(stopped)?;
@@ -270,15 +268,19 @@ pub trait SegmentOptimizer {
         }
 
         let deleted_indexes = proxy_deleted_indexes.read().iter().cloned().collect_vec();
-        let create_indexes = proxy_created_indexes.read().iter().cloned().collect_vec();
+        let create_indexes = proxy_created_indexes.read().clone();
 
         for delete_field_name in &deleted_indexes {
             optimized_segment.delete_field_index(optimized_segment.version(), delete_field_name)?;
             self.check_cancellation(stopped)?;
         }
 
-        for create_field_name in &create_indexes {
-            optimized_segment.create_field_index(optimized_segment.version(), create_field_name)?;
+        for (create_field_name, schema_type) in &create_indexes {
+            optimized_segment.create_field_index(
+                optimized_segment.version(),
+                create_field_name,
+                &Some(*schema_type),
+            )?;
             self.check_cancellation(stopped)?;
         }
 
@@ -337,15 +339,18 @@ pub trait SegmentOptimizer {
 
         let proxy_deleted_points = Arc::new(RwLock::new(HashSet::<PointIdType>::new()));
         let proxy_deleted_indexes = Arc::new(RwLock::new(HashSet::<PayloadKeyType>::new()));
-        let proxy_created_indexes = Arc::new(RwLock::new(HashSet::<PayloadKeyType>::new()));
+        let proxy_created_indexes = Arc::new(RwLock::new(HashMap::<
+            PayloadKeyType,
+            PayloadSchemaType,
+        >::new()));
 
         let proxies = optimizing_segments.iter().map(|sg| {
             ProxySegment::new(
                 sg.clone(),
                 tmp_segment.clone(),
                 proxy_deleted_points.clone(),
-                proxy_deleted_indexes.clone(),
                 proxy_created_indexes.clone(),
+                proxy_deleted_indexes.clone(),
             )
         });
 
@@ -406,9 +411,12 @@ pub trait SegmentOptimizer {
                     .delete_field_index(optimized_segment.version(), deleted_field_name)?;
             }
 
-            for created_field_name in proxy_created_indexes.read().iter() {
-                optimized_segment
-                    .create_field_index(optimized_segment.version(), created_field_name)?;
+            for (created_field_name, schema_type) in proxy_created_indexes.read().iter() {
+                optimized_segment.create_field_index(
+                    optimized_segment.version(),
+                    created_field_name,
+                    &Some(*schema_type),
+                )?;
             }
 
             let (_, proxies) = write_segments.swap(optimized_segment, &proxy_ids);

commit 1b458780eb196ebbbd7fb1f6c5d85ce3b15adb64
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Wed Jun 1 17:23:34 2022 +0200

    On disk payload storage (#634)
    
    * implement on-disk payload storage
    
    * fmt + clippy
    
    * config param for on-disk payload storage
    
    * upd openapi definitions
    
    * add integration test with on-disk payload
    
    * fix clippy
    
    * review fixes
    
    * fmt

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index b43ce955f..2ab8faa4e 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -8,11 +8,11 @@ use parking_lot::{RwLock, RwLockUpgradableReadGuard};
 
 use segment::entry::entry_point::SegmentEntry;
 use segment::segment::Segment;
+use segment::segment_constructor::build_segment;
 use segment::segment_constructor::segment_builder::SegmentBuilder;
-use segment::segment_constructor::simple_segment_constructor::build_simple_segment;
 use segment::types::{
-    HnswConfig, Indexes, PayloadIndexType, PayloadKeyType, PayloadSchemaType, PointIdType,
-    SegmentConfig, StorageType,
+    HnswConfig, Indexes, PayloadIndexType, PayloadKeyType, PayloadSchemaType, PayloadStorageType,
+    PointIdType, SegmentConfig, StorageType,
 };
 
 use crate::collection_manager::holders::proxy_segment::ProxySegment;
@@ -69,11 +69,14 @@ pub trait SegmentOptimizer {
             index: Indexes::Plain {},
             payload_index: Some(PayloadIndexType::Plain),
             storage_type: StorageType::InMemory,
+            payload_storage_type: match collection_params.on_disk_payload {
+                true => PayloadStorageType::OnDisk,
+                false => PayloadStorageType::InMemory,
+            },
         };
-        Ok(LockedSegment::new(build_simple_segment(
+        Ok(LockedSegment::new(build_segment(
             self.collection_path(),
-            config.vector_size,
-            config.distance,
+            &config,
         )?))
     }
 
@@ -120,6 +123,10 @@ pub trait SegmentOptimizer {
             } else {
                 StorageType::InMemory
             },
+            payload_storage_type: match collection_params.on_disk_payload {
+                true => PayloadStorageType::OnDisk,
+                false => PayloadStorageType::InMemory,
+            },
         };
 
         Ok(SegmentBuilder::new(

commit 2601c017de71bbb46bc61df256ea8263a8fe23b9
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Wed Jun 1 18:09:38 2022 +0200

    Smarter defaults (#637)
    
    * auto segments number
    
    * auto segments number
    
    * replace vector number limits with vector size limits
    
    * fmt

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 2ab8faa4e..d155ba329 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -12,7 +12,7 @@ use segment::segment_constructor::build_segment;
 use segment::segment_constructor::segment_builder::SegmentBuilder;
 use segment::types::{
     HnswConfig, Indexes, PayloadIndexType, PayloadKeyType, PayloadSchemaType, PayloadStorageType,
-    PointIdType, SegmentConfig, StorageType,
+    PointIdType, SegmentConfig, StorageType, VECTOR_ELEMENT_SIZE,
 };
 
 use crate::collection_manager::holders::proxy_segment::ProxySegment;
@@ -22,6 +22,8 @@ use crate::collection_manager::holders::segment_holder::{
 use crate::config::CollectionParams;
 use crate::operations::types::{CollectionError, CollectionResult};
 
+const BYTES_IN_KB: usize = 1024;
+
 #[derive(Debug, Clone)]
 pub struct OptimizerThresholds {
     pub memmap_threshold: usize,
@@ -85,9 +87,13 @@ pub trait SegmentOptimizer {
         &self,
         optimizing_segments: &[LockedSegment],
     ) -> CollectionResult<SegmentBuilder> {
-        let total_vectors: usize = optimizing_segments
+        let total_vectors_size: usize = optimizing_segments
             .iter()
-            .map(|s| s.get().read().vectors_count())
+            .map(|s| {
+                let segment = s.get();
+                let locked_segment = segment.read();
+                locked_segment.vectors_count() * locked_segment.vector_dim() * VECTOR_ELEMENT_SIZE
+            })
             .sum();
 
         let have_indexed_fields = optimizing_segments
@@ -97,13 +103,15 @@ pub trait SegmentOptimizer {
         let thresholds = self.threshold_config();
         let collection_params = self.collection_params();
 
-        let is_indexed = total_vectors >= thresholds.indexing_threshold;
+        let is_indexed = total_vectors_size >= thresholds.indexing_threshold * BYTES_IN_KB;
 
         // Create structure index only if there is something to index
-        let is_payload_indexed =
-            total_vectors >= thresholds.payload_indexing_threshold && have_indexed_fields;
+        // ToDo: remove deprecated
+        let is_payload_indexed = total_vectors_size
+            >= thresholds.payload_indexing_threshold * BYTES_IN_KB
+            && have_indexed_fields;
 
-        let is_on_disk = total_vectors >= thresholds.memmap_threshold;
+        let is_on_disk = total_vectors_size >= thresholds.memmap_threshold * BYTES_IN_KB;
 
         let optimized_config = SegmentConfig {
             vector_size: collection_params.vector_size,

commit c15981092ac33c7dde9541ab4a2df558e6abe4e6
Author: Gabriel Velo <gabriel.velo@gmail.com>
Date:   Mon Jun 6 12:14:20 2022 -0300

    [WIP] [real-time index] Implement payloadstorage for structpayloadindex (#642)
    
    * [real-time index] Extend FieldIndex enum and StructPayloadIndex with method from PayloadStorage
    
    * [real-time index] add missing remove_point methods
    
    * [real-time index] add new index to FieldIndex enum
    
    * fix compile
    
    * are you happy fmt
    
    * merge load and remove
    
    * fix test generics
    
    * decrement points count
    
    * remove from histogram
    
    * simplify histogram usage
    
    * [real-time index] remove old tests and fix clippy warnings
    
    * histogram: method to derive range by size (#657)
    
    * [real-time index] add histogram based payload_blocks implementation.
    
    * payload blocks
    
    * fmt
    
    * clippy
    
    * [real-time index] refactor Segment to use PayloadIndex instead of PayloadStorage.
    
    * fix tests
    
    * fmt
    
    * clippy
    
    * rename indexes
    
    * remove redundent params
    
    * add struct payload deletion test + fix delete payload in map index
    
    * remove payload threshold
    
    Co-authored-by: Ivan Pleshkov <pleshkov.ivan@gmail.com>
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index d155ba329..c0948caf0 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -11,8 +11,8 @@ use segment::segment::Segment;
 use segment::segment_constructor::build_segment;
 use segment::segment_constructor::segment_builder::SegmentBuilder;
 use segment::types::{
-    HnswConfig, Indexes, PayloadIndexType, PayloadKeyType, PayloadSchemaType, PayloadStorageType,
-    PointIdType, SegmentConfig, StorageType, VECTOR_ELEMENT_SIZE,
+    HnswConfig, Indexes, PayloadKeyType, PayloadSchemaType, PayloadStorageType, PointIdType,
+    SegmentConfig, StorageType, VECTOR_ELEMENT_SIZE,
 };
 
 use crate::collection_manager::holders::proxy_segment::ProxySegment;
@@ -28,7 +28,6 @@ const BYTES_IN_KB: usize = 1024;
 pub struct OptimizerThresholds {
     pub memmap_threshold: usize,
     pub indexing_threshold: usize,
-    pub payload_indexing_threshold: usize,
 }
 
 /// SegmentOptimizer - trait implementing common functionality of the optimizers
@@ -69,7 +68,6 @@ pub trait SegmentOptimizer {
             vector_size: collection_params.vector_size,
             distance: collection_params.distance,
             index: Indexes::Plain {},
-            payload_index: Some(PayloadIndexType::Plain),
             storage_type: StorageType::InMemory,
             payload_storage_type: match collection_params.on_disk_payload {
                 true => PayloadStorageType::OnDisk,
@@ -96,21 +94,11 @@ pub trait SegmentOptimizer {
             })
             .sum();
 
-        let have_indexed_fields = optimizing_segments
-            .iter()
-            .any(|s| !s.get().read().get_indexed_fields().is_empty());
-
         let thresholds = self.threshold_config();
         let collection_params = self.collection_params();
 
         let is_indexed = total_vectors_size >= thresholds.indexing_threshold * BYTES_IN_KB;
 
-        // Create structure index only if there is something to index
-        // ToDo: remove deprecated
-        let is_payload_indexed = total_vectors_size
-            >= thresholds.payload_indexing_threshold * BYTES_IN_KB
-            && have_indexed_fields;
-
         let is_on_disk = total_vectors_size >= thresholds.memmap_threshold * BYTES_IN_KB;
 
         let optimized_config = SegmentConfig {
@@ -121,11 +109,6 @@ pub trait SegmentOptimizer {
             } else {
                 Indexes::Plain {}
             },
-            payload_index: Some(if is_payload_indexed {
-                PayloadIndexType::Struct
-            } else {
-                PayloadIndexType::Plain
-            }),
             storage_type: if is_on_disk {
                 StorageType::Mmap
             } else {

commit 40245c5e306c380a1e3ba5b69d61f1fca7229bea
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Thu Jun 16 12:38:43 2022 +0200

    Cifs dir delete (#705)
    
    * consume objects before removing the data
    
    * fmt
    
    * rm debug code

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index c0948caf0..1de397d00 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -421,18 +421,23 @@ pub trait SegmentOptimizer {
 
             let has_appendable_segments = write_segments.random_appendable_segment().is_some();
 
+            // Release reference counter of the optimized segments
+            drop(optimizing_segments);
             // Append a temp segment to a collection if it is not empty or there is no other appendable segment
             if tmp_segment.get().read().vectors_count() > 0 || !has_appendable_segments {
                 write_segments.add_locked(tmp_segment);
+                // Only remove data after we ensure the consistency of the collection.
+                // If remove fails - we will still have operational collection with reported error.
+                for proxy in proxies {
+                    proxy.drop_data()?;
+                }
             } else {
+                // Proxy contains pointer to the `tmp_segment`, so they should be removed first
+                for proxy in proxies {
+                    proxy.drop_data()?;
+                }
                 tmp_segment.drop_data()?;
             }
-
-            // Only remove data after we ensure the consistency of the collection.
-            // If remove fails - we will till have operational collection with reported error.
-            for proxy in proxies {
-                proxy.drop_data()?;
-            }
         }
         Ok(true)
     }

commit 10b468620ccbc655f321f21ec1de0b222c7b732f
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Thu Jun 23 20:19:29 2022 +0200

    Storage bug fixes (#736)
    
    * fix: dont use shared tombstone set to estimate number of removed vectors
    
    * fix: unlock collection before removing temp segments and attempt removal with timeout
    
    * clippy & fmt
    
    * review fixes
    
    * fmt
    
    * fmt

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 1de397d00..aa3bedd5f 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -395,7 +395,7 @@ pub trait SegmentOptimizer {
 
         {
             // This block locks all operations with collection. It should be fast
-            let mut write_segments = segments.write();
+            let mut write_segments_guard = segments.write();
             let deleted_points = proxy_deleted_points.read();
             let points_diff = already_remove_points.difference(&deleted_points);
             for &point_id in points_diff {
@@ -417,21 +417,31 @@ pub trait SegmentOptimizer {
                 )?;
             }
 
-            let (_, proxies) = write_segments.swap(optimized_segment, &proxy_ids);
+            let (_, proxies) = write_segments_guard.swap(optimized_segment, &proxy_ids);
 
-            let has_appendable_segments = write_segments.random_appendable_segment().is_some();
+            let has_appendable_segments =
+                write_segments_guard.random_appendable_segment().is_some();
 
             // Release reference counter of the optimized segments
             drop(optimizing_segments);
             // Append a temp segment to a collection if it is not empty or there is no other appendable segment
             if tmp_segment.get().read().vectors_count() > 0 || !has_appendable_segments {
-                write_segments.add_locked(tmp_segment);
+                write_segments_guard.add_locked(tmp_segment);
+
+                // unlock collection for search and updates
+                drop(write_segments_guard);
+                // After the collection is unlocked - we can remove data as slow as we want.
+
                 // Only remove data after we ensure the consistency of the collection.
                 // If remove fails - we will still have operational collection with reported error.
                 for proxy in proxies {
                     proxy.drop_data()?;
                 }
             } else {
+                // unlock collection for search and updates
+                drop(write_segments_guard);
+                // After the collection is unlocked - we can remove data as slow as we want.
+
                 // Proxy contains pointer to the `tmp_segment`, so they should be removed first
                 for proxy in proxies {
                     proxy.drop_data()?;

commit 850e937c2a883e87622b43b3603be9ee1aaf02af
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Mon Jun 27 15:17:09 2022 +0200

    Storage points tracking refactoring (#750)
    
    * segment refactoring
    
    * rm points iterator
    
    * fmt

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index aa3bedd5f..965c8ae51 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -90,7 +90,7 @@ pub trait SegmentOptimizer {
             .map(|s| {
                 let segment = s.get();
                 let locked_segment = segment.read();
-                locked_segment.vectors_count() * locked_segment.vector_dim() * VECTOR_ELEMENT_SIZE
+                locked_segment.points_count() * locked_segment.vector_dim() * VECTOR_ELEMENT_SIZE
             })
             .sum();
 
@@ -197,7 +197,7 @@ pub trait SegmentOptimizer {
         temp_segment: &LockedSegment,
     ) {
         self.unwrap_proxy(segments, proxy_ids);
-        if temp_segment.get().read().vectors_count() > 0 {
+        if temp_segment.get().read().points_count() > 0 {
             let mut write_segments = segments.write();
             write_segments.add_locked(temp_segment.clone());
         }
@@ -425,7 +425,7 @@ pub trait SegmentOptimizer {
             // Release reference counter of the optimized segments
             drop(optimizing_segments);
             // Append a temp segment to a collection if it is not empty or there is no other appendable segment
-            if tmp_segment.get().read().vectors_count() > 0 || !has_appendable_segments {
+            if tmp_segment.get().read().points_count() > 0 || !has_appendable_segments {
                 write_segments_guard.add_locked(tmp_segment);
 
                 // unlock collection for search and updates

commit e983b07a1521cd47771b63006defe54f74d181ce
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Sun Jul 3 01:14:05 2022 +0200

    Parallel hnsw building (#773)
    
    * parallel hnsw building
    
    * improve hnsw payload blocks condition
    
    * update indexing optimizer condition
    
    * fmt

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 965c8ae51..e9da14e00 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -26,6 +26,7 @@ const BYTES_IN_KB: usize = 1024;
 
 #[derive(Debug, Clone)]
 pub struct OptimizerThresholds {
+    pub max_segment_size: usize,
     pub memmap_threshold: usize,
     pub indexing_threshold: usize,
 }

commit 026bd040b001f1c66e16fc911322f1f182d1cf0f
Author: Egor Ivkov <e.o.ivkov@gmail.com>
Date:   Fri Jul 15 15:42:25 2022 +0300

    Add import formatting rules (#820)
    
    * Add import formatting rules
    
    * Review fix: update rusty hook

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index e9da14e00..c304c3390 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -5,7 +5,6 @@ use std::sync::Arc;
 
 use itertools::Itertools;
 use parking_lot::{RwLock, RwLockUpgradableReadGuard};
-
 use segment::entry::entry_point::SegmentEntry;
 use segment::segment::Segment;
 use segment::segment_constructor::build_segment;

commit dc4cccde3d14e18cf6eac51b64909889f1c64dce
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Sun Jul 31 13:03:53 2022 +0200

    Prevent Vector size zero (#876)

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index c304c3390..4a6922c20 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -65,7 +65,7 @@ pub trait SegmentOptimizer {
     fn temp_segment(&self) -> CollectionResult<LockedSegment> {
         let collection_params = self.collection_params();
         let config = SegmentConfig {
-            vector_size: collection_params.vector_size,
+            vector_size: collection_params.vector_size.get() as usize,
             distance: collection_params.distance,
             index: Indexes::Plain {},
             storage_type: StorageType::InMemory,
@@ -102,7 +102,7 @@ pub trait SegmentOptimizer {
         let is_on_disk = total_vectors_size >= thresholds.memmap_threshold * BYTES_IN_KB;
 
         let optimized_config = SegmentConfig {
-            vector_size: collection_params.vector_size,
+            vector_size: collection_params.vector_size.get() as usize,
             distance: collection_params.distance,
             index: if is_indexed {
                 Indexes::Hnsw(self.hnsw_config())

commit 5728a0e670ed61fa0abbeec89d4ef735e2c34f03
Author: Ivan Pleshkov <pleshkov.ivan@gmail.com>
Date:   Tue Aug 2 13:30:31 2022 +0400

    optimizers and indices telemetry (#892)

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 4a6922c20..d5324ecc2 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -4,11 +4,12 @@ use std::sync::atomic::{AtomicBool, Ordering};
 use std::sync::Arc;
 
 use itertools::Itertools;
-use parking_lot::{RwLock, RwLockUpgradableReadGuard};
+use parking_lot::{Mutex, RwLock, RwLockUpgradableReadGuard};
 use segment::entry::entry_point::SegmentEntry;
 use segment::segment::Segment;
 use segment::segment_constructor::build_segment;
 use segment::segment_constructor::segment_builder::SegmentBuilder;
+use segment::telemetry::{TelemetryOperationAggregator, TelemetryOperationTimer};
 use segment::types::{
     HnswConfig, Indexes, PayloadKeyType, PayloadSchemaType, PayloadStorageType, PointIdType,
     SegmentConfig, StorageType, VECTOR_ELEMENT_SIZE,
@@ -20,6 +21,7 @@ use crate::collection_manager::holders::segment_holder::{
 };
 use crate::config::CollectionParams;
 use crate::operations::types::{CollectionError, CollectionResult};
+use crate::telemetry::OptimizerTelemetry;
 
 const BYTES_IN_KB: usize = 1024;
 
@@ -61,6 +63,10 @@ pub trait SegmentOptimizer {
         excluded_ids: &HashSet<SegmentId>,
     ) -> Vec<SegmentId>;
 
+    fn get_telemetry_data(&self) -> OptimizerTelemetry;
+
+    fn get_telemetry_counter(&self) -> Arc<Mutex<TelemetryOperationAggregator>>;
+
     /// Build temp segment
     fn temp_segment(&self) -> CollectionResult<LockedSegment> {
         let collection_params = self.collection_params();
@@ -307,6 +313,9 @@ pub trait SegmentOptimizer {
         ids: Vec<SegmentId>,
         stopped: &AtomicBool,
     ) -> CollectionResult<bool> {
+        let mut timer = TelemetryOperationTimer::new(&self.get_telemetry_counter());
+        timer.set_success(false);
+
         // On the one hand - we want to check consistently if all provided segments are
         // available for optimization (not already under one) and we want to do it before creating a temp segment
         // which is an expensive operation. So we can't not unlock `segments` after the check and before the insert.
@@ -449,6 +458,7 @@ pub trait SegmentOptimizer {
                 tmp_segment.drop_data()?;
             }
         }
+        timer.set_success(true);
         Ok(true)
     }
 }

commit 88c672475e64cfafacbb7d812129d1986387d870
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Wed Aug 3 16:45:34 2022 +0200

    Fix integration test (#903)
    
    * fix integration test problems + overflow
    
    * fmt

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index d5324ecc2..42906d732 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -103,9 +103,11 @@ pub trait SegmentOptimizer {
         let thresholds = self.threshold_config();
         let collection_params = self.collection_params();
 
-        let is_indexed = total_vectors_size >= thresholds.indexing_threshold * BYTES_IN_KB;
+        let is_indexed =
+            total_vectors_size >= thresholds.indexing_threshold.saturating_mul(BYTES_IN_KB);
 
-        let is_on_disk = total_vectors_size >= thresholds.memmap_threshold * BYTES_IN_KB;
+        let is_on_disk =
+            total_vectors_size >= thresholds.memmap_threshold.saturating_mul(BYTES_IN_KB);
 
         let optimized_config = SegmentConfig {
             vector_size: collection_params.vector_size.get() as usize,

commit b9eee55a9fb6d53572622f62756a80e62484009e
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Thu Sep 1 12:50:12 2022 +0200

    Full text search (#963)
    
    * allow additional params for payload field index
    
    * fmt
    
    * wip: full text index building
    
    * fmt
    
    * text search request
    
    * text search request
    
    * full text index persitance and loading
    
    * fmt
    
    * enable fts index in mapping
    
    * clippy
    
    * fix tests + add integration test
    
    * review fixes: extend payload index test
    
    * revert incedental change

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 42906d732..7fe68357b 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -11,7 +11,7 @@ use segment::segment_constructor::build_segment;
 use segment::segment_constructor::segment_builder::SegmentBuilder;
 use segment::telemetry::{TelemetryOperationAggregator, TelemetryOperationTimer};
 use segment::types::{
-    HnswConfig, Indexes, PayloadKeyType, PayloadSchemaType, PayloadStorageType, PointIdType,
+    HnswConfig, Indexes, PayloadFieldSchema, PayloadKeyType, PayloadStorageType, PointIdType,
     SegmentConfig, StorageType, VECTOR_ELEMENT_SIZE,
 };
 
@@ -231,7 +231,7 @@ pub trait SegmentOptimizer {
         optimizing_segments: &[LockedSegment],
         proxy_deleted_points: Arc<RwLock<HashSet<PointIdType>>>,
         proxy_deleted_indexes: Arc<RwLock<HashSet<PayloadKeyType>>>,
-        proxy_created_indexes: Arc<RwLock<HashMap<PayloadKeyType, PayloadSchemaType>>>,
+        proxy_created_indexes: Arc<RwLock<HashMap<PayloadKeyType, PayloadFieldSchema>>>,
         stopped: &AtomicBool,
     ) -> CollectionResult<Segment> {
         let mut segment_builder = self.optimized_segment_builder(optimizing_segments)?;
@@ -281,11 +281,11 @@ pub trait SegmentOptimizer {
             self.check_cancellation(stopped)?;
         }
 
-        for (create_field_name, schema_type) in &create_indexes {
+        for (create_field_name, schema) in create_indexes {
             optimized_segment.create_field_index(
                 optimized_segment.version(),
-                create_field_name,
-                &Some(*schema_type),
+                &create_field_name,
+                Some(&schema),
             )?;
             self.check_cancellation(stopped)?;
         }
@@ -350,7 +350,7 @@ pub trait SegmentOptimizer {
         let proxy_deleted_indexes = Arc::new(RwLock::new(HashSet::<PayloadKeyType>::new()));
         let proxy_created_indexes = Arc::new(RwLock::new(HashMap::<
             PayloadKeyType,
-            PayloadSchemaType,
+            PayloadFieldSchema,
         >::new()));
 
         let proxies = optimizing_segments.iter().map(|sg| {
@@ -424,7 +424,7 @@ pub trait SegmentOptimizer {
                 optimized_segment.create_field_index(
                     optimized_segment.version(),
                     created_field_name,
-                    &Some(*schema_type),
+                    Some(schema_type),
                 )?;
             }
 

commit f6b21861939744e054a861d9771608b7e6b614e7
Author: Ivan Pleshkov <pleshkov.ivan@gmail.com>
Date:   Sun Sep 11 22:59:23 2022 +0400

    [WIP] Many named vectors per point (#958)
    
    * many named vectors per point (segment-level)
    
    * operation result for dim function
    
    * beautifulized vector name
    
    * fix naming bug
    
    * segment version migration
    
    * fmt
    
    * add segment tests
    
    * are you happy clippy
    
    * fix build
    
    * [WIP] many named vectors per point (collection-level) (#975)
    
    * config and search
    
    * fix placeholders for proxy segment move
    
    * remove VectorType from collection
    
    * are you happy fmt
    
    * vectors in grps messages
    
    * create collections with vectors
    
    * segment holder fixes
    
    * are you happy fmt
    
    * remove default vector name placeholders
    
    * are you happy fmt
    
    * are you happy clippy
    
    * fix build
    
    * fix web api
    
    * are you happy clippy
    
    * are you happy fmt
    
    * record vector&vectors
    
    * openapi update
    
    * fix openapi integration tests
    
    * segment builder fix todo
    
    * vector names for update from segment
    
    * remove unwrap
    
    * backward compatibility
    
    * upd openapi
    
    * backward compatible PointStruct
    
    * upd openapi
    
    * fix record back-comp
    
    * fmt
    
    * vector configuration backward compatibility
    
    * fix vetor storage size estimation
    
    * fmt
    
    * multi-vec segment test + index test
    
    * fmt
    
    * api integration tests
    
    * [WIP] Named vectors struct (#1002)
    
    * move to separate file
    
    * named vectors as struct
    
    * use cow
    
    * fix build
    
    * keys iterator
    
    * avoid copy in PointStruct -> get_vectors
    
    * avoid another copy
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 7fe68357b..a712f9793 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -71,8 +71,7 @@ pub trait SegmentOptimizer {
     fn temp_segment(&self) -> CollectionResult<LockedSegment> {
         let collection_params = self.collection_params();
         let config = SegmentConfig {
-            vector_size: collection_params.vector_size.get() as usize,
-            distance: collection_params.distance,
+            vector_data: collection_params.get_all_vector_params()?,
             index: Indexes::Plain {},
             storage_type: StorageType::InMemory,
             payload_storage_type: match collection_params.on_disk_payload {
@@ -96,7 +95,14 @@ pub trait SegmentOptimizer {
             .map(|s| {
                 let segment = s.get();
                 let locked_segment = segment.read();
-                locked_segment.points_count() * locked_segment.vector_dim() * VECTOR_ELEMENT_SIZE
+                locked_segment.points_count()
+                    * locked_segment
+                        .vector_dims()
+                        .values()
+                        .max()
+                        .copied()
+                        .unwrap_or(0)
+                    * VECTOR_ELEMENT_SIZE
             })
             .sum();
 
@@ -110,8 +116,7 @@ pub trait SegmentOptimizer {
             total_vectors_size >= thresholds.memmap_threshold.saturating_mul(BYTES_IN_KB);
 
         let optimized_config = SegmentConfig {
-            vector_size: collection_params.vector_size.get() as usize,
-            distance: collection_params.distance,
+            vector_data: collection_params.get_all_vector_params()?,
             index: if is_indexed {
                 Indexes::Hnsw(self.hnsw_config())
             } else {

commit c333e95019122078be90d58e32e20715725aca2a
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Fri Sep 23 14:37:33 2022 +0200

    fix the filter-based points remove operation in proxy segment (#1050)
    
    * fix the filter-based points remove operation in proxy segment
    
    * propagate primary clause in proxy segment

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index a712f9793..fcf0fb060 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -413,7 +413,7 @@ pub trait SegmentOptimizer {
             // This block locks all operations with collection. It should be fast
             let mut write_segments_guard = segments.write();
             let deleted_points = proxy_deleted_points.read();
-            let points_diff = already_remove_points.difference(&deleted_points);
+            let points_diff = deleted_points.difference(&already_remove_points);
             for &point_id in points_diff {
                 optimized_segment
                     .delete_point(optimized_segment.version(), point_id)

commit ef7b4ac2a74c50c9e7a89984bd11e238e240ced4
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Tue Oct 18 09:55:16 2022 +0200

    propagate payload index to temporary segment during optimization (#1133)

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index fcf0fb060..cef77e730 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -358,27 +358,40 @@ pub trait SegmentOptimizer {
             PayloadFieldSchema,
         >::new()));
 
-        let proxies = optimizing_segments.iter().map(|sg| {
-            ProxySegment::new(
+        let mut proxies = Vec::new();
+        for sg in optimizing_segments.iter() {
+            let mut proxy = ProxySegment::new(
                 sg.clone(),
                 tmp_segment.clone(),
                 proxy_deleted_points.clone(),
                 proxy_created_indexes.clone(),
                 proxy_deleted_indexes.clone(),
-            )
-        });
+            );
+            // Wrapped segment is fresh, so it has no operations
+            // Operation with number 0 will be applied
+            let op_num = 0;
+            proxy.replicate_field_indexes(op_num)?;
+            proxies.push(proxy);
+        }
 
         let proxy_ids: Vec<_> = {
-            // Exclusive lock for the segments operations
+            // Exclusive lock for the segments operations.
             let mut write_segments = RwLockUpgradableReadGuard::upgrade(segment_lock);
-
-            proxies
-                .zip(ids.iter().cloned())
-                .map(|(proxy, idx)| write_segments.swap(proxy, &[idx]).0)
-                .collect()
+            let mut proxy_ids = Vec::new();
+            for (mut proxy, idx) in proxies.into_iter().zip(ids.iter().cloned()) {
+                // replicate_field_indexes for the second time,
+                // because optimized segments could have been changed.
+                // The probability is small, though,
+                // so we can afford this operation under the full collection write lock
+                let op_num = 0;
+                proxy.replicate_field_indexes(op_num)?; // Slow only in case the index is change in the gap between two calls
+                proxy_ids.push(write_segments.swap(proxy, &[idx]).0);
+            }
+            proxy_ids
         };
 
         // ---- SLOW PART -----
+
         let mut optimized_segment = match self.build_new_segment(
             &optimizing_segments,
             proxy_deleted_points.clone(),

commit 516dcd7020e2f54d91ecdda87e08333b17d85574
Author: Ivan Pleshkov <pleshkov.ivan@gmail.com>
Date:   Sun Oct 23 02:48:55 2022 +0400

    Telemetry level of detail (#1049)
    
    * telemetry level of detail
    
    * rename duration aggregator
    
    * are you happy fmt
    
    * move total searches sum
    
    * separate levels
    
    * optional bucket size
    
    * search telemetry improvements
    
    * separate web telemetry into methods
    
    * tonic telemetry methods
    
    * merge optimizations
    
    * are you happy fmt
    
    * better rounding
    
    * qdrant configs on level 1
    
    * provide collection params
    
    * add peers count
    
    * collection points count
    
    * update openapi
    
    * use pattern in actix telemetry
    
    * are you happy fmt
    
    * merge dev
    
    * are you happy fmt
    
    * fix merge conflicts
    
    * update openapi
    
    * fix build
    
    * are you happy fmt
    
    * add exact searches statistics
    
    * process replica set
    
    * update openapi
    
    * fix wrong name
    
    * fix naming
    
    * fix unwrap
    
    * review
    
    * fmt
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index cef77e730..9e68c9780 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -5,11 +5,13 @@ use std::sync::Arc;
 
 use itertools::Itertools;
 use parking_lot::{Mutex, RwLock, RwLockUpgradableReadGuard};
+use segment::common::operation_time_statistics::{
+    OperationDurationStatistics, OperationDurationsAggregator, ScopeDurationMeasurer,
+};
 use segment::entry::entry_point::SegmentEntry;
 use segment::segment::Segment;
 use segment::segment_constructor::build_segment;
 use segment::segment_constructor::segment_builder::SegmentBuilder;
-use segment::telemetry::{TelemetryOperationAggregator, TelemetryOperationTimer};
 use segment::types::{
     HnswConfig, Indexes, PayloadFieldSchema, PayloadKeyType, PayloadStorageType, PointIdType,
     SegmentConfig, StorageType, VECTOR_ELEMENT_SIZE,
@@ -21,7 +23,6 @@ use crate::collection_manager::holders::segment_holder::{
 };
 use crate::config::CollectionParams;
 use crate::operations::types::{CollectionError, CollectionResult};
-use crate::telemetry::OptimizerTelemetry;
 
 const BYTES_IN_KB: usize = 1024;
 
@@ -63,9 +64,9 @@ pub trait SegmentOptimizer {
         excluded_ids: &HashSet<SegmentId>,
     ) -> Vec<SegmentId>;
 
-    fn get_telemetry_data(&self) -> OptimizerTelemetry;
+    fn get_telemetry_data(&self) -> OperationDurationStatistics;
 
-    fn get_telemetry_counter(&self) -> Arc<Mutex<TelemetryOperationAggregator>>;
+    fn get_telemetry_counter(&self) -> Arc<Mutex<OperationDurationsAggregator>>;
 
     /// Build temp segment
     fn temp_segment(&self) -> CollectionResult<LockedSegment> {
@@ -320,7 +321,7 @@ pub trait SegmentOptimizer {
         ids: Vec<SegmentId>,
         stopped: &AtomicBool,
     ) -> CollectionResult<bool> {
-        let mut timer = TelemetryOperationTimer::new(&self.get_telemetry_counter());
+        let mut timer = ScopeDurationMeasurer::new(&self.get_telemetry_counter());
         timer.set_success(false);
 
         // On the one hand - we want to check consistently if all provided segments are

commit 8e8ed800f4cb4995d3449a6c1de0f41960042c8b
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Wed Dec 14 10:11:46 2022 +0100

    Shard build refactoring (#1280)
    
    * refactor some code of shard creation and optimizers
    
    * Shard build refactoring debug (#1279)
    
    * add debig code
    
    * debug code
    
    * debug code
    
    * debug code
    
    * debug code
    
    * debug code
    
    * debug code
    
    * debug code
    
    * debug code
    
    * debug code
    
    * debug code
    
    * refactor stop-checking during the optimization
    
    * remove debug logs
    
    * improve delete-renaming schema
    
    * fmt
    
    * move collection file removing to the async task
    
    * rename check_optimization_stopped into more general check_process_stopped

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 9e68c9780..e84de4b91 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -8,7 +8,7 @@ use parking_lot::{Mutex, RwLock, RwLockUpgradableReadGuard};
 use segment::common::operation_time_statistics::{
     OperationDurationStatistics, OperationDurationsAggregator, ScopeDurationMeasurer,
 };
-use segment::entry::entry_point::SegmentEntry;
+use segment::entry::entry_point::{check_process_stopped, SegmentEntry};
 use segment::segment::Segment;
 use segment::segment_constructor::build_segment;
 use segment::segment_constructor::segment_builder::SegmentBuilder;
@@ -321,6 +321,8 @@ pub trait SegmentOptimizer {
         ids: Vec<SegmentId>,
         stopped: &AtomicBool,
     ) -> CollectionResult<bool> {
+        check_process_stopped(stopped)?;
+
         let mut timer = ScopeDurationMeasurer::new(&self.get_telemetry_counter());
         timer.set_success(false);
 
@@ -350,6 +352,8 @@ pub trait SegmentOptimizer {
             return Ok(false);
         }
 
+        check_process_stopped(stopped)?;
+
         let tmp_segment = self.temp_segment()?;
 
         let proxy_deleted_points = Arc::new(RwLock::new(HashSet::<PointIdType>::new()));
@@ -391,6 +395,11 @@ pub trait SegmentOptimizer {
             proxy_ids
         };
 
+        check_process_stopped(stopped).map_err(|error| {
+            self.handle_cancellation(&segments, &proxy_ids, &tmp_segment);
+            error
+        })?;
+
         // ---- SLOW PART -----
 
         let mut optimized_segment = match self.build_new_segment(
@@ -423,6 +432,11 @@ pub trait SegmentOptimizer {
 
         // ---- SLOW PART ENDS HERE -----
 
+        check_process_stopped(stopped).map_err(|error| {
+            self.handle_cancellation(&segments, &proxy_ids, &tmp_segment);
+            error
+        })?;
+
         {
             // This block locks all operations with collection. It should be fast
             let mut write_segments_guard = segments.write();

commit 128e49fcc3633e361df33818de6cca0aab95da10
Author: Ivan Pleshkov <pleshkov.ivan@gmail.com>
Date:   Fri Mar 3 20:46:17 2023 +0400

    integrate quantized data to storages (#1311)
    
    * integrate quantized data to storages
    
    * revert gitignore
    
    * are you happy clippy
    
    * quantize in optimizer
    
    * provide flag
    
    * fix segfault
    
    * skip quantization flag, update scores
    
    * use quantization flag
    
    * are you happy fmt
    
    * use quantization flag
    
    * quantized search test
    
    * are you happy fmt
    
    * refactor test, refactor scorer choosing
    
    * are you happy fmt
    
    * run quantization on segment builder
    
    * decrease testing parameters
    
    * simplify segment
    
    * update version
    
    * remove use_quantization flag
    
    * provide quantization config
    
    * quantization version up
    
    * euclid dist
    
    * add euclid test
    
    * saveload
    
    * fix initialization bugs
    
    * quantization lib version up
    
    * fix arm build
    
    * refactor scorer selecting
    
    * quant lib version up
    
    * are you happy fmt
    
    * are you happy fmt
    
    * are you happy clippy
    
    * add save/load test for simple storage
    
    * add comments
    
    * quantiles
    
    * quantization mmap
    
    * remove f32
    
    * mmap test
    
    * fix mmap slice
    
    * fix mmap test
    
    * use chunks for quantization storage
    
    * fix build
    
    * are you happy fmt
    
    * update quantization library
    
    * update quantization lib
    
    * update quantization lib
    
    * integrate api changes
    
    * are you happy fmt
    
    * change quantization api
    
    * additional checks in tests
    
    * update quantization version
    
    * fix unit tests
    
    * add quantization to storage config
    
    * use quantization for all cardinality search cases
    
    * Integrate quantization suggestions 2 (#1520)
    
    * review api
    
    * wip: refactor quantization integrations
    
    * wip: refactor quantization integrations
    
    * wip: fmt
    
    * include quantization into snapshot
    
    * fmt
    
    ---------
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index e84de4b91..07747793e 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -14,7 +14,7 @@ use segment::segment_constructor::build_segment;
 use segment::segment_constructor::segment_builder::SegmentBuilder;
 use segment::types::{
     HnswConfig, Indexes, PayloadFieldSchema, PayloadKeyType, PayloadStorageType, PointIdType,
-    SegmentConfig, StorageType, VECTOR_ELEMENT_SIZE,
+    QuantizationConfig, SegmentConfig, StorageType, VECTOR_ELEMENT_SIZE,
 };
 
 use crate::collection_manager::holders::proxy_segment::ProxySegment;
@@ -54,6 +54,9 @@ pub trait SegmentOptimizer {
     /// Get HNSW config
     fn hnsw_config(&self) -> HnswConfig;
 
+    /// Get quantization config
+    fn quantization_config(&self) -> Option<QuantizationConfig>;
+
     /// Get thresholds configuration for the current optimizer
     fn threshold_config(&self) -> &OptimizerThresholds;
 
@@ -79,6 +82,7 @@ pub trait SegmentOptimizer {
                 true => PayloadStorageType::OnDisk,
                 false => PayloadStorageType::InMemory,
             },
+            quantization_config: None,
         };
         Ok(LockedSegment::new(build_segment(
             self.collection_path(),
@@ -132,6 +136,12 @@ pub trait SegmentOptimizer {
                 true => PayloadStorageType::OnDisk,
                 false => PayloadStorageType::InMemory,
             },
+            quantization_config: if is_indexed {
+                // TODO: separate config for applying quantization
+                self.quantization_config()
+            } else {
+                Default::default()
+            },
         };
 
         Ok(SegmentBuilder::new(

commit 66ba8f17af136554e5a5a707c31d8d1fd801b70c
Author: Tim Vise <tim+github@visee.me>
Date:   Mon Apr 10 17:16:56 2023 +0200

    Add vector specific HNSW configuration (#1675)
    
    * Validate VectorConfig/VectorParams, remove obsolete validation
    
    * Add HNSW config diff to vector parameters
    
    * Validate params in collection config
    
    * Add HNSW config to segment vector data config
    
    * Add VectorsConfig params iterator for more elegant conversions
    
    * Prefer vector HNSW config over collection config for building HNSW index
    
    * Base segment vector param HNSW config on collection config
    
    * General improvements
    
    * Rewrite HNSW ef_construct extract function to also consider vector configs
    
    * Update OpenAPI specification
    
    * Add test to check if vector specific HNSW config is persisted
    
    * review changes
    
    * review changes
    
    * Regenerate gRPC docs
    
    * Fix test on Windows
    
    * Regenerate OpenAPI specification
    
    ---------
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 07747793e..3b25944c1 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -75,7 +75,7 @@ pub trait SegmentOptimizer {
     fn temp_segment(&self) -> CollectionResult<LockedSegment> {
         let collection_params = self.collection_params();
         let config = SegmentConfig {
-            vector_data: collection_params.get_all_vector_params()?,
+            vector_data: collection_params.get_all_vector_params(&self.hnsw_config())?,
             index: Indexes::Plain {},
             storage_type: StorageType::InMemory,
             payload_storage_type: match collection_params.on_disk_payload {
@@ -121,7 +121,7 @@ pub trait SegmentOptimizer {
             total_vectors_size >= thresholds.memmap_threshold.saturating_mul(BYTES_IN_KB);
 
         let optimized_config = SegmentConfig {
-            vector_data: collection_params.get_all_vector_params()?,
+            vector_data: collection_params.get_all_vector_params(&self.hnsw_config())?,
             index: if is_indexed {
                 Indexes::Hnsw(self.hnsw_config())
             } else {

commit 868626f409a7bcc4e2537dcf69b9b4bbe2c10208
Author: Tim Vise <tim+github@visee.me>
Date:   Mon Apr 10 21:39:43 2023 +0200

    Add vector specific quantization configuration (#1680)
    
    * Add QuantizationConfigDiff type
    
    * Add quantization config diff to vector parameters
    
    * Prefer vector config over collection config for quantization
    
    * Update OpenAPI specification
    
    * Validate quantization configuration quantile in 0.5-1.0 range
    
    As per https://github.com/qdrant/qdrant/pull/1681
    
    * Add test if check if vector specific quantization config is persisted
    
    * Alias quantization to quantization_config in vector parameters
    
    * Remove quantization config diff, use full vector specific config instead
    
    * Regenerate OpenAPI specification and gRPC docs
    
    * Fix compilation error
    
    * Add error handling to quantization config conversions
    
    * Fix quantization integration test, make HNSW test stricter

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 3b25944c1..d2117896e 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -75,7 +75,8 @@ pub trait SegmentOptimizer {
     fn temp_segment(&self) -> CollectionResult<LockedSegment> {
         let collection_params = self.collection_params();
         let config = SegmentConfig {
-            vector_data: collection_params.get_all_vector_params(&self.hnsw_config())?,
+            vector_data: collection_params
+                .get_all_vector_params(&self.hnsw_config(), self.quantization_config().as_ref())?,
             index: Indexes::Plain {},
             storage_type: StorageType::InMemory,
             payload_storage_type: match collection_params.on_disk_payload {
@@ -121,7 +122,8 @@ pub trait SegmentOptimizer {
             total_vectors_size >= thresholds.memmap_threshold.saturating_mul(BYTES_IN_KB);
 
         let optimized_config = SegmentConfig {
-            vector_data: collection_params.get_all_vector_params(&self.hnsw_config())?,
+            vector_data: collection_params
+                .get_all_vector_params(&self.hnsw_config(), self.quantization_config().as_ref())?,
             index: if is_indexed {
                 Indexes::Hnsw(self.hnsw_config())
             } else {

commit 1c85c9b2359c81897da57ea7dd5e9f0bdbf67791
Author: Tim Vise <tim+github@visee.me>
Date:   Fri Apr 28 10:36:58 2023 +0200

    Add optimizer for many deleted points, make aware of deleted points and vectors (#1758)
    
    * Minor collection optimizer cleanup
    
    * Make optimizers better aware of available vs soft deleted points
    
    * Fix incorrect deleted state on proxy segment for double delete
    
    * Rename upsert_vector to upsert_point, because we work with points
    
    * Refactor point methods for more clear and consistent naming
    
    * Replace internal_size in IdTracker with total_point_count
    
    * Keep track of vector deletion count on storage creation
    
    * Add sparse index optimizer, to optimize indexes with high deletion count
    
    * Add minimum vector count threshold to sparse index optimizer
    
    * Add sparse index optimizer test
    
    * Use consistent naming, write vector in full everywhere
    
    * Simplify vacuum optimizer a bit
    
    * Merge sparse index optimizer into vacuum optimizer
    
    * Improve update_from in segment builder by returning early
    
    * More accurately count vectors in segment optimizer
    
    * Remove random from vacuum optimizer tests to make them more reliable
    
    * Don't expose the total points in segment info, use available points
    
    * Process review feedback
    
    * Compare available vectors against indexed ones in vacuum optimizer
    
    This is much better than using the number of soft-deleted vectors when
    the segment was created for calculations. Not to mention that value had
    other problems as well.
    
    * Remove create_deleted_vector_count field, update vacuum test parameters
    
    * Potentially solve out of bound panic when building index
    
    * Review fixes:
    
    - Propagate deleted flags into payload hnsw building
    - Use `total` number of points for building HNSW instead of number of
      available points
    - minor refactoring of `hnsw_config` copy -> clone
    - Better detection of `indexed_points` in HNSW
    
    * fix assert condition
    
    * Optional named vectors optimizer reveiw 2 (#1794)
    
    * review with Ivan
    
    * fmt
    
    * remove available_vector_count from segment entry
    
    * remove total_point_count from segment entry
    
    ---------
    
    Co-authored-by: Ivan Pleshkov <pleshkov.ivan@gmail.com>
    
    * rollback changes in deleted count in proxy segment
    
    * improve vector threshold detection logic in optimized_segment_builder
    
    * style changes
    
    * fix propagate deleted points to vectors
    
    * Fix typo in method name
    
    ---------
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>
    Co-authored-by: Ivan Pleshkov <pleshkov.ivan@gmail.com>

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index d2117896e..5d3b21b37 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -52,7 +52,7 @@ pub trait SegmentOptimizer {
     fn collection_params(&self) -> CollectionParams;
 
     /// Get HNSW config
-    fn hnsw_config(&self) -> HnswConfig;
+    fn hnsw_config(&self) -> &HnswConfig;
 
     /// Get quantization config
     fn quantization_config(&self) -> Option<QuantizationConfig>;
@@ -76,7 +76,7 @@ pub trait SegmentOptimizer {
         let collection_params = self.collection_params();
         let config = SegmentConfig {
             vector_data: collection_params
-                .get_all_vector_params(&self.hnsw_config(), self.quantization_config().as_ref())?,
+                .get_all_vector_params(self.hnsw_config(), self.quantization_config().as_ref())?,
             index: Indexes::Plain {},
             storage_type: StorageType::InMemory,
             payload_storage_type: match collection_params.on_disk_payload {
@@ -96,36 +96,64 @@ pub trait SegmentOptimizer {
         &self,
         optimizing_segments: &[LockedSegment],
     ) -> CollectionResult<SegmentBuilder> {
-        let total_vectors_size: usize = optimizing_segments
-            .iter()
-            .map(|s| {
-                let segment = s.get();
-                let locked_segment = segment.read();
-                locked_segment.points_count()
-                    * locked_segment
-                        .vector_dims()
-                        .values()
-                        .max()
-                        .copied()
-                        .unwrap_or(0)
-                    * VECTOR_ELEMENT_SIZE
-            })
-            .sum();
+        // Example:
+        //
+        // S1: {
+        //     text_vectors: 10000,
+        //     image_vectors: 100
+        // }
+        // S2: {
+        //     text_vectors: 200,
+        //     image_vectors: 10000
+        // }
+
+        // Example: bytes_count_by_vector_name = {
+        //     text_vectors: 10200 * dim * VECTOR_ELEMENT_SIZE
+        //     image_vectors: 10100 * dim * VECTOR_ELEMENT_SIZE
+        // }
+        let mut bytes_count_by_vector_name = HashMap::new();
+
+        for segment in optimizing_segments {
+            let segment = match segment {
+                LockedSegment::Original(segment) => segment,
+                LockedSegment::Proxy(_) => {
+                    return Err(CollectionError::service_error(
+                        "Proxy segment is not expected here".to_string(),
+                    ))
+                }
+            };
+            let locked_segment = segment.read();
+
+            for (vector_name, dim) in locked_segment.vector_dims() {
+                let available_vectors =
+                    locked_segment.available_vector_count(&vector_name).unwrap();
+                let vector_size = dim * VECTOR_ELEMENT_SIZE * available_vectors;
+                let size = bytes_count_by_vector_name.entry(vector_name).or_insert(0);
+                *size += vector_size;
+            }
+        }
+
+        // Example: maximal_vector_store_size_bytes = 10200 * dim * VECTOR_ELEMENT_SIZE
+        let maximal_vector_store_size_bytes = bytes_count_by_vector_name
+            .values()
+            .max()
+            .copied()
+            .unwrap_or(0);
 
         let thresholds = self.threshold_config();
         let collection_params = self.collection_params();
 
-        let is_indexed =
-            total_vectors_size >= thresholds.indexing_threshold.saturating_mul(BYTES_IN_KB);
+        let is_indexed = maximal_vector_store_size_bytes
+            >= thresholds.indexing_threshold.saturating_mul(BYTES_IN_KB);
 
-        let is_on_disk =
-            total_vectors_size >= thresholds.memmap_threshold.saturating_mul(BYTES_IN_KB);
+        let is_on_disk = maximal_vector_store_size_bytes
+            >= thresholds.memmap_threshold.saturating_mul(BYTES_IN_KB);
 
         let optimized_config = SegmentConfig {
             vector_data: collection_params
-                .get_all_vector_params(&self.hnsw_config(), self.quantization_config().as_ref())?,
+                .get_all_vector_params(self.hnsw_config(), self.quantization_config().as_ref())?,
             index: if is_indexed {
-                Indexes::Hnsw(self.hnsw_config())
+                Indexes::Hnsw(self.hnsw_config().clone())
             } else {
                 Indexes::Plain {}
             },
@@ -213,9 +241,8 @@ pub trait SegmentOptimizer {
     /// # Result
     ///
     /// Rolls back back optimization state.
-    /// All processed changes will still be there, but the collection should be returned
-    /// into state before optimization.
-    ///
+    /// All processed changes will still be there, but the collection should be returned into state
+    /// before optimization.
     fn handle_cancellation(
         &self,
         segments: &LockedSegmentHolder,
@@ -223,7 +250,7 @@ pub trait SegmentOptimizer {
         temp_segment: &LockedSegment,
     ) {
         self.unwrap_proxy(segments, proxy_ids);
-        if temp_segment.get().read().points_count() > 0 {
+        if temp_segment.get().read().available_point_count() > 0 {
             let mut write_segments = segments.write();
             write_segments.add_locked(temp_segment.clone());
         }
@@ -481,7 +508,7 @@ pub trait SegmentOptimizer {
             // Release reference counter of the optimized segments
             drop(optimizing_segments);
             // Append a temp segment to a collection if it is not empty or there is no other appendable segment
-            if tmp_segment.get().read().points_count() > 0 || !has_appendable_segments {
+            if tmp_segment.get().read().available_point_count() > 0 || !has_appendable_segments {
                 write_segments_guard.add_locked(tmp_segment);
 
                 // unlock collection for search and updates

commit df711b7c2e64ec4baf9c086fab2ba68dcdf0966e
Author: Tim Vise <tim+github@visee.me>
Date:   Wed May 17 09:49:55 2023 +0200

    Refactor segment config (#1894)
    
    * Clone current segment config to deprecated type
    
    * Remove segment level quantization config from segment config
    
    * Also deprecate current VectorDataConfig
    
    * Update old segment migration to work with new refactoring
    
    * Move index into vector data config
    
    * Move vector data config migration logic into segment level
    
    * Remove hnsw_config from vector data config
    
    * Rename collection params to vector data conversions function
    
    * Move storage type into vector data config
    
    * Set appendable flag correctly
    
    * Clean up and reformat
    
    * Make segment on disk flag not optional
    
    * Add appendable flag to segment config to replace storage type
    
    * Remove storage type from segment config
    
    * Deprecate storage type enum
    
    * Use consistent variable naming
    
    * Cleanup
    
    * Add segment config migration for v0.5.0 to current
    
    * Bump segment to 0.6.0
    
    * Remove serde defaults for new storage and vector data config types
    
    These default value configurations are not needed anymore, because these
    structs are not used to deserialize old data. All current fields should
    always be available in these structs. When new fields are added in new
    functions, the serde default annotation must be set again.
    
    * Cleanup
    
    * Update OpenAPI specification
    
    This updates the returned data structure on telemetry endpoints, as a
    result of segment configuration refactoring.
    
    * Fix quantization configuration not falling back to collection config
    
    * Fix compiler warning when building in release mode
    
    * Move deprecated type structs into compat module
    
    * Update allow deprecated attributes
    
    * Assign quantization config only in segment optimizer
    
    * Remove unsued parameter
    
    * Add vector storage type enum to vector data config
    
    * Remove appendable and on_disk flags from segment and vector config
    
    * Update OpenAPI specification
    
    * add tests
    
    ---------
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 5d3b21b37..c73f3f19a 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -14,7 +14,7 @@ use segment::segment_constructor::build_segment;
 use segment::segment_constructor::segment_builder::SegmentBuilder;
 use segment::types::{
     HnswConfig, Indexes, PayloadFieldSchema, PayloadKeyType, PayloadStorageType, PointIdType,
-    QuantizationConfig, SegmentConfig, StorageType, VECTOR_ELEMENT_SIZE,
+    QuantizationConfig, SegmentConfig, VectorStorageType, VECTOR_ELEMENT_SIZE,
 };
 
 use crate::collection_manager::holders::proxy_segment::ProxySegment;
@@ -22,6 +22,7 @@ use crate::collection_manager::holders::segment_holder::{
     LockedSegment, LockedSegmentHolder, SegmentId,
 };
 use crate::config::CollectionParams;
+use crate::operations::config_diff::DiffConfig;
 use crate::operations::types::{CollectionError, CollectionResult};
 
 const BYTES_IN_KB: usize = 1024;
@@ -75,15 +76,12 @@ pub trait SegmentOptimizer {
     fn temp_segment(&self) -> CollectionResult<LockedSegment> {
         let collection_params = self.collection_params();
         let config = SegmentConfig {
-            vector_data: collection_params
-                .get_all_vector_params(self.hnsw_config(), self.quantization_config().as_ref())?,
-            index: Indexes::Plain {},
-            storage_type: StorageType::InMemory,
-            payload_storage_type: match collection_params.on_disk_payload {
-                true => PayloadStorageType::OnDisk,
-                false => PayloadStorageType::InMemory,
+            vector_data: collection_params.into_base_vector_data()?,
+            payload_storage_type: if collection_params.on_disk_payload {
+                PayloadStorageType::OnDisk
+            } else {
+                PayloadStorageType::InMemory
             },
-            quantization_config: None,
         };
         Ok(LockedSegment::new(build_segment(
             self.collection_path(),
@@ -149,28 +147,48 @@ pub trait SegmentOptimizer {
         let is_on_disk = maximal_vector_store_size_bytes
             >= thresholds.memmap_threshold.saturating_mul(BYTES_IN_KB);
 
+        let mut vector_data = collection_params.into_base_vector_data()?;
+
+        // If indexing, change to HNSW index and quantization
+        if is_indexed {
+            let collection_hnsw = self.hnsw_config();
+            let collection_quantization = self.quantization_config();
+            vector_data.iter_mut().for_each(|(vector_name, config)| {
+                // Assign HNSW index
+                let param_hnsw = collection_params
+                    .vectors
+                    .get_params(vector_name)
+                    .and_then(|params| params.hnsw_config);
+                let vector_hnsw = param_hnsw
+                    .and_then(|c| c.update(collection_hnsw).ok())
+                    .unwrap_or_else(|| collection_hnsw.clone());
+                config.index = Indexes::Hnsw(vector_hnsw);
+
+                // Assign quantization config
+                let param_quantization = collection_params
+                    .vectors
+                    .get_params(vector_name)
+                    .and_then(|params| params.quantization_config.as_ref());
+                let vector_quantization = param_quantization
+                    .or(collection_quantization.as_ref())
+                    .cloned();
+                config.quantization_config = vector_quantization;
+            });
+        }
+
+        // If storing on disk, set storage type
+        if is_on_disk {
+            vector_data.values_mut().for_each(|config| {
+                config.storage_type = VectorStorageType::Mmap;
+            });
+        }
+
         let optimized_config = SegmentConfig {
-            vector_data: collection_params
-                .get_all_vector_params(self.hnsw_config(), self.quantization_config().as_ref())?,
-            index: if is_indexed {
-                Indexes::Hnsw(self.hnsw_config().clone())
-            } else {
-                Indexes::Plain {}
-            },
-            storage_type: if is_on_disk {
-                StorageType::Mmap
-            } else {
-                StorageType::InMemory
-            },
-            payload_storage_type: match collection_params.on_disk_payload {
-                true => PayloadStorageType::OnDisk,
-                false => PayloadStorageType::InMemory,
-            },
-            quantization_config: if is_indexed {
-                // TODO: separate config for applying quantization
-                self.quantization_config()
+            vector_data,
+            payload_storage_type: if collection_params.on_disk_payload {
+                PayloadStorageType::OnDisk
             } else {
-                Default::default()
+                PayloadStorageType::InMemory
             },
         };
 

commit 8797df02d4f9039db09e46af7b9cb0c26cc87e43
Author: Tim Vise <tim+github@visee.me>
Date:   Wed May 17 13:52:58 2023 +0200

    Don't store version of temp segment until payload indices are converted (#1913)

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index c73f3f19a..90b6562b0 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -8,8 +8,9 @@ use parking_lot::{Mutex, RwLock, RwLockUpgradableReadGuard};
 use segment::common::operation_time_statistics::{
     OperationDurationStatistics, OperationDurationsAggregator, ScopeDurationMeasurer,
 };
+use segment::common::version::StorageVersion;
 use segment::entry::entry_point::{check_process_stopped, SegmentEntry};
-use segment::segment::Segment;
+use segment::segment::{Segment, SegmentVersion};
 use segment::segment_constructor::build_segment;
 use segment::segment_constructor::segment_builder::SegmentBuilder;
 use segment::types::{
@@ -73,7 +74,7 @@ pub trait SegmentOptimizer {
     fn get_telemetry_counter(&self) -> Arc<Mutex<OperationDurationsAggregator>>;
 
     /// Build temp segment
-    fn temp_segment(&self) -> CollectionResult<LockedSegment> {
+    fn temp_segment(&self, save_version: bool) -> CollectionResult<LockedSegment> {
         let collection_params = self.collection_params();
         let config = SegmentConfig {
             vector_data: collection_params.into_base_vector_data()?,
@@ -86,6 +87,7 @@ pub trait SegmentOptimizer {
         Ok(LockedSegment::new(build_segment(
             self.collection_path(),
             &config,
+            save_version,
         )?))
     }
 
@@ -411,7 +413,7 @@ pub trait SegmentOptimizer {
 
         check_process_stopped(stopped)?;
 
-        let tmp_segment = self.temp_segment()?;
+        let tmp_segment = self.temp_segment(false)?;
 
         let proxy_deleted_points = Arc::new(RwLock::new(HashSet::<PointIdType>::new()));
         let proxy_deleted_indexes = Arc::new(RwLock::new(HashSet::<PayloadKeyType>::new()));
@@ -431,11 +433,20 @@ pub trait SegmentOptimizer {
             );
             // Wrapped segment is fresh, so it has no operations
             // Operation with number 0 will be applied
-            let op_num = 0;
-            proxy.replicate_field_indexes(op_num)?;
+            proxy.replicate_field_indexes(0)?;
             proxies.push(proxy);
         }
 
+        // Save segment version once all payload indices have been converted
+        // If this ends up not being saved due to a crash, the segment will not be used
+        match &tmp_segment {
+            LockedSegment::Original(segment) => {
+                let segment_path = &segment.read().current_path;
+                SegmentVersion::save(segment_path)?;
+            }
+            LockedSegment::Proxy(_) => unreachable!(),
+        }
+
         let proxy_ids: Vec<_> = {
             // Exclusive lock for the segments operations.
             let mut write_segments = RwLockUpgradableReadGuard::upgrade(segment_lock);

commit 7c100a19f7c8e205973999eb54a98cd9ef0b50f8
Author: Tim Vise <tim+github@visee.me>
Date:   Wed Jun 14 12:57:06 2023 +0200

    Improve counting vectors in `SegmentInfo` (#2072)
    
    * Correctly count vectors in segment info for normal segment
    
    * Correctly count vectors in segment info for proxy segment
    
    * Simplify available point count method
    
    * Minor improvements
    
    * Add unit test for point and vector counts in segment
    
    * Add unit test for point and vector counts in proxy segment
    
    * Improve vector counting for proxy segment

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 90b6562b0..0e9a1fa29 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -536,6 +536,7 @@ pub trait SegmentOptimizer {
 
             // Release reference counter of the optimized segments
             drop(optimizing_segments);
+
             // Append a temp segment to a collection if it is not empty or there is no other appendable segment
             if tmp_segment.get().read().available_point_count() > 0 || !has_appendable_segments {
                 write_segments_guard.add_locked(tmp_segment);

commit 7fcb4efeff84e726481e60a76094d6c1333a8d73
Author: Roman Titov <ffuugoo@users.noreply.github.com>
Date:   Wed Jun 14 17:02:35 2023 +0200

    Enable `Segment::prefault_mmap_pages` after load and optimization (#1791) (#1992)
    
    * Enable `Segment::preheat_disk_cache` after load and optimization
    
    * Add `black_box` to prevent compiler from optimizing memmap reads (#2081)
    
    ---------
    
    Co-authored-by: Andrey Vasnetsov <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 0e9a1fa29..9f829e48d 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -529,6 +529,8 @@ pub trait SegmentOptimizer {
                 )?;
             }
 
+            optimized_segment.prefault_mmap_pages();
+
             let (_, proxies) = write_segments_guard.swap(optimized_segment, &proxy_ids);
 
             let has_appendable_segments =

commit 34f654568bf2847ddc1485735b160cd3a7c77547
Author: Tim Vise <tim+github@visee.me>
Date:   Mon Aug 28 09:14:37 2023 +0200

    Report optimizer status and history in telemetry (#2475)
    
    * Add name to optimizers
    
    * Track optimizer status in update handler
    
    * Remove unused optimizer telemetry implementation
    
    * Report tracked optimizer status in local shard telemetry
    
    * Keep just the last 16 optimizer trackers and non successful ones
    
    * Also eventually truncate cancelled optimizer statuses
    
    * Fix codespell
    
    * Assert basic optimizer log state in unit test
    
    * Remove repetitive suffix from optimizer names
    
    * Loosen requirements for optimizer status test to prevent flakyness

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 9f829e48d..0cc7950aa 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -44,6 +44,9 @@ pub struct OptimizerThresholds {
 /// The selection of the candidates for optimization and the configuration
 /// of resulting segment are up to concrete implementations.
 pub trait SegmentOptimizer {
+    /// Get name describing this optimizer
+    fn name(&self) -> &str;
+
     /// Get path of the whole collection
     fn collection_path(&self) -> &Path;
 

commit a18573b26503d33b89bb076c346196b6517d5a4e
Author: Josh Soref <2119212+jsoref@users.noreply.github.com>
Date:   Thu Sep 14 05:38:23 2023 -0400

    Spelling (#2658)
    
    * spelling: accumulating
    
    Signed-off-by: Josh Soref <2119212+jsoref@users.noreply.github.com>
    
    * spelling: and
    
    Signed-off-by: Josh Soref <2119212+jsoref@users.noreply.github.com>
    
    * spelling: back
    
    Signed-off-by: Josh Soref <2119212+jsoref@users.noreply.github.com>
    
    * spelling: batching
    
    Signed-off-by: Josh Soref <2119212+jsoref@users.noreply.github.com>
    
    * spelling: been
    
    Signed-off-by: Josh Soref <2119212+jsoref@users.noreply.github.com>
    
    * spelling: benchmark
    
    Signed-off-by: Josh Soref <2119212+jsoref@users.noreply.github.com>
    
    * spelling: collections
    
    Signed-off-by: Josh Soref <2119212+jsoref@users.noreply.github.com>
    
    * spelling: confusion
    
    Signed-off-by: Josh Soref <2119212+jsoref@users.noreply.github.com>
    
    * spelling: consensus
    
    Signed-off-by: Josh Soref <2119212+jsoref@users.noreply.github.com>
    
    * spelling: decrease
    
    Signed-off-by: Josh Soref <2119212+jsoref@users.noreply.github.com>
    
    * spelling: equal
    
    Signed-off-by: Josh Soref <2119212+jsoref@users.noreply.github.com>
    
    * spelling: github
    
    Signed-off-by: Josh Soref <2119212+jsoref@users.noreply.github.com>
    
    * spelling: minimal
    
    Signed-off-by: Josh Soref <2119212+jsoref@users.noreply.github.com>
    
    * spelling: nonexistent
    
    Signed-off-by: Josh Soref <2119212+jsoref@users.noreply.github.com>
    
    * spelling: oversampling
    
    Signed-off-by: Josh Soref <2119212+jsoref@users.noreply.github.com>
    
    * spelling: paths
    
    Signed-off-by: Josh Soref <2119212+jsoref@users.noreply.github.com>
    
    * spelling: points
    
    Signed-off-by: Josh Soref <2119212+jsoref@users.noreply.github.com>
    
    * spelling: prevent
    
    Signed-off-by: Josh Soref <2119212+jsoref@users.noreply.github.com>
    
    * spelling: protobuf
    
    Signed-off-by: Josh Soref <2119212+jsoref@users.noreply.github.com>
    
    * spelling: proxied
    
    Signed-off-by: Josh Soref <2119212+jsoref@users.noreply.github.com>
    
    * spelling: randomness
    
    Signed-off-by: Josh Soref <2119212+jsoref@users.noreply.github.com>
    
    * spelling: recover
    
    Signed-off-by: Josh Soref <2119212+jsoref@users.noreply.github.com>
    
    ---------
    
    Signed-off-by: Josh Soref <2119212+jsoref@users.noreply.github.com>

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 0cc7950aa..2b25edfc6 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -263,7 +263,7 @@ pub trait SegmentOptimizer {
     ///
     /// # Result
     ///
-    /// Rolls back back optimization state.
+    /// Rolls back optimization state.
     /// All processed changes will still be there, but the collection should be returned into state
     /// before optimization.
     fn handle_cancellation(

commit 4f983e495db72336b2311dc2abe95a11eab8c620
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Fri Sep 29 16:23:24 2023 +0200

    Promote operation error to dedicated file (#2736)

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 2b25edfc6..4debaccd1 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -5,11 +5,12 @@ use std::sync::Arc;
 
 use itertools::Itertools;
 use parking_lot::{Mutex, RwLock, RwLockUpgradableReadGuard};
+use segment::common::operation_error::check_process_stopped;
 use segment::common::operation_time_statistics::{
     OperationDurationStatistics, OperationDurationsAggregator, ScopeDurationMeasurer,
 };
 use segment::common::version::StorageVersion;
-use segment::entry::entry_point::{check_process_stopped, SegmentEntry};
+use segment::entry::entry_point::SegmentEntry;
 use segment::segment::{Segment, SegmentVersion};
 use segment::segment_constructor::build_segment;
 use segment::segment_constructor::segment_builder::SegmentBuilder;

commit 3fc1f9656418995d21d156bd83f6f3611a99ee96
Author: Ivan Pleshkov <pleshkov.ivan@gmail.com>
Date:   Fri Dec 1 13:10:58 2023 +0100

    Sparse index segment and collection config (#2802)
    
    * quantization storage as separate entity
    
    sparse index try to extend segment types
    
    fix build
    
    fix async scorer
    
    codespell
    
    update openapi
    
    update vector index
    
    remove code duplications
    
    more fixes
    
    more fixes
    
    fix build
    
    fix deserialization test
    
    remove transform_into
    
    are you happy clippy
    
    update openapi
    
    update openapi
    
    are you happy clippy
    
    fix build
    
    optional serialize
    
    more defaults
    
    update openapi
    
    fix comments
    
    generic transpose_map_into_named_vector
    
    rename fields in tests
    
    remove obsolete parts
    
    only named sparse config
    
    VectorStruct without unnamed sparse
    
    NamedVectorStruct without unnamed sparse
    
    remove obsolete test
    
    update openapi
    
    mmap index
    
    revert preprocess function
    
    are you happy fmt
    
    update openapi
    
    fix build
    
    fix tests
    
    are you happy fmt
    
    fix for client generation
    
    fix sparse segment creation
    
    fix basic sparse test
    
    fix conflicts
    
    remove obsolete convertion
    
    fix build
    
    config diffs
    
    update openapi
    
    review remarks
    
    update openapi
    
    fix batch upsert
    
    add failing test showing bad ids matching
    
    fix sparse vector insertion
    
    remove on_disk flag
    
    update openapi
    
    revert debug assert
    
    simplify conversions
    
    update openapi
    
    remove on disk storage flag
    
    update openapi
    
    default for vector config
    
    update openapi comment
    
    remove diffs
    
    update openapi
    
    * enable consensus test
    
    * add comment
    
    * update openapi

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 4debaccd1..4624eb155 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -82,6 +82,7 @@ pub trait SegmentOptimizer {
         let collection_params = self.collection_params();
         let config = SegmentConfig {
             vector_data: collection_params.into_base_vector_data()?,
+            sparse_vector_data: collection_params.into_sparse_vector_data()?,
             payload_storage_type: if collection_params.on_disk_payload {
                 PayloadStorageType::OnDisk
             } else {
@@ -154,6 +155,7 @@ pub trait SegmentOptimizer {
             >= thresholds.memmap_threshold.saturating_mul(BYTES_IN_KB);
 
         let mut vector_data = collection_params.into_base_vector_data()?;
+        let sparse_vector_data = collection_params.into_sparse_vector_data()?;
 
         // If indexing, change to HNSW index and quantization
         if is_indexed {
@@ -191,6 +193,7 @@ pub trait SegmentOptimizer {
 
         let optimized_config = SegmentConfig {
             vector_data,
+            sparse_vector_data,
             payload_storage_type: if collection_params.on_disk_payload {
                 PayloadStorageType::OnDisk
             } else {

commit 9f260907c58090abf6e2debff1c7ef6728c83282
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Tue Dec 5 00:24:30 2023 +0100

    Fix optimizer change sparse index on disk (#3143)
    
    * Estimate sparse vector dimension for optimizer (#3142)
    
    * Estimate sparse vector dimension for optimizer
    
    * add hack explanation + todo
    
    Fix optimizer change sparse index on disk
    
    fix index to disk
    
    is appendable fix
    
    add index files if necessary
    
    fix test
    
    * fix tests
    
    ---------
    
    Co-authored-by: Ivan Pleshkov <pleshkov.ivan@gmail.com>

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 4624eb155..8c86e832d 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -155,7 +155,7 @@ pub trait SegmentOptimizer {
             >= thresholds.memmap_threshold.saturating_mul(BYTES_IN_KB);
 
         let mut vector_data = collection_params.into_base_vector_data()?;
-        let sparse_vector_data = collection_params.into_sparse_vector_data()?;
+        let mut sparse_vector_data = collection_params.into_sparse_vector_data()?;
 
         // If indexing, change to HNSW index and quantization
         if is_indexed {
@@ -189,6 +189,19 @@ pub trait SegmentOptimizer {
             vector_data.values_mut().for_each(|config| {
                 config.storage_type = VectorStorageType::Mmap;
             });
+
+            sparse_vector_data
+                .iter_mut()
+                .for_each(|(vector_name, config)| {
+                    // Assign sparse index on disk
+                    if let Some(sparse_config) = &collection_params.sparse_vectors {
+                        if let Some(params) = sparse_config.get(vector_name) {
+                            if let Some(index) = params.index.as_ref() {
+                                config.index = Some(*index);
+                            }
+                        }
+                    }
+                });
         }
 
         let optimized_config = SegmentConfig {

commit 17827a33a1f1948df306701d8ab7560028eb2203
Author: Ivan Pleshkov <pleshkov.ivan@gmail.com>
Date:   Wed Dec 6 13:28:17 2023 +0100

    Internal sparse configs for segment (#3168)
    
    * internal sparse vector segment configs
    
    update openapi
    
    fix build
    
    provide correct index type to fixtures
    
    rename grpc stuff
    
    optional search threshold
    
    update api consistency
    
    * review fix
    
    * rollback rename
    
    * reger docs
    
    ---------
    
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 8c86e832d..7a5020723 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -11,6 +11,7 @@ use segment::common::operation_time_statistics::{
 };
 use segment::common::version::StorageVersion;
 use segment::entry::entry_point::SegmentEntry;
+use segment::index::sparse_index::sparse_index_config::SparseIndexType;
 use segment::segment::{Segment, SegmentVersion};
 use segment::segment_constructor::build_segment;
 use segment::segment_constructor::segment_builder::SegmentBuilder;
@@ -189,20 +190,33 @@ pub trait SegmentOptimizer {
             vector_data.values_mut().for_each(|config| {
                 config.storage_type = VectorStorageType::Mmap;
             });
+        }
 
-            sparse_vector_data
-                .iter_mut()
-                .for_each(|(vector_name, config)| {
-                    // Assign sparse index on disk
-                    if let Some(sparse_config) = &collection_params.sparse_vectors {
-                        if let Some(params) = sparse_config.get(vector_name) {
-                            if let Some(index) = params.index.as_ref() {
-                                config.index = Some(*index);
-                            }
-                        }
+        sparse_vector_data
+            .iter_mut()
+            .for_each(|(vector_name, config)| {
+                // Assign sparse index on disk
+                if let Some(sparse_config) = &collection_params.sparse_vectors {
+                    if let Some(params) = sparse_config.get(vector_name) {
+                        let config_on_disk = params
+                            .index
+                            .and_then(|index_params| index_params.on_disk)
+                            .unwrap_or(false);
+
+                        // If mmap OR index is exceeded
+                        let is_big = is_on_disk || is_indexed;
+
+                        let index_type = match (config_on_disk, is_big) {
+                            (true, true) => SparseIndexType::Mmap, // Big and configured on disk
+                            (true, false) => SparseIndexType::MutableRam, // Small
+                            (false, true) => SparseIndexType::ImmutableRam, // Big and configured in RAM
+                            (false, false) => SparseIndexType::MutableRam,  // Small
+                        };
+
+                        config.index.index_type = index_type;
                     }
-                });
-        }
+                }
+            });
 
         let optimized_config = SegmentConfig {
             vector_data,

commit 76aafec2136b8e8ee4459406748f19df86d60101
Author: Tim Vise <tim+github@visee.me>
Date:   Wed Dec 6 13:31:09 2023 +0100

    Hotfix for infinite optimization loop with `on_disk` and `mmap_threshold` fighting (#3167)
    
    * If on_disk is explicitly configured, prefer it over mmap threshold
    
    * review fix
    
    * Fix vector size in index optimizer not taking point count into account
    
    * Refactor vector size name, storage name is less ambiguous
    
    * Add on_disk and memmap_threshold conflict test
    
    * Return early in require optimization check, don't do useless work
    
    ---------
    
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 7a5020723..d3cc9f5d3 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -149,17 +149,17 @@ pub trait SegmentOptimizer {
         let thresholds = self.threshold_config();
         let collection_params = self.collection_params();
 
-        let is_indexed = maximal_vector_store_size_bytes
+        let threshold_is_indexed = maximal_vector_store_size_bytes
             >= thresholds.indexing_threshold.saturating_mul(BYTES_IN_KB);
 
-        let is_on_disk = maximal_vector_store_size_bytes
+        let threshold_is_on_disk = maximal_vector_store_size_bytes
             >= thresholds.memmap_threshold.saturating_mul(BYTES_IN_KB);
 
         let mut vector_data = collection_params.into_base_vector_data()?;
         let mut sparse_vector_data = collection_params.into_sparse_vector_data()?;
 
         // If indexing, change to HNSW index and quantization
-        if is_indexed {
+        if threshold_is_indexed {
             let collection_hnsw = self.hnsw_config();
             let collection_quantization = self.quantization_config();
             vector_data.iter_mut().for_each(|(vector_name, config)| {
@@ -185,10 +185,28 @@ pub trait SegmentOptimizer {
             });
         }
 
-        // If storing on disk, set storage type
-        if is_on_disk {
-            vector_data.values_mut().for_each(|config| {
-                config.storage_type = VectorStorageType::Mmap;
+        // If storing on disk, set storage type in current segment (not in collection config)
+        if threshold_is_on_disk {
+            vector_data.iter_mut().for_each(|(vector_name, config)| {
+                // Check whether on_disk is explicitly configured, if not, set it to true
+                let config_on_disk = collection_params
+                    .vectors
+                    .get_params(vector_name)
+                    .and_then(|config| config.on_disk);
+
+                match config_on_disk {
+                    Some(true) => config.storage_type = VectorStorageType::Mmap, // Both agree, but prefer mmap storage type
+                    Some(false) => {}, // on_disk=false wins, do nothing
+                    None => config.storage_type = VectorStorageType::Mmap, // Mmap threshold wins
+                }
+
+                // If we explicitly configure on_disk, but the segment storage type uses something
+                // that doesn't match, warn about it
+                if let Some(config_on_disk) = config_on_disk {
+                    if config_on_disk != config.storage_type.is_on_disk() {
+                        log::warn!("Collection config for vector {vector_name} has on_disk={config_on_disk:?} configured, but storage type for segment doesn't match it");
+                    }
+                }
             });
         }
 

commit 2ec1b2fab4a11c0db66e4de70ab1242db558964f
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Wed Dec 6 13:51:11 2023 +0100

    Fix dev build (#3172)

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index d3cc9f5d3..9f853e5fc 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -222,7 +222,7 @@ pub trait SegmentOptimizer {
                             .unwrap_or(false);
 
                         // If mmap OR index is exceeded
-                        let is_big = is_on_disk || is_indexed;
+                        let is_big = threshold_is_on_disk || threshold_is_indexed;
 
                         let index_type = match (config_on_disk, is_big) {
                             (true, true) => SparseIndexType::Mmap, // Big and configured on disk

commit cafab322d08b1dd38204063e756a9c1b59bd9a72
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Wed Dec 6 18:40:24 2023 +0100

    Optimizer detects on_disk update sparse vector index (#3160)
    
    * Optimizer detects on_disk update sparse vector index
    
    * Rename sparse vector index on disk check function to remove ambiguity
    
    * Appendable segments always have sparse index in memory
    
    Don't try to force these segments to have their index on disk if
    our collection configuration tells us to do it that way.
    
    * Fix compilation warning
    
    * Resolve merge conflict
    
    * In indexing optimizer, only put sparse vectors on disk if big
    
    * fix mutable->immutable optimization
    
    * fix negation
    
    * Disable config mismatch for sparse now, update sparse vector storage decision tree
    
    * Fix compilation error due to TODO
    
    * Fix flipped boolean
    
    * simplify config mismatch logic
    
    * simplify logic further
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 9f853e5fc..e4c51e1a3 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -219,16 +219,15 @@ pub trait SegmentOptimizer {
                         let config_on_disk = params
                             .index
                             .and_then(|index_params| index_params.on_disk)
-                            .unwrap_or(false);
+                            .unwrap_or(threshold_is_on_disk);
 
                         // If mmap OR index is exceeded
                         let is_big = threshold_is_on_disk || threshold_is_indexed;
 
-                        let index_type = match (config_on_disk, is_big) {
+                        let index_type = match (is_big, config_on_disk) {
                             (true, true) => SparseIndexType::Mmap, // Big and configured on disk
-                            (true, false) => SparseIndexType::MutableRam, // Small
-                            (false, true) => SparseIndexType::ImmutableRam, // Big and configured in RAM
-                            (false, false) => SparseIndexType::MutableRam,  // Small
+                            (true, false) => SparseIndexType::ImmutableRam, // Big and not on disk nor reached threshold
+                            (false, _) => SparseIndexType::MutableRam,      // Small
                         };
 
                         config.index.index_type = index_type;

commit 19514265330ac9a1049b9439517deb104a5a19ed
Author: Tim Vise <tim+github@visee.me>
Date:   Wed Jan 31 11:56:34 2024 +0100

    Dynamic CPU saturation internals (#3364)
    
    * Move CPU count function to common, fix wrong CPU count in visited list
    
    * Change default number of rayon threads to 8
    
    * Use CPU budget and CPU permits for optimizer tasks to limit utilization
    
    * Respect configured thread limits, use new sane defaults in config
    
    * Fix spelling issues
    
    * Fix test compilation error
    
    * Improve breaking if there is no CPU budget
    
    * Block optimizations until CPU budget, fix potentially getting stuck
    
    Our optimization worker now blocks until CPU budget is available to
    perform the task.
    
    Fix potential issue where optimization worker could get stuck. This
    would happen if no optimization task is started because there's no
    available CPU budget. This ensures the worker is woken up again to
    retry.
    
    * Utilize n-1 CPUs with optimization tasks
    
    * Better handle situations where CPU budget is drained
    
    * Dynamically scale rayon CPU count based on CPU size
    
    * Fix incorrect default for max_indexing_threads conversion
    
    * Respect max_indexing_threads for collection
    
    * Make max_indexing_threads optional, use none to set no limit
    
    * Update property documentation and comments
    
    * Property max_optimization_threads is per shard, not per collection
    
    * If we reached shard optimization limit, skip further checks
    
    * Add remaining TODOs
    
    * Fix spelling mistake
    
    * Align gRPC comment blocks
    
    * Fix compilation errors since last rebase
    
    * Make tests aware of CPU budget
    
    * Use new CPU budget calculation function everywhere
    
    * Make CPU budget configurable in settings, move static budget to common
    
    * Do not use static CPU budget, instance it and pass it through
    
    * Update CPU budget description
    
    * Move heuristic into defaults
    
    * Fix spelling issues
    
    * Move cpu_budget property to a better place
    
    * Move some things around
    
    * Minor review improvements
    
    * Use range match statement for CPU count heuristics
    
    * Systems with 1 or 2 CPUs do not keep cores unallocated by default
    
    * Fix compilation errors since last rebase
    
    * Update lib/segment/src/types.rs
    
    Co-authored-by: Luis Cosso <luis.cossio@qdrant.com>
    
    * Update lib/storage/src/content_manager/toc/transfer.rs
    
    Co-authored-by: Luis Cosso <luis.cossio@qdrant.com>
    
    * Rename cpu_budget to optimizer_cpu_budget
    
    * Update OpenAPI specification
    
    * Require at least half of the desired CPUs for optimizers
    
    This prevents running optimizations with just one CPU, which could be
    very slow.
    
    * Don't use wildcard in CPU heuristic match statements
    
    * Rename cpu_budget setting to optimizer_cpu_budget
    
    * Update CPU budget comments
    
    * Spell acquire correctly
    
    * Change if-else into match
    
    Co-authored-by: Luis Cosso <luis.cossio@qdrant.com>
    
    * Rename max_rayon_threads to num_rayon_threads, add explanation
    
    * Explain limit in update handler
    
    * Remove numbers for automatic selection of indexing threads
    
    * Inline max_workers variable
    
    * Remove CPU budget from ShardTransferConsensus trait, it is in collection
    
    * small allow(dead_code) => cfg(test)
    
    * Remove now obsolete lazy_static
    
    * Fix incorrect CPU calculation in CPU saturation test
    
    * Make waiting for CPU budget async, don't block current thread
    
    * Prevent deadlock on optimizer signal channel
    
    Do not block the optimization worker task anymore to wait for CPU budget
    to be available. That prevents our optimizer signal channel from being
    drained, blocking incoming updates because the cannot send another
    optimizer signal. Now, prevent blocking this task all together and
    retrigger the optimizers separately when CPU budget is available again.
    
    * Fix incorrect CPU calculation in optimization cancel test
    
    * Rename CPU budget wait function to notify
    
    * Detach API changes from CPU saturation internals
    
    This allows us to merge into a patch version of Qdrant. We can
    reintroduce the API changes in the upcoming minor release to make all of
    it fully functional.
    
    ---------
    
    Co-authored-by: Luis Cosso <luis.cossio@qdrant.com>
    Co-authored-by: Luis Cosso <luis.cossio@outlook.com>

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index e4c51e1a3..80dd8fbf1 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -3,6 +3,7 @@ use std::path::Path;
 use std::sync::atomic::{AtomicBool, Ordering};
 use std::sync::Arc;
 
+use common::cpu::CpuPermit;
 use itertools::Itertools;
 use parking_lot::{Mutex, RwLock, RwLockUpgradableReadGuard};
 use segment::common::operation_error::check_process_stopped;
@@ -348,6 +349,7 @@ pub trait SegmentOptimizer {
         proxy_deleted_points: Arc<RwLock<HashSet<PointIdType>>>,
         proxy_deleted_indexes: Arc<RwLock<HashSet<PayloadKeyType>>>,
         proxy_created_indexes: Arc<RwLock<HashMap<PayloadKeyType, PayloadFieldSchema>>>,
+        permit: CpuPermit,
         stopped: &AtomicBool,
     ) -> CollectionResult<Segment> {
         let mut segment_builder = self.optimized_segment_builder(optimizing_segments)?;
@@ -373,7 +375,7 @@ pub trait SegmentOptimizer {
                 .insert(field.to_owned(), schema_type.to_owned());
         }
 
-        let mut optimized_segment: Segment = segment_builder.build(stopped)?;
+        let mut optimized_segment: Segment = segment_builder.build(permit, stopped)?;
 
         // Delete points in 2 steps
         // First step - delete all points with read lock
@@ -429,6 +431,7 @@ pub trait SegmentOptimizer {
         &self,
         segments: LockedSegmentHolder,
         ids: Vec<SegmentId>,
+        permit: CpuPermit,
         stopped: &AtomicBool,
     ) -> CollectionResult<bool> {
         check_process_stopped(stopped)?;
@@ -526,6 +529,7 @@ pub trait SegmentOptimizer {
             proxy_deleted_points.clone(),
             proxy_deleted_indexes.clone(),
             proxy_created_indexes.clone(),
+            permit,
             stopped,
         ) {
             Ok(segment) => segment,

commit d39a483017d14971051e30be5023dd4e969163b6
Author: xzfc <5121426+xzfc@users.noreply.github.com>
Date:   Tue Feb 20 14:55:57 2024 +0000

    Refactor: introduce details level enum (#3612)

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 80dd8fbf1..b2c58273e 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -4,6 +4,7 @@ use std::sync::atomic::{AtomicBool, Ordering};
 use std::sync::Arc;
 
 use common::cpu::CpuPermit;
+use common::types::TelemetryDetail;
 use itertools::Itertools;
 use parking_lot::{Mutex, RwLock, RwLockUpgradableReadGuard};
 use segment::common::operation_error::check_process_stopped;
@@ -75,7 +76,7 @@ pub trait SegmentOptimizer {
         excluded_ids: &HashSet<SegmentId>,
     ) -> Vec<SegmentId>;
 
-    fn get_telemetry_data(&self) -> OperationDurationStatistics;
+    fn get_telemetry_data(&self, detail: TelemetryDetail) -> OperationDurationStatistics;
 
     fn get_telemetry_counter(&self) -> Arc<Mutex<OperationDurationsAggregator>>;
 

commit 19f43f5b30a81509fd8221f059824caa30fb2a84
Author: xzfc <5121426+xzfc@users.noreply.github.com>
Date:   Thu Feb 22 10:39:33 2024 +0000

    Prometheus histogram support (#3552)
    
    * Get rid of Arc in SegmentOptimizer::get_telemetry_counter()
    
    * Get rid of SegmentOptimizer::get_telemetry_data
    
    * Prometheus histogram support
    
    * Fixes, and sparse buckets
    
    * Preallocate in convert_histogram, merge_histograms
    
    * debug_assert to check boundaries are sorted
    
    * Generate histograms when details_level >= 3 or in /metrics

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index b2c58273e..18a0dcbe4 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -4,12 +4,11 @@ use std::sync::atomic::{AtomicBool, Ordering};
 use std::sync::Arc;
 
 use common::cpu::CpuPermit;
-use common::types::TelemetryDetail;
 use itertools::Itertools;
 use parking_lot::{Mutex, RwLock, RwLockUpgradableReadGuard};
 use segment::common::operation_error::check_process_stopped;
 use segment::common::operation_time_statistics::{
-    OperationDurationStatistics, OperationDurationsAggregator, ScopeDurationMeasurer,
+    OperationDurationsAggregator, ScopeDurationMeasurer,
 };
 use segment::common::version::StorageVersion;
 use segment::entry::entry_point::SegmentEntry;
@@ -76,9 +75,7 @@ pub trait SegmentOptimizer {
         excluded_ids: &HashSet<SegmentId>,
     ) -> Vec<SegmentId>;
 
-    fn get_telemetry_data(&self, detail: TelemetryDetail) -> OperationDurationStatistics;
-
-    fn get_telemetry_counter(&self) -> Arc<Mutex<OperationDurationsAggregator>>;
+    fn get_telemetry_counter(&self) -> &Mutex<OperationDurationsAggregator>;
 
     /// Build temp segment
     fn temp_segment(&self, save_version: bool) -> CollectionResult<LockedSegment> {
@@ -437,7 +434,7 @@ pub trait SegmentOptimizer {
     ) -> CollectionResult<bool> {
         check_process_stopped(stopped)?;
 
-        let mut timer = ScopeDurationMeasurer::new(&self.get_telemetry_counter());
+        let mut timer = ScopeDurationMeasurer::new(self.get_telemetry_counter());
         timer.set_success(false);
 
         // On the one hand - we want to check consistently if all provided segments are

commit 41c817c2a16f270dcab376e94b2ec0c5e7d8f149
Author: Tim Vise <tim+github@visee.me>
Date:   Thu Apr 4 10:52:59 2024 +0200

    Non-blocking snapshots (#3420)
    
    * Initial non-blocking snapshot implementation
    
    * Minor refactoring
    
    * Add some comments, improve log messages
    
    * Propagate proxy segment changes into wrapped segment when unproxying
    
    * Use upgradable read lock for propagating proxy segment changes
    
    * Extract proxy/unproxy functions for segments, better error handling
    
    * Don't stop early on error, always clean up proxied segments
    
    * Propagate proxy changes in two batches to minimize write locking
    
    * Use upgradable read lock when propagating proxy changes in two batches
    
    * Do not fall back to non-appendable segment configurations
    
    * Resolve remaining TODOs
    
    * Use LockedSegmentHolder type alias everywhere
    
    * Better state handling in method to proxy all segments
    
    * When proxying all segments, lock only after creating temporary segment
    
    * Pass actual proxied segments around to minimize segment holder locking
    
    * Propagate proxy segment changes to wrapped on drop, not to writable
    
    * Minor improvements
    
    * Fix proxy logic returning non-proxied segments
    
    * Share single segment holder lock and upgrade/downgrade it
    
    * Minor improvements
    
    * Make appendable segment check more efficient
    
    * Do not explicitly drop segments lock, it's not necessary
    
    * Add consensus test to assert data consistency while snapshotting
    
    * Fix incorrect documentation
    
    * Extract payload storage type decision logic to collection params function
    
    * Resolve TODO, we always expect to get a shard here
    
    * Only upgrade propagate to wrapped readers if lists are not empty
    
    * Set correct operation versions
    
    * review fixes
    
    ---------
    
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 18a0dcbe4..6dff479b8 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -443,12 +443,12 @@ pub trait SegmentOptimizer {
         //
         // On the other hand - we do not want to hold write lock during the segment creation.
         // Solution in the middle - is a upgradable lock. It ensures consistency after the check and allows to perform read operation.
-        let segment_lock = segments.upgradable_read();
+        let segments_lock = segments.upgradable_read();
 
         let optimizing_segments: Vec<_> = ids
             .iter()
             .cloned()
-            .map(|id| segment_lock.get(id))
+            .map(|id| segments_lock.get(id))
             .filter_map(|x| x.cloned())
             .collect();
 
@@ -501,7 +501,7 @@ pub trait SegmentOptimizer {
 
         let proxy_ids: Vec<_> = {
             // Exclusive lock for the segments operations.
-            let mut write_segments = RwLockUpgradableReadGuard::upgrade(segment_lock);
+            let mut write_segments = RwLockUpgradableReadGuard::upgrade(segments_lock);
             let mut proxy_ids = Vec::new();
             for (mut proxy, idx) in proxies.into_iter().zip(ids.iter().cloned()) {
                 // replicate_field_indexes for the second time,
@@ -592,7 +592,7 @@ pub trait SegmentOptimizer {
             // Release reference counter of the optimized segments
             drop(optimizing_segments);
 
-            // Append a temp segment to a collection if it is not empty or there is no other appendable segment
+            // Append a temp segment to collection if it is not empty or there is no other appendable segment
             if tmp_segment.get().read().available_point_count() > 0 || !has_appendable_segments {
                 write_segments_guard.add_locked(tmp_segment);
 

commit 42e8db910e296fafca3d026cfe95703b6d5b8a69
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Wed Apr 10 16:09:21 2024 +0200

    Use idiomatic Rust naming (#4007)

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 6dff479b8..d66d160ce 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -81,8 +81,8 @@ pub trait SegmentOptimizer {
     fn temp_segment(&self, save_version: bool) -> CollectionResult<LockedSegment> {
         let collection_params = self.collection_params();
         let config = SegmentConfig {
-            vector_data: collection_params.into_base_vector_data()?,
-            sparse_vector_data: collection_params.into_sparse_vector_data()?,
+            vector_data: collection_params.to_base_vector_data()?,
+            sparse_vector_data: collection_params.to_sparse_vector_data()?,
             payload_storage_type: if collection_params.on_disk_payload {
                 PayloadStorageType::OnDisk
             } else {
@@ -154,8 +154,8 @@ pub trait SegmentOptimizer {
         let threshold_is_on_disk = maximal_vector_store_size_bytes
             >= thresholds.memmap_threshold.saturating_mul(BYTES_IN_KB);
 
-        let mut vector_data = collection_params.into_base_vector_data()?;
-        let mut sparse_vector_data = collection_params.into_sparse_vector_data()?;
+        let mut vector_data = collection_params.to_base_vector_data()?;
+        let mut sparse_vector_data = collection_params.to_sparse_vector_data()?;
 
         // If indexing, change to HNSW index and quantization
         if threshold_is_indexed {

commit 1d724579dfd6ed5adeda31429bab5821cab5af30
Author: xzfc <5121426+xzfc@users.noreply.github.com>
Date:   Thu May 16 06:47:47 2024 +0000

    InvertedIndexImmutableRam and index migrations (#4220)
    
    * Move StorageVersion from segment crate to common/io
    
    * Refine StorageVersion API
    
    * Move methods from SparseVectorDataConfig to enum SparseIndexType
    
    * Introduce InvertedIndexImmutableRam
    
    * Add migrate
    
    * Don't migrate

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index d66d160ce..e9583c723 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -4,13 +4,13 @@ use std::sync::atomic::{AtomicBool, Ordering};
 use std::sync::Arc;
 
 use common::cpu::CpuPermit;
+use io::storage_version::StorageVersion;
 use itertools::Itertools;
 use parking_lot::{Mutex, RwLock, RwLockUpgradableReadGuard};
 use segment::common::operation_error::check_process_stopped;
 use segment::common::operation_time_statistics::{
     OperationDurationsAggregator, ScopeDurationMeasurer,
 };
-use segment::common::version::StorageVersion;
 use segment::entry::entry_point::SegmentEntry;
 use segment::index::sparse_index::sparse_index_config::SparseIndexType;
 use segment::segment::{Segment, SegmentVersion};

commit aad9db1fe9c5d22dce24e1de27a92a28f7453c8d
Author: Tim Vise <tim+github@visee.me>
Date:   Mon May 27 19:03:02 2024 +0200

    Fix missing segments, use correct path for new segment created during snapshot (#4332)
    
    * Put temporary segment in correct path
    
    * Use shard directory rather than collection directory in test
    
    * Fix collection path getter, it actually returns segments path
    
    * Use segments path for temporary segment
    
    * The build segment function actually wants the segments path
    
    * Refactor parameter name

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index e9583c723..0203fa483 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -50,8 +50,8 @@ pub trait SegmentOptimizer {
     /// Get name describing this optimizer
     fn name(&self) -> &str;
 
-    /// Get path of the whole collection
-    fn collection_path(&self) -> &Path;
+    /// Get path of the the shard
+    fn segments_path(&self) -> &Path;
 
     /// Get temp path, where optimized segments could be temporary stored
     fn temp_path(&self) -> &Path;
@@ -90,7 +90,7 @@ pub trait SegmentOptimizer {
             },
         };
         Ok(LockedSegment::new(build_segment(
-            self.collection_path(),
+            self.segments_path(),
             &config,
             save_version,
         )?))
@@ -245,7 +245,7 @@ pub trait SegmentOptimizer {
         };
 
         Ok(SegmentBuilder::new(
-            self.collection_path(),
+            self.segments_path(),
             self.temp_path(),
             &optimized_config,
         )?)

commit 8e6f8d4575a4613f7d863f76088b3096c9d7be77
Author: Tim Vise <tim+github@visee.me>
Date:   Tue May 28 13:31:54 2024 +0200

    On shard load, ensure we have any appendable segments (#4342)
    
    * Extract logic for creating temporary segment during segment proxying
    
    * Simplify check for having an appendable segment
    
    * Fix incorrect documentation
    
    * When loading shard, ensure we have any appendable segments or create one
    
    * Use correct parameter name
    
    * In debug builds, crash when there's no appendable segment on start

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 0203fa483..dc13a1775 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -50,7 +50,7 @@ pub trait SegmentOptimizer {
     /// Get name describing this optimizer
     fn name(&self) -> &str;
 
-    /// Get path of the the shard
+    /// Get the path of the segments directory
     fn segments_path(&self) -> &Path;
 
     /// Get temp path, where optimized segments could be temporary stored
@@ -586,8 +586,7 @@ pub trait SegmentOptimizer {
 
             let (_, proxies) = write_segments_guard.swap(optimized_segment, &proxy_ids);
 
-            let has_appendable_segments =
-                write_segments_guard.random_appendable_segment().is_some();
+            let has_appendable_segments = write_segments_guard.has_appendable_segment();
 
             // Release reference counter of the optimized segments
             drop(optimizing_segments);

commit 21a3fb5f38a796f37883017adc993d0322bbca8f
Author: Ivan Pleshkov <pleshkov.ivan@gmail.com>
Date:   Tue May 28 16:38:56 2024 +0200

    Use correct vector storage size (#4312)
    
    * use correct vector storage size
    
    * remove dim from segment entry
    
    * are you happy fmt
    
    * codespell and proportions
    
    * remove obsolete comment
    
    * remove `try_vector_dim`
    
    * are you happy fmt
    
    * remove todo
    
    * revert code of conduct
    
    * check div 0
    
    * Simplify a bit with max iterator
    
    * Update lib/segment/src/index/hnsw_index/hnsw.rs
    
    Co-authored-by: Tim Vise <tim+github@visee.me>
    
    * are you happy fmt
    
    * Update lib/segment/src/index/plain_payload_index.rs
    
    Co-authored-by: Tim Vise <tim+github@visee.me>
    
    * review fix
    
    * set full scan threshold 0 for test
    
    * use u128 also for multivector storages
    
    * fix sparse vector size calculation
    
    * Move size calculation into if-branch
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>
    Co-authored-by: Tim Vise <tim+github@visee.me>

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index dc13a1775..3bf82795f 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -18,7 +18,7 @@ use segment::segment_constructor::build_segment;
 use segment::segment_constructor::segment_builder::SegmentBuilder;
 use segment::types::{
     HnswConfig, Indexes, PayloadFieldSchema, PayloadKeyType, PayloadStorageType, PointIdType,
-    QuantizationConfig, SegmentConfig, VectorStorageType, VECTOR_ELEMENT_SIZE,
+    QuantizationConfig, SegmentConfig, VectorStorageType,
 };
 
 use crate::collection_manager::holders::proxy_segment::ProxySegment;
@@ -129,10 +129,8 @@ pub trait SegmentOptimizer {
             };
             let locked_segment = segment.read();
 
-            for (vector_name, dim) in locked_segment.vector_dims() {
-                let available_vectors =
-                    locked_segment.available_vector_count(&vector_name).unwrap();
-                let vector_size = dim * VECTOR_ELEMENT_SIZE * available_vectors;
+            for vector_name in locked_segment.vector_names() {
+                let vector_size = locked_segment.available_vectors_size_in_bytes(&vector_name)?;
                 let size = bytes_count_by_vector_name.entry(vector_name).or_insert(0);
                 *size += vector_size;
             }

commit 96ecd2cca8ba311282b5d72c9e41ed71ddca036d
Author: Ivan Pleshkov <pleshkov.ivan@gmail.com>
Date:   Tue Jun 4 11:16:11 2024 +0200

    Fix hnsw full scan threshold (#4369)
    
    * fix hnsw full scan threshold
    
    * add test
    
    * are you happy clippy
    
    * separate open_vector_storage
    
    * remove public fields from builder
    
    * wip: do not create segment in builder before build
    
    * avoid arc in storage test and low-level loading functions
    
    * WIP: remove internal segment from SegmentBuilder
    
    * fmt
    
    * finalize segment builder fixes
    
    * Revert "are you happy clippy"
    
    This reverts commit c04afa698995f75f8b589737c2a794aee03824d8.
    
    * Revert "add test"
    
    This reverts commit 8e7ad6207ed042f25dcd07a16fac7c109b9c5a9e.
    
    * Revert "fix hnsw full scan threshold"
    
    This reverts commit 8904443fcb849cca30885b0b6980b0113ed25c16.
    
    * remove _daatabse from builder
    
    * fix optimizer test
    
    * fix id tracker versions persistence
    
    * do flush for segment components on build
    
    ---------
    
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 3bf82795f..277ef20f1 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -363,12 +363,10 @@ pub trait SegmentOptimizer {
         }
 
         for field in proxy_deleted_indexes.read().iter() {
-            segment_builder.indexed_fields.remove(field);
+            segment_builder.remove_indexed_field(field);
         }
         for (field, schema_type) in proxy_created_indexes.read().iter() {
-            segment_builder
-                .indexed_fields
-                .insert(field.to_owned(), schema_type.to_owned());
+            segment_builder.add_indexed_field(field.to_owned(), schema_type.to_owned());
         }
 
         let mut optimized_segment: Segment = segment_builder.build(permit, stopped)?;

commit a7f2e7a3c9861c90630917b96e5f59db70cedbe5
Author: Tim Vise <tim+github@visee.me>
Date:   Thu Jun 6 20:11:00 2024 +0200

    Fix deadlock caused by concurrent snapshot and optimization (#4402)
    
    * Rename segment addition functions, clarify this generates a new ID
    
    * Don't randomize segment IDs, auto increment to prevent duplicates
    
    * Rename swap to swap_new
    
    * On snapshot unproxy, put segments back with their original segment ID
    
    * Add sanity check to optimizer unproxy, must swap same number of segments
    
    * Clean up
    
    * Extend snapshot test, assert we end up with the same segment IDs

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 277ef20f1..93200c2d5 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -279,7 +279,7 @@ pub trait SegmentOptimizer {
                     LockedSegment::Proxy(proxy_segment) => {
                         let wrapped_segment = proxy_segment.read().wrapped_segment.clone();
                         let (restored_id, _proxies) =
-                            segments_lock.swap(wrapped_segment, &[proxy_id]);
+                            segments_lock.swap_new(wrapped_segment, &[proxy_id]);
                         restored_segment_ids.push(restored_id);
                     }
                 }
@@ -320,7 +320,7 @@ pub trait SegmentOptimizer {
         self.unwrap_proxy(segments, proxy_ids);
         if temp_segment.get().read().available_point_count() > 0 {
             let mut write_segments = segments.write();
-            write_segments.add_locked(temp_segment.clone());
+            write_segments.add_new_locked(temp_segment.clone());
         }
     }
 
@@ -506,7 +506,7 @@ pub trait SegmentOptimizer {
                 // so we can afford this operation under the full collection write lock
                 let op_num = 0;
                 proxy.replicate_field_indexes(op_num)?; // Slow only in case the index is change in the gap between two calls
-                proxy_ids.push(write_segments.swap(proxy, &[idx]).0);
+                proxy_ids.push(write_segments.swap_new(proxy, &[idx]).0);
             }
             proxy_ids
         };
@@ -580,7 +580,12 @@ pub trait SegmentOptimizer {
 
             optimized_segment.prefault_mmap_pages();
 
-            let (_, proxies) = write_segments_guard.swap(optimized_segment, &proxy_ids);
+            let (_, proxies) = write_segments_guard.swap_new(optimized_segment, &proxy_ids);
+            debug_assert_eq!(
+                proxies.len(),
+                proxy_ids.len(),
+                "swapped different number of proxies on unwrap, missing or incorrect segment IDs?"
+            );
 
             let has_appendable_segments = write_segments_guard.has_appendable_segment();
 
@@ -589,7 +594,7 @@ pub trait SegmentOptimizer {
 
             // Append a temp segment to collection if it is not empty or there is no other appendable segment
             if tmp_segment.get().read().available_point_count() > 0 || !has_appendable_segments {
-                write_segments_guard.add_locked(tmp_segment);
+                write_segments_guard.add_new_locked(tmp_segment);
 
                 // unlock collection for search and updates
                 drop(write_segments_guard);

commit 106002c3034ac9eddc3e4cc3d2027a3f3aaa900f
Author: Tim Vise <tim+github@visee.me>
Date:   Mon Jun 10 18:45:53 2024 +0200

    Ensure we have any segment within capacity, otherwise add new one (#4416)
    
    * Extract logic for creating thresholds config
    
    * Put collection params and threshold config in update handler
    
    * Add function to add a new appendable segment if all are over capacity
    
    * Make new method static, call it before each optimization loop
    
    * Update error message formatting
    
    * Use exact point count in replication consensus test
    
    * Add a test to assert segment creation when all are over capacity
    
    * Suffix optimizer thresholds with _kb to clarify unit
    
    * Move segment capacity check logic, run if optimizers are disabled
    
    * fix: add -> mul
    
    ---------
    
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 93200c2d5..af3d64e0f 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -33,9 +33,9 @@ const BYTES_IN_KB: usize = 1024;
 
 #[derive(Debug, Clone)]
 pub struct OptimizerThresholds {
-    pub max_segment_size: usize,
-    pub memmap_threshold: usize,
-    pub indexing_threshold: usize,
+    pub max_segment_size_kb: usize,
+    pub memmap_threshold_kb: usize,
+    pub indexing_threshold_kb: usize,
 }
 
 /// SegmentOptimizer - trait implementing common functionality of the optimizers
@@ -147,10 +147,10 @@ pub trait SegmentOptimizer {
         let collection_params = self.collection_params();
 
         let threshold_is_indexed = maximal_vector_store_size_bytes
-            >= thresholds.indexing_threshold.saturating_mul(BYTES_IN_KB);
+            >= thresholds.indexing_threshold_kb.saturating_mul(BYTES_IN_KB);
 
         let threshold_is_on_disk = maximal_vector_store_size_bytes
-            >= thresholds.memmap_threshold.saturating_mul(BYTES_IN_KB);
+            >= thresholds.memmap_threshold_kb.saturating_mul(BYTES_IN_KB);
 
         let mut vector_data = collection_params.to_base_vector_data()?;
         let mut sparse_vector_data = collection_params.to_sparse_vector_data()?;

commit ac9313e00bc9fffebbacc4672d1cb157b2178063
Author: Tim Vise <tim+github@visee.me>
Date:   Tue Jun 11 12:59:05 2024 +0200

    When selecting a segment for writing, select the smallest one (#4440)
    
    * Preallocate list of entires to prevent some unnecessary reallocations
    
    * Implement Copy for OptimizerThresholds
    
    * Add shard holder function get smallest segment
    
    * Take the smallest segment in the segments updater
    
    * Add test to assert inserting into smallest segment
    
    * Fix compilation warnings

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index af3d64e0f..ce6f1daea 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -31,7 +31,7 @@ use crate::operations::types::{CollectionError, CollectionResult};
 
 const BYTES_IN_KB: usize = 1024;
 
-#[derive(Debug, Clone)]
+#[derive(Debug, Clone, Copy)]
 pub struct OptimizerThresholds {
     pub max_segment_size_kb: usize,
     pub memmap_threshold_kb: usize,

commit c99caba0a37deab98b7758c760c7a53f7f60d7d9
Author: tellet-q <166374656+tellet-q@users.noreply.github.com>
Date:   Sat Jun 29 22:40:18 2024 +0200

    Add test for OOD during indexing (#4267)
    
    * Add test for OOD during indexing
    
    * Start indexing right after 1st OOD message
    
    * Only send search request after insert loop
    
    * Fail early when encountering out-of-storage during optimization (#4578)
    
    * fs4@0.8.4
    
    * fail early on low storage
    
    * move `dir_size` to `common`
    
    * move the ood bailout to `SegmentOptimizer::optimized_segment_builder`
    
    * drop dead code
    
    * move dir_size to common subcrate
    
    ---------
    
    Co-authored-by: generall <andrey@vasnetsov.com>
    
    ---------
    
    Co-authored-by: xhjkl <xhjkl@users.noreply.github.com>
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index ce6f1daea..bb9484427 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -4,6 +4,7 @@ use std::sync::atomic::{AtomicBool, Ordering};
 use std::sync::Arc;
 
 use common::cpu::CpuPermit;
+use common::disk::dir_size;
 use io::storage_version::StorageVersion;
 use itertools::Itertools;
 use parking_lot::{Mutex, RwLock, RwLockUpgradableReadGuard};
@@ -118,6 +119,10 @@ pub trait SegmentOptimizer {
         // }
         let mut bytes_count_by_vector_name = HashMap::new();
 
+        // Counting up how much space do the segments being optimized actually take on the fs.
+        // If there was at least one error while reading the size, this will be `None`.
+        let mut space_occupied = Some(0u64);
+
         for segment in optimizing_segments {
             let segment = match segment {
                 LockedSegment::Original(segment) => segment,
@@ -134,6 +139,28 @@ pub trait SegmentOptimizer {
                 let size = bytes_count_by_vector_name.entry(vector_name).or_insert(0);
                 *size += vector_size;
             }
+
+            space_occupied = space_occupied
+                .and_then(|x| dir_size(locked_segment.data_path()).ok().map(|y| x + y));
+        }
+
+        let space_needed = space_occupied.map(|x| 2 * x);
+        let space_available = fs4::available_space(self.temp_path()).ok();
+
+        match (space_available, space_needed) {
+            (Some(space_available), Some(space_needed)) => {
+                if space_available < space_needed {
+                    return Err(CollectionError::service_error(
+                        "Not enough space available for optimization".to_string(),
+                    ));
+                }
+            }
+            _ => {
+                log::warn!(
+                    "Could not estimate available storage space in `{}`; will try optimizing anyway",
+                    self.name()
+                );
+            }
         }
 
         // Example: maximal_vector_store_size_bytes = 10200 * dim * VECTOR_ELEMENT_SIZE
@@ -511,9 +538,8 @@ pub trait SegmentOptimizer {
             proxy_ids
         };
 
-        check_process_stopped(stopped).map_err(|error| {
+        check_process_stopped(stopped).inspect_err(|_| {
             self.handle_cancellation(&segments, &proxy_ids, &tmp_segment);
-            error
         })?;
 
         // ---- SLOW PART -----

commit 00c40c00602b0ec922aba51deb34f22088064a4b
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Mon Jul 1 13:30:56 2024 +0200

    add better logging and check for directory exists before free space check (#4588)

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index bb9484427..dc1501b77 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -140,12 +140,45 @@ pub trait SegmentOptimizer {
                 *size += vector_size;
             }
 
-            space_occupied = space_occupied
-                .and_then(|x| dir_size(locked_segment.data_path()).ok().map(|y| x + y));
+            space_occupied =
+                space_occupied.and_then(|acc| match dir_size(locked_segment.data_path()) {
+                    Ok(size) => Some(size + acc),
+                    Err(err) => {
+                        log::debug!(
+                            "Could not estimate size of segment `{}`: {}",
+                            locked_segment.data_path().display(),
+                            err
+                        );
+                        None
+                    }
+                });
         }
 
         let space_needed = space_occupied.map(|x| 2 * x);
-        let space_available = fs4::available_space(self.temp_path()).ok();
+
+        // Ensure temp_path exists
+
+        if !self.temp_path().exists() {
+            std::fs::create_dir_all(self.temp_path()).map_err(|err| {
+                CollectionError::service_error(format!(
+                    "Could not create temp directory `{}`: {}",
+                    self.temp_path().display(),
+                    err
+                ))
+            })?;
+        }
+
+        let space_available = match fs4::available_space(self.temp_path()) {
+            Ok(available) => Some(available),
+            Err(err) => {
+                log::debug!(
+                    "Could not estimate available storage space in `{}`: {}",
+                    self.temp_path().display(),
+                    err
+                );
+                None
+            }
+        };
 
         match (space_available, space_needed) {
             (Some(space_available), Some(space_needed)) => {

commit 6650e5885f6b622161741fb7ecfe181b81a346bf
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Wed Jul 17 19:45:22 2024 +0200

    Merge pull request #4403
    
    * add immutable_id_tracker
    
    * add dirty flag in test
    
    * don't use immutable_id_tracker for now
    
    * improve and integrate new immutable_id_tracker
    
    * split external_to_internal into two BTreeMaps
    
    * apply rquested changes
    
    * delay mmap writes until flush
    
    * remove unnecessary clone
    
    * single source of truth for file path
    
    * use custom de/serialization for more performance
    
    * disable id tracker and fix codespell
    
    * improve code & test
    
    * Other minor nitpicks
    
    * Apply suggestions from code review
    
    * fix rebase issues
    
    * basic custom mappings storage implementation
    
    * add tests & fix bugs
    
    * add more tests and fix bugs
    
    * undo .codespellrc
    
    * disable immutable_id_tracker completely for now
    
    * fix clippy
    
    * Remove unnecessary pub
    
    * minor renaming

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index dc1501b77..830c8eab9 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -676,6 +676,7 @@ pub trait SegmentOptimizer {
                 tmp_segment.drop_data()?;
             }
         }
+
         timer.set_success(true);
         Ok(true)
     }

commit 38522784b76c5e27dce2e71e8b22defcac68da75
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Thu Jul 18 11:43:56 2024 +0200

    Basic defragmentation (#4610)
    
    * sorting
    
    * migrate tests and move logic into SegmentBuilder
    
    * add test and improve implementation
    
    * improve code
    
    * review
    
    * code review improvements
    
    * add index building to test
    
    * Do not clone ranges
    
    * Resolve clippy warnings due to recent PR on dev
    
    * review suggestions
    
    * Defragmentation in api (#4684)
    
    * add tenant config to api
    
    * deduplicate used defragmentation keys
    
    * rename is_tenant to is_primary
    
    * use all values to defrag key
    
    * rename is_primary -> is_tenant
    
    * update schema
    
    ---------
    
    Co-authored-by: generall <andrey@vasnetsov.com>
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 830c8eab9..e49feb3f2 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -412,16 +412,36 @@ pub trait SegmentOptimizer {
 
         self.check_cancellation(stopped)?;
 
-        for segment in optimizing_segments {
-            match segment {
-                LockedSegment::Original(segment_arc) => {
-                    let segment_guard = segment_arc.read();
-                    segment_builder.update_from(&segment_guard, stopped)?;
+        let segments: Vec<_> = optimizing_segments
+            .iter()
+            .map(|i| match i {
+                LockedSegment::Original(o) => o.clone(),
+                LockedSegment::Proxy(_) => {
+                    panic!("Trying to optimize a segment that is already being optimized!")
                 }
-                LockedSegment::Proxy(_) => panic!("Attempt to optimize segment which is already currently under optimization. Should never happen"),
-            }
+            })
+            .collect();
+
+        let mut defragmentation_keys = HashSet::new();
+        for segment in &segments {
+            let payload_index = &segment.read().payload_index;
+            let payload_index = payload_index.borrow();
+
+            let keys = payload_index
+                .config()
+                .indexed_fields
+                .iter()
+                .filter_map(|(key, schema)| schema.is_tenant().then_some(key))
+                .cloned();
+            defragmentation_keys.extend(keys);
         }
 
+        if !defragmentation_keys.is_empty() {
+            segment_builder.set_defragment_keys(defragmentation_keys.into_iter().collect());
+        }
+
+        segment_builder.update(&segments, stopped)?;
+
         for field in proxy_deleted_indexes.read().iter() {
             segment_builder.remove_indexed_field(field);
         }

commit 624b29daa431fe3683174e738aba0c0c5e625119
Author: xzfc <5121426+xzfc@users.noreply.github.com>
Date:   Sat Aug 3 20:00:03 2024 +0000

    Integration tests for on-disk payload indices (#4819)
    
    * refactor: let SegmentBuilder::update take unlocked segments
    
    * style: split long lines
    
    * refactor: introduce TestSegments
    
    * test: add tests for mmap indices

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index e49feb3f2..da6773485 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -1,4 +1,5 @@
 use std::collections::{HashMap, HashSet};
+use std::ops::Deref;
 use std::path::Path;
 use std::sync::atomic::{AtomicBool, Ordering};
 use std::sync::Arc;
@@ -440,7 +441,13 @@ pub trait SegmentOptimizer {
             segment_builder.set_defragment_keys(defragmentation_keys.into_iter().collect());
         }
 
-        segment_builder.update(&segments, stopped)?;
+        {
+            let segment_guards = segments.iter().map(|segment| segment.read()).collect_vec();
+            segment_builder.update(
+                &segment_guards.iter().map(Deref::deref).collect_vec(),
+                stopped,
+            )?;
+        }
 
         for field in proxy_deleted_indexes.read().iter() {
             segment_builder.remove_indexed_field(field);

commit 438ce237bdfec0824dd6a76ed8279e02d034489e
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Tue Sep 3 20:27:43 2024 +0200

    Fix Clippy 1.81 (#5012)

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index da6773485..aed8d9133 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -635,9 +635,8 @@ pub trait SegmentOptimizer {
 
         // ---- SLOW PART ENDS HERE -----
 
-        check_process_stopped(stopped).map_err(|error| {
+        check_process_stopped(stopped).inspect_err(|_error| {
             self.handle_cancellation(&segments, &proxy_ids, &tmp_segment);
-            error
         })?;
 
         {

commit 70c46bbb6f49739acac3ee7ce55074029a40b5a1
Author: Kumar Shivendu <kshivendu1@gmail.com>
Date:   Tue Sep 10 16:52:38 2024 +0530

    Track number of points optimized and expose in telemetry (#5000)
    
    * Track number of points optimized and expose in telemetry
    
    * refactor
    
    * openapi specs
    
    * remove dbg
    
    * Return num points optimized from optimize() func
    
    * fmt
    
    * fix
    
    * fix type in tests
    
    * Store total points indexed on shard level instead of optimization level
    
    * fmt
    
    * fix test
    
    * trigger ci
    
    * fix openapi schema
    
    * review fixes
    
    * fmt
    
    * improvements and fix test
    
    * review fixes
    
    * use const for indexing optimizer name
    
    * fmt
    
    * return segment id from optimize() func
    
    * review fixes
    
    * fix
    
    * fix
    
    * fik
    
    * minor var name improvement
    
    * Use Option<usize> to return segment id
    
    * Use segment ID type rather than ambiguous usize
    
    * fix test
    
    * avoid intermediate check
    
    * review fixes
    
    * Rename total_indexed_points to total_optimized_points
    
    * Update openapi schema
    
    * optimize() should return number of points in new segment instead of segment id
    
    * add else condition
    
    * take read lock
    
    * fmt
    
    * remove flaky assert
    
    * Count points on new segment without locking
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index aed8d9133..f8f650525 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -508,13 +508,15 @@ pub trait SegmentOptimizer {
     /// New optimized segment should be added into `segments`.
     /// If there were any record changes during the optimization - an additional plain segment will be created.
     ///
+    /// Returns id of the created optimized segment. If no optimization was done - returns None
+    ///
     fn optimize(
         &self,
         segments: LockedSegmentHolder,
         ids: Vec<SegmentId>,
         permit: CpuPermit,
         stopped: &AtomicBool,
-    ) -> CollectionResult<bool> {
+    ) -> CollectionResult<usize> {
         check_process_stopped(stopped)?;
 
         let mut timer = ScopeDurationMeasurer::new(self.get_telemetry_counter());
@@ -543,7 +545,7 @@ pub trait SegmentOptimizer {
 
         if !all_segments_ok {
             // Cancel the optimization
-            return Ok(false);
+            return Ok(0);
         }
 
         check_process_stopped(stopped)?;
@@ -665,6 +667,8 @@ pub trait SegmentOptimizer {
 
             optimized_segment.prefault_mmap_pages();
 
+            let point_count = optimized_segment.available_point_count();
+
             let (_, proxies) = write_segments_guard.swap_new(optimized_segment, &proxy_ids);
             debug_assert_eq!(
                 proxies.len(),
@@ -701,9 +705,10 @@ pub trait SegmentOptimizer {
                 }
                 tmp_segment.drop_data()?;
             }
-        }
 
-        timer.set_success(true);
-        Ok(true)
+            timer.set_success(true);
+
+            Ok(point_count)
+        }
     }
 }

commit b3b22793769d2a18b5be99beb96b29fbf275521e
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Sat Sep 14 20:53:07 2024 +0200

    Allow explicit populate of mmap (#4923)
    
    * expose mmap populate
    
    * expose mmap populate in open_read_mmap
    
    * FOR TEST, REVERSE IT: make InRamChunkedMmap default
    
    * enable populate advise on unix
    
    * fix clippy
    
    * unix -> linux
    
    * Update lib/collection/src/config.rs
    
    * clippy fixes
    
    * resolve conflicts
    
    * fmt
    
    * Runtime check for PopulateRead
    
    ---------
    
    Co-authored-by: xzfc <xzfcpw@gmail.com>

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index f8f650525..5778ee38d 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -637,9 +637,8 @@ pub trait SegmentOptimizer {
 
         // ---- SLOW PART ENDS HERE -----
 
-        check_process_stopped(stopped).inspect_err(|_error| {
-            self.handle_cancellation(&segments, &proxy_ids, &tmp_segment);
-        })?;
+        check_process_stopped(stopped)
+            .inspect_err(|_| self.handle_cancellation(&segments, &proxy_ids, &tmp_segment))?;
 
         {
             // This block locks all operations with collection. It should be fast

commit 6886766c8a09f40cee9c09e39827c2f7f53faca3
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Thu Oct 17 12:33:29 2024 +0200

    do cleanup on cancellation (#5253)

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 5778ee38d..32af8651e 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -9,7 +9,7 @@ use common::disk::dir_size;
 use io::storage_version::StorageVersion;
 use itertools::Itertools;
 use parking_lot::{Mutex, RwLock, RwLockUpgradableReadGuard};
-use segment::common::operation_error::check_process_stopped;
+use segment::common::operation_error::{check_process_stopped, OperationResult};
 use segment::common::operation_time_statistics::{
     OperationDurationsAggregator, ScopeDurationMeasurer,
 };
@@ -376,13 +376,17 @@ pub trait SegmentOptimizer {
         &self,
         segments: &LockedSegmentHolder,
         proxy_ids: &[SegmentId],
-        temp_segment: &LockedSegment,
-    ) {
+        temp_segment: LockedSegment,
+    ) -> OperationResult<()> {
         self.unwrap_proxy(segments, proxy_ids);
         if temp_segment.get().read().available_point_count() > 0 {
             let mut write_segments = segments.write();
-            write_segments.add_new_locked(temp_segment.clone());
+            write_segments.add_new_locked(temp_segment);
+        } else {
+            // Temp segment is already removed from proxy, so nobody could write to it in between
+            temp_segment.drop_data()?;
         }
+        Ok(())
     }
 
     /// Function to wrap slow part of optimization. Performs proxy rollback in case of cancellation.
@@ -600,9 +604,10 @@ pub trait SegmentOptimizer {
             proxy_ids
         };
 
-        check_process_stopped(stopped).inspect_err(|_| {
-            self.handle_cancellation(&segments, &proxy_ids, &tmp_segment);
-        })?;
+        if let Err(e) = check_process_stopped(stopped) {
+            self.handle_cancellation(&segments, &proxy_ids, tmp_segment)?;
+            return Err(CollectionError::from(e));
+        }
 
         // ---- SLOW PART -----
 
@@ -617,7 +622,8 @@ pub trait SegmentOptimizer {
             Ok(segment) => segment,
             Err(error) => {
                 if matches!(error, CollectionError::Cancelled { .. }) {
-                    self.handle_cancellation(&segments, &proxy_ids, &tmp_segment);
+                    self.handle_cancellation(&segments, &proxy_ids, tmp_segment)?;
+                    return Err(error);
                 }
                 return Err(error);
             }
@@ -637,8 +643,10 @@ pub trait SegmentOptimizer {
 
         // ---- SLOW PART ENDS HERE -----
 
-        check_process_stopped(stopped)
-            .inspect_err(|_| self.handle_cancellation(&segments, &proxy_ids, &tmp_segment))?;
+        if let Err(e) = check_process_stopped(stopped) {
+            self.handle_cancellation(&segments, &proxy_ids, tmp_segment)?;
+            return Err(CollectionError::from(e));
+        }
 
         {
             // This block locks all operations with collection. It should be fast

commit 6c162656f3a23a6e6601a58cf69f44bdcea0ab00
Author: Luis Cosso <luis.cossio@qdrant.com>
Date:   Wed Nov 13 08:49:42 2024 -0600

    Backward compatibility for mmap payload storage (#5398)
    
    * support mmap storage backward compat
    
    * fix clippy
    
    * review fixes + bump + restore Cargo.lock
    
    * fix clippy
    
    * map_err instead of match
    
    * add sanity tests for payload storage trait
    
    * fix clippy
    
    * error conversion
    
    * test persistance too
    
    * add config to enable mmap storage (#5434)

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 32af8651e..8233daacb 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -19,8 +19,8 @@ use segment::segment::{Segment, SegmentVersion};
 use segment::segment_constructor::build_segment;
 use segment::segment_constructor::segment_builder::SegmentBuilder;
 use segment::types::{
-    HnswConfig, Indexes, PayloadFieldSchema, PayloadKeyType, PayloadStorageType, PointIdType,
-    QuantizationConfig, SegmentConfig, VectorStorageType,
+    HnswConfig, Indexes, PayloadFieldSchema, PayloadKeyType, PointIdType, QuantizationConfig,
+    SegmentConfig, VectorStorageType,
 };
 
 use crate::collection_manager::holders::proxy_segment::ProxySegment;
@@ -85,11 +85,7 @@ pub trait SegmentOptimizer {
         let config = SegmentConfig {
             vector_data: collection_params.to_base_vector_data()?,
             sparse_vector_data: collection_params.to_sparse_vector_data()?,
-            payload_storage_type: if collection_params.on_disk_payload {
-                PayloadStorageType::OnDisk
-            } else {
-                PayloadStorageType::InMemory
-            },
+            payload_storage_type: collection_params.payload_storage_type(),
         };
         Ok(LockedSegment::new(build_segment(
             self.segments_path(),
@@ -296,11 +292,7 @@ pub trait SegmentOptimizer {
         let optimized_config = SegmentConfig {
             vector_data,
             sparse_vector_data,
-            payload_storage_type: if collection_params.on_disk_payload {
-                PayloadStorageType::OnDisk
-            } else {
-                PayloadStorageType::InMemory
-            },
+            payload_storage_type: collection_params.payload_storage_type(),
         };
 
         Ok(SegmentBuilder::new(

commit ddde62196b8d6415d87b7efcef7a705d0dd8359e
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Wed Nov 27 16:46:58 2024 +0100

    Points inconsistency during cancel of optimization (#5527)
    
    * test to demonstrate inconsistency failure during optimization
    
    * Fix apply points mixing point versions, only apply operation to latest (#5528)
    
    * Patch apply points, only apply point operations to latest point version
    
    * Simplify test, we don't need owned strings in JSON macro
    
    * Use ahash in apply points because we map to simple numbers
    
    * Minor improvements
    
    * preserve the update version before doing move_if_exists in proxy (#5531)
    
    * preserve the update version before doing move_if_exists in proxy
    
    * Simplify move_if_exists with flat if statements and early return
    
    * Fix inverted condition, consider deleted if delete is new
    
    * Use hash map keys iterator as more idiomatic Rust
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>
    Co-authored-by: Tim Vise <tim+github@visee.me>
    
    ---------
    
    Co-authored-by: Tim Vise <tim+github@visee.me>
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 8233daacb..ed516c447 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -20,7 +20,7 @@ use segment::segment_constructor::build_segment;
 use segment::segment_constructor::segment_builder::SegmentBuilder;
 use segment::types::{
     HnswConfig, Indexes, PayloadFieldSchema, PayloadKeyType, PointIdType, QuantizationConfig,
-    SegmentConfig, VectorStorageType,
+    SegmentConfig, SeqNumberType, VectorStorageType,
 };
 
 use crate::collection_manager::holders::proxy_segment::ProxySegment;
@@ -250,7 +250,7 @@ pub trait SegmentOptimizer {
 
                 match config_on_disk {
                     Some(true) => config.storage_type = VectorStorageType::Mmap, // Both agree, but prefer mmap storage type
-                    Some(false) => {}, // on_disk=false wins, do nothing
+                    Some(false) => {} // on_disk=false wins, do nothing
                     None => config.storage_type = VectorStorageType::Mmap, // Mmap threshold wins
                 }
 
@@ -399,7 +399,7 @@ pub trait SegmentOptimizer {
     fn build_new_segment(
         &self,
         optimizing_segments: &[LockedSegment],
-        proxy_deleted_points: Arc<RwLock<HashSet<PointIdType>>>,
+        proxy_deleted_points: Arc<RwLock<HashMap<PointIdType, SeqNumberType>>>,
         proxy_deleted_indexes: Arc<RwLock<HashSet<PayloadKeyType>>>,
         proxy_created_indexes: Arc<RwLock<HashMap<PayloadKeyType, PayloadFieldSchema>>>,
         permit: CpuPermit,
@@ -460,7 +460,7 @@ pub trait SegmentOptimizer {
         //
         // Use collection copy to prevent long time lock of `proxy_deleted_points`
         let deleted_points_snapshot: Vec<PointIdType> =
-            proxy_deleted_points.read().iter().cloned().collect();
+            proxy_deleted_points.read().keys().copied().collect();
 
         for &point_id in &deleted_points_snapshot {
             optimized_segment
@@ -548,7 +548,8 @@ pub trait SegmentOptimizer {
 
         let tmp_segment = self.temp_segment(false)?;
 
-        let proxy_deleted_points = Arc::new(RwLock::new(HashSet::<PointIdType>::new()));
+        let proxy_deleted_points =
+            Arc::new(RwLock::new(HashMap::<PointIdType, SeqNumberType>::new()));
         let proxy_deleted_indexes = Arc::new(RwLock::new(HashSet::<PayloadKeyType>::new()));
         let proxy_created_indexes = Arc::new(RwLock::new(HashMap::<
             PayloadKeyType,
@@ -626,7 +627,7 @@ pub trait SegmentOptimizer {
         // - exclude already removed points from post-optimization removing
         let already_remove_points = {
             let mut all_removed_points: HashSet<_> =
-                proxy_deleted_points.read().iter().cloned().collect();
+                proxy_deleted_points.read().keys().copied().collect();
             for existing_point in optimized_segment.iter_points() {
                 all_removed_points.remove(&existing_point);
             }
@@ -644,7 +645,9 @@ pub trait SegmentOptimizer {
             // This block locks all operations with collection. It should be fast
             let mut write_segments_guard = segments.write();
             let deleted_points = proxy_deleted_points.read();
-            let points_diff = deleted_points.difference(&already_remove_points);
+            let points_diff = deleted_points
+                .keys()
+                .filter(|&point_id| !already_remove_points.contains(point_id));
             for &point_id in points_diff {
                 optimized_segment
                     .delete_point(optimized_segment.version(), point_id)

commit a11f6ae05d160b045d7a9dda567af0fc582cffb6
Author: Tim Vise <tim+github@visee.me>
Date:   Fri Nov 29 16:37:59 2024 +0100

    Keep temporary segment if not empty (#5550)
    
    * Keep temporary segment if not empty
    
    * Fix is empty condition
    
    * Swap condition because it's cheaper
    
    * Keep segment if no appendable segment or if not empty

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index ed516c447..5dba07772 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -371,7 +371,7 @@ pub trait SegmentOptimizer {
         temp_segment: LockedSegment,
     ) -> OperationResult<()> {
         self.unwrap_proxy(segments, proxy_ids);
-        if temp_segment.get().read().available_point_count() > 0 {
+        if !temp_segment.get().read().is_empty() {
             let mut write_segments = segments.write();
             write_segments.add_new_locked(temp_segment);
         } else {
@@ -684,7 +684,7 @@ pub trait SegmentOptimizer {
             drop(optimizing_segments);
 
             // Append a temp segment to collection if it is not empty or there is no other appendable segment
-            if tmp_segment.get().read().available_point_count() > 0 || !has_appendable_segments {
+            if !has_appendable_segments || !tmp_segment.get().read().is_empty() {
                 write_segments_guard.add_new_locked(tmp_segment);
 
                 // unlock collection for search and updates

commit 3e550da32a929997269f5bd1336e34b29286dc69
Author: Tim Vise <tim+github@visee.me>
Date:   Tue Dec 3 16:37:33 2024 +0100

    In proxy segment, apply proxied point delete with its operation version (#5573)
    
    * In proxy segment, apply proxied point delete with its operation version
    
    * Also use correct delete version when propagating changes back to wrapped
    
    * Also delete points with version in segment optimizer build new segment
    
    * Track original/operation point versions separately on proxied deletes
    
    * Fix test compilation failures
    
    * Use explicit types when initializing default

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 5dba07772..4b3e2b58a 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -8,7 +8,7 @@ use common::cpu::CpuPermit;
 use common::disk::dir_size;
 use io::storage_version::StorageVersion;
 use itertools::Itertools;
-use parking_lot::{Mutex, RwLock, RwLockUpgradableReadGuard};
+use parking_lot::{Mutex, RwLockUpgradableReadGuard};
 use segment::common::operation_error::{check_process_stopped, OperationResult};
 use segment::common::operation_time_statistics::{
     OperationDurationsAggregator, ScopeDurationMeasurer,
@@ -18,12 +18,9 @@ use segment::index::sparse_index::sparse_index_config::SparseIndexType;
 use segment::segment::{Segment, SegmentVersion};
 use segment::segment_constructor::build_segment;
 use segment::segment_constructor::segment_builder::SegmentBuilder;
-use segment::types::{
-    HnswConfig, Indexes, PayloadFieldSchema, PayloadKeyType, PointIdType, QuantizationConfig,
-    SegmentConfig, SeqNumberType, VectorStorageType,
-};
+use segment::types::{HnswConfig, Indexes, QuantizationConfig, SegmentConfig, VectorStorageType};
 
-use crate::collection_manager::holders::proxy_segment::ProxySegment;
+use crate::collection_manager::holders::proxy_segment::{self, ProxySegment};
 use crate::collection_manager::holders::segment_holder::{
     LockedSegment, LockedSegmentHolder, SegmentId,
 };
@@ -399,9 +396,9 @@ pub trait SegmentOptimizer {
     fn build_new_segment(
         &self,
         optimizing_segments: &[LockedSegment],
-        proxy_deleted_points: Arc<RwLock<HashMap<PointIdType, SeqNumberType>>>,
-        proxy_deleted_indexes: Arc<RwLock<HashSet<PayloadKeyType>>>,
-        proxy_created_indexes: Arc<RwLock<HashMap<PayloadKeyType, PayloadFieldSchema>>>,
+        proxy_deleted_points: proxy_segment::LockedRmSet,
+        proxy_deleted_indexes: proxy_segment::LockedFieldsSet,
+        proxy_created_indexes: proxy_segment::LockedFieldsMap,
         permit: CpuPermit,
         stopped: &AtomicBool,
     ) -> CollectionResult<Segment> {
@@ -459,12 +456,15 @@ pub trait SegmentOptimizer {
         // Second step - delete all the rest points with full write lock
         //
         // Use collection copy to prevent long time lock of `proxy_deleted_points`
-        let deleted_points_snapshot: Vec<PointIdType> =
-            proxy_deleted_points.read().keys().copied().collect();
+        let deleted_points_snapshot = proxy_deleted_points
+            .read()
+            .iter()
+            .map(|(point_id, versions)| (*point_id, *versions))
+            .collect::<Vec<_>>();
 
-        for &point_id in &deleted_points_snapshot {
+        for (point_id, versions) in deleted_points_snapshot {
             optimized_segment
-                .delete_point(optimized_segment.version(), point_id)
+                .delete_point(versions.operation_version, point_id)
                 .unwrap();
         }
 
@@ -547,23 +547,18 @@ pub trait SegmentOptimizer {
         check_process_stopped(stopped)?;
 
         let tmp_segment = self.temp_segment(false)?;
-
-        let proxy_deleted_points =
-            Arc::new(RwLock::new(HashMap::<PointIdType, SeqNumberType>::new()));
-        let proxy_deleted_indexes = Arc::new(RwLock::new(HashSet::<PayloadKeyType>::new()));
-        let proxy_created_indexes = Arc::new(RwLock::new(HashMap::<
-            PayloadKeyType,
-            PayloadFieldSchema,
-        >::new()));
+        let proxy_deleted_points = proxy_segment::LockedRmSet::default();
+        let proxy_created_indexes = proxy_segment::LockedFieldsMap::default();
+        let proxy_deleted_indexes = proxy_segment::LockedFieldsSet::default();
 
         let mut proxies = Vec::new();
         for sg in optimizing_segments.iter() {
             let mut proxy = ProxySegment::new(
                 sg.clone(),
                 tmp_segment.clone(),
-                proxy_deleted_points.clone(),
-                proxy_created_indexes.clone(),
-                proxy_deleted_indexes.clone(),
+                Arc::clone(&proxy_deleted_points),
+                Arc::clone(&proxy_created_indexes),
+                Arc::clone(&proxy_deleted_indexes),
             );
             // Wrapped segment is fresh, so it has no operations
             // Operation with number 0 will be applied
@@ -606,9 +601,9 @@ pub trait SegmentOptimizer {
 
         let mut optimized_segment = match self.build_new_segment(
             &optimizing_segments,
-            proxy_deleted_points.clone(),
-            proxy_deleted_indexes.clone(),
-            proxy_created_indexes.clone(),
+            Arc::clone(&proxy_deleted_points),
+            Arc::clone(&proxy_deleted_indexes),
+            Arc::clone(&proxy_created_indexes),
             permit,
             stopped,
         ) {
@@ -646,11 +641,18 @@ pub trait SegmentOptimizer {
             let mut write_segments_guard = segments.write();
             let deleted_points = proxy_deleted_points.read();
             let points_diff = deleted_points
-                .keys()
-                .filter(|&point_id| !already_remove_points.contains(point_id));
-            for &point_id in points_diff {
+                .iter()
+                .filter(|&(point_id, _version)| !already_remove_points.contains(point_id));
+            let optimized_segment_version = optimized_segment.version();
+            for (&point_id, &versions) in points_diff {
+                // Delete points here with their operation version, that'll bump the optimized
+                // segment version and will ensure we flush the new changes
+                debug_assert!(
+                    versions.operation_version >= optimized_segment_version,
+                    "proxied point deletes should have newer version than segment",
+                );
                 optimized_segment
-                    .delete_point(optimized_segment.version(), point_id)
+                    .delete_point(versions.operation_version, point_id)
                     .unwrap();
             }
 

commit a55ca9dc8fcd6d6494d88148ea2103e9d053af4a
Author: Tim Vise <tim+github@visee.me>
Date:   Mon Dec 9 13:10:13 2024 +0100

     In proxy segment, apply proxied index changes with its operation version (#5581)
    
    * Reimplement index change tracking in proxy segment, include version
    
    Keep track of index changes in a single map and keep track of the
    operation version. Propagate this operation version when applying the
    index change to ensure segments are properly flushed.
    
    * Add proxy changed indexes collection type that enforces index ordering
    
    * Apply index changes before point deletions, add some debug assertions
    
    * Minor improvements

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 4b3e2b58a..163520069 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -20,7 +20,7 @@ use segment::segment_constructor::build_segment;
 use segment::segment_constructor::segment_builder::SegmentBuilder;
 use segment::types::{HnswConfig, Indexes, QuantizationConfig, SegmentConfig, VectorStorageType};
 
-use crate::collection_manager::holders::proxy_segment::{self, ProxySegment};
+use crate::collection_manager::holders::proxy_segment::{self, ProxyIndexChange, ProxySegment};
 use crate::collection_manager::holders::segment_holder::{
     LockedSegment, LockedSegmentHolder, SegmentId,
 };
@@ -386,8 +386,7 @@ pub trait SegmentOptimizer {
     ///
     /// * `optimizing_segments` - Segments to optimize
     /// * `proxy_deleted_points` - Holds a set of points, deleted while optimization was running
-    /// * `proxy_deleted_indexes` - Holds a set of Indexes, deleted while optimization was running
-    /// * `proxy_created_indexes` - Holds a set of Indexes, created while optimization was running
+    /// * `proxy_changed_indexes` - Holds a set of indexes changes, created or deleted while optimization was running
     /// * `stopped` - flag to check if optimization was cancelled by external thread
     ///
     /// # Result
@@ -397,8 +396,7 @@ pub trait SegmentOptimizer {
         &self,
         optimizing_segments: &[LockedSegment],
         proxy_deleted_points: proxy_segment::LockedRmSet,
-        proxy_deleted_indexes: proxy_segment::LockedFieldsSet,
-        proxy_created_indexes: proxy_segment::LockedFieldsMap,
+        proxy_changed_indexes: proxy_segment::LockedIndexChanges,
         permit: CpuPermit,
         stopped: &AtomicBool,
     ) -> CollectionResult<Segment> {
@@ -442,49 +440,52 @@ pub trait SegmentOptimizer {
             )?;
         }
 
-        for field in proxy_deleted_indexes.read().iter() {
-            segment_builder.remove_indexed_field(field);
-        }
-        for (field, schema_type) in proxy_created_indexes.read().iter() {
-            segment_builder.add_indexed_field(field.to_owned(), schema_type.to_owned());
+        // Apply index changes to segment builder
+        // Indexes are only used for defragmentation in segment builder, so versions are ignored
+        for (field_name, change) in proxy_changed_indexes.read().iter_unordered() {
+            match change {
+                ProxyIndexChange::Create(schema, _) => {
+                    segment_builder.add_indexed_field(field_name.to_owned(), schema.to_owned());
+                }
+                ProxyIndexChange::Delete(_) => {
+                    segment_builder.remove_indexed_field(field_name);
+                }
+            }
         }
 
         let mut optimized_segment: Segment = segment_builder.build(permit, stopped)?;
 
-        // Delete points in 2 steps
-        // First step - delete all points with read lock
-        // Second step - delete all the rest points with full write lock
-        //
-        // Use collection copy to prevent long time lock of `proxy_deleted_points`
+        // Apply index changes before point deletions
+        // Point deletions bump the segment version, can cause index changes to be ignored
+        let old_optimized_segment_version = optimized_segment.version();
+        for (field_name, change) in proxy_changed_indexes.read().iter_ordered() {
+            debug_assert!(
+                change.version() >= old_optimized_segment_version,
+                "proxied index change should have newer version than segment",
+            );
+            match change {
+                ProxyIndexChange::Create(schema, version) => {
+                    optimized_segment.create_field_index(*version, field_name, Some(schema))?;
+                }
+                ProxyIndexChange::Delete(version) => {
+                    optimized_segment.delete_field_index(*version, field_name)?;
+                }
+            }
+            self.check_cancellation(stopped)?;
+        }
+
+        // Delete points
         let deleted_points_snapshot = proxy_deleted_points
             .read()
             .iter()
             .map(|(point_id, versions)| (*point_id, *versions))
             .collect::<Vec<_>>();
-
         for (point_id, versions) in deleted_points_snapshot {
             optimized_segment
                 .delete_point(versions.operation_version, point_id)
                 .unwrap();
         }
 
-        let deleted_indexes = proxy_deleted_indexes.read().iter().cloned().collect_vec();
-        let create_indexes = proxy_created_indexes.read().clone();
-
-        for delete_field_name in &deleted_indexes {
-            optimized_segment.delete_field_index(optimized_segment.version(), delete_field_name)?;
-            self.check_cancellation(stopped)?;
-        }
-
-        for (create_field_name, schema) in create_indexes {
-            optimized_segment.create_field_index(
-                optimized_segment.version(),
-                &create_field_name,
-                Some(&schema),
-            )?;
-            self.check_cancellation(stopped)?;
-        }
-
         Ok(optimized_segment)
     }
 
@@ -548,8 +549,7 @@ pub trait SegmentOptimizer {
 
         let tmp_segment = self.temp_segment(false)?;
         let proxy_deleted_points = proxy_segment::LockedRmSet::default();
-        let proxy_created_indexes = proxy_segment::LockedFieldsMap::default();
-        let proxy_deleted_indexes = proxy_segment::LockedFieldsSet::default();
+        let proxy_index_changes = proxy_segment::LockedIndexChanges::default();
 
         let mut proxies = Vec::new();
         for sg in optimizing_segments.iter() {
@@ -557,8 +557,7 @@ pub trait SegmentOptimizer {
                 sg.clone(),
                 tmp_segment.clone(),
                 Arc::clone(&proxy_deleted_points),
-                Arc::clone(&proxy_created_indexes),
-                Arc::clone(&proxy_deleted_indexes),
+                Arc::clone(&proxy_index_changes),
             );
             // Wrapped segment is fresh, so it has no operations
             // Operation with number 0 will be applied
@@ -602,8 +601,7 @@ pub trait SegmentOptimizer {
         let mut optimized_segment = match self.build_new_segment(
             &optimizing_segments,
             Arc::clone(&proxy_deleted_points),
-            Arc::clone(&proxy_deleted_indexes),
-            Arc::clone(&proxy_created_indexes),
+            Arc::clone(&proxy_index_changes),
             permit,
             stopped,
         ) {
@@ -639,16 +637,35 @@ pub trait SegmentOptimizer {
         {
             // This block locks all operations with collection. It should be fast
             let mut write_segments_guard = segments.write();
+            let old_optimized_segment_version = optimized_segment.version();
+
+            // Apply index changes before point deletions
+            // Point deletions bump the segment version, can cause index changes to be ignored
+            for (field_name, change) in proxy_index_changes.read().iter_ordered() {
+                debug_assert!(
+                    change.version() >= old_optimized_segment_version,
+                    "proxied index change should have newer version than segment",
+                );
+                match change {
+                    ProxyIndexChange::Create(schema, version) => {
+                        optimized_segment.create_field_index(*version, field_name, Some(schema))?;
+                    }
+                    ProxyIndexChange::Delete(version) => {
+                        optimized_segment.delete_field_index(*version, field_name)?;
+                    }
+                }
+                self.check_cancellation(stopped)?;
+            }
+
             let deleted_points = proxy_deleted_points.read();
             let points_diff = deleted_points
                 .iter()
                 .filter(|&(point_id, _version)| !already_remove_points.contains(point_id));
-            let optimized_segment_version = optimized_segment.version();
             for (&point_id, &versions) in points_diff {
                 // Delete points here with their operation version, that'll bump the optimized
                 // segment version and will ensure we flush the new changes
                 debug_assert!(
-                    versions.operation_version >= optimized_segment_version,
+                    versions.operation_version >= old_optimized_segment_version,
                     "proxied point deletes should have newer version than segment",
                 );
                 optimized_segment
@@ -656,19 +673,6 @@ pub trait SegmentOptimizer {
                     .unwrap();
             }
 
-            for deleted_field_name in proxy_deleted_indexes.read().iter() {
-                optimized_segment
-                    .delete_field_index(optimized_segment.version(), deleted_field_name)?;
-            }
-
-            for (created_field_name, schema_type) in proxy_created_indexes.read().iter() {
-                optimized_segment.create_field_index(
-                    optimized_segment.version(),
-                    created_field_name,
-                    Some(schema_type),
-                )?;
-            }
-
             optimized_segment.prefault_mmap_pages();
 
             let point_count = optimized_segment.available_point_count();
@@ -677,7 +681,7 @@ pub trait SegmentOptimizer {
             debug_assert_eq!(
                 proxies.len(),
                 proxy_ids.len(),
-                "swapped different number of proxies on unwrap, missing or incorrect segment IDs?"
+                "swapped different number of proxies on unwrap, missing or incorrect segment IDs?",
             );
 
             let has_appendable_segments = write_segments_guard.has_appendable_segment();

commit f5bec253b6e1d9931dec902b0ccaa68475cad05f
Author: Tim Vise <tim+github@visee.me>
Date:   Mon Dec 9 15:33:26 2024 +0100

    Minor refactoring during testing (#5610)

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 163520069..15cffb0ef 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -506,7 +506,6 @@ pub trait SegmentOptimizer {
     /// If there were any record changes during the optimization - an additional plain segment will be created.
     ///
     /// Returns id of the created optimized segment. If no optimization was done - returns None
-    ///
     fn optimize(
         &self,
         segments: LockedSegmentHolder,
@@ -584,8 +583,7 @@ pub trait SegmentOptimizer {
                 // because optimized segments could have been changed.
                 // The probability is small, though,
                 // so we can afford this operation under the full collection write lock
-                let op_num = 0;
-                proxy.replicate_field_indexes(op_num)?; // Slow only in case the index is change in the gap between two calls
+                proxy.replicate_field_indexes(0)?; // Slow only in case the index is change in the gap between two calls
                 proxy_ids.push(write_segments.swap_new(proxy, &[idx]).0);
             }
             proxy_ids

commit 3d950185fa157f24ad1ef3b50f6cf4b6f094a40c
Author: Tim Vise <tim+github@visee.me>
Date:   Tue Dec 17 10:56:13 2024 +0100

    Fix debug assertion for propagating segment proxy deletes (#5655)
    
    * Fix debug assertion for propagating segment proxy deletes
    
    Point delete operations don't have to be newer than the segment version,
    but they should not be older than the point version of the wrapped
    segment.
    
    * Update assert messages

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 15cffb0ef..ac78d89a1 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -663,8 +663,9 @@ pub trait SegmentOptimizer {
                 // Delete points here with their operation version, that'll bump the optimized
                 // segment version and will ensure we flush the new changes
                 debug_assert!(
-                    versions.operation_version >= old_optimized_segment_version,
-                    "proxied point deletes should have newer version than segment",
+                    versions.operation_version
+                        >= optimized_segment.point_version(point_id).unwrap_or(0),
+                    "proxied point deletes should have newer version than point in segment",
                 );
                 optimized_segment
                     .delete_point(versions.operation_version, point_id)

commit 38f478ddf7a9d03a1c783c5599f3b6ae33a05195
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Thu Jan 16 14:25:55 2025 +0100

    Measure payload read IO (#5773)
    
    * Measure read io for payload storage
    
    * Add Hardware Counter to update functions
    
    * Fix tests and benches
    
    * Rename (some) *_measured functions back to original

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index ac78d89a1..025821a5e 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -4,6 +4,7 @@ use std::path::Path;
 use std::sync::atomic::{AtomicBool, Ordering};
 use std::sync::Arc;
 
+use common::counter::hardware_counter::HardwareCounterCell;
 use common::cpu::CpuPermit;
 use common::disk::dir_size;
 use io::storage_version::StorageVersion;
@@ -482,7 +483,11 @@ pub trait SegmentOptimizer {
             .collect::<Vec<_>>();
         for (point_id, versions) in deleted_points_snapshot {
             optimized_segment
-                .delete_point(versions.operation_version, point_id)
+                .delete_point(
+                    versions.operation_version,
+                    point_id,
+                    &HardwareCounterCell::disposable(), // Internal operation, no need for measurement.
+                )
                 .unwrap();
         }
 
@@ -668,7 +673,11 @@ pub trait SegmentOptimizer {
                     "proxied point deletes should have newer version than point in segment",
                 );
                 optimized_segment
-                    .delete_point(versions.operation_version, point_id)
+                    .delete_point(
+                        versions.operation_version,
+                        point_id,
+                        &HardwareCounterCell::disposable(), // Internal operation, no measurement needed!
+                    )
                     .unwrap();
             }
 

commit caed5729e5b7ff3db9dcb4531a4af0929b186682
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Thu Feb 20 09:05:00 2025 +0100

    IO resource usage permit (#6015)
    
    * rename cpu_budget -> resource_budget
    
    * clippy
    
    * add io budget to resources
    
    * fmt
    
    * move budget structures into a separate file
    
    * add extend permit function
    
    * dont extend existing permit
    
    * switch from IO to CPU permit
    
    * do not release resource before aquiring an extension
    
    * fmt
    
    * Review remarks
    
    * Improve resource permit number assertion
    
    * Make resource permit replace_with only acquire extra needed permits
    
    * Remove obsolete drop implementation
    
    * allocate IO budget same as CPU
    
    * review fixes
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 025821a5e..2c50b14a0 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -4,8 +4,8 @@ use std::path::Path;
 use std::sync::atomic::{AtomicBool, Ordering};
 use std::sync::Arc;
 
+use common::budget::{ResourceBudget, ResourcePermit};
 use common::counter::hardware_counter::HardwareCounterCell;
-use common::cpu::CpuPermit;
 use common::disk::dir_size;
 use io::storage_version::StorageVersion;
 use itertools::Itertools;
@@ -398,7 +398,8 @@ pub trait SegmentOptimizer {
         optimizing_segments: &[LockedSegment],
         proxy_deleted_points: proxy_segment::LockedRmSet,
         proxy_changed_indexes: proxy_segment::LockedIndexChanges,
-        permit: CpuPermit,
+        permit: ResourcePermit, // IO resources for copying data
+        resource_budget: ResourceBudget,
         stopped: &AtomicBool,
     ) -> CollectionResult<Segment> {
         let mut segment_builder = self.optimized_segment_builder(optimizing_segments)?;
@@ -454,7 +455,52 @@ pub trait SegmentOptimizer {
             }
         }
 
-        let mut optimized_segment: Segment = segment_builder.build(permit, stopped)?;
+        // 000 - acquired
+        // +++ - blocked on waiting
+        //
+        // Case: 1 indexation job at a time, long indexing
+        //
+        //  IO limit = 1
+        // CPU limit = 2                         Next optimization
+        //                                                   loop
+        //                                       
+        //                                       
+        //  IO 0  00000000000000                  000000000
+        // CPU 1              00000000000000000
+        //     2              00000000000000000
+        //
+        //
+        //  IO 0  ++++++++++++++00000000000000000
+        // CPU 1                       ++++++++0000000000
+        //     2                       ++++++++0000000000
+        //
+        //
+        //  Case: 1 indexing job at a time, short indexation
+        //
+        //
+        //   IO limit = 1
+        //  CPU limit = 2
+        //
+        //
+        //   IO 0  000000000000   ++++++++0000000000
+        //  CPU 1            00000
+        //      2            00000
+        //
+        //   IO 0  ++++++++++++00000000000   +++++++
+        //  CPU 1                       00000
+        //      2                       00000
+        // At this stage workload shifts from IO to CPU, so we can release IO permit
+
+        // Use same number of threads for indexing as for IO.
+        // This ensures that IO is equally distributed between optimization jobs.
+        let desired_cpus = permit.num_io as usize;
+        let indexing_permit = resource_budget
+            .replace_with(permit, desired_cpus, 0, stopped)
+            .map_err(|_| CollectionError::Cancelled {
+                description: "optimization cancelled while waiting for budget".to_string(),
+            })?;
+
+        let mut optimized_segment: Segment = segment_builder.build(indexing_permit, stopped)?;
 
         // Apply index changes before point deletions
         // Point deletions bump the segment version, can cause index changes to be ignored
@@ -515,7 +561,8 @@ pub trait SegmentOptimizer {
         &self,
         segments: LockedSegmentHolder,
         ids: Vec<SegmentId>,
-        permit: CpuPermit,
+        permit: ResourcePermit,
+        resource_budget: ResourceBudget,
         stopped: &AtomicBool,
     ) -> CollectionResult<usize> {
         check_process_stopped(stopped)?;
@@ -606,6 +653,7 @@ pub trait SegmentOptimizer {
             Arc::clone(&proxy_deleted_points),
             Arc::clone(&proxy_index_changes),
             permit,
+            resource_budget,
             stopped,
         ) {
             Ok(segment) => segment,

commit 8ad2b34265448ec01b89d4093de5fbb1a86dcd4d
Author: Tim Vise <tim+github@visee.me>
Date:   Tue Feb 25 11:21:25 2025 +0100

    Bump Rust edition to 2024 (#6042)
    
    * Bump Rust edition to 2024
    
    * gen is a reserved keyword now
    
    * Remove ref mut on references
    
    * Mark extern C as unsafe
    
    * Wrap unsafe function bodies in unsafe block
    
    * Geo hash implements Copy, don't reference but pass by value instead
    
    * Replace secluded self import with parent
    
    * Update execute_cluster_read_operation with new match semantics
    
    * Fix lifetime issue
    
    * Replace map_or with is_none_or
    
    * set_var is unsafe now
    
    * Reformat

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 2c50b14a0..77fd95e4b 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -1,8 +1,8 @@
 use std::collections::{HashMap, HashSet};
 use std::ops::Deref;
 use std::path::Path;
-use std::sync::atomic::{AtomicBool, Ordering};
 use std::sync::Arc;
+use std::sync::atomic::{AtomicBool, Ordering};
 
 use common::budget::{ResourceBudget, ResourcePermit};
 use common::counter::hardware_counter::HardwareCounterCell;
@@ -10,7 +10,7 @@ use common::disk::dir_size;
 use io::storage_version::StorageVersion;
 use itertools::Itertools;
 use parking_lot::{Mutex, RwLockUpgradableReadGuard};
-use segment::common::operation_error::{check_process_stopped, OperationResult};
+use segment::common::operation_error::{OperationResult, check_process_stopped};
 use segment::common::operation_time_statistics::{
     OperationDurationsAggregator, ScopeDurationMeasurer,
 };
@@ -124,7 +124,7 @@ pub trait SegmentOptimizer {
                 LockedSegment::Proxy(_) => {
                     return Err(CollectionError::service_error(
                         "Proxy segment is not expected here".to_string(),
-                    ))
+                    ));
                 }
             };
             let locked_segment = segment.read();

commit 7726126e5b73424e07216c135465ab8e7e665c56
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Fri Mar 21 11:30:14 2025 +0100

    (Potentially) Fix missing CF on flush on payload index change (#6214)
    
    * Handle missing CF on the payload index flush level
    
    * move error handling on the payload field level, so other fields are not skipped if one fails to flush
    
    * Merge nested match statement
    
    ---------
    
    Co-authored-by: timvisee <tim@visee.me>

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 77fd95e4b..ee4e35101 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -502,6 +502,13 @@ pub trait SegmentOptimizer {
 
         let mut optimized_segment: Segment = segment_builder.build(indexing_permit, stopped)?;
 
+        // Delete points
+        let deleted_points_snapshot = proxy_deleted_points
+            .read()
+            .iter()
+            .map(|(point_id, versions)| (*point_id, *versions))
+            .collect::<Vec<_>>();
+
         // Apply index changes before point deletions
         // Point deletions bump the segment version, can cause index changes to be ignored
         let old_optimized_segment_version = optimized_segment.version();
@@ -521,12 +528,6 @@ pub trait SegmentOptimizer {
             self.check_cancellation(stopped)?;
         }
 
-        // Delete points
-        let deleted_points_snapshot = proxy_deleted_points
-            .read()
-            .iter()
-            .map(|(point_id, versions)| (*point_id, *versions))
-            .collect::<Vec<_>>();
         for (point_id, versions) in deleted_points_snapshot {
             optimized_segment
                 .delete_point(
@@ -688,15 +689,13 @@ pub trait SegmentOptimizer {
         {
             // This block locks all operations with collection. It should be fast
             let mut write_segments_guard = segments.write();
-            let old_optimized_segment_version = optimized_segment.version();
 
             // Apply index changes before point deletions
             // Point deletions bump the segment version, can cause index changes to be ignored
             for (field_name, change) in proxy_index_changes.read().iter_ordered() {
-                debug_assert!(
-                    change.version() >= old_optimized_segment_version,
-                    "proxied index change should have newer version than segment",
-                );
+                // Warn: change version might be lower than the segment version,
+                // because we might already applied the change earlier in optimization.
+                // Applied optimizations are not removed from `proxy_index_changes`.
                 match change {
                     ProxyIndexChange::Create(schema, version) => {
                         optimized_segment.create_field_index(*version, field_name, Some(schema))?;

commit 5cd7239b61d1a6944984132283f762850275670f
Author: Jojii <15957865+JojiiOfficial@users.noreply.github.com>
Date:   Mon Mar 24 19:39:17 2025 +0100

    Measure Payload Index IO Writes (#6137)
    
    * Prepare measurement of index creation + Remove vector deletion
    measurement
    
    * add hw_counter to add_point functions
    
    * Adjust add_point(..) function signatures
    
    * Add new measurement type: payload index IO write
    
    * Measure payload index IO writes
    
    * Some Hw measurement performance improvements
    
    * Review remarks
    
    * Fix measurements in distributed setups
    
    * review fixes
    
    ---------
    
    Co-authored-by: generall <andrey@vasnetsov.com>

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index ee4e35101..3ece7cfe0 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -393,6 +393,7 @@ pub trait SegmentOptimizer {
     /// # Result
     ///
     /// Constructs optimized segment
+    #[allow(clippy::too_many_arguments)]
     fn build_new_segment(
         &self,
         optimizing_segments: &[LockedSegment],
@@ -401,6 +402,7 @@ pub trait SegmentOptimizer {
         permit: ResourcePermit, // IO resources for copying data
         resource_budget: ResourceBudget,
         stopped: &AtomicBool,
+        hw_counter: &HardwareCounterCell,
     ) -> CollectionResult<Segment> {
         let mut segment_builder = self.optimized_segment_builder(optimizing_segments)?;
 
@@ -500,7 +502,8 @@ pub trait SegmentOptimizer {
                 description: "optimization cancelled while waiting for budget".to_string(),
             })?;
 
-        let mut optimized_segment: Segment = segment_builder.build(indexing_permit, stopped)?;
+        let mut optimized_segment: Segment =
+            segment_builder.build(indexing_permit, stopped, hw_counter)?;
 
         // Delete points
         let deleted_points_snapshot = proxy_deleted_points
@@ -519,7 +522,12 @@ pub trait SegmentOptimizer {
             );
             match change {
                 ProxyIndexChange::Create(schema, version) => {
-                    optimized_segment.create_field_index(*version, field_name, Some(schema))?;
+                    optimized_segment.create_field_index(
+                        *version,
+                        field_name,
+                        Some(schema),
+                        hw_counter,
+                    )?;
                 }
                 ProxyIndexChange::Delete(version) => {
                     optimized_segment.delete_field_index(*version, field_name)?;
@@ -530,11 +538,7 @@ pub trait SegmentOptimizer {
 
         for (point_id, versions) in deleted_points_snapshot {
             optimized_segment
-                .delete_point(
-                    versions.operation_version,
-                    point_id,
-                    &HardwareCounterCell::disposable(), // Internal operation, no need for measurement.
-                )
+                .delete_point(versions.operation_version, point_id, hw_counter)
                 .unwrap();
         }
 
@@ -599,6 +603,8 @@ pub trait SegmentOptimizer {
 
         check_process_stopped(stopped)?;
 
+        let hw_counter = HardwareCounterCell::disposable(); // Internal operation, no measurement needed!
+
         let tmp_segment = self.temp_segment(false)?;
         let proxy_deleted_points = proxy_segment::LockedRmSet::default();
         let proxy_index_changes = proxy_segment::LockedIndexChanges::default();
@@ -613,7 +619,7 @@ pub trait SegmentOptimizer {
             );
             // Wrapped segment is fresh, so it has no operations
             // Operation with number 0 will be applied
-            proxy.replicate_field_indexes(0)?;
+            proxy.replicate_field_indexes(0, &hw_counter)?;
             proxies.push(proxy);
         }
 
@@ -636,7 +642,7 @@ pub trait SegmentOptimizer {
                 // because optimized segments could have been changed.
                 // The probability is small, though,
                 // so we can afford this operation under the full collection write lock
-                proxy.replicate_field_indexes(0)?; // Slow only in case the index is change in the gap between two calls
+                proxy.replicate_field_indexes(0, &hw_counter)?; // Slow only in case the index is change in the gap between two calls
                 proxy_ids.push(write_segments.swap_new(proxy, &[idx]).0);
             }
             proxy_ids
@@ -656,6 +662,7 @@ pub trait SegmentOptimizer {
             permit,
             resource_budget,
             stopped,
+            &hw_counter,
         ) {
             Ok(segment) => segment,
             Err(error) => {
@@ -698,7 +705,12 @@ pub trait SegmentOptimizer {
                 // Applied optimizations are not removed from `proxy_index_changes`.
                 match change {
                     ProxyIndexChange::Create(schema, version) => {
-                        optimized_segment.create_field_index(*version, field_name, Some(schema))?;
+                        optimized_segment.create_field_index(
+                            *version,
+                            field_name,
+                            Some(schema),
+                            &hw_counter,
+                        )?;
                     }
                     ProxyIndexChange::Delete(version) => {
                         optimized_segment.delete_field_index(*version, field_name)?;
@@ -720,11 +732,7 @@ pub trait SegmentOptimizer {
                     "proxied point deletes should have newer version than point in segment",
                 );
                 optimized_segment
-                    .delete_point(
-                        versions.operation_version,
-                        point_id,
-                        &HardwareCounterCell::disposable(), // Internal operation, no measurement needed!
-                    )
+                    .delete_point(versions.operation_version, point_id, &hw_counter)
                     .unwrap();
             }
 

commit 33e7aa0b981da077cf1c52f6f0d441442c2d3cb3
Author: Arnaud Gourlay <arnaud.gourlay@gmail.com>
Date:   Tue Apr 1 12:26:37 2025 +0200

    Fixes for Clippy 1.86 (#6290)

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 3ece7cfe0..028e969bc 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -553,8 +553,7 @@ pub trait SegmentOptimizer {
     ///
     /// * `segments` - segments holder
     /// * `ids` - list of segment ids to perform optimization on. All segments will be merged into single one
-    /// * `stopped` - flag for early stopping of the optimization.
-    ///               If appears to be `true` - optimization process should be cancelled, all segments unwrapped
+    /// * `stopped` - flag for early stopping of the optimization. If appears to be `true` - optimization process should be cancelled, all segments unwrapped.
     ///
     /// # Result
     ///

commit 6e0ddbafa950250daff35ebe44fb3ec6afad944f
Author: Andrey Vasnetsov <andrey@vasnetsov.com>
Date:   Wed Apr 9 10:54:30 2025 +0200

    disk cache hygiene (#6323)
    
    * wip: implement explicit populate and clear_cache functions for all components
    
    * fmt
    
    * implement clear and populate for vector storages
    
    * fmt
    
    * implement clear and populate for payload storage
    
    * wip: implement explicit populate and clear_cache functions payload indexes
    
    * implement explicit populate and clear_cache functions payload indexes
    
    * fix clippy on CI
    
    * only compile posix_fadvise on linux
    
    * only compile posix_fadvise on linux
    
    * implement explicit populate and clear_cache functions for quantized vectors
    
    * fmt
    
    * remove post-load prefault
    
    * fix typo
    
    * implement is-on-disk for payload indexes, implement clear on drop for segment, implement clear after segment build
    
    * fmt
    
    * also evict quantized vectors after optimization
    
    * re-use and replace advise_dontneed

diff --git a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
index 028e969bc..2bf9a1acb 100644
--- a/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
+++ b/lib/collection/src/collection_manager/optimizers/segment_optimizer.rs
@@ -735,8 +735,6 @@ pub trait SegmentOptimizer {
                     .unwrap();
             }
 
-            optimized_segment.prefault_mmap_pages();
-
             let point_count = optimized_segment.available_point_count();
 
             let (_, proxies) = write_segments_guard.swap_new(optimized_segment, &proxy_ids);

