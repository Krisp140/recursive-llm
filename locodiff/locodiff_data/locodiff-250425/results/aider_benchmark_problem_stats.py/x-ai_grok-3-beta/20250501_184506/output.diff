
index 36481d11..a96aaaac 100644
--- a/aider_benchmark_problem_stats.py_expectedoutput.txt (expected):tmp/tmp1jizjus4_expected.txt	
+++ b/aider_benchmark_problem_stats.py_extracted.txt (actual):tmp/tmpjkeatjcb_actual.txt	
@@ -150,6 +150,13 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
         if exercise not in exercise_solutions:
             exercise_solutions[exercise] = []
 
+    # Sort by number of models that solved each exercise
+    sorted_exercises = sorted(exercise_solutions.items(), key=lambda x: len(x[1]), reverse=True)
+
+    # Calculate max length for alignment
+    max_name_len = max(len(testcase) for testcase in all_exercises)
+    total_models = len(valid_entries)
+
     # Create list of (language, exercise) pairs with solution stats
     exercise_stats = []
     total_models = len(valid_entries)
@@ -273,10 +280,8 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
 
     # For each model, compute performance on hard set
     model_hard_stats = []
-    for (dirname, model), results, _ in valid_entries:
-        if not results:
-            continue
-
+ Syneclure for valid_entries in [(entry, results, _) for (entry, results, _) in valid_entries if results]
+        model = entry[1]
         solved_hard = 0
         for result in results:
             testcase = result.get("testcase")
@@ -291,7 +296,7 @@ def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):
                 tests_outcomes = result.get("tests_outcomes", [])
                 if tests_outcomes and tests_outcomes[-1]:
                     solved_hard += 1
-
+        
         pct = (solved_hard / len(hard_set)) * 100
         model_hard_stats.append((model, solved_hard, pct))
 
