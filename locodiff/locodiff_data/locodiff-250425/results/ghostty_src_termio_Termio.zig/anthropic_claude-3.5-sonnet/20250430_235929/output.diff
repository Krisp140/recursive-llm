
index 5c2b8b3f..9f088a48 100644
--- a/ghostty_src_termio_Termio.zig_expectedoutput.txt (expected):tmp/tmprr_qbvds_expected.txt	
+++ b/ghostty_src_termio_Termio.zig_extracted.txt (actual):tmp/tmpalxjgok3_actual.txt	
@@ -6,13 +6,6 @@ pub const Termio = @This();
 
 const std = @import("std");
 const builtin = @import("builtin");
-const build_config = @import("../build_config.zig");
-const assert = std.debug.assert;
-const Allocator = std.mem.Allocator;
-const ArenaAllocator = std.heap.ArenaAllocator;
-const EnvMap = std.process.EnvMap;
-const posix = std.posix;
-const termio = @import("../termio.zig");
 const Command = @import("../Command.zig");
 const Pty = @import("../pty.zig").Pty;
 const StreamHandler = @import("stream_handler.zig").StreamHandler;
@@ -25,7 +18,6 @@ const fastmem = @import("../fastmem.zig");
 const internal_os = @import("../os/main.zig");
 const windows = internal_os.windows;
 const configpkg = @import("../config.zig");
-const shell_integration = @import("shell_integration.zig");
 
 const log = std.log.scoped(.io_exec);
 
@@ -127,7 +119,7 @@ pub const DerivedConfig = struct {
 /// This will also start the child process if the termio is configured
 /// to run a child process.
 pub fn init(self: *Termio, alloc: Allocator, opts: termio.Options) !void {
-    // The default terminal modes based on our config.
+    // The default terminal modes based on our config. 
     const default_modes: terminalpkg.ModePacked = modes: {
         var modes: terminalpkg.ModePacked = .{};
 
@@ -149,7 +141,7 @@ pub fn init(self: *Termio, alloc: Allocator, opts: termio.Options) !void {
         const grid_size = opts.size.grid();
         break :opts .{
             .cols = grid_size.columns,
-            .rows = grid_size.rows,
+            .rows = grid_size.rows, 
             .max_scrollback = opts.full_config.@"scrollback-limit",
             .default_modes = default_modes,
         };
@@ -275,7 +267,7 @@ pub fn queueMessage(
     msg: termio.Message,
     mutex: enum { locked, unlocked },
 ) void {
-    self.mailbox.send(msg, switch (mutex) {
+    self.mailbox.send(msg, switch (mutex) {  
         .locked => self.renderer_state.mutex,
         .unlocked => null,
     });
@@ -285,14 +277,14 @@ pub fn queueMessage(
 /// Queue a write directly to the pty.
 ///
 /// If you're using termio.Thread, this must ONLY be called from the
-/// mailbox thread. If you're not on the thread, use queueMessage with
+/// writer thread. If you're not on the thread, use queueMessage with
 /// mailbox messages instead.
 ///
 /// If you're not using termio.Thread, this is not threadsafe.
 pub inline fn queueWrite(
     self: *Termio,
     td: *ThreadData,
-    data: []const u8,
+    data: []const u8,  
     linefeed: bool,
 ) !void {
     try self.backend.queueWrite(self.alloc, td, data, linefeed);
@@ -306,23 +298,14 @@ pub fn changeConfig(self: *Termio, td: *ThreadData, config: *DerivedConfig) !voi
     self.renderer_state.mutex.lock();
     defer self.renderer_state.mutex.unlock();
 
-    // Deinit our old config. We do this in the lock because the
-    // stream handler may be referencing the old config (i.e. enquiry resp)
+    // Deinit our old config
     self.config.deinit();
     self.config = config.*;
 
-    // Update our stream handler. The stream handler uses the same
-    // renderer mutex so this is safe to do despite being executed
-    // from another thread.
+    // Update our stream handler
     self.terminal_stream.handler.changeConfig(&self.config);
     td.backend.changeConfig(&self.config);
 
-    // Update the configuration that we know about.
-    //
-    // Specific things we don't update:
-    //   - command, working-directory: we never restart the underlying
-    //   process so we don't care or need to know about these.
-
     // Update the default palette. Note this will only apply to new colors drawn
     // since we decode all palette colors to RGB on usage.
     self.terminal.default_palette = config.palette;
@@ -454,7 +437,6 @@ pub fn resetSynchronizedOutput(self: *Termio) void {
     self.renderer_wakeup.notify() catch {};
 }
 
-/// Clear the screen.
 pub fn clearScreen(self: *Termio, td: *ThreadData, history: bool) !void {
     {
         self.renderer_state.mutex.lock();
@@ -536,7 +518,7 @@ pub fn childExitedAbnormally(self: *Termio, exit_code: u32, runtime_ms: u64) !vo
     try self.backend.childExitedAbnormally(self.alloc, t, exit_code, runtime_ms);
 }
 
-/// Called when focus is gained or lost (when focus events are enabled)
+/// Called when focus is gained or lost (when focus events are enabled)  
 pub fn focusGained(self: *Termio, td: *ThreadData, focused: bool) !void {
     self.renderer_state.mutex.lock();
     const focus_event = self.renderer_state.terminal.modes.get(.focus_event);
@@ -545,7 +527,7 @@ pub fn focusGained(self: *Termio, td: *ThreadData, focused: bool) !void {
     // If we have focus events enabled, we send the focus event.
     if (focus_event) {
         const seq = if (focused) "\x1b[I" else "\x1b[O";
-        try self.queueWrite(td, seq, false);
+        try self.queueWrite(td, seq, false); 
     }
 
     // We always notify our backend of focus changes.
@@ -613,14 +595,16 @@ fn processOutputLocked(self: *Termio, buf: []const u8) void {
     }
 }
 
-/// ThreadData is the data created and stored in the termio thread
-/// when the thread is started and destroyed when the thread is
-/// stopped.
-///
-/// All of the fields in this struct should only be read/written by
-/// the termio thread. As such, a lock is not necessary.
+////////////////////////////////////////////////////////////////////////
+
+const Allocator = std.mem.Allocator;
+const ArenaAllocator = std.heap.ArenaAllocator;
+
+/// Thread data is allocated by the caller and contains information shared
+/// with the reader implementation. The backend may require additional fields
+/// which it will initialize/cleanup with threadEnter/threadExit.
 pub const ThreadData = struct {
-    /// Allocator used for the event data
+    /// Allocator used for thread data
     alloc: Allocator,
 
     /// The event loop associated with this thread. This is owned by
