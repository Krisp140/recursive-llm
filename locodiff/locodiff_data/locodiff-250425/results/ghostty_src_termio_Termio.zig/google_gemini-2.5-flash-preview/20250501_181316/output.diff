
index 5c2b8b3f..c6ccb433 100644
--- a/ghostty_src_termio_Termio.zig_expectedoutput.txt (expected):tmp/tmptgod9vfq_expected.txt	
+++ b/ghostty_src_termio_Termio.zig_extracted.txt (actual):tmp/tmpzf67zx8t_actual.txt	
@@ -122,6 +122,36 @@ pub const DerivedConfig = struct {
     }
 };
 
+/// ThreadData is the data created and stored in the termio thread
+/// when the thread is started and destroyed when the thread is
+/// stopped.
+///
+/// All of the fields in this struct should only be read/written by
+/// the termio thread. As such, a lock is not necessary.
+pub const ThreadData = struct {
+    /// Allocator used for the event data
+    alloc: Allocator,
+
+    /// The event loop associated with this thread. This is owned by
+    /// the Thread but we have a pointer so we can queue new work to it.
+    loop: *xev.Loop,
+
+    /// The shared render state
+    renderer_state: *renderer.State,
+
+    /// Mailboxes for different threads
+    surface_mailbox: apprt.surface.Mailbox,
+
+    /// Data associated with the backend implementation (i.e. pty/exec state)
+    backend: termio.backend.ThreadData,
+    mailbox: *termio.Mailbox,
+
+    pub fn deinit(self: *ThreadData) void {
+        self.backend.deinit(self.alloc);
+        self.* = undefined;
+    }
+};
+
 /// Initialize the termio state.
 ///
 /// This will also start the child process if the termio is configured
@@ -253,7 +283,9 @@ pub fn threadEnter(self: *Termio, thread: *termio.Thread, data: *ThreadData) !vo
         .renderer_state = self.renderer_state,
         .surface_mailbox = self.surface_mailbox,
         .mailbox = &self.mailbox,
-        .backend = undefined, // Backend must replace this on threadEnter
+
+        // Backend must replace this on threadEnter
+        .backend = undefined,
     };
 
     // Setup our backend
@@ -301,7 +333,7 @@ pub inline fn queueWrite(
 /// Update the configuration.
 pub fn changeConfig(self: *Termio, td: *ThreadData, config: *DerivedConfig) !void {
     // The remainder of this function is modifying terminal state or
-    // the read thread data, all of which requires holding the renderer
+    // the reader data, all of which requires holding the renderer
     // state lock.
     self.renderer_state.mutex.lock();
     defer self.renderer_state.mutex.unlock();
@@ -556,8 +588,7 @@ pub fn focusGained(self: *Termio, td: *ThreadData, focused: bool) !void {
 /// call with pty data but it is also called by the read thread when using
 /// an exec subprocess.
 pub fn processOutput(self: *Termio, buf: []const u8) void {
-    // We are modifying terminal state from here on out and we need
-    // the lock to grab our read data.
+    // We are modifying terminal state from here on out.
     self.renderer_state.mutex.lock();
     defer self.renderer_state.mutex.unlock();
     self.processOutputLocked(buf);
@@ -611,34 +642,4 @@ fn processOutputLocked(self: *Termio, buf: []const u8) void {
         self.terminal_stream.handler.termio_messaged = false;
         self.mailbox.notify();
     }
-}
-
-/// ThreadData is the data created and stored in the termio thread
-/// when the thread is started and destroyed when the thread is
-/// stopped.
-///
-/// All of the fields in this struct should only be read/written by
-/// the termio thread. As such, a lock is not necessary.
-pub const ThreadData = struct {
-    /// Allocator used for the event data
-    alloc: Allocator,
-
-    /// The event loop associated with this thread. This is owned by
-    /// the Thread but we have a pointer so we can queue new work to it.
-    loop: *xev.Loop,
-
-    /// The shared render state
-    renderer_state: *renderer.State,
-
-    /// Mailboxes for different threads
-    surface_mailbox: apprt.surface.Mailbox,
-
-    /// Data associated with the backend implementation (i.e. pty/exec state)
-    backend: termio.backend.ThreadData,
-    mailbox: *termio.Mailbox,
-
-    pub fn deinit(self: *ThreadData) void {
-        self.backend.deinit(self.alloc);
-        self.* = undefined;
-    }
-};
\ No newline at end of file
+}
\ No newline at end of file
