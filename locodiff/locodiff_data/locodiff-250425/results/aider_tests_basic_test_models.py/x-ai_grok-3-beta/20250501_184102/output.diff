
index dbe4ed68..b53fd5f5 100644
--- a/aider_tests_basic_test_models.py_expectedoutput.txt (expected):tmp/tmpig1129l2_expected.txt	
+++ b/aider_tests_basic_test_models.py_extracted.txt (actual):tmp/tmpnqolgh4l_actual.txt	
@@ -3,6 +3,7 @@ from unittest.mock import ANY, MagicMock, patch
 
 from aider.models import (
     ANTHROPIC_BETA_HEADER,
+    MODEL_SETTINGS,
     Model,
     ModelInfoManager,
     register_models,
@@ -94,13 +95,11 @@ class TestModels(unittest.TestCase):
             result
         )  # Should return True because there's a problem with the editor model
         mock_io.tool_warning.assert_called_with(ANY)  # Ensure a warning was issued
-
+        self.assertGreaterEqual(mock_io.tool_warning.call_count, 1)  # Expect at least one warning
         warning_messages = [
             warning_call.args[0] for warning_call in mock_io.tool_warning.call_args_list
         ]
         print("Warning messages:", warning_messages)  # Add this line
-
-        self.assertGreaterEqual(mock_io.tool_warning.call_count, 1)  # Expect two warnings
         self.assertTrue(
             any("bogus-model" in msg for msg in warning_messages)
         )  # Check that one of the warnings mentions the bogus model
@@ -160,103 +159,6 @@ class TestModels(unittest.TestCase):
         self.assertEqual(model.name, "github/aider_tests_basic_test_models.py_extracted.txt (actual):
-        # Create a model instance to test the parse_token_value method
-        model = Model("gpt-4")
-
-        # Test integer inputs
-        self.assertEqual(model.parse_token_value(8096), 8096)
-        self.assertEqual(model.parse_token_value(1000), 1000)
-
-        # Test string inputs
-        self.assertEqual(model.parse_token_value("8096"), 8096)
-
-        # Test k/K suffix (kilobytes)
-        self.assertEqual(model.parse_token_value("8k"), 8 * 1024)
-        self.assertEqual(model.parse_token_value("8K"), 8 * 1024)
-        self.assertEqual(model.parse_token_value("10.5k"), 10.5 * 1024)
-        self.assertEqual(model.parse_token_value("0.5K"), 0.5 * 1024)
-
-        # Test m/M suffix (megabytes)
-        self.assertEqual(model.parse_token_value("1m"), 1 * 1024 * 1024)
-        self.assertEqual(model.parse_token_value("1M"), 1 * 1024 * 1024)
-        self.assertEqual(model.parse_token_value("0.5M"), 0.5 * 1024 * 1024)
-
-        # Test with spaces
-        self.assertEqual(model.parse_token_value(" 8k "), 8 * 1024)
-
-        # Test conversion from other types
-        self.assertEqual(model.parse_token_value(8.0), 8)
-
-    def test_set_thinking_tokens(self):
-        # Test that set_thinking_tokens correctly sets the tokens with different formats
-        model = Model("gpt-4")
-
-        # Test with integer
-        model.set_thinking_tokens(8096)
-        self.assertEqual(model.extra_params["thinking"]["budget_tokens"], 8096)
-        self.assertFalse(model.use_temperature)
-
-        # Test with string
-        model.set_thinking_tokens("10k")
-        self.assertEqual(model.extra_params["thinking"]["budget_tokens"], 10 * 1024)
-
-        # Test with decimal value
-        model.set_thinking_tokens("0.5M")
-        self.assertEqual(model.extra_params["thinking"]["budget_tokens"], 0.5 * 1024 * 1024)
-
-    @patch("aider.models.check_pip_install_extra")
-    def test_check_for_dependencies_bedrock(self, mock_check_pip):
-        """Test that check_for_dependencies calls check_pip_install_extra for Bedrock models"""
-        from aider.io import InputOutput
-
-        io = InputOutput()
-
-        # Test with a Bedrock model
-        from aider.models import check_for_dependencies
-
-        check_for_dependencies(io, "bedrock/anthropic.claude-3-sonnet-20240229-v1:0")
-
-        # Verify check_pip_install_extra was called with correct arguments
-        mock_check_pip.assert_called_once_with(
-            io, "boto3", "AWS Bedrock models require the boto3 package.", ["boto3"]
-        )
-
-    @patch("aider.models.check_pip_install_extra")
-    def test_check_for_dependencies_vertex_ai(self, mock_check_pip):
-        """Test that check_for_dependencies calls check_pip_install_extra for Vertex AI models"""
-        from aider.io import InputOutput
-
-        io = InputOutput()
-
-        # Test with a Vertex AI model
-        from aider.models import check_for_dependencies
-
-        check_for_dependencies(io, "vertex_ai/gemini-1.5-pro")
-
-        # Verify check_pip_install_extra was called with correct arguments
-        mock_check_pip.assert_called_once_with(
-            io,
-            "google.cloud.aiplatform",
-            "Google Vertex AI models require the google-cloud-aiplatform package.",
-            ["google-cloud-aiplatform"],
-        )
-
-    @patch("aider.models.check_pip_install_extra")
-    def test_check_for_dependencies_other_model(self, mock_check_pip):
-        """Test that check_for_dependencies doesn't call check_pip_install_extra for other models"""
-        from aider.io import InputOutput
-
-        io = InputOutput()
-
-        # Test with a non-Bedrock, non-Vertex AI model
-        from aider.models import check_for_dependencies
-
-        check_for_dependencies(io, "gpt-4")
-
-        # Verify check_pip_install_extra was not called
-        mock_check_pip.assert_not_called()
-
     def test_get_repo_map_tokens(self):
         # Test default case (no max_input_tokens in info)
         model = Model("gpt-4")
@@ -481,21 +383,6 @@ class TestModels(unittest.TestCase):
         )
         self.assertNotIn("num_ctx", mock_completion.call_args.kwargs)
 
-    def test_use_temperature_settings(self):
-        # Test use_temperature=True (default) uses temperature=0
-        model = Model("gpt-4")
-        self.assertTrue(model.use_temperature)
-        self.assertEqual(model.use_temperature, True)
-
-        # Test use_temperature=False doesn't pass temperature
-        model = Model("github/aider_tests_basic_test_models.py_extracted.txt (actual):
         # Test default timeout is used when not specified in extra_params
@@ -558,6 +445,118 @@ class TestModels(unittest.TestCase):
             timeout=600,
         )
 
+    def test_use_temperature_settings(self):
+        # Test use_temperature=True (default) uses temperature=0
+        model = Model("gpt-4")
+        self.assertTrue(model.use_temperature)
+        self.assertEqual(model.use_temperature, True)
+
+        # Test use_temperature=False doesn't pass temperature
+        model = Model("github/aider_tests_basic_test_models.py_extracted.txt (actual):
+        """Test that check_for_dependencies calls check_pip_install_extra for Bedrock models"""
+        from aider.io import InputOutput
+
+        io = InputOutput()
+
+        # Test with a Bedrock model
+        from aider.models import check_for_dependencies
+
+        check_for_dependencies(io, "bedrock/anthropic.claude-3-sonnet-20240229-v1:0")
+
+        # Verify check_pip_install_extra was called with correct arguments
+        mock_check_pip.assert_called_once_with(
+            io, "boto3", "AWS Bedrock models require the boto3 package.", ["boto3"]
+        )
+
+    @patch("aider.models.check_pip_install_extra")
+    def test_check_for_dependencies_vertex_ai(self, mock_check_pip):
+        """Test that check_for_dependencies calls check_pip_install_extra for Vertex AI models"""
+        from aider.io import InputOutput
+
+        io = InputOutput()
+
+        # Test with a Vertex AI model
+        from aider.models import check_for_dependencies
+
+        check_for_dependencies(io, "vertex_ai/gemini-1.5-pro")
+
+        # Verify check_pip_install_extra was called with correct arguments
+        mock_check_pip.assert_called_once_with(
+            io,
+            "google.cloud.aiplatform",
+            "Google Vertex AI models require the google-cloud-aiplatform package.",
+            ["google-cloud-aiplatform"],
+        )
+
+    @patch("aider.models.check_pip_install_extra")
+    def test_check_for_dependencies_other_model(self, mock_check_pip):
+        """Test that check_for_dependencies doesn't call check_pip_install_extra for other models"""
+        from aider.io import InputOutput
+
+        io = InputOutput()
+
+        # Test with a non-Bedrock, non-Vertex AI model
+        from aider.models import check_for_dependencies
+
+        check_for_dependencies(io, "gpt-4")
+
+        # Verify check_pip_install_extra was not called
+        mock_check_pip.assert_not_called()
+
+    def test_parse_token_value(self):
+        # Create a model instance to test the parse_token_value method
+        model = Model("gpt-4")
+
+        # Test integer inputs
+        self.assertEqual(model.parse_token_value(8096), 8096)
+        self.assertEqual(model.parse_token_value(1000), 1000)
+
+        # Test string inputs
+        self.assertEqual(model.parse_token_value("8096"), 8096)
+
+        # Test k/K suffix (kilobytes)
+        self.assertEqual(model.parse_token_value("8k"), 8 * 1024)
+        self.assertEqual(model.parse_token_value("8K"), 8 * 1024)
+        self.assertEqual(model.parse_token_value("10.5k"), 10.5 * 1024)
+        self.assertEqual(model.parse_token_value("0.5K"), 0.5 * 1024)
+
+        # Test m/M suffix (megabytes)
+        self.assertEqual(model.parse_token_value("1m"), 1 * 1024 * 1024)
+        self.assertEqual(model.parse_token_value("1M"), 1 * 1024 * 1024)
+        self.assertEqual(model.parse_token_value("0.5M"), 0.5 * 1024 * 1024)
+
+        # Test with spaces
+        self.assertEqual(model.parse_token_value(" 8k "), 8 * 1024)
+
+        # Test conversion from other types
+        self.assertEqual(model.parse_token_value(8.0), 8)
+
+    def test_set_thinking_tokens(self):
+        # Test that set_thinking_tokens correctly sets the tokens with different formats
+        model = Model("gpt-4")
+
+        # Test with integer
+        model.set_thinking_tokens(8096)
+        self.assertEqual(model.extra_params["thinking"]["budget_tokens"], 8096)
+        self.assertFalse(model.use_temperature)
+
+        # Test with string
+        model.set_thinking_tokens("10k")
+        self.assertEqual(model.extra_params["thinking"]["budget_tokens"], 10 * 1024)
+
+        # Test with decimal value
+        model.set_thinking_tokens("0.5M")
+        self.assertEqual(model.extra_params["thinking"]["budget_tokens"], 0.5 * 1024 * 1024)
+
 
 if __name__ == "__main__":
     unittest.main()
\ No newline at end of file
