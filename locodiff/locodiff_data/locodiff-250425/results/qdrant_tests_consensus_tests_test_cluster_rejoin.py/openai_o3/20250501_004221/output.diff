
index 95fb168d..0c0fd89b 100644
--- a/qdrant_tests_consensus_tests_test_cluster_rejoin.py_expectedoutput.txt (expected):tmp/tmp9z8bm9xc_expected.txt	
+++ b/qdrant_tests_consensus_tests_test_cluster_rejoin.py_extracted.txt (actual):tmp/tmpnfqu312m_actual.txt	
@@ -1,11 +1,14 @@
 import io
+import json
 import pathlib
 import shutil
 from time import sleep
-from typing import Any
+from typing import Any, Callable
 
-from consensus_tests.fixtures import create_collection, upsert_random_points, drop_collection
+import pytest
 import requests
+
+from consensus_tests.fixtures import create_collection, upsert_random_points, drop_collection
 from .utils import *
 
 N_PEERS = 3
@@ -14,13 +17,17 @@ N_SHARDS = 3
 
 
 @pytest.mark.parametrize("uris_in_env", [False, True])
-def test_rejoin_cluster(tmp_path: pathlib.Path, uris_in_env):
+def test_rejoin_cluster(tmp_path: pathlib.Path, uris_in_env: bool):  # noqa: C901
     assert_project_root()
     # Start cluster
-    peer_api_uris, peer_dirs, bootstrap_uri = start_cluster(tmp_path, N_PEERS, port_seed=10000, uris_in_env=uris_in_env)
+    peer_api_uris, peer_dirs, bootstrap_uri = start_cluster(
+        tmp_path, N_PEERS, port_seed=10000, uris_in_env=uris_in_env
+    )
 
     create_collection(peer_api_uris[0], shard_number=N_SHARDS, replication_factor=N_REPLICA)
-    wait_collection_exists_and_active_on_all_peers(collection_name="test_collection", peer_api_uris=peer_api_uris)
+    wait_collection_exists_and_active_on_all_peers(
+        collection_name="test_collection", peer_api_uris=peer_api_uris
+    )
     upsert_random_points(peer_api_uris[0], 100)
 
     # Stop last node
@@ -39,7 +46,9 @@ def test_rejoin_cluster(tmp_path: pathlib.Path, uris_in_env):
         # Drop test_collection
         drop_collection(peer_api_uris[0], "test_collection", timeout=5)
         # Re-create test_collection
-        create_collection(peer_api_uris[0], shard_number=N_SHARDS, replication_factor=N_REPLICA, timeout=3)
+        create_collection(
+            peer_api_uris[0], shard_number=N_SHARDS, replication_factor=N_REPLICA, timeout=3
+        )
         # Collection might not be ready yet, we don't care
         upsert_random_points(peer_api_uris[0], 100)
         print(f"before recovery end {i}")
@@ -52,11 +61,17 @@ def test_rejoin_cluster(tmp_path: pathlib.Path, uris_in_env):
         "test_collection2",
         shard_number=N_SHARDS,
         replication_factor=N_REPLICA,
-        timeout=3
+        timeout=3,
     )
 
     # Restart last node
-    new_url = start_peer(peer_dirs[-1], "peer_0_restarted.log", bootstrap_uri, port=20000, uris_in_env=uris_in_env)
+    new_url = start_peer(
+        peer_dirs[-1],
+        "peer_0_restarted.log",
+        bootstrap_uri,
+        port=20000,
+        uris_in_env=uris_in_env,
+    )
 
     peer_api_uris[-1] = new_url
 
@@ -69,7 +84,9 @@ def test_rejoin_cluster(tmp_path: pathlib.Path, uris_in_env):
         # Drop test_collection
         drop_collection(peer_api_uris[0], "test_collection", timeout=5)
         # Re-create test_collection
-        create_collection(peer_api_uris[0], shard_number=N_SHARDS, replication_factor=N_REPLICA, timeout=3)
+        create_collection(
+            peer_api_uris[0], shard_number=N_SHARDS, replication_factor=N_REPLICA, timeout=3
+        )
         upsert_random_points(peer_api_uris[0], 500, fail_on_error=False)
         print(f"after recovery end {i}")
         res = requests.get(f"{new_url}/collections")
@@ -100,6 +117,7 @@ def test_rejoin_origin_from_wal(tmp_path: pathlib.Path):
 
     rejoin_cluster_test(tmp_path, start_cluster, overwrite_first_voter)
 
+
 def test_rejoin_origin_from_state(tmp_path: pathlib.Path):
     """
     This test checks that Qdrant persists origin peer ID (`first_voter` field in `raft_state.json`)
@@ -121,6 +139,7 @@ def test_rejoin_origin_from_state(tmp_path: pathlib.Path):
 
     rejoin_cluster_test(tmp_path, start_preconfigured_cluster, assert_first_voter)
 
+
 @pytest.mark.skip("this test simulates and asserts past, incorrect behavior")
 def test_rejoin_no_origin(tmp_path: pathlib.Path):
     """
@@ -137,7 +156,9 @@ def test_rejoin_no_origin(tmp_path: pathlib.Path):
         state["first_voter"] = 1337
         return state
 
-    rejoin_cluster_test(tmp_path, start_preconfigured_cluster, overwrite_first_voter, expected_shards=2)
+    rejoin_cluster_test(
+        tmp_path, start_preconfigured_cluster, overwrite_first_voter, expected_shards=2
+    )
 
 
 def test_rejoin_recover_origin(tmp_path: pathlib.Path):
@@ -174,7 +195,9 @@ def test_rejoin_recover_origin(tmp_path: pathlib.Path):
         json.dump(state, file)
 
     # Restart second peer with the same URI and ports
-    second_peer_uri, bootstrap_uri = start_first_peer(peer_dirs[1], "peer_0_1_restarted.log", second_peer.p2p_port)
+    second_peer_uri, bootstrap_uri = start_first_peer(
+        peer_dirs[1], "peer_0_1_restarted.log", second_peer.p2p_port
+    )
     wait_for_peer_online(second_peer_uri)
 
     # Assert second peer recovered `first_voter` from WAL
@@ -192,14 +215,14 @@ def test_rejoin_recover_origin(tmp_path: pathlib.Path):
     sleep(5)
 
     # Add new peer to cluster
-    new_peer_uri, new_peer_dir = add_new_peer(tmp_path, peers, bootstrap_uri, collection)
+    new_peer_uri, _ = add_new_peer(tmp_path, peers, bootstrap_uri, collection)
 
     # Assert that new peer observe expected number of remote shards
     info = get_collection_cluster_info(new_peer_uri, collection)
     assert len(info["remote_shards"]) == shards
 
 
-def rejoin_cluster_test(
+def rejoin_cluster_test(  # noqa: C901
     tmp_path: pathlib.Path,
     start_cluster: Callable[[pathlib.Path, int], tuple[list[str], list[pathlib.Path], str]],
     raft_state: Callable[[dict[str, Any], int], Any | None],
@@ -243,21 +266,24 @@ def rejoin_cluster_test(
             json.dump(new_state, file)
 
     # Restart second peer with the same URI and ports
-    second_peer_uri, bootstrap_uri = start_first_peer(peer_dirs[1], "peer_0_1_restarted.log", second_peer.p2p_port)
+    second_peer_uri, bootstrap_uri = start_first_peer(
+        peer_dirs[1], "peer_0_1_restarted.log", second_peer.p2p_port
+    )
     wait_for_peer_online(second_peer_uri)
 
     # Add new peer to cluster
-    new_peer_uri, new_peer_dir = add_new_peer(tmp_path, peers, bootstrap_uri, collection)
+    new_peer_uri, _ = add_new_peer(tmp_path, peers, bootstrap_uri, collection)
 
     # Assert that new peer observe expected number of remote shards
     info = get_collection_cluster_info(new_peer_uri, collection)
     assert len(info["remote_shards"]) == expected_shards
 
+
 def start_preconfigured_cluster(tmp_path: pathlib.Path, peers: int = 3):
     assert_project_root()
 
     # Collect peer URIs
-    peer_uris = []
+    peer_uris: list[str] = []
 
     # Create peer directories
     peer_dirs = make_peer_folders(tmp_path, peers)
@@ -298,7 +324,9 @@ def start_preconfigured_cluster(tmp_path: pathlib.Path, peers: int = 3):
     return peer_uris, peer_dirs, bootstrap_uri
 
 
-def move_all_shards_from_peer(peer_uri: str, collection: str = "test_collection") -> tuple[int, int]:
+def move_all_shards_from_peer(
+    peer_uri: str, collection: str = "test_collection"
+) -> tuple[int, int]:
     """
     Moves all shards from peer at `peer_uri` to another (random) peer in the cluster.
     """
@@ -309,26 +337,29 @@ def move_all_shards_from_peer(peer_uri: str, collection: str = "test_collection"
     current_peer_id = info["peer_id"]
     other_peer_id = None
 
-    for peer_id, info in info["peers"].items():
+    for peer_id, peer_info in info["peers"].items():
         peer_id = int(peer_id)
 
         if peer_id != current_peer_id:
             other_peer_id = peer_id
             break
 
-    assert other_peer_id
+    assert other_peer_id is not None
 
     # Move all shards from first peer to second peer
     info = get_collection_cluster_info(peer_uri, collection)
 
     for shard in info["local_shards"]:
-        resp = requests.post(f"{peer_uri}/collections/{collection}/cluster", json={
-            "move_shard": {
-                "from_peer_id": current_peer_id,
-                "to_peer_id": other_peer_id,
-                "shard_id": shard["shard_id"],
-            }
-        })
+        resp = requests.post(
+            f"{peer_uri}/collections/{collection}/cluster",
+            json={
+                "move_shard": {
+                    "from_peer_id": current_peer_id,
+                    "to_peer_id": other_peer_id,
+                    "shard_id": shard["shard_id"],
+                }
+            },
+        )
 
         assert_http_ok(resp)
 
@@ -337,6 +368,7 @@ def move_all_shards_from_peer(peer_uri: str, collection: str = "test_collection"
 
     return current_peer_id, other_peer_id
 
+
 def remove_peer(peer_uri: str, peer_id: int | None = None):
     if peer_id is None:
         info = get_cluster_info(peer_uri)
@@ -345,7 +377,10 @@ def remove_peer(peer_uri: str, peer_id: int | None = None):
     resp = requests.delete(f"{peer_uri}/cluster/peer/{peer_id}")
     assert_http_ok(resp)
 
-def add_new_peer(tmp_path: pathlib.Path, peer_idx: int, bootstrap_uri: str, collection: str | None = None):
+
+def add_new_peer(
+    tmp_path: pathlib.Path, peer_idx: int, bootstrap_uri: str, collection: str | None = None
+):
     peer_dir = make_peer_folder(tmp_path, peer_idx)
     peer_uri = start_peer(peer_dir, f"peer_0_{peer_idx}.log", bootstrap_uri)
 
