```rust
use std::collections::HashSet;
use std::path::{Path, PathBuf};
use std::sync::Arc;

use parking_lot::Mutex;
use segment::common::operation_time_statistics::OperationDurationsAggregator;
use segment::types::{HnswConfig, QuantizationConfig, SegmentType};

use crate::collection_manager::holders::segment_holder::{
    LockedSegmentHolder, SegmentHolder, SegmentId,
};
use crate::collection_manager::optimizers::segment_optimizer::{
    OptimizerThresholds, SegmentOptimizer,
};
use crate::config::CollectionParams;

const BYTES_IN_KB: usize = 1024;

/// Looks for the segments, which require to be indexed.
///
/// If segment is too large, but still does not have indexes - it is time to create some indexes.
/// The process of index creation is slow and CPU-bounded, so it is convenient to perform
/// index building in a same way as segment re-creation.
pub struct IndexingOptimizer {
    default_segments_number: usize,
    thresholds_config: OptimizerThresholds,
    segments_path: PathBuf,
    collection_temp_dir: PathBuf,
    collection_params: CollectionParams,
    hnsw_config: HnswConfig,
    quantization_config: Option<QuantizationConfig>,
    telemetry_durations_aggregator: Mutex<OperationDurationsAggregator>,
}

impl IndexingOptimizer {
    pub fn new(
        default_segments_number: usize,
        thresholds_config: OptimizerThresholds,
        segments_path: PathBuf,
        collection_temp_dir: PathBuf,
        collection_params: CollectionParams,
        hnsw_config: HnswConfig,
        quantization_config: Option<QuantizationConfig>,
    ) -> Self {
        IndexingOptimizer {
            default_segments_number,
            thresholds_config,
            segments_path,
            collection_temp_dir,
            collection_params,
            hnsw_config,
            quantization_config,
            telemetry_durations_aggregator: OperationDurationsAggregator::new(),
        }
    }

    fn smallest_indexed_segment(
        segments: &SegmentHolder,
        excluded_ids: &HashSet<SegmentId>,
    ) -> Option<(SegmentId, usize)> {
        segments
            .iter()
            .filter(|(idx, _)| !excluded_ids.contains(idx))
            .filter_map(|(idx, segment)| {
                let segment_entry = segment.get();
                let read_segment = segment_entry.read();
                let vector_size = read_segment
                    .max_available_vectors_size_in_bytes()
                    .unwrap_or(0);
    
                if read_segment.segment_type() == SegmentType::Special {
                    return None; // Never optimize already optimized segment
                }
    
                let segment_config = read_segment.config();
                let is_any_vector_indexed = segment_config.is_any_vector_indexed();
                let is_any_on_disk = segment_config.is_any_on_disk();
    
                if !(is_any_vector_indexed || is_any_on_disk) {
                    return None;
                }
    
                Some((idx, vector_size))
            })
            .min_by_key(|(_, vector_size)| *vector_size)
            .map(|(idx, size)| (idx, size))
    }
    
    fn worst_segment(
        &self,
        segments: LockedSegmentHolder,
        excluded_ids: &HashSet<SegmentId>,
    ) -> Vec<SegmentId> {
        let segments_read_guard = segments.read();
        let candidates: Vec<_> = segments_read_guard
            .iter()
            .filter(|(idx, _)| !excluded_ids.contains(idx))
            .filter_map(|(idx, segment)| {
                let segment_entry = segment.get();
                let read_segment = segment_entry.read();
                let vector_size = read_segment.max_available_vectors_size_in_bytes().unwrap_or(0);
    
                if read_segment.segment_type() == SegmentType::Special {
                    return None; // Never optimize already optimized segment
                }
    
                let segment_config = read_segment.config();
    
                let indexing_threshold_bytes = self
                    .thresholds_config
                    .indexing_threshold_kb
                    .saturating_mul(BYTES_IN_KB);
                let mmap_threshold_bytes = self
                    .thresholds_config
                    .memmap_threshold_kb
                    .saturating_mul(BYTES_IN_KB);
                let mut require_optimization = false;
    
                for (vector_name, vector_config) in self.collection_params.vectors.params_iter() {
                    if let Some(vector_data) = segment_config.vector_data.get(vector_name) {
                        let is_indexed = vector_data.index.is_indexed();
                        let is_on_disk = vector_data.storage_type.is_on_disk();
                        let storage_size_bytes = read_segment
                            .available_vectors_size_in_bytes(vector_name)
                            .unwrap_or_default();
    
                        let is_big_for_index = storage_size_bytes >= indexing_threshold_bytes;
                        let is_big_for_mmap = storage_size_bytes >= mmap_threshold_bytes;
    
                        let optimize_for_index = is_big_for_index && !is_indexed;
                        let optimize_for_mmap = if let Some(on_disk_config) = vector_config.on_disk
                        {
                            on_disk_config && !is_on_disk
                        } else {
                            is_big_for_mmap && !is_on_disk
                        };
    
                        if optimize_for_index || optimize_for_mmap {
                            require_optimization = true;
                            break;
                        }
                    }
                }
    
                if !require_optimization {
                    if let Some(sparse_vectors_params) =
                        self.collection_params.sparse_vectors.as_ref()
                    {
                        for sparse_vector_name in sparse_vectors_params.keys() {
                            if let Some(sparse_vector_data) =
                                segment_config.sparse_vector_data.get(sparse_vector_name)
                            {
                                let is_index_immutable =
                                    sparse_vector_data.index.index_type.is_immutable();
    
                                let storage_size = read_segment
                                    .available_vectors_size_in_bytes(sparse_vector_name)
                                    .unwrap_or_default();
    
                                let is_big_for_index = storage_size >= indexing_threshold_bytes;
                                let is_big_for_mmap = storage_size >= mmap_threshold_bytes;
    
                                if is_big_for_index && !is_index_immutable {
                                    require_optimization = true;
                                    break;
                                }
                            }
                        }
                    }
                }
    
                require_optimization.then_some((idx, vector_size))
            })
            .collect();
    
        // Select the largest unindexed segment, return if none
        let selected_segment = candidates
            .iter()
            .max_by_key(|(_, vector_size)| *vector_size);
        if selected_segment.is_none() {
            return vec![];
        }
        let (selected_segment_id, selected_segment_size) = *selected_segment.unwrap();
    
        let number_of_segments = segments_read_guard.len();
    
        // If the number of segments if equal or bigger than the default_segments_number
        // We want to make sure that we at least do not increase number of segments after optimization, thus we take more than one segment to optimize
    
        if number_of_segments < self.default_segments_number {
            return vec![selected_segment_id];
        }
    
        // It is better for scheduling if indexing optimizer optimizes 2 segments.
        // Because result of the optimization is usually 2 segment - it should preserve
        // overall count of segments.
    
        // Find the smallest unindexed to check if we can index together
        let smallest_unindexed = candidates
            .iter()
            .min_by_key(|(_, vector_size)| *vector_size);
        if let Some((idx, size)) = smallest_unindexed {
            if *idx != selected_segment_id
                && selected_segment_size + size
                    < self
                        .thresholds_config
                        .max_segment_size_kb
                        .saturating_mul(BYTES_IN_KB)
            {
                return vec![selected_segment_id, *idx];
            }
        }
    
        // Find smallest indexed to check if we can reindex together
        let smallest_indexed = Self::smallest_indexed_segment(&segments_read_guard, excluded_ids);
        if let Some((idx, size)) = smallest_indexed {
            if idx != selected_segment_id
                && selected_segment_size + size
                    < self
                        .thresholds_config
                        .max_segment_size_kb
                        .saturating_mul(BYTES_IN_KB)
            {
                return vec![selected_segment_id, idx];
            }
        }
    
        vec![selected_segment_id]
    }
}

impl SegmentOptimizer for IndexingOptimizer {
    fn name(&self) -> &'static str {
        "indexing"
    }

    fn segments_path(&self) -> &Path {
        self.segments_path.as_path()
    }

    fn temp_path(&self) -> &Path {
        self.collection_temp_dir.as_path()
    }

    fn collection_params(&self) -> CollectionParams {
        self.collection_params.clone()
    }

    fn hnsw_config(&self) -> &HnswConfig {
        &self.hnsw_config
    }

    fn quantization_config(&self) -> Option<QuantizationConfig> {
        self.quantization_config.clone()
    }

    fn threshold_config(&self) -> &OptimizerThresholds {
        &self.thresholds_config
    }

    fn check_condition(
        &self,
        segments: LockedSegmentHolder,
        excluded_ids: &HashSet<SegmentId>,
    ) -> Vec<SegmentId> {
        self.worst_segment(segments, excluded_ids)
    }

    fn get_telemetry_counter(&self) -> &Mutex<OperationDurationsAggregator> {
        &self.telemetry_durations_aggregator
    }
}
```