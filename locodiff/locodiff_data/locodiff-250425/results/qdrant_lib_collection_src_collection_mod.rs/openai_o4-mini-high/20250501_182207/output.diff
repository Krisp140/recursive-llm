
index 71e15f9b..451ae85b 100644
--- a/qdrant_lib_collection_src_collection_mod.rs_expectedoutput.txt (expected):tmp/tmp0i5ryhez_expected.txt	
+++ b/qdrant_lib_collection_src_collection_mod.rs_extracted.txt (actual):tmp/tmpezasvldo_actual.txt	
@@ -29,9 +29,6 @@ use tokio::sync::{Mutex, RwLock, RwLockWriteGuard};
 
 use crate::collection::payload_index_schema::PayloadIndexSchema;
 use crate::collection_state::{ShardInfo, State};
-use crate::common::collection_size_stats::{
-    CollectionSizeAtomicStats, CollectionSizeStats, CollectionSizeStatsCache,
-};
 use crate::common::is_ready::IsReady;
 use crate::config::CollectionConfigInternal;
 use crate::operations::config_diff::{DiffConfig, OptimizersConfigDiff};
@@ -42,20 +39,17 @@ use crate::save_on_disk::SaveOnDisk;
 use crate::shards::channel_service::ChannelService;
 use crate::shards::collection_shard_distribution::CollectionShardDistribution;
 use crate::shards::local_shard::clock_map::RecoveryPoint;
-use crate::shards::replica_set::ReplicaState::{Active, Dead, Initializing, Listener};
 use crate::shards::replica_set::{
     ChangePeerFromState, ChangePeerState, ReplicaState, ShardReplicaSet,
 };
+use crate::shards::resharding::ReshardKey;
 use crate::shards::shard::{PeerId, ShardId};
-use crate::shards::shard_holder::shard_mapping::ShardKeyMapping;
 use crate::shards::shard_holder::{LockedShardHolder, ShardHolder, shard_not_found_error};
 use crate::shards::transfer::helpers::check_transfer_conflicts_strict;
 use crate::shards::transfer::transfer_tasks_pool::{TaskResult, TransferTasksPool};
 use crate::shards::transfer::{ShardTransfer, ShardTransferMethod};
 use crate::shards::{CollectionId, replica_set};
-use crate::telemetry::{
-    CollectionConfigTelemetry, CollectionTelemetry, CollectionsAggregatedTelemetry,
-};
+use crate::telemetry::{CollectionConfigTelemetry, CollectionTelemetry, CollectionsAggregatedTelemetry};
 
 /// Collection's data is split into several shards.
 pub struct Collection {
@@ -85,6 +79,7 @@ pub struct Collection {
     update_runtime: Handle,
     // Search runtime handle.
     search_runtime: Handle,
+    // Budget for optimizer CPU/IO usage.
     optimizer_resource_budget: ResourceBudget,
     // Cached statistics of collection size, may be outdated.
     collection_stats_cache: CollectionSizeStatsCache,
@@ -93,7 +88,6 @@ pub struct Collection {
 }
 
 pub type RequestShardTransfer = Arc<dyn Fn(ShardTransfer) + Send + Sync>;
-
 pub type OnTransferFailure = Arc<dyn Fn(ShardTransfer, CollectionId, &str) + Send + Sync>;
 pub type OnTransferSuccess = Arc<dyn Fn(ShardTransfer, CollectionId) + Send + Sync>;
 
@@ -107,7 +101,6 @@ impl Collection {
         collection_config: &CollectionConfigInternal,
         shared_storage_config: Arc<SharedStorageConfig>,
         shard_distribution: CollectionShardDistribution,
-        shard_key_mapping: Option<ShardKeyMapping>,
         channel_service: ChannelService,
         on_replica_failure: ChangePeerFromState,
         request_shard_transfer: RequestShardTransfer,
@@ -120,23 +113,31 @@ impl Collection {
         let start_time = std::time::Instant::now();
 
         let mut shard_holder = ShardHolder::new(path)?;
-        shard_holder.set_shard_key_mappings(shard_key_mapping.clone().unwrap_or_default())?;
+        shard_holder.set_shard_key_mappings(
+            collection_config
+                .params
+                .shard_key_mapping
+                .clone()
+                .unwrap_or_default(),
+        )?;
 
         let payload_index_schema = Arc::new(Self::load_payload_index_schema(path)?);
 
         let shared_collection_config = Arc::new(RwLock::new(collection_config.clone()));
+        let mut effective_optimizers_config = collection_config.optimizer_config.clone();
+        if let Some(overwrite) = optimizers_overwrite.clone() {
+            effective_optimizers_config = overwrite.update(&effective_optimizers_config)?;
+        }
+
         for (shard_id, mut peers) in shard_distribution.shards {
             let is_local = peers.remove(&this_peer_id);
 
-            let mut effective_optimizers_config = collection_config.optimizer_config.clone();
-            if let Some(optimizers_overwrite) = optimizers_overwrite.clone() {
-                effective_optimizers_config =
-                    optimizers_overwrite.update(&effective_optimizers_config)?;
-            }
-
-            let shard_key = shard_key_mapping
+            let shard_key = collection_config
+                .params
+                .shard_key_mapping
                 .as_ref()
                 .and_then(|mapping| mapping.shard_key(shard_id));
+
             let replica_set = ShardReplicaSet::build(
                 shard_id,
                 shard_key.clone(),
@@ -148,9 +149,8 @@ impl Collection {
                 abort_shard_transfer.clone(),
                 path,
                 shared_collection_config.clone(),
-                effective_optimizers_config,
+                effective_optimizers_config.clone(),
                 shared_storage_config.clone(),
-                payload_index_schema.clone(),
                 channel_service.clone(),
                 update_runtime.clone().unwrap_or_else(Handle::current),
                 search_runtime.clone().unwrap_or_else(Handle::current),
@@ -159,14 +159,14 @@ impl Collection {
             )
             .await?;
 
-            shard_holder.add_shard(shard_id, replica_set, shard_key)?;
+            shard_holder.add_shard(shard_id, replica_set, shard_key.clone())?;
         }
 
         let locked_shard_holder = Arc::new(LockedShardHolder::new(shard_holder));
-
-        let collection_stats_cache = CollectionSizeStatsCache::new_with_values(
-            Self::estimate_collection_size_stats(&locked_shard_holder).await,
-        );
+        let local_stats_cache =
+            LocalDataStatsCache::new_with_values(Self::calculate_local_shards_stats(&locked_shard_holder).await);
+        let collection_stats_cache =
+            CollectionSizeStatsCache::new_with_values(Self::estimate_collection_size_stats(&locked_shard_holder).await);
 
         // Once the config is persisted - the collection is considered to be successfully created.
         CollectionVersion::save(path)?;
@@ -176,9 +176,9 @@ impl Collection {
             id: name.clone(),
             shards_holder: locked_shard_holder,
             collection_config: shared_collection_config,
-            optimizers_overwrite,
-            payload_index_schema,
             shared_storage_config,
+            payload_index_schema,
+            optimizers_overwrite,
             this_peer_id,
             path: path.to_owned(),
             snapshots_path: snapshots_path.to_owned(),
@@ -206,7 +206,7 @@ impl Collection {
         snapshots_path: &Path,
         shared_storage_config: Arc<SharedStorageConfig>,
         channel_service: ChannelService,
-        on_replica_failure: replica_set::ChangePeerFromState,
+        on_replica_failure: ChangePeerFromState,
         request_shard_transfer: RequestShardTransfer,
         abort_shard_transfer: replica_set::AbortShardTransfer,
         search_runtime: Option<Handle>,
@@ -218,15 +218,13 @@ impl Collection {
         let stored_version = CollectionVersion::load(path)
             .expect("Can't read collection version")
             .expect("Collection version is not found");
-
         let app_version = CollectionVersion::current();
 
         if stored_version > app_version {
             panic!("Collection version is greater than application version");
         }
-
         if stored_version != app_version {
-            if Self::can_upgrade_storage(&stored_version, &app_version) {
+            if Self::can_upgrade_storage(&stored_version, &app_version.parse().unwrap()) {
                 log::info!("Migrating collection {stored_version} -> {app_version}");
                 CollectionVersion::save(path)
                     .unwrap_or_else(|err| panic!("Can't save collection version {err}"));
@@ -247,23 +245,17 @@ impl Collection {
         });
         collection_config.validate_and_warn();
 
-        let mut shard_holder = ShardHolder::new(path).expect("Can not create shard holder");
+        let mut shard_holder = ShardHolder::new(path)?;
+        let shard_key_mapping = CollectionConfigInternal::load_shard_mapping(path)
+            .expect("Can't load or initialize shard key mapping");
 
-        let mut effective_optimizers_config = collection_config.optimizer_config.clone();
+        shard_holder.set_shard_key_mappings(shard_key_mapping.clone())?;
 
-        if let Some(optimizers_overwrite) = optimizers_overwrite.clone() {
-            effective_optimizers_config = optimizers_overwrite
-                .update(&effective_optimizers_config)
-                .expect("Can not apply optimizer overwrite");
-        }
+        let payload_index_schema = Arc::new(Self::load_payload_index_schema(path)
+            .expect("Can't load or initialize payload index schema"));
 
         let shared_collection_config = Arc::new(RwLock::new(collection_config.clone()));
 
-        let payload_index_schema = Arc::new(
-            Self::load_payload_index_schema(path)
-                .expect("Can't load or initialize payload index schema"),
-        );
-
         shard_holder
             .load_shards(
                 path,
@@ -271,7 +263,6 @@ impl Collection {
                 shared_collection_config.clone(),
                 effective_optimizers_config,
                 shared_storage_config.clone(),
-                payload_index_schema.clone(),
                 channel_service.clone(),
                 on_replica_failure.clone(),
                 abort_shard_transfer.clone(),
@@ -279,22 +270,21 @@ impl Collection {
                 update_runtime.clone().unwrap_or_else(Handle::current),
                 search_runtime.clone().unwrap_or_else(Handle::current),
                 optimizer_resource_budget.clone(),
+                shard_key_mapping.clone(),
             )
             .await;
 
         let locked_shard_holder = Arc::new(LockedShardHolder::new(shard_holder));
-
-        let collection_stats_cache = CollectionSizeStatsCache::new_with_values(
-            Self::estimate_collection_size_stats(&locked_shard_holder).await,
-        );
+        let collection_stats_cache =
+            CollectionSizeStatsCache::new_with_values(Self::estimate_collection_size_stats(&locked_shard_holder).await);
 
         Self {
             id: collection_id.clone(),
             shards_holder: locked_shard_holder,
             collection_config: shared_collection_config,
-            optimizers_overwrite,
-            payload_index_schema,
             shared_storage_config,
+            payload_index_schema,
+            optimizers_overwrite,
             this_peer_id,
             path: path.to_owned(),
             snapshots_path: snapshots_path.to_owned(),
@@ -318,12 +308,6 @@ impl Collection {
     /// If major version is different, then it is not compatible.
     /// If the difference in consecutive versions is greater than 1 in patch,
     /// then the collection is not compatible with the current version.
-    ///
-    /// Example:
-    ///   0.4.0 -> 0.4.1 = true
-    ///   0.4.0 -> 0.4.2 = false
-    ///   0.4.0 -> 0.5.0 = false
-    ///   0.4.0 -> 0.5.1 = false
     pub fn can_upgrade_storage(stored: &Version, app: &Version) -> bool {
         if stored.major != app.major {
             return false;
@@ -341,10 +325,6 @@ impl Collection {
         self.id.clone()
     }
 
-    pub async fn uuid(&self) -> Option<uuid::Uuid> {
-        self.collection_config.read().await.uuid
-    }
-
     pub async fn get_shard_keys(&self) -> Vec<ShardKey> {
         self.shards_holder
             .read()
@@ -400,20 +380,13 @@ impl Collection {
             replica_set.peer_state(peer_id),
         );
 
-        let current_state = replica_set.peer_state(peer_id);
-
-        // Validation:
-        //
-        // 1. Check that peer exists in the cluster (peer might *not* exist, if it was removed from
-        //    the cluster right before `SetShardReplicaSet` was proposed)
+        // 1. Check that peer or existing replica is in cluster
         let peer_exists = self
             .channel_service
             .id_to_address
             .read()
             .contains_key(&peer_id);
-
         let replica_exists = replica_set.peer_state(peer_id).is_some();
-
         if !peer_exists && !replica_exists {
             return Err(CollectionError::bad_input(format!(
                 "Can't set replica {peer_id}:{shard_id} state to {new_state:?}, \
@@ -422,7 +395,8 @@ impl Collection {
             )));
         }
 
-        // 2. Check that `from_state` matches current state
+        // 2. Check that from_state matches current
+        let current_state = replica_set.peer_state(peer_id);
         if from_state.is_some() && current_state != from_state {
             return Err(CollectionError::bad_input(format!(
                 "Replica {peer_id} of shard {shard_id} has state {current_state:?}, but expected {from_state:?}"
@@ -430,79 +404,49 @@ impl Collection {
         }
 
         // 3. Do not deactivate the last active replica
-        //
-        // `is_last_active_replica` counts both `Active` and `ReshardingScaleDown` replicas!
         if replica_set.is_last_active_replica(peer_id) && !new_state.is_active() {
             return Err(CollectionError::bad_input(format!(
                 "Cannot deactivate the last active replica {peer_id} of shard {shard_id}"
             )));
         }
 
-        // Update replica status
+        // Abort resharding if relevant
+        if matches!(current_state, Some(ReplicaState::Resharding | ReplicaState::ReshardingScaleDown))
+            && new_state == ReplicaState::Dead
+        {
+            drop(shard_holder);
+            if let Some(state) = self.resharding_state().await.filter(|s| s.peer_id == peer_id) {
+                self.abort_resharding(state.key(), false).await?;
+            }
+            return Ok(());
+        }
+
         replica_set
             .ensure_replica_with_state(peer_id, new_state)
             .await?;
 
         if new_state == ReplicaState::Dead {
-            let resharding_state = shard_holder.resharding_state.read().clone();
             let related_transfers = shard_holder.get_related_transfers(shard_id, peer_id);
-
-            // Functions below lock `shard_holder`!
             drop(shard_holder);
-
-            let mut abort_resharding_result = CollectionResult::Ok(());
-
-            // Abort resharding, if resharding shard is marked as `Dead`.
-            //
-            // This branch should only be triggered, if resharding is currently at `MigratingPoints`
-            // stage, because target shard should be marked as `Active`, when all resharding transfers
-            // are successfully completed, and so the check *right above* this one would be triggered.
-            //
-            // So, if resharding reached `ReadHashRingCommitted`, this branch *won't* be triggered,
-            // and resharding *won't* be cancelled. The update request should *fail* with "failed to
-            // update all replicas of a shard" error.
-            //
-            // If resharding reached `ReadHashRingCommitted`, and this branch is triggered *somehow*,
-            // then `Collection::abort_resharding` call should return an error, so no special handling
-            // is needed.
-            let is_resharding = current_state
-                .as_ref()
-                .is_some_and(ReplicaState::is_resharding);
-            if is_resharding {
-                if let Some(state) = resharding_state {
-                    abort_resharding_result = self.abort_resharding(state.key(), false).await;
-                }
-            }
-
-            // Terminate transfer if source or target replicas are now dead
             for transfer in related_transfers {
                 self.abort_shard_transfer(transfer.key(), None).await?;
             }
-
-            // Propagate resharding errors now
-            abort_resharding_result?;
         }
 
         // If not initialized yet, we need to check if it was initialized by this call
         if !self.is_initialized.check_ready() {
             let state = self.state().await;
-
             let mut is_ready = true;
-
-            for (_shard_id, shard_info) in state.shards {
-                let all_replicas_active = shard_info.replicas.into_iter().all(|(_, state)| {
-                    matches!(
-                        state,
-                        ReplicaState::Active | ReplicaState::ReshardingScaleDown
-                    )
-                });
-
-                if !all_replicas_active {
+            for (_id, shard_info) in state.shards {
+                let all_active = shard_info
+                    .replicas
+                    .into_iter()
+                    .all(|(_, st)| matches!(st, ReplicaState::Active | ReplicaState::ReshardingScaleDown));
+                if !all_active {
                     is_ready = false;
                     break;
                 }
             }
-
             if is_ready {
                 self.is_initialized.make_ready();
             }
@@ -513,14 +457,12 @@ impl Collection {
 
     pub async fn shard_recovery_point(&self, shard_id: ShardId) -> CollectionResult<RecoveryPoint> {
         let shard_holder_read = self.shards_holder.read().await;
-
         let shard = shard_holder_read.get_shard(shard_id);
         let Some(replica_set) = shard else {
             return Err(CollectionError::NotFound {
                 what: format!("Shard {shard_id}"),
             });
         };
-
         replica_set.shard_recovery_point().await
     }
 
@@ -530,14 +472,12 @@ impl Collection {
         cutoff: &RecoveryPoint,
     ) -> CollectionResult<()> {
         let shard_holder_read = self.shards_holder.read().await;
-
         let shard = shard_holder_read.get_shard(shard_id);
         let Some(replica_set) = shard else {
             return Err(CollectionError::NotFound {
                 what: format!("Shard {shard_id}"),
             });
         };
-
         replica_set.update_shard_cutoff_point(cutoff).await
     }
 
@@ -550,28 +490,29 @@ impl Collection {
             shards: shards_holder
                 .get_shards()
                 .map(|(shard_id, replicas)| {
-                    let shard_info = ShardInfo {
-                        replicas: replicas.peers(),
-                    };
+                    let shard_info = ShardInfo { replicas: replicas.peers() };
                     (shard_id, shard_info)
                 })
                 .collect(),
-            resharding,
             transfers,
             shards_key_mapping: shards_holder.get_shard_key_to_ids_mapping(),
             payload_index_schema: self.payload_index_schema.read().clone(),
+            resharding,
         }
     }
 
-    pub async fn remove_shards_at_peer(&self, peer_id: PeerId) -> CollectionResult<()> {
-        // Abort resharding, if shards are removed from peer driving resharding
-        // (which *usually* means the *peer* is being removed from consensus)
-        let resharding_state = self
-            .resharding_state()
-            .await
-            .filter(|state| state.peer_id == peer_id);
+    pub async fn apply_state(
+        &self,
+        state: State,
+        this_peer_id: PeerId,
+        abort_transfer: impl FnMut(ShardTransfer),
+    ) -> CollectionResult<()> {
+        state.apply(this_peer_id, self, abort_transfer).await
+    }
 
-        if let Some(state) = resharding_state {
+    pub async fn remove_shards_at_peer(&self, peer_id: PeerId) -> CollectionResult<()> {
+        // Abort resharding if shards driving resharding are removed
+        if let Some(state) = self.resharding_state().await.filter(|s| s.peer_id == peer_id) {
             if let Err(err) = self.abort_resharding(state.key(), true).await {
                 log::error!(
                     "Failed to abort resharding {} while removing peer {peer_id}: {err}",
@@ -579,12 +520,7 @@ impl Collection {
                 );
             }
         }
-
-        self.shards_holder
-            .read()
-            .await
-            .remove_shards_at_peer(peer_id)
-            .await
+        self.shards_holder.read().await.remove_shards_at_peer(peer_id).await
     }
 
     pub async fn sync_local_state(
@@ -595,26 +531,20 @@ impl Collection {
         on_convert_to_listener: ChangePeerState,
         on_convert_from_listener: ChangePeerState,
     ) -> CollectionResult<()> {
-        // Check for disabled replicas
         let shard_holder = self.shards_holder.read().await;
 
-        let get_shard_transfers = |shard_id, from| {
-            shard_holder
-                .get_transfers(|transfer| transfer.shard_id == shard_id && transfer.from == from)
-        };
-
+        // Sync replica sets
         for replica_set in shard_holder.all_shards() {
-            replica_set.sync_local_state(get_shard_transfers)?;
+            replica_set.sync_local_state(|shard_id, from| {
+                shard_holder.get_transfers(|t| t.shard_id == shard_id && t.from == from)
+            })?;
         }
 
         // Check for un-reported finished transfers
         let outgoing_transfers = shard_holder.get_outgoing_transfers(self.this_peer_id);
         let tasks_lock = self.transfer_tasks.lock().await;
         for transfer in outgoing_transfers {
-            match tasks_lock
-                .get_task_status(&transfer.key())
-                .map(|s| s.result)
-            {
+            match tasks_lock.get_task_status(&transfer.key()).map(|s| s.result) {
                 None => {
                     log::debug!(
                         "Transfer {:?} does not exist, but not reported as cancelled. Reporting now.",
@@ -640,130 +570,101 @@ impl Collection {
             }
         }
 
-        // Count how many transfers we are now proposing
-        // We must track this here so we can reference it when checking for tranfser limits,
-        // because transfers we propose now will not be in the consensus state within the lifetime
-        // of this function
+        // Check for proper replica states and possibly auto-recover
         let mut proposed = HashMap::<PeerId, usize>::new();
-
-        // Check for proper replica states
         for replica_set in shard_holder.all_shards() {
             let this_peer_id = replica_set.this_peer_id();
             let shard_id = replica_set.shard_id;
-
             let peers = replica_set.peers();
             let this_peer_state = peers.get(&this_peer_id).copied();
 
-            if this_peer_state == Some(Initializing) {
-                // It is possible, that collection creation didn't report
-                // Try to activate shard, as the collection clearly exists
+            // If initializing, finish init
+            if this_peer_state == Some(ReplicaState::Initializing) {
                 on_finish_init(this_peer_id, shard_id);
                 continue;
             }
 
+            // Listener <-> Active conversion
+            let is_last_active =
+                peers.values().filter(|&&st| st == ReplicaState::Active).count() == 1;
             if self.shared_storage_config.node_type == NodeType::Listener {
-                // We probably should not switch node type during resharding, so we only check for `Active`,
-                // but not `ReshardingScaleDown` replica state here...
-                let is_last_active = peers.values().filter(|&&state| state == Active).count() == 1;
-
-                if this_peer_state == Some(Active) && !is_last_active {
-                    // Convert active node from active to listener
+                if this_peer_state == Some(ReplicaState::Active) && !is_last_active {
                     on_convert_to_listener(this_peer_id, shard_id);
                     continue;
                 }
-            } else if this_peer_state == Some(Listener) {
-                // Convert listener node to active
+            } else if this_peer_state == Some(ReplicaState::Listener) {
                 on_convert_from_listener(this_peer_id, shard_id);
                 continue;
             }
 
-            // Don't automatically recover replicas if started in recovery mode
+            // Only recover dead replicas
             if self.shared_storage_config.recovery_mode.is_some() {
                 continue;
             }
-
-            // Don't recover replicas if not dead
-            let is_dead = this_peer_state == Some(Dead);
-            if !is_dead {
+            if this_peer_state != Some(ReplicaState::Dead) {
                 continue;
             }
 
-            // Try to find dead replicas with no active transfers
             let transfers = shard_holder.get_transfers(|_| true);
 
-            // Respect shard transfer limit, consider already proposed transfers in our counts
-            let (mut incoming, outgoing) = shard_holder.count_shard_transfer_io(this_peer_id);
+            // Respect shard transfer limits
+            let (mut incoming, outgoing) = shard_holder.count_shard_transfer_io(&this_peer_id);
             incoming += proposed.get(&this_peer_id).copied().unwrap_or(0);
             if self.check_auto_shard_transfer_limit(incoming, outgoing) {
                 log::trace!(
-                    "Postponing automatic shard {shard_id} transfer to stay below limit on this node (incoming: {incoming}, outgoing: {outgoing})",
+                    "Postponing automatic shard {shard_id} transfer to stay below limit on this node \
+                     (incoming: {incoming}, outgoing: {outgoing})",
                 );
                 continue;
             }
 
-            // Select shard transfer method, prefer user configured method or choose one now
-            // If all peers are 1.8+, we try WAL delta transfer, otherwise we use the default method
-            let shard_transfer_method = self
-                .shared_storage_config
-                .default_shard_transfer_method
-                .unwrap_or_else(|| {
-                    let all_support_wal_delta = self
-                        .channel_service
-                        .all_peers_at_version(&Version::new(1, 8, 0));
-                    if all_support_wal_delta {
-                        ShardTransferMethod::WalDelta
-                    } else {
-                        ShardTransferMethod::default()
-                    }
-                });
-
-            // Try to find a replica to transfer from
-            //
-            // `active_remote_shards` includes `Active` and `ReshardingScaleDown` replicas!
+            // Try remote sources
             for replica_id in replica_set.active_remote_shards() {
+                let shard_transfer_method = self
+                    .shared_storage_config
+                    .default_shard_transfer_method
+                    .unwrap_or_else(|| {
+                        let all_support = self
+                            .channel_service
+                            .all_peers_at_version(&Version::new(1, 8, 0));
+                        if all_support {
+                            ShardTransferMethod::WalDelta
+                        } else {
+                            ShardTransferMethod::default()
+                        }
+                    });
                 let transfer = ShardTransfer {
                     from: replica_id,
                     to: this_peer_id,
                     shard_id,
                     to_shard_id: None,
                     sync: true,
-                    // For automatic shard transfers, always select some default method from this point on
                     method: Some(shard_transfer_method),
                 };
-
                 if check_transfer_conflicts_strict(&transfer, transfers.iter()).is_some() {
-                    continue; // this transfer won't work
+                    continue;
                 }
-
-                // Respect shard transfer limit, consider already proposed transfers in our counts
-                let (incoming, mut outgoing) = shard_holder.count_shard_transfer_io(replica_id);
-                outgoing += proposed.get(&replica_id).copied().unwrap_or(0);
-                if self.check_auto_shard_transfer_limit(incoming, outgoing) {
+                if let Err(err) = replica_set.health_check(replica_id).await {
                     log::trace!(
-                        "Postponing automatic shard {shard_id} transfer to stay below limit on peer {replica_id} (incoming: {incoming}, outgoing: {outgoing})",
+                        "Replica {replica_id}/{}/{} is not available to request shard \
+                         transfer from: {err}",
+                        self.id,
+                        shard_id,
                     );
                     continue;
                 }
 
-                // TODO: Should we, maybe, throttle/backoff this requests a bit?
-                if let Err(err) = replica_set.health_check(replica_id).await {
-                    // TODO: This is rather verbose, not sure if we want to log this at all... :/
+                // Apply limits
+                let (inc2, mut out2) = shard_holder.count_shard_transfer_io(&replica_id);
+                out2 += proposed.get(&replica_id).copied().unwrap_or(0);
+                if self.check_auto_shard_transfer_limit(inc2, out2) {
                     log::trace!(
-                        "Replica {replica_id}/{}:{} is not available \
-                         to request shard transfer from: \
-                         {err}",
-                        self.id,
-                        replica_set.shard_id,
+                        "Postponing automatic shard {shard_id} transfer to stay below limit on peer \
+                         {replica_id} (incoming: {inc2}, outgoing: {out2})",
                     );
                     continue;
                 }
 
-                log::debug!(
-                    "Recovering shard {}:{shard_id} on peer {this_peer_id} by requesting it from {replica_id}",
-                    self.name(),
-                );
-
-                // Update our counters for proposed transfers, then request (propose) shard transfer
                 *proposed.entry(transfer.from).or_default() += 1;
                 *proposed.entry(transfer.to).or_default() += 1;
                 self.request_shard_transfer(transfer);
@@ -775,79 +676,50 @@ impl Collection {
     }
 
     pub async fn get_telemetry_data(&self, detail: TelemetryDetail) -> CollectionTelemetry {
-        let (shards_telemetry, transfers, resharding) = {
-            if detail.level >= DetailsLevel::Level3 {
-                let shards_holder = self.shards_holder.read().await;
-                let mut shards_telemetry = Vec::new();
-                for shard in shards_holder.all_shards() {
-                    shards_telemetry.push(shard.get_telemetry_data(detail).await)
-                }
-                (
-                    Some(shards_telemetry),
-                    Some(shards_holder.get_shard_transfer_info(&*self.transfer_tasks.lock().await)),
-                    Some(
-                        shards_holder
-                            .get_resharding_operations_info()
-                            .unwrap_or_default(),
-                    ),
-                )
-            } else {
-                (None, None, None)
+        let (shards, transfers, resharding) = if detail.level >= DetailsLevel::Level3 {
+            let shards_holder = self.shards_holder.read().await;
+            let mut list = Vec::new();
+            for shard in shards_holder.all_shards() {
+                list.push(shard.get_telemetry_data(detail).await);
             }
+            (
+                Some(list),
+                Some(shards_holder.get_shard_transfer_info(&*self.transfer_tasks.lock().await)),
+                Some(shards_holder.get_resharding_operations_info().unwrap_or_default()),
+            )
+        } else {
+            (None, None, None)
         };
 
-        let shard_clean_tasks = self.clean_local_shards_statuses();
-
+        let shard_clean = self.clean_local_shards_statuses();
         CollectionTelemetry {
             id: self.name(),
             init_time_ms: self.init_time.as_millis() as u64,
             config: CollectionConfigTelemetry::from(self.collection_config.read().await.clone()),
-            shards: shards_telemetry,
+            shards,
             transfers,
             resharding,
-            shard_clean_tasks: (!shard_clean_tasks.is_empty()).then_some(shard_clean_tasks),
+            shard_clean_tasks: (!shard_clean.is_empty()).then_some(shard_clean),
         }
     }
 
     pub async fn get_aggregated_telemetry_data(&self) -> CollectionsAggregatedTelemetry {
         let shards_holder = self.shards_holder.read().await;
-
-        let mut shard_optimization_statuses = Vec::new();
-        let mut vectors = 0;
-
+        let mut statuses = Vec::new();
+        let mut total_vectors = 0;
         for shard in shards_holder.all_shards() {
-            let shard_optimization_status = shard
-                .get_optimization_status()
-                .await
-                .unwrap_or(OptimizersStatus::Ok);
-
-            shard_optimization_statuses.push(shard_optimization_status);
-
-            vectors += shard.get_size_stats().await.num_vectors;
+            let status = shard.get_optimization_status().await.unwrap_or(OptimizersStatus::Ok);
+            statuses.push(status);
+            total_vectors += shard.get_size_stats().await.num_vectors;
         }
-
-        let optimizers_status = shard_optimization_statuses
-            .into_iter()
-            .max()
-            .unwrap_or(OptimizersStatus::Ok);
-
+        let overall = statuses.into_iter().max().unwrap_or(OptimizersStatus::Ok);
         CollectionsAggregatedTelemetry {
-            vectors,
-            optimizers_status,
+            vectors: total_vectors,
+            optimizers_status: overall,
             params: self.collection_config.read().await.params.clone(),
         }
     }
 
-    pub async fn effective_optimizers_config(&self) -> CollectionResult<OptimizersConfig> {
-        let config = self.collection_config.read().await;
-
-        if let Some(optimizers_overwrite) = self.optimizers_overwrite.clone() {
-            Ok(optimizers_overwrite.update(&config.optimizer_config)?)
-        } else {
-            Ok(config.optimizer_config.clone())
-        }
-    }
-
     pub async fn lock_updates(&self) -> RwLockWriteGuard<()> {
         self.updates_lock.write().await
     }
@@ -875,13 +747,13 @@ impl Collection {
     async fn estimate_collection_size_stats(
         shards_holder: &Arc<RwLock<ShardHolder>>,
     ) -> Option<CollectionSizeStats> {
-        let shard_lock = shards_holder.read().await;
-        shard_lock.estimate_collection_size_stats().await
+        let lock = shards_holder.read().await;
+        lock.estimate_collection_size_stats().await
     }
 
-    /// Returns estimations of collection sizes. This values are cached and might be not 100% up to date.
-    /// The cache gets updated every 32 calls.
-    pub(crate) async fn estimated_collection_stats(&self) -> Option<&CollectionSizeAtomicStats> {
+    pub(crate) async fn estimated_collection_stats(
+        &self,
+    ) -> Option<&CollectionSizeAtomicStats> {
         self.collection_stats_cache
             .get_or_update_cache(|| Self::estimate_collection_size_stats(&self.shards_holder))
             .await
