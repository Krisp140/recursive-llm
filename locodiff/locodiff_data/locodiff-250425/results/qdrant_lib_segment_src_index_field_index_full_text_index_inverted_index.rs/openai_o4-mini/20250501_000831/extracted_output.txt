use std::collections::{BTreeSet, HashMap};

use common::types::PointOffsetType;
use serde::{Deserialize, Serialize};

use crate::common::operation_error::OperationResult;
use crate::index::field_index::{CardinalityEstimation, PayloadBlockCondition, PrimaryCondition};
use crate::types::{FieldCondition, Match, PayloadKeyType};

pub type TokenId = u32;

#[derive(Default, Serialize, Deserialize, Debug, Clone)]
pub struct Document {
    tokens: Vec<TokenId>,
}

impl Document {
    pub fn new(mut tokens: Vec<TokenId>) -> Self {
        tokens.sort_unstable();
        Self { tokens }
    }

    pub fn len(&self) -> usize {
        self.tokens.len()
    }

    pub fn is_empty(&self) -> bool {
        self.tokens.is_empty()
    }

    pub fn tokens(&self) -> &[TokenId] {
        &self.tokens
    }

    pub fn check(&self, token: TokenId) -> bool {
        self.tokens.binary_search(&token).is_ok()
    }
}

#[derive(Debug, Clone)]
pub struct ParsedQuery {
    pub tokens: Vec<Option<TokenId>>,
}

impl ParsedQuery {
    pub fn check_match(&self, document: &Document) -> bool {
        if self.tokens.contains(&None) {
            return false;
        }
        self.tokens
            .iter()
            .all(|query_token| document.check(query_token.unwrap()))
    }
}

pub trait InvertedIndex {
    /// Obtain a mutable reference to the vocabulary map so that
    /// `document_from_tokens` can assign new token IDs.
    fn get_vocab_mut(&mut self) -> &mut HashMap<String, TokenId>;

    /// Build a `Document` from a set of text tokens, updating the
    /// vocabulary map as needed.
    fn document_from_tokens(&mut self, tokens: &BTreeSet<String>) -> Document {
        let vocab = self.get_vocab_mut();
        let mut document_tokens = Vec::new();
        for token in tokens {
            let id = match vocab.get(token) {
                Some(&id) => id,
                None => {
                    let next_id = vocab.len() as TokenId;
                    vocab.insert(token.clone(), next_id);
                    next_id
                }
            };
            document_tokens.push(id);
        }
        Document::new(document_tokens)
    }

    /// Index a new document under the given point offset.
    fn index_document(
        &mut self,
        idx: PointOffsetType,
        document: Document,
    ) -> OperationResult<()>;

    /// Remove a document by point offset. Returns `true` if a document
    /// was actually removed.
    fn remove_document(&mut self, idx: PointOffsetType) -> bool;

    /// Filter the indexed points by the parsed full-text query.
    fn filter(&self, query: &ParsedQuery) -> Box<dyn Iterator<Item = PointOffsetType> + '_>;

    /// Get the length of the posting list for a given token ID, if any.
    fn get_posting_len(&self, token_id: TokenId) -> Option<usize>;

    /// Estimate the cardinality of a query under a given field condition.
    fn estimate_cardinality(
        &self,
        query: &ParsedQuery,
        condition: &FieldCondition,
    ) -> CardinalityEstimation {
        let points_count = self.points_count();
        let posting_lengths: Option<Vec<usize>> = query
            .tokens
            .iter()
            .map(|&opt_id| match opt_id {
                None => None,
                Some(id) => self.get_posting_len(id),
            })
            .collect();
        if posting_lengths.is_none() || points_count == 0 {
            return CardinalityEstimation {
                primary_clauses: vec![PrimaryCondition::Condition(Box::new(condition.clone()))],
                min: 0,
                exp: 0,
                max: 0,
            };
        }
        let postings = posting_lengths.unwrap();
        if postings.is_empty() {
            return CardinalityEstimation {
                primary_clauses: vec![PrimaryCondition::Condition(Box::new(condition.clone()))],
                min: 0,
                exp: 0,
                max: 0,
            };
        }
        let smallest = postings.iter().min().copied().unwrap();
        if postings.len() == 1 {
            CardinalityEstimation {
                primary_clauses: vec![PrimaryCondition::Condition(Box::new(condition.clone()))],
                min: smallest,
                exp: smallest,
                max: smallest,
            }
        } else {
            let expected_frac: f64 = postings
                .iter()
                .map(|&len| len as f64 / points_count as f64)
                .product();
            let exp = (expected_frac * points_count as f64) as usize;
            CardinalityEstimation {
                primary_clauses: vec![PrimaryCondition::Condition(Box::new(condition.clone()))],
                min: 0,
                exp,
                max: smallest,
            }
        }
    }

    /// Iterate over the vocabulary, yielding `(token_str, posting_len)`.
    fn vocab_with_postings_len_iter(&self) -> impl Iterator<Item = (&str, usize)> + '_;

    /// Build payload-block conditions for tokens whose posting lists
    /// meet or exceed `threshold`.
    fn payload_blocks(
        &self,
        threshold: usize,
        key: PayloadKeyType,
    ) -> impl Iterator<Item = PayloadBlockCondition> + '_ {
        let map_filter = move |(token_str, len): (&str, usize)| {
            if len >= threshold {
                Some(PayloadBlockCondition {
                    condition: FieldCondition::new_match(key.clone(), Match::new_text(token_str)),
                    cardinality: len,
                })
            } else {
                None
            }
        };
        self.vocab_with_postings_len_iter().filter_map(map_filter)
    }

    /// Check whether a specific point matches the parsed query.
    fn check_match(&self, parsed_query: &ParsedQuery, point_id: PointOffsetType) -> bool;

    /// True if there is no document indexed under `point_id`.
    fn values_is_empty(&self, point_id: PointOffsetType) -> bool;

    /// Number of tokens indexed for a given point.
    fn values_count(&self, point_id: PointOffsetType) -> usize;

    /// Total number of indexed points.
    fn points_count(&self) -> usize;

    /// Map a token string to its assigned `TokenId`, if any.
    fn get_token_id(&self, token: &str) -> Option<TokenId>;
}

#[cfg(test)]
mod tests {
    use std::collections::BTreeSet;
    use rand::Rng;
    use rand::seq::SliceRandom;
    use rstest::rstest;

    use super::{Document, InvertedIndex, ParsedQuery, TokenId};
    use crate::index::field_index::full_text_index::immutable_inverted_index::ImmutableInvertedIndex;
    use crate::index::field_index::full_text_index::mutable_inverted_index::MutableInvertedIndex;
    use crate::index::field_index::full_text_index::mmap_inverted_index::MmapInvertedIndex;

    fn generate_word() -> String {
        let mut rng = rand::rng();
        let len = rng.random_range(1..=3);
        rng.sample_iter(rand::distr::Alphanumeric)
            .take(len)
            .map(char::from)
            .collect()
    }

    fn generate_query() -> Vec<String> {
        let mut rng = rand::rng();
        let len = rng.random_range(1..=2);
        (0..len).map(|_| generate_word()).collect()
    }

    fn to_parsed_query(
        query: Vec<String>,
        token_to_id: impl Fn(String) -> Option<TokenId>,
    ) -> ParsedQuery {
        ParsedQuery {
            tokens: query.into_iter().map(token_to_id).collect(),
        }
    }

    fn mutable_inverted_index(indexed_count: u32, deleted_count: u32) -> MutableInvertedIndex {
        let mut idx = MutableInvertedIndex::default();
        for pt in 0..indexed_count {
            let doc_len = rand::rng().random_range(10..=30);
            let tokens: BTreeSet<String> = (0..doc_len).map(|_| generate_word()).collect();
            let doc = idx.document_from_tokens(&tokens);
            idx.index_document(pt, doc).unwrap();
        }
        let mut pts: Vec<_> = (0..indexed_count).collect();
        pts.shuffle(&mut rand::rng());
        for &pt in &pts[..deleted_count as usize] {
            idx.remove_document(pt);
        }
        idx
    }

    #[test]
    fn test_mutable_to_immutable() {
        let mut_idx = mutable_inverted_index(2000, 400);
        let mut_cloned = mut_idx.clone();
        let imm = ImmutableInvertedIndex::from(mut_idx);

        assert!(imm.vocab.len() < mut_cloned.vocab.len());
        assert!(imm.postings.len() < mut_cloned.postings.len());
        assert!(!imm.vocab.is_empty());

        assert!(imm.vocab.iter().all(|(tok, &new_id)| {
            let new_post = &imm.postings[new_id as usize];
            let &orig_id = &mut_cloned.vocab[tok];
            let orig_post = mut_cloned.postings[orig_id as usize].as_ref().unwrap();
            new_post
                .iter()
                .all(|&pt| orig_post.contains(&pt))
                && orig_post
                    .iter()
                    .all(|&pt| new_post.contains(&pt))
        }));
    }

    #[rstest]
    #[case(2000, 400)]
    #[case(2000, 2000)]
    #[case(1111, 1110)]
    #[case(1111, 0)]
    #[case(10, 2)]
    #[case(0, 0)]
    #[test]
    fn test_immutable_to_mmap(
        #[case] indexed_count: u32,
        #[case] deleted_count: u32,
    ) {
        let mut_idx = mutable_inverted_index(indexed_count, deleted_count);
        let imm = ImmutableInvertedIndex::from(mut_idx.clone());
        let dir = tempfile::tempdir().unwrap().into_path();
        MmapInvertedIndex::create(dir.clone(), imm.clone()).unwrap();

        let mmap = MmapInvertedIndex::open(dir.clone(), false).unwrap();
        for (tok, &tid) in &imm.vocab {
            assert_eq!(mmap.get_token_id(tok), Some(tid));
        }
        for (tid, post) in imm.postings.iter().enumerate() {
            let reader = mmap.postings.get(tid as u32).unwrap();
            for &pt in post.iter() {
                assert!(reader.contains(pt));
            }
        }
        for (&pt, &cnt) in imm.point_to_tokens_count.iter().enumerate() {
            assert_eq!(mmap.deleted_points.get(pt).unwrap(), cnt.is_none());
            assert_eq!(mmap.point_to_tokens_count[pt], cnt.unwrap_or(0));
        }
        assert_eq!(mmap.active_points_count, imm.points_count());
    }

    #[test]
    fn test_mmap_index_congruence() {
        let indexed = 10000;
        let deleted = 500;
        let mut_idx = mutable_inverted_index(indexed, deleted);
        let imm = ImmutableInvertedIndex::from(mut_idx.clone());
        let dir = tempfile::tempdir().unwrap().into_path();
        MmapInvertedIndex::create(dir.clone(), imm.clone()).unwrap();
        let mut mmap_idx = MmapInvertedIndex::open(dir, false).unwrap();

        let queries: Vec<_> = (0..100).map(|_| generate_query()).collect();
        let mut_qs: Vec<_> = queries
            .clone()
            .into_iter()
            .map(|q| to_parsed_query(q, |tok| mut_idx.vocab.get(&tok).copied()))
            .collect();
        let imm_qs: Vec<_> = queries
            .into_iter()
            .map(|q| to_parsed_query(q, |tok| mmap_idx.get_token_id(&tok)))
            .collect();

        for (mq, iq) in mut_qs.iter().zip(&imm_qs) {
            let mf: Vec<_> = mut_idx.filter(mq).collect();
            let imf: Vec<_> = mmap_idx.filter(iq).collect();
            assert_eq!(mf, imf);
        }

        let to_delete: Vec<_> = (0..deleted)
            .map(|_| rand::rng().random_range(0..indexed))
            .collect();
        for &pt in &to_delete {
            mut_idx.remove_document(pt);
            mmap_idx.remove_document(pt);
        }

        for (mq, iq) in mut_qs.iter().zip(&imm_qs) {
            let mf: Vec<_> = mut_idx.filter(mq).collect();
            let imf: Vec<_> = mmap_idx.filter(iq).collect();
            assert_eq!(mf, imf);
        }
    }
}