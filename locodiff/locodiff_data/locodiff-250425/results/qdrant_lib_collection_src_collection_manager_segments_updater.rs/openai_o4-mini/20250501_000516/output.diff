
index 3c0816e9..2255ae5f 100644
--- a/qdrant_lib_collection_src_collection_manager_segments_updater.rs_expectedoutput.txt (expected):tmp/tmprt5gt3ne_expected.txt	
+++ b/qdrant_lib_collection_src_collection_manager_segments_updater.rs_extracted.txt (actual):tmp/tmpz2ropelm_actual.txt	
@@ -1,8 +1,8 @@
 //! A collection of functions for updating points and payloads stored in segments
 
+use ahash::{AHashMap, AHashSet};
 use std::sync::atomic::AtomicBool;
 
-use ahash::{AHashMap, AHashSet};
 use common::counter::hardware_counter::HardwareCounterCell;
 use itertools::iproduct;
 use parking_lot::{RwLock, RwLockWriteGuard};
@@ -17,27 +17,27 @@ use segment::types::{
 };
 
 use crate::collection_manager::holders::segment_holder::SegmentHolder;
-use crate::operations::FieldIndexOperations;
 use crate::operations::payload_ops::PayloadOps;
 use crate::operations::point_ops::{
     PointInsertOperationsInternal, PointOperations, PointStructPersisted,
 };
 use crate::operations::types::{CollectionError, CollectionResult};
 use crate::operations::vector_ops::{PointVectorsPersisted, VectorOperations};
+use crate::operations::FieldIndexOperations;
 
+/// Checks for any point IDs that have not been processed.
 pub(crate) fn check_unprocessed_points(
     points: &[PointIdType],
     processed: &AHashSet<PointIdType>,
 ) -> CollectionResult<usize> {
     let first_missed_point = points.iter().copied().find(|p| !processed.contains(p));
-
     match first_missed_point {
         None => Ok(processed.len()),
         Some(missed_point_id) => Err(CollectionError::PointNotFound { missed_point_id }),
     }
 }
 
-/// Tries to delete points from all segments, returns number of actually deleted points
+/// Deletes specific points by ID.
 pub(crate) fn delete_points(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
@@ -45,39 +45,31 @@ pub(crate) fn delete_points(
     hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     let mut total_deleted_points = 0;
-
     for batch in ids.chunks(VECTOR_OP_BATCH_SIZE) {
-        let deleted_points = segments.apply_points(
-            batch,
-            |_| (),
-            |id, _idx, write_segment, ()| write_segment.delete_point(op_num, id, hw_counter),
-        )?;
-
+        let deleted_points =
+            segments.apply_points(batch, |_| (), |id, _idx, write_segment, ()| {
+                write_segment.delete_point(op_num, id, hw_counter)
+            })?;
         total_deleted_points += deleted_points;
     }
-
     Ok(total_deleted_points)
 }
 
-/// Update the specified named vectors of a point, keeping unspecified vectors intact.
+/// Update named vectors on existing points, merging multiple updates per ID.
 pub(crate) fn update_vectors(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
     points: Vec<PointVectorsPersisted>,
     hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
-    // Build a map of vectors to update per point, merge updates on same point ID
     let mut points_map: AHashMap<PointIdType, NamedVectors> = AHashMap::new();
     for point in points {
         let PointVectorsPersisted { id, vector } = point;
         let named_vector = NamedVectors::from(vector);
-
         let entry = points_map.entry(id).or_default();
         entry.merge(named_vector);
     }
-
     let ids: Vec<PointIdType> = points_map.keys().copied().collect();
-
     let mut total_updated_points = 0;
     for batch in ids.chunks(VECTOR_OP_BATCH_SIZE) {
         let updated_points = segments.apply_points_with_conditional_move(
@@ -87,32 +79,24 @@ pub(crate) fn update_vectors(
                 let vectors = points_map[&id].clone();
                 write_segment.update_vectors(op_num, id, vectors, hw_counter)
             },
-            |id, owned_vectors, _| {
-                for (vector_name, vector_ref) in points_map[&id].iter() {
-                    owned_vectors.insert(vector_name.to_owned(), vector_ref.to_owned());
-                }
-            },
             |_| false,
             hw_counter,
         )?;
         check_unprocessed_points(batch, &updated_points)?;
         total_updated_points += updated_points.len();
     }
-
     Ok(total_updated_points)
 }
 
-const VECTOR_OP_BATCH_SIZE: usize = 512;
-
-/// Delete the given named vectors for the given points, keeping other vectors intact.
+/// Delete named vectors on specific points.
 pub(crate) fn delete_vectors(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
     points: &[PointIdType],
     vector_names: &[VectorNameBuf],
+    hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     let mut total_deleted_points = 0;
-
     for batch in points.chunks(VECTOR_OP_BATCH_SIZE) {
         let deleted_points = segments.apply_points(
             batch,
@@ -120,19 +104,17 @@ pub(crate) fn delete_vectors(
             |id, _idx, write_segment, ()| {
                 let mut res = true;
                 for name in vector_names {
-                    res &= write_segment.delete_vector(op_num, id, name)?;
+                    res &= write_segment.delete_vector(op_num, id, name, hw_counter)?;
                 }
                 Ok(res)
             },
         )?;
-
         total_deleted_points += deleted_points;
     }
-
     Ok(total_deleted_points)
 }
 
-/// Delete the given named vectors for points matching the given filter, keeping other vectors intact.
+/// Delete named vectors by filter.
 pub(crate) fn delete_vectors_by_filter(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
@@ -141,12 +123,10 @@ pub(crate) fn delete_vectors_by_filter(
     hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     let affected_points = points_by_filter(segments, filter, hw_counter)?;
-    delete_vectors(segments, op_num, &affected_points, vector_names)
+    delete_vectors(segments, op_num, &affected_points, vector_names, hw_counter)
 }
 
-/// Batch size when modifying payload.
-const PAYLOAD_OP_BATCH_SIZE: usize = 512;
-
+/// Upsert full payload on specific points.
 pub(crate) fn overwrite_payload(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
@@ -155,7 +135,6 @@ pub(crate) fn overwrite_payload(
     hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     let mut total_updated_points = 0;
-
     for batch in points.chunks(PAYLOAD_OP_BATCH_SIZE) {
         let updated_points = segments.apply_points_with_conditional_move(
             op_num,
@@ -167,14 +146,12 @@ pub(crate) fn overwrite_payload(
             |segment| segment.get_indexed_fields().is_empty(),
             hw_counter,
         )?;
-
         total_updated_points += updated_points.len();
-        check_unprocessed_points(batch, &updated_points)?;
     }
-
     Ok(total_updated_points)
 }
 
+/// Overwrite payload by filter.
 pub(crate) fn overwrite_payload_by_filter(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
@@ -186,6 +163,7 @@ pub(crate) fn overwrite_payload_by_filter(
     overwrite_payload(segments, op_num, payload, &affected_points, hw_counter)
 }
 
+/// Set partial payload on points.
 pub(crate) fn set_payload(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
@@ -195,7 +173,6 @@ pub(crate) fn set_payload(
     hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     let mut total_updated_points = 0;
-
     for chunk in points.chunks(PAYLOAD_OP_BATCH_SIZE) {
         let updated_points = segments.apply_points_with_conditional_move(
             op_num,
@@ -206,36 +183,20 @@ pub(crate) fn set_payload(
                 None => old_payload.merge(payload),
             },
             |segment| {
-                segment.get_indexed_fields().keys().all(|indexed_path| {
-                    !indexed_path.is_affected_by_value_set(&payload.0, key.as_ref())
-                })
+                segment
+                    .get_indexed_fields()
+                    .keys()
+                    .all(|indexed_path| !indexed_path.is_affected_by_value_set(&payload.0, key.as_ref()))
             },
             hw_counter,
         )?;
-
         check_unprocessed_points(chunk, &updated_points)?;
         total_updated_points += updated_points.len();
     }
-
     Ok(total_updated_points)
 }
 
-fn points_by_filter(
-    segments: &SegmentHolder,
-    filter: &Filter,
-    hw_counter: &HardwareCounterCell,
-) -> CollectionResult<Vec<PointIdType>> {
-    let mut affected_points: Vec<PointIdType> = Vec::new();
-    // we don’t want to cancel this filtered read
-    let is_stopped = AtomicBool::new(false);
-    segments.for_each_segment(|s| {
-        let points = s.read_filtered(None, None, Some(filter), &is_stopped, hw_counter);
-        affected_points.extend_from_slice(points.as_slice());
-        Ok(true)
-    })?;
-    Ok(affected_points)
-}
-
+/// Set payload by filter.
 pub(crate) fn set_payload_by_filter(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
@@ -248,6 +209,7 @@ pub(crate) fn set_payload_by_filter(
     set_payload(segments, op_num, payload, &affected_points, key, hw_counter)
 }
 
+/// Delete partial payload.
 pub(crate) fn delete_payload(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
@@ -256,7 +218,6 @@ pub(crate) fn delete_payload(
     hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     let mut total_deleted_points = 0;
-
     for batch in points.chunks(PAYLOAD_OP_BATCH_SIZE) {
         let updated_points = segments.apply_points_with_conditional_move(
             op_num,
@@ -268,28 +229,21 @@ pub(crate) fn delete_payload(
                 }
                 Ok(res)
             },
-            |_, _, payload| {
-                for key in keys {
-                    payload.remove(key);
-                }
-            },
             |segment| {
-                iproduct!(segment.get_indexed_fields().keys(), keys).all(
-                    |(indexed_path, path_to_delete)| {
+                iproduct!(segment.get_indexed_fields().keys(), keys)
+                    .all(|(indexed_path, path_to_delete)| {
                         !indexed_path.is_affected_by_value_remove(path_to_delete)
-                    },
-                )
+                    })
             },
             hw_counter,
         )?;
-
         check_unprocessed_points(batch, &updated_points)?;
         total_deleted_points += updated_points.len();
     }
-
     Ok(total_deleted_points)
 }
 
+/// Delete payload by filter.
 pub(crate) fn delete_payload_by_filter(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
@@ -301,6 +255,7 @@ pub(crate) fn delete_payload_by_filter(
     delete_payload(segments, op_num, &affected_points, keys, hw_counter)
 }
 
+/// Clear payload on points.
 pub(crate) fn clear_payload(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
@@ -308,7 +263,6 @@ pub(crate) fn clear_payload(
     hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     let mut total_updated_points = 0;
-
     for batch in points.chunks(PAYLOAD_OP_BATCH_SIZE) {
         let updated_points = segments.apply_points_with_conditional_move(
             op_num,
@@ -321,11 +275,10 @@ pub(crate) fn clear_payload(
         check_unprocessed_points(batch, &updated_points)?;
         total_updated_points += updated_points.len();
     }
-
     Ok(total_updated_points)
 }
 
-/// Clear Payloads from all segments matching the given filter
+/// Clear payload by filter.
 pub(crate) fn clear_payload_by_filter(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
@@ -336,213 +289,7 @@ pub(crate) fn clear_payload_by_filter(
     clear_payload(segments, op_num, &points_to_clear, hw_counter)
 }
 
-pub(crate) fn create_field_index(
-    segments: &SegmentHolder,
-    op_num: SeqNumberType,
-    field_name: PayloadKeyTypeRef,
-    field_schema: Option<&PayloadFieldSchema>,
-    hw_counter: &HardwareCounterCell,
-) -> CollectionResult<usize> {
-    segments
-        .apply_segments(|write_segment| {
-            let Some((schema, index)) =
-                write_segment.build_field_index(op_num, field_name, field_schema, hw_counter)?
-            else {
-                return Ok(false);
-            };
-
-            write_segment.with_upgraded(|segment| {
-                segment.apply_field_index(op_num, field_name.to_owned(), schema, index)
-            })
-        })
-        .map_err(Into::into)
-}
-
-pub(crate) fn delete_field_index(
-    segments: &SegmentHolder,
-    op_num: SeqNumberType,
-    field_name: PayloadKeyTypeRef,
-) -> CollectionResult<usize> {
-    segments
-        .apply_segments(|write_segment| {
-            write_segment.with_upgraded(|segment| segment.delete_field_index(op_num, field_name))
-        })
-        .map_err(Into::into)
-}
-
-/// Upsert to a point ID with the specified vectors and payload in the given segment.
-///
-/// Returns
-/// - Ok(true) if the operation was successful and point replaced existing value
-/// - Ok(false) if the operation was successful and point was inserted
-/// - Err if the operation failed
-fn upsert_with_payload(
-    segment: &mut RwLockWriteGuard<dyn SegmentEntry>,
-    op_num: SeqNumberType,
-    point_id: PointIdType,
-    vectors: NamedVectors,
-    payload: Option<&Payload>,
-    hw_counter: &HardwareCounterCell,
-) -> OperationResult<bool> {
-    let mut res = segment.upsert_point(op_num, point_id, vectors, hw_counter)?;
-    if let Some(full_payload) = payload {
-        res &= segment.set_full_payload(op_num, point_id, full_payload, hw_counter)?;
-    }
-    Ok(res)
-}
-
-/// Sync points within a given [from_id; to_id) range.
-///
-/// 1. Retrieve existing points for a range
-/// 2. Remove points, which are not present in the sync operation
-/// 3. Retrieve overlapping points, detect which one of them are changed
-/// 4. Select new points
-/// 5. Upsert points which differ from the stored ones
-///
-/// Returns:
-///     (number of deleted points, number of new points, number of updated points)
-pub(crate) fn sync_points(
-    segments: &SegmentHolder,
-    op_num: SeqNumberType,
-    from_id: Option<PointIdType>,
-    to_id: Option<PointIdType>,
-    points: &[PointStructPersisted],
-    hw_counter: &HardwareCounterCell,
-) -> CollectionResult<(usize, usize, usize)> {
-    let id_to_point: AHashMap<PointIdType, _> = points.iter().map(|p| (p.id, p)).collect();
-    let sync_points: AHashSet<_> = points.iter().map(|p| p.id).collect();
-    // 1. Retrieve existing points for a range
-    let stored_point_ids: AHashSet<_> = segments
-        .iter()
-        .flat_map(|(_, segment)| segment.get().read().read_range(from_id, to_id))
-        .collect();
-    // 2. Remove points, which are not present in the sync operation
-    let points_to_remove: Vec<_> = stored_point_ids.difference(&sync_points).copied().collect();
-    let deleted = delete_points(segments, op_num, points_to_remove.as_slice(), hw_counter)?;
-    // 3. Retrieve overlapping points, detect which one of them are changed
-    let existing_point_ids: Vec<_> = stored_point_ids
-        .intersection(&sync_points)
-        .copied()
-        .collect();
-
-    let mut points_to_update: Vec<_> = Vec::new();
-    // we don’t want to cancel this filtered read
-    let is_stopped = AtomicBool::new(false);
-    let _num_updated =
-        segments.read_points(existing_point_ids.as_slice(), &is_stopped, |id, segment| {
-            let all_vectors = match segment.all_vectors(id) {
-                Ok(v) => v,
-                Err(OperationError::InconsistentStorage { .. }) => NamedVectors::default(),
-                Err(e) => return Err(e),
-            };
-            let payload = segment.payload(id, hw_counter)?;
-            let point = id_to_point.get(&id).unwrap();
-            if point.get_vectors() != all_vectors {
-                points_to_update.push(*point);
-                Ok(true)
-            } else {
-                let payload_match = match point.payload {
-                    Some(ref p) => p == &payload,
-                    None => Payload::default() == payload,
-                };
-                if !payload_match {
-                    points_to_update.push(*point);
-                    Ok(true)
-                } else {
-                    Ok(false)
-                }
-            }
-        })?;
-
-    // 4. Select new points
-    let num_updated = points_to_update.len();
-    let mut num_new = 0;
-    sync_points.difference(&stored_point_ids).for_each(|id| {
-        num_new += 1;
-        points_to_update.push(*id_to_point.get(id).unwrap());
-    });
-
-    // 5. Upsert points which differ from the stored ones
-    let num_replaced = upsert_points(segments, op_num, points_to_update, hw_counter)?;
-    debug_assert!(
-        num_replaced <= num_updated,
-        "number of replaced points cannot be greater than points to update ({num_replaced} <= {num_updated})",
-    );
-
-    Ok((deleted, num_new, num_updated))
-}
-
-/// Checks point id in each segment, update point if found.
-/// All not found points are inserted into random segment.
-/// Returns: number of updated points.
-pub(crate) fn upsert_points<'a, T>(
-    segments: &SegmentHolder,
-    op_num: SeqNumberType,
-    points: T,
-    hw_counter: &HardwareCounterCell,
-) -> CollectionResult<usize>
-where
-    T: IntoIterator<Item = &'a PointStructPersisted>,
-{
-    let points_map: AHashMap<PointIdType, _> = points.into_iter().map(|p| (p.id, p)).collect();
-    let ids: Vec<PointIdType> = points_map.keys().copied().collect();
-
-    // Update points in writable segments
-    let updated_points = segments.apply_points_with_conditional_move(
-        op_num,
-        &ids,
-        |id, write_segment| {
-            let point = points_map[&id];
-            upsert_with_payload(
-                write_segment,
-                op_num,
-                id,
-                point.get_vectors(),
-                point.payload.as_ref(),
-                hw_counter,
-            )
-        },
-        |id, vectors, old_payload| {
-            let point = points_map[&id];
-            for (name, vec) in point.get_vectors() {
-                vectors.insert(name.into(), vec.to_owned());
-            }
-            if let Some(payload) = &point.payload {
-                *old_payload = payload.clone();
-            }
-        },
-        |_| false,
-        hw_counter,
-    )?;
-
-    let mut res = updated_points.len();
-    // Insert new points, which was not updated or existed
-    let new_point_ids = ids.iter().copied().filter(|x| !updated_points.contains(x));
-
-    {
-        let default_write_segment = segments.smallest_appendable_segment().ok_or_else(|| {
-            CollectionError::service_error("No appendable segments exists, expected at least one")
-        })?;
-
-        let segment_arc = default_write_segment.get();
-        let mut write_segment = segment_arc.write();
-        for point_id in new_point_ids {
-            let point = points_map[&point_id];
-            res += usize::from(upsert_with_payload(
-                &mut write_segment,
-                op_num,
-                point_id,
-                point.get_vectors(),
-                point.payload.as_ref(),
-                hw_counter,
-            )?);
-        }
-        RwLockWriteGuard::unlock_fair(write_segment);
-    };
-
-    Ok(res)
-}
-
+/// Process a single point operation.
 pub(crate) fn process_point_operation(
     segments: &RwLock<SegmentHolder>,
     op_num: SeqNumberType,
@@ -599,6 +346,7 @@ pub(crate) fn process_point_operation(
     }
 }
 
+/// Process a single vector operation.
 pub(crate) fn process_vector_operation(
     segments: &RwLock<SegmentHolder>,
     op_num: SeqNumberType,
@@ -609,15 +357,26 @@ pub(crate) fn process_vector_operation(
         VectorOperations::UpdateVectors(operation) => {
             update_vectors(&segments.read(), op_num, operation.points, hw_counter)
         }
-        VectorOperations::DeleteVectors(ids, vector_names) => {
-            delete_vectors(&segments.read(), op_num, &ids.points, &vector_names)
-        }
+        VectorOperations::DeleteVectors(ids, vector_names) => delete_vectors(
+            &segments.read(),
+            op_num,
+            &ids.points,
+            &vector_names,
+            hw_counter,
+        ),
         VectorOperations::DeleteVectorsByFilter(filter, vector_names) => {
-            delete_vectors_by_filter(&segments.read(), op_num, &filter, &vector_names, hw_counter)
+            delete_vectors_by_filter(
+                &segments.read(),
+                op_num,
+                &filter,
+                &vector_names,
+                hw_counter,
+            )
         }
     }
 }
 
+/// Process a single payload or index operation.
 pub(crate) fn process_payload_operation(
     segments: &RwLock<SegmentHolder>,
     op_num: SeqNumberType,
@@ -628,14 +387,7 @@ pub(crate) fn process_payload_operation(
         PayloadOps::SetPayload(sp) => {
             let payload: Payload = sp.payload;
             if let Some(points) = sp.points {
-                set_payload(
-                    &segments.read(),
-                    op_num,
-                    &payload,
-                    &points,
-                    &sp.key,
-                    hw_counter,
-                )
+                set_payload(&segments.read(), op_num, &payload, &points, &sp.key, hw_counter)
             } else if let Some(filter) = sp.filter {
                 set_payload_by_filter(
                     &segments.read(),
@@ -683,30 +435,7 @@ pub(crate) fn process_payload_operation(
     }
 }
 
-pub(crate) fn process_field_index_operation(
-    segments: &RwLock<SegmentHolder>,
-    op_num: SeqNumberType,
-    field_index_operation: &FieldIndexOperations,
-    hw_counter: &HardwareCounterCell,
-) -> CollectionResult<usize> {
-    match field_index_operation {
-        FieldIndexOperations::CreateIndex(index_data) => create_field_index(
-            &segments.read(),
-            op_num,
-            &index_data.field_name,
-            index_data.field_schema.as_ref(),
-            hw_counter,
-        ),
-        FieldIndexOperations::DeleteIndex(field_name) => {
-            delete_field_index(&segments.read(), op_num, field_name)
-        }
-    }
-}
-
-/// Max amount of points to delete in a batched deletion iteration.
-const DELETION_BATCH_SIZE: usize = 512;
-
-/// Deletes points from all segments matching the given filter
+/// Delete points matching a filter in batches.
 pub(crate) fn delete_points_by_filter(
     segments: &SegmentHolder,
     op_num: SeqNumberType,
@@ -714,20 +443,16 @@ pub(crate) fn delete_points_by_filter(
     hw_counter: &HardwareCounterCell,
 ) -> CollectionResult<usize> {
     let mut total_deleted = 0;
-    // we don’t want to cancel this filtered read
     let is_stopped = AtomicBool::new(false);
-    let mut points_to_delete: AHashMap<_, _> = segments
+    let mut points_to_delete: AHashMap<_, Vec<_>> = segments
         .iter()
         .map(|(segment_id, segment)| {
             (
                 *segment_id,
-                segment.get().read().read_filtered(
-                    None,
-                    None,
-                    Some(filter),
-                    &is_stopped,
-                    hw_counter,
-                ),
+                segment
+                    .get()
+                    .read()
+                    .read_filtered(None, None, Some(filter), &is_stopped, hw_counter),
             )
         })
         .collect();
@@ -739,21 +464,224 @@ pub(crate) fn delete_points_by_filter(
         if curr_points.is_empty() {
             return Ok(false);
         }
-
         let mut deleted_in_batch = 0;
         while let Some(point_id) = curr_points.pop() {
             if s.delete_point(op_num, point_id, hw_counter)? {
                 total_deleted += 1;
                 deleted_in_batch += 1;
             }
-
             if deleted_in_batch >= DELETION_BATCH_SIZE {
                 break;
             }
         }
-
         Ok(true)
     })?;
 
     Ok(total_deleted)
-}
\ No newline at end of file
+}
+
+/// Collect point IDs matching a filter.
+fn points_by_filter(
+    segments: &SegmentHolder,
+    filter: &Filter,
+    hw_counter: &HardwareCounterCell,
+) -> CollectionResult<Vec<PointIdType>> {
+    let mut affected_points: Vec<PointIdType> = Vec::new();
+    let is_stopped = AtomicBool::new(false);
+    segments.for_each_segment(|s| {
+        let points = s.read_filtered(None, None, Some(filter), &is_stopped, hw_counter);
+        affected_points.extend_from_slice(points.as_slice());
+        Ok(true)
+    })?;
+    Ok(affected_points)
+}
+
+/// Read a contiguous range of points, synchronize them,
+/// and return counts of deleted, new, and updated points.
+pub(crate) fn sync_points(
+    segments: &SegmentHolder,
+    op_num: SeqNumberType,
+    from_id: Option<PointIdType>,
+    to_id: Option<PointIdType>,
+    points: &[PointStructPersisted],
+    hw_counter: &HardwareCounterCell,
+) -> CollectionResult<(usize, usize, usize)> {
+    let id_to_point: AHashMap<PointIdType, &PointStructPersisted> =
+        points.iter().map(|p| (p.id, p)).collect();
+    let sync_points: AHashSet<_> = points.iter().map(|p| p.id).collect();
+    let stored_point_ids: AHashSet<_> = segments
+        .iter()
+        .flat_map(|(_, segment)| segment.get().read().read_range(from_id, to_id))
+        .collect();
+
+    let points_to_remove: Vec<_> = stored_point_ids.difference(&sync_points).copied().collect();
+    let deleted = delete_points(segments, op_num, points_to_remove.as_slice(), hw_counter)?;
+
+    let existing_point_ids: Vec<_> = stored_point_ids.intersection(&sync_points).copied().collect();
+    let mut points_to_update: Vec<_> = Vec::new();
+    let is_stopped = AtomicBool::new(false);
+    let _num_updated = segments.read_points(&existing_point_ids, &is_stopped, |id, segment| {
+        let all_vectors = match segment.all_vectors(id) {
+            Ok(v) => v,
+            Err(OperationError::InconsistentStorage { .. }) => NamedVectors::default(),
+            Err(e) => return Err(e),
+        };
+        let payload = segment.payload(id, hw_counter)?;
+        let point = id_to_point[&id];
+        if point.get_vectors() != all_vectors {
+            points_to_update.push(*point);
+            Ok(true)
+        } else {
+            let payload_match = match &point.payload {
+                Some(p) => p == &payload,
+                None => Payload::default() == payload,
+            };
+            if !payload_match {
+                points_to_update.push(*point);
+                Ok(true)
+            } else {
+                Ok(false)
+            }
+        }
+    })?;
+
+    let num_updated = points_to_update.len();
+    let mut num_new = 0;
+    sync_points
+        .difference(&stored_point_ids)
+        .for_each(|id| {
+            num_new += 1;
+            points_to_update.push(*id_to_point.get(id).unwrap());
+        });
+
+    let num_replaced = upsert_points(segments, op_num, points_to_update.iter(), hw_counter)?;
+    debug_assert!(
+        num_replaced <= num_updated,
+        "number of replaced points cannot be greater than points to update ({num_replaced} <= {num_updated})"
+    );
+
+    Ok((deleted, num_new, num_updated))
+}
+
+/// Upsert points (add or replace) given full vectors & optional payload.
+pub(crate) fn upsert_points<'a, T>(
+    segments: &SegmentHolder,
+    op_num: SeqNumberType,
+    points: T,
+    hw_counter: &HardwareCounterCell,
+) -> CollectionResult<usize>
+where
+    T: IntoIterator<Item = &'a PointStructPersisted>,
+{
+    let points_map: AHashMap<_, _> = points.into_iter().map(|p| (p.id, p)).collect();
+    let ids: Vec<PointIdType> = points_map.keys().copied().collect();
+
+    let updated_points = segments.apply_points_with_conditional_move(
+        op_num,
+        &ids,
+        |id, write_segment| {
+            let point = points_map[&id];
+            upsert_with_payload(
+                write_segment,
+                op_num,
+                id,
+                point.get_vectors(),
+                point.payload.as_ref(),
+                hw_counter,
+            )
+        },
+        |id, vectors, old_payload| {
+            let point = points_map[&id];
+            for (name, vec) in point.get_vectors() {
+                vectors.insert(name.into(), vec.to_owned());
+            }
+            if let Some(payload) = &point.payload {
+                *old_payload = payload.clone();
+            }
+        },
+        |_| false,
+        hw_counter,
+    )?;
+
+    let mut res = updated_points.len();
+    let new_point_ids = ids.iter().copied().filter(|x| !updated_points.contains(x));
+    {
+        let default_write_segment = segments.smallest_appendable_segment().ok_or_else(|| {
+            CollectionError::service_error("No appendable segments exists, expected at least one")
+        })?;
+        let segment_arc = default_write_segment.get();
+        let mut write_segment = segment_arc.write();
+        for point_id in new_point_ids {
+            let point = points_map[&point_id];
+            res += usize::from(
+                upsert_with_payload(
+                    &mut write_segment,
+                    op_num,
+                    point_id,
+                    point.get_vectors(),
+                    point.payload.as_ref(),
+                    hw_counter,
+                )?,
+            );
+        }
+        RwLockWriteGuard::unlock_fair(write_segment);
+    }
+    Ok(res)
+}
+
+/// Add or update a single point with full vectors & optional payload.
+fn upsert_with_payload(
+    segment: &mut RwLockWriteGuard<dyn SegmentEntry>,
+    op_num: SeqNumberType,
+    point_id: PointIdType,
+    vectors: NamedVectors,
+    payload: Option<&Payload>,
+    hw_counter: &HardwareCounterCell,
+) -> OperationResult<bool> {
+    let mut res = segment.upsert_point(op_num, point_id, vectors, hw_counter)?;
+    if let Some(full_payload) = payload {
+        res &= segment.set_full_payload(op_num, point_id, full_payload, hw_counter)?;
+    }
+    Ok(res)
+}
+
+/// Create a new field index non-blocking.
+pub(crate) fn create_field_index(
+    segments: &SegmentHolder,
+    op_num: SeqNumberType,
+    field_name: PayloadKeyTypeRef,
+    field_schema: Option<&PayloadFieldSchema>,
+    hw_counter: &HardwareCounterCell,
+) -> CollectionResult<usize> {
+    segments
+        .apply_segments(|write_segment| {
+            let Some((schema, index)) =
+                write_segment.build_field_index(op_num, field_name, field_schema, hw_counter)?
+            else {
+                return Ok(false);
+            };
+            write_segment.with_upgraded(|segment| {
+                segment.apply_field_index(op_num, field_name.to_owned(), schema, index)
+            })
+        })
+        .map_err(Into::into)
+}
+
+/// Delete an existing field index non-blocking.
+pub(crate) fn delete_field_index(
+    segments: &SegmentHolder,
+    op_num: SeqNumberType,
+    field_name: PayloadKeyTypeRef,
+) -> CollectionResult<usize> {
+    segments
+        .apply_segments(|write_segment| {
+            write_segment.with_upgraded(|segment| segment.delete_field_index(op_num, field_name))
+        })
+        .map_err(Into::into)
+}
+
+// Constants
+const DELETION_BATCH_SIZE: usize = 512;
+const PAYLOAD_OP_BATCH_SIZE: usize = 512;
+const VECTOR_OP_BATCH_SIZE: usize = 512;
+```
\ No newline at end of file
