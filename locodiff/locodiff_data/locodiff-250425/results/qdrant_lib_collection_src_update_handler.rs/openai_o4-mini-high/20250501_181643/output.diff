
index cb922e86..d2103458 100644
--- a/qdrant_lib_collection_src_update_handler.rs_expectedoutput.txt (expected):tmp/tmp0wn6ek90_expected.txt	
+++ b/qdrant_lib_collection_src_update_handler.rs_extracted.txt (actual):tmp/tmpb1cqb759_actual.txt	
@@ -1,8 +1,8 @@
 use std::cmp::min;
 use std::collections::HashSet;
 use std::path::{Path, PathBuf};
-use std::sync::Arc;
 use std::sync::atomic::{AtomicBool, AtomicU64, AtomicUsize, Ordering};
+use std::sync::Arc;
 
 use common::budget::ResourceBudget;
 use common::counter::hardware_accumulator::HwMeasurementAcc;
@@ -10,43 +10,32 @@ use common::counter::hardware_counter::HardwareCounterCell;
 use common::panic;
 use itertools::Itertools;
 use log::{debug, error, info, trace, warn};
-use parking_lot::Mutex;
-use segment::common::operation_error::OperationResult;
 use segment::index::hnsw_index::num_rayon_threads;
 use segment::types::SeqNumberType;
 use tokio::runtime::Handle;
-use tokio::sync::mpsc::{self, Receiver, Sender};
 use tokio::sync::{Mutex as TokioMutex, oneshot};
+use tokio::sync::mpsc::{self, Receiver, Sender};
 use tokio::task::{self, JoinHandle};
-use tokio::time::error::Elapsed;
-use tokio::time::{Duration, timeout};
+use tokio::time::{Duration, error::Elapsed, timeout};
 
 use crate::collection::payload_index_schema::PayloadIndexSchema;
 use crate::collection_manager::collection_updater::CollectionUpdater;
 use crate::collection_manager::holders::segment_holder::LockedSegmentHolder;
-use crate::collection_manager::optimizers::segment_optimizer::{
-    OptimizerThresholds, SegmentOptimizer,
-};
+use crate::collection_manager::optimizers::segment_optimizer::{OptimizerThresholds, SegmentOptimizer};
 use crate::collection_manager::optimizers::{Tracker, TrackerLog, TrackerStatus};
-use crate::common::stoppable_task::{StoppableTaskHandle, spawn_stoppable};
+use crate::common::stoppable_task::{spawn_stoppable, StoppableTaskHandle};
 use crate::config::CollectionParams;
-use crate::operations::CollectionUpdateOperations;
 use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{CollectionError, CollectionResult};
+use crate::operations::CollectionUpdateOperations;
 use crate::save_on_disk::SaveOnDisk;
 use crate::shards::local_shard::LocalShardClocks;
 use crate::wal::WalError;
 use crate::wal_delta::LockedWal;
 
-/// Interval at which the optimizer worker cleans up old optimization handles
-///
-/// The longer the duration, the longer it  takes for panicked tasks to be reported.
-const OPTIMIZER_CLEANUP_INTERVAL: Duration = Duration::from_secs(5);
-
 pub type Optimizer = dyn SegmentOptimizer + Sync + Send;
 
 /// Information, required to perform operation and notify regarding the result
-#[derive(Debug)]
 pub struct OperationData {
     /// Sequential number of the operation
     pub op_num: SeqNumberType,
@@ -56,6 +45,7 @@ pub struct OperationData {
     pub wait: bool,
     /// Callback notification channel
     pub sender: Option<oneshot::Sender<CollectionResult<usize>>>,
+    /// Hardware measurements accumulator
     pub hw_measurements: HwMeasurementAcc,
 }
 
@@ -90,14 +80,12 @@ pub struct UpdateHandler {
     /// List of used optimizers
     pub optimizers: Arc<Vec<Arc<Optimizer>>>,
     /// Log of optimizer statuses
-    optimizers_log: Arc<Mutex<TrackerLog>>,
+    optimizers_log: Arc<parking_lot::Mutex<TrackerLog>>,
     /// Total number of optimized points since last start
     total_optimized_points: Arc<AtomicUsize>,
-    /// Global CPU budget in number of cores for all optimization tasks.
-    /// Assigns CPU permits to tasks to limit overall resource utilization.
+    /// Global resource budget for CPU and I/O permits
     optimizer_resource_budget: ResourceBudget,
-    /// How frequent can we flush data
-    /// This parameter depends on the optimizer config and should be updated accordingly.
+    /// How frequent can we flush data; depends on optimizer config
     pub flush_interval_sec: u64,
     segments: LockedSegmentHolder,
     /// Process, that listens updates signals and perform updates
@@ -108,39 +96,30 @@ pub struct UpdateHandler {
     flush_worker: Option<JoinHandle<()>>,
     /// Sender to stop flush worker
     flush_stop: Option<oneshot::Sender<()>>,
-    runtime_handle: Handle,
-    /// WAL, required for operations
-    wal: LockedWal,
-    /// Always keep this WAL version and later and prevent acknowledging/truncating from the WAL.
-    /// This is used when other bits of code still depend on information in the WAL, such as the
-    /// queue proxy shard.
-    /// Defaults to `u64::MAX` to allow acknowledging all confirmed versions.
+    shard_path: PathBuf,
+    /// Whether we have ever triggered optimizers since starting.
+    has_triggered_optimizers: Arc<AtomicBool>,
+    clocks: LocalShardClocks,
     pub(super) wal_keep_from: Arc<AtomicU64>,
     optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
     /// Maximum number of concurrent optimization jobs in this update handler.
-    /// This parameter depends on the optimizer config and should be updated accordingly.
     pub max_optimization_threads: Option<usize>,
-    /// Highest and cutoff clocks for the shard WAL.
-    clocks: LocalShardClocks,
-    shard_path: PathBuf,
-    /// Whether we have ever triggered optimizers since starting.
-    has_triggered_optimizers: Arc<AtomicBool>,
 }
 
 impl UpdateHandler {
     #[allow(clippy::too_many_arguments)]
     pub fn new(
-        shared_storage_config: Arc<SharedStorageConfig>,
+        shared_storage_config: Arc<SharedStorageConfig>>,
         payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,
         optimizers: Arc<Vec<Arc<Optimizer>>>,
-        optimizers_log: Arc<Mutex<TrackerLog>>,
-        total_optimized_points: Arc<AtomicUsize>,
+        optimizers_log: Arc<parking_lot::Mutex<TrackerLog>>,
+        total_optimized_points: Arc<AtomicUsize>>,
         optimizer_resource_budget: ResourceBudget,
         runtime_handle: Handle,
         segments: LockedSegmentHolder,
         wal: LockedWal,
         flush_interval_sec: u64,
-        max_optimization_threads: Option<usize>,
+        max_optimization_threads: Option<usize>>,
         clocks: LocalShardClocks,
         shard_path: PathBuf,
     ) -> UpdateHandler {
@@ -157,39 +136,43 @@ impl UpdateHandler {
             flush_worker: None,
             flush_stop: None,
             runtime_handle,
-            wal,
+            shard_path,
+            has_triggered_optimizers: Default::default(),
+            clocks,
             wal_keep_from: Arc::new(u64::MAX.into()),
             flush_interval_sec,
+            update_handle: None,
             optimization_handles: Arc::new(TokioMutex::new(vec![])),
             max_optimization_threads,
-            clocks,
-            shard_path,
-            has_triggered_optimizers: Default::default(),
         }
     }
 
-    pub fn run_workers(&mut self, update_receiver: Receiver<UpdateSignal>) {
-        let (tx, rx) = mpsc::channel(self.shared_storage_config.update_queue_size);
+    pub fn run_workers(&mut self, mut update_receiver: Receiver<UpdateSignal>) {
+        let (opt_tx, opt_rx) = mpsc::channel(self.shared_storage_config.update_queue_size);
+        // Start optimizer worker
         self.optimizer_worker = Some(self.runtime_handle.spawn(Self::optimization_worker_fn(
             self.optimizers.clone(),
-            tx.clone(),
-            rx,
+            opt_tx.clone(),
+            opt_rx,
             self.segments.clone(),
             self.wal.clone(),
             self.optimization_handles.clone(),
             self.optimizers_log.clone(),
-            self.total_optimized_points.clone(),
             self.optimizer_resource_budget.clone(),
             self.max_optimization_threads,
             self.has_triggered_optimizers.clone(),
             self.payload_index_schema.clone(),
         )));
+
+        // Start update worker
         self.update_worker = Some(self.runtime_handle.spawn(Self::update_worker_fn(
             update_receiver,
-            tx,
+            opt_tx,
             self.wal.clone(),
             self.segments.clone(),
         )));
+
+        // Start flush worker
         let (flush_tx, flush_rx) = oneshot::channel();
         self.flush_worker = Some(self.runtime_handle.spawn(Self::flush_worker(
             self.segments.clone(),
@@ -204,73 +187,91 @@ impl UpdateHandler {
     }
 
     pub fn stop_flush_worker(&mut self) {
-        if let Some(flush_stop) = self.flush_stop.take() {
-            if let Err(()) = flush_stop.send(()) {
+        if let Some(fl) = self.flush_stop.take() {
+            if fl.send(()).is_err() {
                 warn!("Failed to stop flush worker as it is already stopped.");
             }
         }
     }
 
-    /// Gracefully wait before all optimizations stop
-    /// If some optimization is in progress - it will be finished before shutdown.
     pub async fn wait_workers_stops(&mut self) -> CollectionResult<()> {
-        let maybe_handle = self.update_worker.take();
-        if let Some(handle) = maybe_handle {
-            handle.await?;
+        if let Some(u) = self.update_worker.take() {
+            u.await?;
         }
-        let maybe_handle = self.optimizer_worker.take();
-        if let Some(handle) = maybe_handle {
-            handle.await?;
+        if let Some(o) = self.optimizer_worker.take() {
+            o.await?;
         }
-        let maybe_handle = self.flush_worker.take();
-        if let Some(handle) = maybe_handle {
-            handle.await?;
+        if let Some(f) = self.flush_worker.take() {
+            f.await?;
         }
-
-        let mut opt_handles_guard = self.optimization_handles.lock().await;
-        let opt_handles = std::mem::take(&mut *opt_handles_guard);
-        let stopping_handles = opt_handles
-            .into_iter()
-            .filter_map(|h| h.stop())
-            .collect_vec();
-
-        for res in stopping_handles {
-            res.await?;
+        let mut opt_handles = self.optimization_handles.lock().await;
+        let finished = std::mem::take(&mut *opt_handles);
+        for h in finished {
+            if let Some(jh) = h.stop() {
+                jh.await?;
+            }
         }
-
         Ok(())
     }
 
     /// Checks if there are any failed operations.
     /// If so - attempts to re-apply all failed operations.
     async fn try_recover(segments: LockedSegmentHolder, wal: LockedWal) -> CollectionResult<usize> {
-        // Try to re-apply everything starting from the first failed operation
-        let first_failed_operation_option = segments.read().failed_operation.iter().cloned().min();
-        match first_failed_operation_option {
-            None => {}
-            Some(first_failed_op) => {
-                let wal_lock = wal.lock().await;
-                for (op_num, operation) in wal_lock.read(first_failed_op) {
-                    CollectionUpdater::update(
-                        &segments,
-                        op_num,
-                        operation.operation,
-                        &HardwareCounterCell::disposable(), // Internal operation, no measurement needed
-                    )?;
-                }
+        let first_failed = segments.read().failed_operation.iter().cloned().min();
+        if let Some(op) = first_failed {
+            let lock = wal.lock().await;
+            for (num, op_data) in lock.read(op) {
+                CollectionUpdater::update(&segments, num, op_data)?;
             }
-        };
+        }
         Ok(0)
     }
 
-    /// Checks conditions for all optimizers until there is no suggested segment
-    /// Starts a task for each optimization
-    /// Returns handles for started tasks
+    /// Checks the optimizer conditions.
+    ///
+    /// Returns (ever_triggered, any_pending).
+    pub(crate) fn check_optimizer_conditions(&self) -> (bool, bool) {
+        let triggered = self.has_triggered_optimizers.load(Ordering::Relaxed);
+        let excluded = HashSet::default();
+        let pending = self.optimizers.iter().any(|opt| {
+            !opt.check_condition(self.segments.clone(), &excluded).is_empty()
+        });
+        (triggered, pending)
+    }
+
+    /// Ensure there is at least one appendable segment with enough capacity
+    pub(super) fn ensure_appendable_segment_with_capacity(
+        segments: &LockedSegmentHolder,
+        segments_path: &Path,
+        params: &CollectionParams,
+        thresholds: &OptimizerThresholds,
+        payload_schema: &PayloadIndexSchema,
+    ) -> OperationResult<()> {
+        let need_new = {
+            let read = segments.read();
+            read
+                .appendable_segments_ids()
+                .into_iter()
+                .filter_map(|id| read.get(id))
+                .all(|segment| {
+                    let max_vec_bytes = segment.get().read().max_available_vectors_size_in_bytes().unwrap_or_default();
+                    let max_bytes = thresholds.max_segment_size_kb.saturating_mul(segment::common::BYTES_IN_KB);
+                    max_vec_bytes >= max_bytes
+                })
+        };
+        if need_new {
+            debug!("Creating new appendable segment, all existing segments are over capacity");
+            segments.write().create_appendable_segment(segments_path, params, payload_schema)?;
+        }
+        Ok(())
+    }
+
+    /// Launch optimizations tasks up to optional limit
     pub(crate) fn launch_optimization<F>(
         optimizers: Arc<Vec<Arc<Optimizer>>>,
-        optimizers_log: Arc<Mutex<TrackerLog>>,
-        total_optimized_points: Arc<AtomicUsize>,
-        optimizer_resource_budget: &ResourceBudget,
+        optimizers_log: Arc<parking_lot::Mutex<TrackerLog>>,
+        total_optimized_points: Arc<AtomicUsize>>,
+        resource_budget: &ResourceBudget,
         segments: LockedSegmentHolder,
         callback: F,
         limit: Option<usize>,
@@ -278,132 +279,80 @@ impl UpdateHandler {
     where
         F: Fn(bool) + Send + Clone + Sync + 'static,
     {
-        let mut scheduled_segment_ids = HashSet::<_>::default();
-        let mut handles = vec![];
+        let mut scheduled = HashSet::default();
+        let mut handles = Vec::new();
 
         'outer: for optimizer in optimizers.iter() {
             loop {
-                // Return early if we reached the optimization job limit
-                if limit.map(|extra| handles.len() >= extra).unwrap_or(false) {
-                    log::trace!("Reached optimization job limit, postponing other optimizations");
+                if limit.map(|l| handles.len() >= l).unwrap_or(false) {
+                    trace!("Reached optimization job limit, postponing other optimizations");
                     break 'outer;
                 }
-
-                let nonoptimal_segment_ids =
-                    optimizer.check_condition(segments.clone(), &scheduled_segment_ids);
-                if nonoptimal_segment_ids.is_empty() {
+                let nsi = optimizer.check_condition(segments.clone(), &scheduled);
+                if nsi.is_empty() {
                     break;
                 }
-
-                debug!("Optimizing segments: {:?}", &nonoptimal_segment_ids);
-
-                // Determine how many Resources we prefer for optimization task, acquire permit for it
-                // And use same amount of IO threads as CPUs
-                let max_indexing_threads = optimizer.hnsw_config().max_indexing_threads;
-                let desired_io = num_rayon_threads(max_indexing_threads);
-                let Some(mut permit) = optimizer_resource_budget.try_acquire(0, desired_io) else {
-                    // If there is no Resource budget, break outer loop and return early
-                    // If we have no handles (no optimizations) trigger callback so that we wake up
-                    // our optimization worker to try again later, otherwise it could get stuck
-                    log::trace!(
-                        "No available IO permit for {} optimizer, postponing",
-                        optimizer.name(),
-                    );
+                scheduled.extend(&nsi);
+                // Acquire resource permits: CPU=0, IO = num_rayon_threads(...)
+                let desired_io = num_rayon_threads(optimizer.hnsw_config().max_indexing_threads);
+                let Some(mut permit) = resource_budget.try_acquire(0, desired_io) else {
+                    trace!("No available IO permit for {} optimizer, postponing", optimizer.name());
                     if handles.is_empty() {
                         callback(false);
                     }
                     break 'outer;
                 };
-                log::trace!(
-                    "Acquired {} IO permit for {} optimizer",
-                    permit.num_io,
-                    optimizer.name(),
-                );
+                trace!("Acquired {} IO permit for {} optimizer", permit.num_io, optimizer.name());
 
                 let permit_callback = callback.clone();
-
                 permit.set_on_release(move || {
                     // Notify scheduler that resource budget changed
                     permit_callback(false);
                 });
 
-                let optimizer = optimizer.clone();
-                let optimizers_log = optimizers_log.clone();
-                let total_optimized_points = total_optimized_points.clone();
-                let segments = segments.clone();
-                let nsi = nonoptimal_segment_ids.clone();
-                scheduled_segment_ids.extend(&nsi);
-                let callback = callback.clone();
+                let optimizer_cloned = optimizer.clone();
+                let log_clone = optimizers_log.clone();
+                let segments_clone = segments.clone();
+                let total_clone = total_optimized_points.clone();
+                let cb_clone = callback.clone();
 
                 let handle = spawn_stoppable(
-                    // Stoppable task
-                    {
-                        let resource_budget = optimizer_resource_budget.clone();
-                        let segments = segments.clone();
-                        move |stopped| {
-                            // Track optimizer status
-                            let tracker = Tracker::start(optimizer.as_ref().name(), nsi.clone());
-                            let tracker_handle = tracker.handle();
-                            optimizers_log.lock().register(tracker);
-
-                            // Optimize and handle result
-                            match optimizer.as_ref().optimize(
-                                segments.clone(),
-                                nsi,
-                                permit,
-                                resource_budget,
-                                stopped,
-                            ) {
-                                // Perform some actions when optimization if finished
-                                Ok(optimized_points) => {
-                                    let is_optimized = optimized_points > 0;
-                                    total_optimized_points
-                                        .fetch_add(optimized_points, Ordering::Relaxed);
-                                    tracker_handle.update(TrackerStatus::Done);
-                                    callback(is_optimized);
-                                    is_optimized
-                                }
-                                // Handle and report errors
-                                Err(error) => match error {
+                    move |stopped| {
+                        // Track optimizer status
+                        let tracker = Tracker::start(optimizer_cloned.name(), nsi.clone());
+                        let tracker_handle = tracker.handle();
+                        log_clone.lock().register(tracker);
+
+                        match optimizer_cloned.as_ref().optimize(segments_clone.clone(), nsi.clone(), permit.clone(), stopped) {
+                            Ok(points) => {
+                                let done = points > 0;
+                                total_clone.fetch_add(points, Ordering::Relaxed);
+                                tracker_handle.update(TrackerStatus::Done);
+                                cb_clone(done);
+                                done
+                            }
+                            Err(err) => {
+                                match err {
                                     CollectionError::Cancelled { description } => {
-                                        debug!("Optimization cancelled - {description}");
-                                        tracker_handle
-                                            .update(TrackerStatus::Cancelled(description));
+                                        debug!("Optimization cancelled - {}", description);
+                                        tracker_handle.update(TrackerStatus::Cancelled(description));
                                         false
                                     }
                                     _ => {
-                                        segments.write().report_optimizer_error(error.clone());
-
-                                        // Error of the optimization can not be handled by API user
-                                        // It is only possible to fix after full restart,
-                                        // so the best available action here is to stop whole
-                                        // optimization thread and log the error
-                                        log::error!("Optimization error: {error}");
-
-                                        tracker_handle
-                                            .update(TrackerStatus::Error(error.to_string()));
-
-                                        panic!("Optimization error: {error}");
+                                        segments_clone.write().report_optimizer_error(err.clone());
+                                        error!("Optimization error: {}", err);
+                                        tracker_handle.update(TrackerStatus::Error(err.to_string()));
+                                        panic!("Optimization error: {err}");
                                     }
-                                },
+                                }
                             }
                         }
                     },
-                    // Panic handler
                     Some(Box::new(move |panic_payload| {
-                        let message = panic::downcast_str(&panic_payload).unwrap_or("");
-                        let separator = if !message.is_empty() { ": " } else { "" };
-
-                        warn!(
-                            "Optimization task panicked, collection may be in unstable state\
-                             {separator}{message}"
-                        );
-
-                        segments
-                            .write()
-                            .report_optimizer_error(CollectionError::service_error(format!(
-                                "Optimization task panicked{separator}{message}"
-                            )));
+                        let msg = panic::downcast_str(&panic_payload).unwrap_or("");
+                        let sep = if !msg.is_empty() { ": " } else { "" };
+                        warn!("Optimization task panicked{sep}{msg}");
+                        segments_clone.write().report_optimizer_error(CollectionError::service_error(format!("Optimization task panicked{sep}{msg}")));
                     })),
                 );
                 handles.push(handle);
@@ -413,134 +362,53 @@ impl UpdateHandler {
         handles
     }
 
-    /// Ensure there is at least one appendable segment with enough capacity
-    ///
-    /// If there is no appendable segment, or all are at or over capacity, a new empty one is
-    /// created.
-    ///
-    /// Capacity is determined based on `optimizers.max_segment_size_kb`.
-    pub(super) fn ensure_appendable_segment_with_capacity(
-        segments: &LockedSegmentHolder,
-        segments_path: &Path,
-        collection_params: &CollectionParams,
-        thresholds_config: &OptimizerThresholds,
-        payload_index_schema: &PayloadIndexSchema,
-    ) -> OperationResult<()> {
-        let no_segment_with_capacity = {
-            let segments_read = segments.read();
-            segments_read
-                .appendable_segments_ids()
-                .into_iter()
-                .filter_map(|segment_id| segments_read.get(segment_id))
-                .all(|segment| {
-                    let max_vector_size_bytes = segment
-                        .get()
-                        .read()
-                        .max_available_vectors_size_in_bytes()
-                        .unwrap_or_default();
-                    let max_segment_size_bytes = thresholds_config
-                        .max_segment_size_kb
-                        .saturating_mul(segment::common::BYTES_IN_KB);
-
-                    max_vector_size_bytes >= max_segment_size_bytes
-                })
-        };
-
-        if no_segment_with_capacity {
-            log::debug!("Creating new appendable segment, all existing segments are over capacity");
-            segments.write().create_appendable_segment(
-                segments_path,
-                collection_params,
-                payload_index_schema,
-            )?;
-        }
-
-        Ok(())
-    }
-
-    /// Checks the optimizer conditions.
-    ///
-    /// This function returns a tuple of two booleans:
-    /// - The first indicates if any optimizers have been triggered since startup.
-    /// - The second indicates if there are any pending/suboptimal optimizers.
-    pub(crate) fn check_optimizer_conditions(&self) -> (bool, bool) {
-        // Check if Qdrant triggered any optimizations since starting at all
-        let has_triggered_any_optimizers = self.has_triggered_optimizers.load(Ordering::Relaxed);
-
-        let excluded_ids = HashSet::<_>::default();
-        let has_suboptimal_optimizers = self.optimizers.iter().any(|optimizer| {
-            let nonoptimal_segment_ids =
-                optimizer.check_condition(self.segments.clone(), &excluded_ids);
-            !nonoptimal_segment_ids.is_empty()
-        });
-
-        (has_triggered_any_optimizers, has_suboptimal_optimizers)
-    }
-
-    #[allow(clippy::too_many_arguments)]
     pub(crate) async fn process_optimization(
         optimizers: Arc<Vec<Arc<Optimizer>>>,
         segments: LockedSegmentHolder,
         optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
-        optimizers_log: Arc<Mutex<TrackerLog>>,
-        total_optimized_points: Arc<AtomicUsize>,
-        optimizer_resource_budget: &ResourceBudget,
+        optimizers_log: Arc<parking_lot::Mutex<TrackerLog>>,
+        total_optimized_points: Arc<AtomicUsize>>,
+        resource_budget: &ResourceBudget,
         sender: Sender<OptimizerSignal>,
         limit: usize,
+        has_triggered_optimizers: Arc<AtomicBool>,
+        payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,
     ) {
         let mut new_handles = Self::launch_optimization(
             optimizers.clone(),
             optimizers_log,
             total_optimized_points,
-            optimizer_resource_budget,
+            resource_budget,
             segments.clone(),
-            move |_optimization_result| {
-                // After optimization is finished, we still need to check if there are
-                // some further optimizations possible.
-                // If receiver is already dead - we do not care.
-                // If channel is full - optimization will be triggered by some other signal
+            move |res| {
+                // Retrigger optimizers
                 let _ = sender.try_send(OptimizerSignal::Nop);
             },
             Some(limit),
         );
-        let mut handles = optimization_handles.lock().await;
-        handles.append(&mut new_handles);
+        let mut guard = optimization_handles.lock().await;
+        guard.append(&mut new_handles);
+        guard.retain(|h| !h.is_finished());
     }
 
-    /// Cleanup finalized optimization task handles
-    ///
-    /// This finds and removes completed tasks from our list of optimization handles.
-    /// It also propagates any panics (and unknown errors) so we properly handle them if desired.
-    ///
-    /// It is essential to call this every once in a while for handling panics in time.
-    ///
-    /// Returns true if any optimization handle was finished, joined and removed.
     async fn cleanup_optimization_handles(
         optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
     ) -> bool {
-        // Remove finished handles
-        let finished_handles: Vec<_> = {
-            let mut handles = optimization_handles.lock().await;
-            (0..handles.len())
-                .filter(|i| handles[*i].is_finished())
-                .collect::<Vec<_>>()
-                .into_iter()
-                .rev()
-                .map(|i| handles.swap_remove(i))
-                .collect()
-        };
-
-        let finished_any = !finished_handles.is_empty();
-
-        // Finalize all finished handles to propagate panics
-        for handle in finished_handles {
-            handle.join_and_handle_panic().await;
+        let mut handles = optimization_handles.lock().await;
+        let mut finished = Vec::new();
+        for i in (0..handles.len()).rev() {
+            if handles[i].is_finished() {
+                finished.push(handles.swap_remove(i));
+            }
         }
-
-        finished_any
+        let cleaned_any = !finished.is_empty();
+        drop(handles);
+        for h in finished {
+            h.join_and_handle_panic().await;
+        }
+        cleaned_any
     }
 
-    #[allow(clippy::too_many_arguments)]
     async fn optimization_worker_fn(
         optimizers: Arc<Vec<Arc<Optimizer>>>,
         sender: Sender<OptimizerSignal>,
@@ -548,117 +416,69 @@ impl UpdateHandler {
         segments: LockedSegmentHolder,
         wal: LockedWal,
         optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,
-        optimizers_log: Arc<Mutex<TrackerLog>>,
-        total_optimized_points: Arc<AtomicUsize>,
-        optimizer_resource_budget: ResourceBudget,
-        max_handles: Option<usize>,
+        optimizers_log: Arc<parking_lot::Mutex<TrackerLog>>,
+        total_optimized_points: Arc<AtomicUsize>>,
+        resource_budget: ResourceBudget,
+        max_handles: Option<usize>>,
         has_triggered_optimizers: Arc<AtomicBool>,
         payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,
     ) {
         let max_handles = max_handles.unwrap_or(usize::MAX);
-        let max_indexing_threads = optimizers
-            .first()
-            .map(|optimizer| optimizer.hnsw_config().max_indexing_threads)
-            .unwrap_or_default();
-
-        // Asynchronous task to trigger optimizers once CPU budget is available again
-        let mut resource_available_trigger: Option<JoinHandle<()>> = None;
+        let max_idx_threads = optimizers.first().map(|o| o.hnsw_config().max_indexing_threads).unwrap_or_default();
+        let mut trigger_handle: Option<JoinHandle<()>> = None;
 
         loop {
-            let result = timeout(OPTIMIZER_CLEANUP_INTERVAL, receiver.recv()).await;
-
-            let cleaned_any =
-                Self::cleanup_optimization_handles(optimization_handles.clone()).await;
-
-            // Either continue below here with the worker, or reloop/break
-            // Decision logic doing one of three things:
-            // 1. run optimizers
-            // 2. reloop and wait for next signal
-            // 3. break here and stop the optimization worker
-            let ignore_max_handles = match result {
-                // Regular optimizer signal: run optimizers: do 1
+            let recv = timeout(Duration::from_secs(5), receiver.recv()).await;
+            let cleaned = Self::cleanup_optimization_handles(optimization_handles.clone()).await;
+
+            let ignore_max = match recv {
                 Ok(Some(OptimizerSignal::Operation(_))) => false,
-                // Optimizer signal ignoring max handles: do 1
                 Ok(Some(OptimizerSignal::Nop)) => true,
-                // Hit optimizer cleanup interval, did clean up a task: do 1
-                Err(Elapsed { .. }) if cleaned_any => {
-                    // This branch prevents a race condition where optimizers would get stuck
-                    // If the optimizer cleanup interval was triggered and we did clean any task we
-                    // must run optimizers now. If we don't there may not be any other ongoing
-                    // tasks that'll trigger this for us. If we don't run optimizers here we might
-                    // get stuck into yellow state until a new update operation is received.
-                    // See: <https://github.com/qdrant/qdrant/pull/5111>
-                    log::warn!(
-                        "Cleaned a optimization handle after timeout, explicitly triggering optimizers",
-                    );
+                Err(Elapsed { .. }) if cleaned => {
+                    warn!("Cleaned a optimization handle after timeout, explicitly triggering optimizers");
                     true
                 }
-                // Hit optimizer cleanup interval, did not clean up a task: do 2
-                Err(Elapsed { .. }) => continue,
-                // Channel closed or received stop signal: do 3
-                Ok(None | Some(OptimizerSignal::Stop)) => break,
+                _ => break,
             };
 
             has_triggered_optimizers.store(true, Ordering::Relaxed);
 
-            // Ensure we have at least one appendable segment with enough capacity
-            // Source required parameters from first optimizer
-            if let Some(optimizer) = optimizers.first() {
-                let result = Self::ensure_appendable_segment_with_capacity(
+            if let Some(opt) = optimizers.first() {
+                if let Err(e) = Self::ensure_appendable_segment_with_capacity(
                     &segments,
-                    optimizer.segments_path(),
-                    &optimizer.collection_params(),
-                    optimizer.threshold_config(),
+                    opt.segments_path(),
+                    &opt.collection_params(),
+                    opt.threshold_config(),
                     &payload_index_schema.read(),
-                );
-                if let Err(err) = result {
-                    log::error!(
-                        "Failed to ensure there are appendable segments with capacity: {err}"
-                    );
-                    panic!("Failed to ensure there are appendable segments with capacity: {err}");
+                ) {
+                    error!("Failed to ensure appendable segment: {e}");
+                    panic!("Failed to ensure appendable segment: {e}");
                 }
             }
 
-            // If not forcing, wait on next signal if we have too many handles
-            if !ignore_max_handles && optimization_handles.lock().await.len() >= max_handles {
+            if !ignore_max && optimization_handles.lock().await.len() >= max_handles {
                 continue;
             }
-
-            if Self::try_recover(segments.clone(), wal.clone())
-                .await
-                .is_err()
-            {
+            if Self::try_recover(segments.clone(), wal.clone()).await.is_err() {
                 continue;
             }
 
-            // Continue if we have enough resource budget available to start an optimization
-            // Otherwise skip now and start a task to trigger the optimizer again once resource
-            // budget becomes available
-            let desired_cpus = 0;
-            let desired_io = num_rayon_threads(max_indexing_threads);
-            if !optimizer_resource_budget.has_budget(desired_cpus, desired_io) {
-                let trigger_active = resource_available_trigger
-                    .as_ref()
-                    .is_some_and(|t| !t.is_finished());
-                if !trigger_active {
-                    resource_available_trigger.replace(trigger_optimizers_on_resource_budget(
-                        optimizer_resource_budget.clone(),
-                        desired_cpus,
-                        desired_io,
-                        sender.clone(),
-                    ));
+            if !resource_budget.has_budget(0, num_rayon_threads(max_idx_threads)) {
+                if trigger_handle.as_ref().map_or(true, |h| h.is_finished()) {
+                    trigger_handle = Some(task::spawn({
+                        let rb = resource_budget.clone();
+                        let tx = sender.clone();
+                        async move {
+                            rb.notify_on_budget_available(0, num_rayon_threads(max_idx_threads)).await;
+                            let _ = tx.send(OptimizerSignal::Nop).await;
+                        }
+                    }));
                 }
                 continue;
             }
-
-            // Determine optimization handle limit based on max handles we allow
-            // Not related to the CPU budget, but a different limit for the maximum number
-            // of concurrent concrete optimizations per shard as configured by the user in
-            // the Qdrant configuration.
-            // Skip if we reached limit, an ongoing optimization that finishes will trigger this loop again
             let limit = max_handles.saturating_sub(optimization_handles.lock().await.len());
             if limit == 0 {
-                log::trace!("Skipping optimization check, we reached optimization thread limit");
+                trace!("Skipping optimization check, reached thread limit");
                 continue;
             }
 
@@ -668,11 +488,12 @@ impl UpdateHandler {
                 optimization_handles.clone(),
                 optimizers_log.clone(),
                 total_optimized_points.clone(),
-                &optimizer_resource_budget,
+                &resource_budget,
                 sender.clone(),
                 limit,
-            )
-            .await;
+                has_triggered_optimizers.clone(),
+                payload_index_schema.clone(),
+            ).await;
         }
     }
 
@@ -684,74 +505,32 @@ impl UpdateHandler {
     ) {
         while let Some(signal) = receiver.recv().await {
             match signal {
-                UpdateSignal::Operation(OperationData {
-                    op_num,
-                    operation,
-                    sender,
-                    wait,
-                    hw_measurements,
-                }) => {
+                UpdateSignal::Operation(OperationData { op_num, operation, sender, wait, hw_measurements }) => {
                     let flush_res = if wait {
-                        wal.lock().await.flush().map_err(|err| {
-                            CollectionError::service_error(format!(
-                                "Can't flush WAL before operation {op_num} - {err}"
-                            ))
-                        })
+                        wal.lock().await.flush().map_err(|e| CollectionError::service_error(format!("Can't flush WAL before operation {op_num} - {e}")))
                     } else {
                         Ok(())
                     };
-
-                    let operation_result = flush_res.and_then(|_| {
-                        CollectionUpdater::update(
-                            &segments,
-                            op_num,
-                            operation,
-                            &hw_measurements.get_counter_cell(),
-                        )
+                    let op_result = flush_res.and_then(|_| {
+                        CollectionUpdater::update(&segments, op_num, operation, &hw_measurements.get_counter_cell())
                     });
-
-                    let res = match operation_result {
-                        Ok(update_res) => optimize_sender
-                            .send(OptimizerSignal::Operation(op_num))
-                            .await
-                            .and(Ok(update_res))
-                            .map_err(|send_err| send_err.into()),
-                        Err(err) => Err(err),
-                    };
-
-                    if let Some(feedback) = sender {
-                        feedback.send(res).unwrap_or_else(|_| {
-                            debug!("Can't report operation {op_num} result. Assume already not required");
-                        });
-                    };
+                    if let Some(cb) = sender {
+                        cb.send(op_result).unwrap_or_else(|_| debug!("Can't report operation {op_num} result. Assume already not required"));
+                    }
                 }
                 UpdateSignal::Stop => {
-                    optimize_sender
-                        .send(OptimizerSignal::Stop)
-                        .await
-                        .unwrap_or_else(|_| debug!("Optimizer already stopped"));
+                    optimize_sender.send(OptimizerSignal::Stop).await.unwrap_or_else(|_| debug!("Optimizer already stopped"));
                     break;
                 }
-                UpdateSignal::Nop => optimize_sender
-                    .send(OptimizerSignal::Nop)
-                    .await
-                    .unwrap_or_else(|_| {
-                        info!(
-                            "Can't notify optimizers, assume process is dead. Restart is required"
-                        );
-                    }),
-                UpdateSignal::Plunger(callback_sender) => {
-                    callback_sender.send(()).unwrap_or_else(|_| {
-                        debug!("Can't notify sender, assume nobody is waiting anymore");
-                    });
+                UpdateSignal::Nop => {
+                    optimize_sender.send(OptimizerSignal::Nop).await.unwrap_or_else(|_| info!("Can't notify optimizers, assume process is dead. Restart is required"));
+                }
+                UpdateSignal::Plunger(cb) => {
+                    let _ = cb.send(());
                 }
             }
         }
-        // Transmitter was destroyed
-        optimize_sender
-            .send(OptimizerSignal::Stop)
-            .await
-            .unwrap_or_else(|_| debug!("Optimizer already stopped"));
+        optimize_sender.send(OptimizerSignal::Stop).await.unwrap_or_else(|_| debug!("Optimizer already stopped"));
     }
 
     async fn flush_worker(
@@ -764,61 +543,44 @@ impl UpdateHandler {
         shard_path: PathBuf,
     ) {
         loop {
-            // Stop flush worker on signal or if sender was dropped
-            // Even if timer did not finish
             tokio::select! {
                 _ = tokio::time::sleep(Duration::from_secs(flush_interval_sec)) => {},
                 _ = &mut stop_receiver => {
                     debug!("Stopping flush worker for shard {}", shard_path.display());
                     return;
                 }
-            }
+            };
 
             trace!("Attempting flushing");
             let wal_flash_job = wal.lock().await.flush_async();
-
             if let Err(err) = wal_flash_job.join() {
                 error!("Failed to flush wal: {err:?}");
-                segments
-                    .write()
-                    .report_optimizer_error(WalError::WriteWalError(format!(
-                        "WAL flush error: {err:?}"
-                    )));
+                segments.write().report_optimizer_error(WalError::WriteWalError(format!("WAL flush error: {err:?}")));
                 continue;
             }
 
-            let confirmed_version = Self::flush_segments(segments.clone());
-            let confirmed_version = match confirmed_version {
-                Ok(version) => version,
-                Err(err) => {
-                    error!("Failed to flush: {err}");
-                    segments.write().report_optimizer_error(err);
+            let confirmed = match Self::flush_segments(segments.clone()) {
+                Ok(v) => v,
+                Err(e) => {
+                    error!("Failed to flush: {e}");
+                    segments.write().report_optimizer_error(e);
                     continue;
                 }
             };
 
-            // Acknowledge confirmed version in WAL, but don't acknowledge the specified
-            // `keep_from` index or higher.
-            // This is to prevent truncating WAL entries that other bits of code still depend on
-            // such as the queue proxy shard.
-            // Default keep_from is `u64::MAX` to allow acknowledging all confirmed.
-            let keep_from = wal_keep_from.load(std::sync::atomic::Ordering::Relaxed);
-
-            // If we should keep the first message, do not acknowledge at all
+            let keep_from = wal_keep_from.load(Ordering::Relaxed);
             if keep_from == 0 {
                 continue;
             }
-
-            let ack = confirmed_version.min(keep_from.saturating_sub(1));
+            let ack = confirmed.min(keep_from.saturating_sub(1));
 
             if let Err(err) = clocks.store_if_changed(&shard_path).await {
-                log::warn!("Failed to store clock maps to disk: {err}");
-                segments.write().report_optimizer_error(err);
+                warn!("Failed to store clock maps to disk: {err}");
+                segments.write().report_optimizer_error(err.clone());
             }
-
             if let Err(err) = wal.lock().await.ack(ack) {
-                log::warn!("Failed to acknowledge WAL version: {err}");
-                segments.write().report_optimizer_error(err);
+                warn!("Failed to acknowledge WAL version: {err}");
+                segments.write().report_optimizer_error(err.clone());
             }
         }
     }
@@ -827,33 +589,12 @@ impl UpdateHandler {
     ///
     /// # Errors
     /// Returns an error on flush failure
-    fn flush_segments(segments: LockedSegmentHolder) -> OperationResult<SeqNumberType> {
-        let read_segments = segments.read();
-        let flushed_version = read_segments.flush_all(false, false)?;
-        Ok(match read_segments.failed_operation.iter().cloned().min() {
-            None => flushed_version,
-            Some(failed_operation) => min(failed_operation, flushed_version),
+    fn flush_segments(segments: LockedSegmentHolder) -> segment::common::operation_error::OperationResult<SeqNumberType> {
+        let read = segments.read();
+        let flushed = read.flush_all(false, false)?;
+        Ok(match read.failed_operation.iter().cloned().min() {
+            None => flushed,
+            Some(failed) => min(failed, flushed),
         })
     }
-}
-
-/// Trigger optimizers when CPU budget is available
-fn trigger_optimizers_on_resource_budget(
-    optimizer_resource_budget: ResourceBudget,
-    desired_cpus: usize,
-    desired_io: usize,
-    sender: Sender<OptimizerSignal>,
-) -> JoinHandle<()> {
-    task::spawn(async move {
-        log::trace!("Skipping optimization checks, waiting for CPU budget to be available");
-        optimizer_resource_budget
-            .notify_on_budget_available(desired_cpus, desired_io)
-            .await;
-        log::trace!("Continue optimization checks, new CPU budget available");
-
-        // Trigger optimizers with Nop operation
-        sender.send(OptimizerSignal::Nop).await.unwrap_or_else(|_| {
-            log::info!("Can't notify optimizers, assume process is dead. Restart is required")
-        });
-    })
 }
\ No newline at end of file
