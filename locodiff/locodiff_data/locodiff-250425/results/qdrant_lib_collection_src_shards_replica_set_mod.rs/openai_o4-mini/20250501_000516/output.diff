
index 1a8af404..de4f38d2 100644
--- a/qdrant_lib_collection_src_shards_replica_set_mod.rs_expectedoutput.txt (expected):tmp/tmp3gqj8ork_expected.txt	
+++ b/qdrant_lib_collection_src_shards_replica_set_mod.rs_extracted.txt (actual):tmp/tmp37hkids__actual.txt	
@@ -8,7 +8,6 @@ mod telemetry;
 mod update;
 
 use std::collections::{HashMap, HashSet};
-use std::ops::Deref as _;
 use std::path::{Path, PathBuf};
 use std::sync::Arc;
 use std::time::Duration;
@@ -23,76 +22,33 @@ use serde::{Deserialize, Serialize};
 use tokio::runtime::Handle;
 use tokio::sync::{Mutex, RwLock};
 
-use super::CollectionId;
 use super::local_shard::LocalShard;
 use super::local_shard::clock_map::RecoveryPoint;
 use super::remote_shard::RemoteShard;
 use super::transfer::ShardTransfer;
+use super::CollectionId;
 use crate::collection::payload_index_schema::PayloadIndexSchema;
 use crate::common::collection_size_stats::CollectionSizeStats;
 use crate::common::snapshots_manager::SnapshotStorageManager;
 use crate::config::CollectionConfigInternal;
+use crate::operations::{CollectionUpdateOperations, point_ops};
 use crate::operations::shared_storage_config::SharedStorageConfig;
 use crate::operations::types::{CollectionError, CollectionResult, UpdateResult, UpdateStatus};
-use crate::operations::{CollectionUpdateOperations, point_ops};
 use crate::optimizers_builder::OptimizersConfig;
 use crate::save_on_disk::SaveOnDisk;
 use crate::shards::channel_service::ChannelService;
 use crate::shards::dummy_shard::DummyShard;
-use crate::shards::replica_set::clock_set::ClockSet;
 use crate::shards::shard::{PeerId, Shard, ShardId};
 use crate::shards::shard_config::ShardConfig;
 
-//    â”‚    Collection Created
-//    â”‚
-//    â–¼
-//  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
-//  â”‚              â”‚
-//  â”‚ Initializing â”‚
-//  â”‚              â”‚
-//  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
-//         â”‚  Report created    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
-//         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º           â”‚
-//             Activate         â”‚ Consensus â”‚
-//        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤           â”‚
-//        â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
-//  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”   User Promote           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
-//  â”‚             â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º          â”‚
-//  â”‚ Active      â”‚                          â”‚ Listener â”‚
-//  â”‚             â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚          â”‚
-//  â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚Transfer      â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
-//     â”‚                      â”‚Finished         â”‚
-//     â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚Update
-//     â”‚Update         â”‚               â”‚        â”‚Failure
-//     â”‚Failure        â”‚ Partial       â”œâ”€â”€â”€â”    â”‚
-//     â”‚               â”‚               â”‚   â”‚    â”‚
-//     â”‚               â””â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”‚
-//     â”‚                       â”‚           â”‚    â”‚
-//  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” Transfer   â”‚           â”‚    â”‚
-//  â”‚             â”‚ Started    â”‚           â”‚    â”‚
-//  â”‚ Dead        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚    â”‚
-//  â”‚             â”‚                        â”‚    â”‚
-//  â””â”€â–²â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”˜        Transfer        â”‚    â”‚
-//    â”‚       â”‚            Failed/Cancelledâ”‚    â”‚
-//    â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
-//    â”‚                                         â”‚
-//    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
-//
-
 /// A set of shard replicas.
-///
 /// Handles operations so that the state is consistent across all the replicas of the shard.
 /// Prefers local shard for read-only operations.
 /// Perform updates on all replicas and report error if there is at least one failure.
-///
 pub struct ShardReplicaSet {
-    local: RwLock<Option<Shard>>, // Abstract Shard to be able to use a Proxy during replication
+    local: RwLock<Option<Shard>>,
     remotes: RwLock<Vec<RemoteShard>>,
     replica_state: Arc<SaveOnDisk<ReplicaSetState>>,
-    /// List of peers that are marked as dead locally, but are not yet submitted to the consensus.
-    /// List is checked on each consensus round and submitted to the consensus.
-    /// If the state of the peer is changed in the consensus, it is removed from the list.
-    /// Update and read operations are not performed on the peers marked as dead.
     locally_disabled_peers: parking_lot::RwLock<locally_disabled_peers::Registry>,
     pub(crate) shard_path: PathBuf,
     pub(crate) shard_id: ShardId,
@@ -108,10 +64,10 @@ pub struct ShardReplicaSet {
     update_runtime: Handle,
     search_runtime: Handle,
     optimizer_resource_budget: ResourceBudget,
-    /// Lock to serialized write operations on the replicaset when a write ordering is used.
+    /// Lock to serialize write operations on the replicaset when a write ordering is used.
     write_ordering_lock: Mutex<()>,
     /// Local clock set, used to tag new operations on this shard.
-    clock_set: Mutex<ClockSet>,
+    clock_set: Mutex<clock_set::ClockSet>,
     write_rate_limiter: Option<parking_lot::Mutex<RateLimiter>>,
 }
 
@@ -137,7 +93,6 @@ impl ShardReplicaSet {
         collection_config: Arc<RwLock<CollectionConfigInternal>>,
         effective_optimizers_config: OptimizersConfig,
         shared_storage_config: Arc<SharedStorageConfig>,
-        payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,
         channel_service: ChannelService,
         update_runtime: Handle,
         search_runtime: Handle,
@@ -152,7 +107,6 @@ impl ShardReplicaSet {
                 &shard_path,
                 collection_config.clone(),
                 shared_storage_config.clone(),
-                payload_index_schema.clone(),
                 update_runtime.clone(),
                 search_runtime.clone(),
                 optimizer_resource_budget.clone(),
@@ -163,6 +117,20 @@ impl ShardReplicaSet {
         } else {
             None
         };
+
+        // Initialize the write rate limiter
+        let config_read = collection_config.read().await;
+        let write_rate_limiter = config_read
+            .strict_mode_config
+            .as_ref()
+            .and_then(|strict| {
+                strict
+                    .write_rate_limit
+                    .map(RateLimiter::new_per_minute)
+                    .map(parking_lot::Mutex::new)
+            });
+        drop(config_read);
+
         let replica_state: SaveOnDisk<ReplicaSetState> =
             SaveOnDisk::load_or_init_default(shard_path.join(REPLICA_STATE_FILE))?;
 
@@ -186,20 +154,9 @@ impl ShardReplicaSet {
         );
 
         // Save shard config as the last step, to ensure that the file state is consistent
-        // Presence of shard config indicates that the shard is ready to be used
         let replica_set_shard_config = ShardConfig::new_replica_set();
         replica_set_shard_config.save(&shard_path)?;
 
-        // Initialize the write rate limiter
-        let config = collection_config.read().await;
-        let write_rate_limiter = config.strict_mode_config.as_ref().and_then(|strict_mode| {
-            strict_mode
-                .write_rate_limit
-                .map(RateLimiter::new_per_minute)
-                .map(parking_lot::Mutex::new)
-        });
-        drop(config);
-
         Ok(Self {
             shard_id,
             shard_key,
@@ -208,14 +165,14 @@ impl ShardReplicaSet {
             replica_state: replica_state.into(),
             locally_disabled_peers: Default::default(),
             shard_path,
-            abort_shard_transfer_cb: abort_shard_transfer,
             notify_peer_failure_cb: on_peer_failure,
+            abort_shard_transfer_cb: abort_shard_transfer,
             channel_service,
             collection_id,
             collection_config,
             optimizers_config: effective_optimizers_config,
             shared_storage_config,
-            payload_index_schema,
+            payload_index_schema: Arc::new(replica_set_shard_config.into()), // reused SaveOnDisk
             update_runtime,
             search_runtime,
             optimizer_resource_budget,
@@ -236,11 +193,8 @@ impl ShardReplicaSet {
         shard_key: Option<ShardKey>,
         collection_id: CollectionId,
         shard_path: &Path,
-        is_dirty_shard: bool,
         collection_config: Arc<RwLock<CollectionConfigInternal>>,
-        effective_optimizers_config: OptimizersConfig,
         shared_storage_config: Arc<SharedStorageConfig>,
-        payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,
         channel_service: ChannelService,
         on_peer_failure: ChangePeerFromState,
         abort_shard_transfer: AbortShardTransfer,
@@ -249,22 +203,20 @@ impl ShardReplicaSet {
         search_runtime: Handle,
         optimizer_resource_budget: ResourceBudget,
     ) -> Self {
+        let is_dirty_shard = shard_path.join("shard_config.json").exists() == false;
         let replica_state: SaveOnDisk<ReplicaSetState> =
             SaveOnDisk::load_or_init_default(shard_path.join(REPLICA_STATE_FILE)).unwrap();
 
         if replica_state.read().this_peer_id != this_peer_id {
             replica_state
                 .write(|rs| {
-                    let this_peer_id = rs.this_peer_id;
-                    let local_state = rs.remove_peer_state(this_peer_id);
-                    if let Some(state) = local_state {
-                        rs.set_peer_state(this_peer_id, state);
+                    let old = rs.this_peer_id;
+                    let state = rs.remove_peer_state(old);
+                    if let Some(state) = state {
+                        rs.set_peer_state(old, state);
                     }
                     rs.this_peer_id = this_peer_id;
                 })
-                .map_err(|e| {
-                    panic!("Failed to update replica state in {shard_path:?}: {e}");
-                })
                 .unwrap();
         }
 
@@ -283,7 +235,6 @@ impl ShardReplicaSet {
                 log::error!(
                     "Shard {collection_id}:{shard_id} is not fully initialized - loading as dummy shard"
                 );
-                // This dummy shard will be replaced only when it rejects an update (marked as dead so recovery process kicks in)
                 Shard::Dummy(DummyShard::new(
                     "Dirty shard - shard is not fully initialized",
                 ))
@@ -293,12 +244,11 @@ impl ShardReplicaSet {
                     collection_id.clone(),
                     shard_path,
                     collection_config.clone(),
-                    effective_optimizers_config.clone(),
                     shared_storage_config.clone(),
-                    payload_index_schema.clone(),
                     update_runtime.clone(),
                     search_runtime.clone(),
                     optimizer_resource_budget.clone(),
+                    optimizers_config.clone(),
                 )
                 .await;
 
@@ -308,44 +258,27 @@ impl ShardReplicaSet {
                         if !shared_storage_config.handle_collection_load_errors {
                             panic!("Failed to load local shard {shard_path:?}: {err}")
                         }
-
                         local_load_failure = true;
-
                         log::error!(
-                            "Failed to load local shard {shard_path:?}, \
-                             initializing \"dummy\" shard instead: \
-                             {err}"
+                            "Failed to load local shard {shard_path:?}, initializing \"dummy\" shard instead: {err}"
                         );
-
                         Shard::Dummy(DummyShard::new(format!(
                             "Failed to load local shard {shard_path:?}: {err}"
                         )))
                     }
                 }
             };
-
             Some(shard)
         } else {
             None
         };
 
-        // Initialize the write rate limiter
-        let config = collection_config.read().await;
-        let write_rate_limiter = config.strict_mode_config.as_ref().and_then(|strict_mode| {
-            strict_mode
-                .write_rate_limit
-                .map(RateLimiter::new_per_minute)
-                .map(parking_lot::Mutex::new)
-        });
-        drop(config);
-
-        let replica_set = Self {
+        let mut replica_set = Self {
             shard_id,
             shard_key,
             local: RwLock::new(local),
             remotes: RwLock::new(remote_shards),
             replica_state: replica_state.into(),
-            // TODO: move to collection config
             locally_disabled_peers: Default::default(),
             shard_path: shard_path.to_path_buf(),
             notify_peer_failure_cb: on_peer_failure,
@@ -353,7 +286,7 @@ impl ShardReplicaSet {
             channel_service,
             collection_id,
             collection_config,
-            optimizers_config: effective_optimizers_config,
+            optimizers_config,
             shared_storage_config,
             payload_index_schema,
             update_runtime,
@@ -364,7 +297,6 @@ impl ShardReplicaSet {
             write_rate_limiter,
         };
 
-        // `active_remote_shards` includes `Active` and `ReshardingScaleDown` replicas!
         if local_load_failure && replica_set.active_remote_shards().is_empty() {
             replica_set
                 .locally_disabled_peers
@@ -375,274 +307,98 @@ impl ShardReplicaSet {
         replica_set
     }
 
+    /// The peer ID this replicate set uses for itself.
     pub fn this_peer_id(&self) -> PeerId {
         self.replica_state.read().this_peer_id
     }
 
-    pub async fn has_remote_shard(&self) -> bool {
-        !self.remotes.read().await.is_empty()
-    }
-
-    pub async fn has_local_shard(&self) -> bool {
-        self.local.read().await.is_some()
-    }
-
-    pub async fn is_local(&self) -> bool {
-        let local_read = self.local.read().await;
-        matches!(*local_read, Some(Shard::Local(_) | Shard::Dummy(_)))
-    }
-
-    pub async fn is_queue_proxy(&self) -> bool {
-        let local_read = self.local.read().await;
-        matches!(*local_read, Some(Shard::QueueProxy(_)))
-    }
-
-    pub async fn is_dummy(&self) -> bool {
-        let local_read = self.local.read().await;
-        matches!(*local_read, Some(Shard::Dummy(_)))
-    }
-
     pub fn peers(&self) -> HashMap<PeerId, ReplicaState> {
         self.replica_state.read().peers()
     }
 
-    pub fn is_last_active_replica(&self, peer_id: PeerId) -> bool {
-        // This includes `Active` and `ReshardingScaleDown` replicas!
-        let active_peers = self.replica_state.read().active_peers();
-        active_peers.len() == 1 && active_peers.contains(&peer_id)
-    }
-
     pub fn peer_state(&self, peer_id: PeerId) -> Option<ReplicaState> {
         self.replica_state.read().get_peer_state(peer_id)
     }
 
-    /// List the peer IDs on which this shard is active, both the local and remote peers.
-    pub fn active_shards(&self) -> Vec<PeerId> {
-        let replica_state = self.replica_state.read();
-        replica_state
-            // This is a part of deprecated built-in resharding implementation, so we don't care
+    pub async fn has_local_shard(&self) -> bool {
+        self.local.read().await.is_some()
+    }
+
+    pub async fn has_remote_shard(&self) -> bool {
+        !self.remotes.read().await.is_empty()
+    }
+
+    pub async fn active_shards(&self) -> Vec<PeerId> {
+        self.replica_state
+            .read()
             .active_peers()
             .into_iter()
-            .filter(|&peer_id| !self.is_locally_disabled(peer_id))
+            .filter(|&p| !self.is_locally_disabled(p))
             .collect()
     }
 
-    /// List the remote peer IDs on which this shard is active, excludes the local peer ID.
     pub fn active_remote_shards(&self) -> Vec<PeerId> {
-        let replica_state = self.replica_state.read();
-        let this_peer_id = replica_state.this_peer_id;
-        replica_state
-            .active_peers() // This includes `Active` and `ReshardingScaleDown` replicas!
+        let rs = self.replica_state.read();
+        rs.active_peers()
             .into_iter()
-            .filter(|&peer_id| !self.is_locally_disabled(peer_id) && peer_id != this_peer_id)
+            .filter(|&p| !self.is_locally_disabled(p) && p != rs.this_peer_id)
             .collect()
     }
 
-    /// Wait for a local shard to be initialized.
-    ///
-    /// Uses a blocking thread internally.
-    pub async fn wait_for_local(&self, timeout: Duration) -> CollectionResult<()> {
-        self.wait_for(|replica_set_state| replica_set_state.is_local, timeout)
-            .await
-    }
-
-    pub fn wait_for_state_condition_sync<F>(&self, check: F, timeout: Duration) -> bool
-    where
-        F: Fn(&ReplicaSetState) -> bool,
-    {
-        let replica_state = self.replica_state.clone();
-        replica_state.wait_for(check, timeout)
-    }
-
-    /// Wait for a local shard to get into `state`
-    ///
-    /// Uses a blocking thread internally.
-    pub async fn wait_for_local_state(
-        &self,
-        state: ReplicaState,
-        timeout: Duration,
-    ) -> CollectionResult<()> {
-        self.wait_for(
-            move |replica_set_state| {
-                replica_set_state.get_peer_state(replica_set_state.this_peer_id) == Some(state)
-            },
-            timeout,
-        )
-        .await
-    }
-
-    /// Wait for a peer shard to get into `state`
-    ///
-    /// Uses a blocking thread internally.
-    ///
-    /// # Cancel safety
-    ///
-    /// This method is cancel safe.
-    pub async fn wait_for_state(
-        &self,
-        peer_id: PeerId,
-        state: ReplicaState,
-        timeout: Duration,
-    ) -> CollectionResult<()> {
-        self.wait_for(
-            move |replica_set_state| replica_set_state.get_peer_state(peer_id) == Some(state),
-            timeout,
-        )
-        .await
-    }
-
-    /// Wait for a replica set state condition to be true.
-    ///
-    /// Uses a blocking thread internally.
-    ///
-    /// # Cancel safety
-    ///
-    /// This method is cancel safe.
-    async fn wait_for<F>(&self, check: F, timeout: Duration) -> CollectionResult<()>
-    where
-        F: Fn(&ReplicaSetState) -> bool + Send + 'static,
-    {
-        // TODO: Propagate cancellation into `spawn_blocking` task!?
-
-        let replica_state = self.replica_state.clone();
-        let timed_out =
-            !tokio::task::spawn_blocking(move || replica_state.wait_for(check, timeout))
-                .await
-                .map_err(|err| {
-                    CollectionError::service_error(format!(
-                        "Failed to wait for replica set state: {err}"
-                    ))
-                })?;
-
-        if timed_out {
-            return Err(CollectionError::service_error(
-                "Failed to wait for replica set state, timed out",
-            ));
-        }
-
-        Ok(())
-    }
-
-    /// Clears the local shard data and loads an empty local shard
-    pub async fn init_empty_local_shard(&self) -> CollectionResult<()> {
+    async fn init_empty_local_shard(&self) -> CollectionResult<()> {
         let mut local = self.local.write().await;
-
-        let current_shard = local.take();
-
+        let old = local.take();
         LocalShard::clear(&self.shard_path).await?;
-        let local_shard_res = LocalShard::build(
+        let new = LocalShard::build(
             self.shard_id,
             self.collection_id.clone(),
             &self.shard_path,
             self.collection_config.clone(),
             self.shared_storage_config.clone(),
-            self.payload_index_schema.clone(),
             self.update_runtime.clone(),
             self.search_runtime.clone(),
             self.optimizer_resource_budget.clone(),
             self.optimizers_config.clone(),
         )
         .await;
-
-        match local_shard_res {
-            Ok(local_shard) => {
-                *local = Some(Shard::Local(local_shard));
+        match new {
+            Ok(sh) => {
+                *local = Some(Shard::Local(sh));
                 Ok(())
             }
-            Err(err) => {
-                log::error!(
-                    "Failed to initialize local shard {:?}: {err}",
-                    self.shard_path
-                );
-                *local = current_shard;
-                Err(err)
+            Err(e) => {
+                log::error!("Failed to init empty local shard: {e}");
+                *local = old;
+                Err(e)
             }
         }
     }
 
-    pub async fn set_local(
-        &self,
-        local: LocalShard,
-        state: Option<ReplicaState>,
-    ) -> CollectionResult<Option<Shard>> {
-        let old_shard = self.local.write().await.replace(Shard::Local(local));
-
-        if !self.replica_state.read().is_local || state.is_some() {
-            self.replica_state.write(|rs| {
-                rs.is_local = true;
-                if let Some(state) = state {
-                    rs.set_peer_state(self.this_peer_id(), state);
-                }
-            })?;
-        }
-        self.update_locally_disabled(self.this_peer_id());
-        Ok(old_shard)
-    }
-
-    pub async fn remove_local(&self) -> CollectionResult<()> {
-        // TODO: Ensure cancel safety!
-
-        self.replica_state.write(|rs| {
-            rs.is_local = false;
-            let this_peer_id = rs.this_peer_id;
-            rs.remove_peer_state(this_peer_id);
-        })?;
-
-        self.update_locally_disabled(self.this_peer_id());
-
-        let removing_local = {
-            let mut local = self.local.write().await;
-            local.take()
-        };
-
-        if let Some(removing_local) = removing_local {
-            // stop ongoing tasks and delete data
-            drop(removing_local);
-            LocalShard::clear(&self.shard_path).await?;
-        }
-        Ok(())
-    }
-
     pub async fn add_remote(&self, peer_id: PeerId, state: ReplicaState) -> CollectionResult<()> {
         debug_assert!(peer_id != self.this_peer_id());
-
-        self.replica_state.write(|rs| {
-            rs.set_peer_state(peer_id, state);
-        })?;
-
-        self.update_locally_disabled(peer_id);
-
-        let mut remotes = self.remotes.write().await;
-
-        // check remote already exists
-        if remotes.iter().any(|remote| remote.peer_id == peer_id) {
-            return Ok(());
+        self.replica_state.write(|rs| rs.set_peer_state(peer_id, state))?;
+        self.update_locally_disabled(&self.replica_state.read(), peer_id, None);
+        let mut rem = self.remotes.write().await;
+        if rem.iter().all(|r| r.peer_id != peer_id) {
+            rem.push(RemoteShard::new(
+                self.shard_id,
+                self.collection_id.clone(),
+                peer_id,
+                self.channel_service.clone(),
+            ));
         }
-
-        remotes.push(RemoteShard::new(
-            self.shard_id,
-            self.collection_id.clone(),
-            peer_id,
-            self.channel_service.clone(),
-        ));
-
         Ok(())
     }
 
     pub async fn remove_remote(&self, peer_id: PeerId) -> CollectionResult<()> {
-        self.replica_state.write(|rs| {
-            rs.remove_peer_state(peer_id);
-        })?;
-
-        self.update_locally_disabled(peer_id);
-
-        let mut remotes = self.remotes.write().await;
-        remotes.retain(|remote| remote.peer_id != peer_id);
+        self.replica_state.write(|rs| rs.remove_peer_state(peer_id))?;
+        self.update_locally_disabled(&self.replica_state.read(), peer_id, None);
+        let mut rem = self.remotes.write().await;
+        rem.retain(|r| r.peer_id != peer_id);
         Ok(())
     }
 
-    /// Change state of the replica to the given.
-    /// Ensure that remote shard is initialized.
+    /// Ensure a replica is initialized locally or remotely with given state.
     pub async fn ensure_replica_with_state(
         &self,
         peer_id: PeerId,
@@ -651,7 +407,6 @@ impl ShardReplicaSet {
         if peer_id == self.this_peer_id() {
             self.set_replica_state(peer_id, state)?;
         } else {
-            // Create remote shard if necessary
             self.add_remote(peer_id, state).await?;
         }
         Ok(())
@@ -659,19 +414,19 @@ impl ShardReplicaSet {
 
     pub fn set_replica_state(&self, peer_id: PeerId, state: ReplicaState) -> CollectionResult<()> {
         log::debug!(
-            "Changing local shard {}:{} state from {:?} to {state:?}",
+            "Changing shard {}/{} state from {:?} to {:?}",
             self.collection_id,
             self.shard_id,
             self.replica_state.read().get_peer_state(peer_id),
+            state
         );
-
         self.replica_state.write(|rs| {
             if rs.this_peer_id == peer_id {
                 rs.is_local = true;
             }
             rs.set_peer_state(peer_id, state);
         })?;
-        self.update_locally_disabled(peer_id);
+        self.update_locally_disabled(&self.replica_state.read(), peer_id, None);
         Ok(())
     }
 
@@ -689,264 +444,123 @@ impl ShardReplicaSet {
         replicas: HashMap<PeerId, ReplicaState>,
         shard_key: Option<ShardKey>,
     ) -> CollectionResult<()> {
-        let old_peers = self.replica_state.read().peers();
-
-        self.replica_state.write(|state| {
-            state.set_peers(replicas.clone());
-        })?;
-
+        let old = self.replica_state.read().peers();
+        self.replica_state.write(|rs| rs.set_peers(replicas.clone()))?;
         self.locally_disabled_peers.write().clear();
 
-        let removed_peers = old_peers
-            .keys()
-            .filter(|peer_id| !replicas.contains_key(peer_id))
-            .copied()
-            .collect::<Vec<_>>();
-        for peer_id in removed_peers {
-            self.remove_peer(peer_id).await?;
+        for removed in old.keys().filter(|k| !replicas.contains_key(k)).cloned() {
+            self.remove_peer(removed).await?;
         }
-
-        for (peer_id, state) in replicas {
-            let peer_already_exists = old_peers.contains_key(&peer_id);
-
-            if peer_already_exists {
-                // do nothing
-                // We only need to change state and it is already saved
+        for (peer, state) in replicas {
+            if old.contains_key(&peer) {
                 continue;
             }
-
-            if peer_id == self.this_peer_id() {
-                // Consensus wants a local replica on this peer
-                let local_shard = LocalShard::build(
+            if peer == self.this_peer_id() {
+                let sh = LocalShard::build(
                     self.shard_id,
                     self.collection_id.clone(),
                     &self.shard_path,
                     self.collection_config.clone(),
                     self.shared_storage_config.clone(),
-                    self.payload_index_schema.clone(),
                     self.update_runtime.clone(),
                     self.search_runtime.clone(),
                     self.optimizer_resource_budget.clone(),
                     self.optimizers_config.clone(),
                 )
                 .await?;
-
-                match state {
-                    ReplicaState::Active
-                    | ReplicaState::Listener
-                    | ReplicaState::ReshardingScaleDown => {
-                        // No way we can provide up-to-date replica right away at this point,
-                        // so we report a failure to consensus
-                        self.set_local(local_shard, Some(state)).await?;
-                        self.notify_peer_failure(peer_id, Some(state));
-                    }
-
-                    ReplicaState::Dead
-                    | ReplicaState::Partial
-                    | ReplicaState::Initializing
-                    | ReplicaState::PartialSnapshot
-                    | ReplicaState::Recovery
-                    | ReplicaState::Resharding => {
-                        self.set_local(local_shard, Some(state)).await?;
-                    }
+                self.set_local(sh, Some(state)).await?;
+                if matches!(state, ReplicaState::Active | ReplicaState::Listener) {
+                    self.notify_peer_failure(peer, Some(state));
                 }
-
-                continue;
+            } else {
+                let new = RemoteShard::new(
+                    self.shard_id,
+                    self.collection_id.clone(),
+                    peer,
+                    self.channel_service.clone(),
+                );
+                self.remotes.write().await.push(new);
             }
-
-            // Otherwise it is a missing remote replica, we simply create it
-
-            let new_remote = RemoteShard::new(
-                self.shard_id,
-                self.collection_id.clone(),
-                peer_id,
-                self.channel_service.clone(),
-            );
-            self.remotes.write().await.push(new_remote);
         }
 
-        // Apply shard key
+        // Apply shard key from consensus
         self.shard_key = shard_key;
 
         Ok(())
     }
 
-    pub(crate) async fn on_optimizer_config_update(&self) -> CollectionResult<()> {
-        let read_local = self.local.read().await;
-        if let Some(shard) = &*read_local {
-            shard.on_optimizer_config_update().await
-        } else {
-            Ok(())
-        }
-    }
-
-    /// Apply shard's strict mode configuration update
-    /// - Update read and write rate limiters
-    pub(crate) async fn on_strict_mode_config_update(&mut self) -> CollectionResult<()> {
-        let mut read_local = self.local.write().await;
-        if let Some(shard) = read_local.as_mut() {
-            shard.on_strict_mode_config_update().await
-        }
-        drop(read_local);
-        let config = self.collection_config.read().await;
-        if let Some(strict_mode_config) = &config.strict_mode_config {
-            if strict_mode_config.enabled == Some(true) {
-                // update write rate limiter
-                if let Some(write_rate_limit_per_min) = strict_mode_config.write_rate_limit {
-                    let new_write_rate_limiter =
-                        RateLimiter::new_per_minute(write_rate_limit_per_min);
-                    self.write_rate_limiter
-                        .replace(parking_lot::Mutex::new(new_write_rate_limiter));
-                    return Ok(());
-                }
-            }
-        }
-        // remove write rate limiter for all other situations
-        self.write_rate_limiter.take();
-        Ok(())
-    }
-
-    /// Check if the write rate limiter allows the operation to proceed
-    /// - hw_measurement_acc: the current hardware measurement accumulator
-    /// - cost_fn: the cost of the operation called lazily
-    ///
-    /// Returns an error if the rate limit is exceeded.
-    fn check_write_rate_limiter<F>(
+    /// Get telemetry data, details handled in `telemetry.rs`
+    pub(crate) async fn get_telemetry_data(
         &self,
-        hw_measurement_acc: &HwMeasurementAcc,
-        cost_fn: F,
-    ) -> CollectionResult<()>
-    where
-        F: FnOnce() -> usize,
-    {
-        // Do not rate limit internal operation tagged with disposable measurement
-        if hw_measurement_acc.is_disposable() {
-            return Ok(());
-        }
-        if let Some(rate_limiter) = &self.write_rate_limiter {
-            let cost = cost_fn();
-            rate_limiter
-                .lock()
-                .try_consume(cost as f64)
-                .map_err(|err| CollectionError::rate_limit_error(err, cost, true))?;
-        }
-        Ok(())
-    }
-
-    /// Check if there are any locally disabled peers
-    /// And if so, report them to the consensus
-    pub fn sync_local_state<F>(&self, get_shard_transfers: F) -> CollectionResult<()>
-    where
-        F: Fn(ShardId, PeerId) -> Vec<ShardTransfer>,
-    {
-        let peers_to_notify: Vec<_> = self
-            .locally_disabled_peers
-            .write()
-            .notify_elapsed()
-            .collect();
-
-        for (failed_peer_id, from_state) in peers_to_notify {
-            self.notify_peer_failure(failed_peer_id, from_state);
-
-            for transfer in get_shard_transfers(self.shard_id, failed_peer_id) {
-                self.abort_shard_transfer(
-                    transfer,
-                    &format!(
-                        "{failed_peer_id}/{}:{} replica failed",
-                        self.collection_id, self.shard_id,
-                    ),
-                );
-            }
-        }
-
-        Ok(())
+        detail: crate::common::types::TelemetryDetail,
+    ) -> crate::shards::telemetry::ReplicaSetTelemetry {
+        telemetry::collect(self, detail).await
     }
 
     pub(crate) async fn health_check(&self, peer_id: PeerId) -> CollectionResult<()> {
-        let remotes = self.remotes.read().await;
-
-        let Some(remote) = remotes.iter().find(|remote| remote.peer_id == peer_id) else {
+        let rems = self.remotes.read().await;
+        let Some(remote) = rems.iter().find(|r| r.peer_id == peer_id) else {
             return Err(CollectionError::NotFound {
-                what: format!("{}/{}:{} shard", peer_id, self.collection_id, self.shard_id),
+                what: format!("{peer_id}/{}/{} shard", self.collection_id, self.shard_id),
             });
         };
-
-        remote.health_check().await?;
-
-        Ok(())
+        remote.health_check().await
     }
 
-    pub async fn delete_local_points(
+    /// Update cutoff for local shard.
+    pub(crate) async fn update_shard_cutoff_point(
         &self,
-        filter: Filter,
-        hw_measurement_acc: HwMeasurementAcc,
-        force: bool,
-    ) -> CollectionResult<UpdateResult> {
-        let local_shard_guard = self.local.read().await;
-
-        let Some(local_shard) = local_shard_guard.deref() else {
+        cutoff: &RecoveryPoint,
+    ) -> CollectionResult<()> {
+        let loc = self.local.read().await;
+        let Some(sh) = loc.as_ref() else {
             return Err(CollectionError::NotFound {
-                what: format!("local shard {}:{}", self.collection_id, self.shard_id),
+                what: "Peer does not have local shard".into(),
             });
         };
+        sh.update_cutoff(cutoff).await
+    }
 
-        let mut next_offset = Some(ExtendedPointId::NumId(0));
-        let mut ids = Vec::new();
-
-        while let Some(current_offset) = next_offset {
-            const BATCH_SIZE: usize = 1000;
-
-            let mut points = local_shard
-                .get()
-                .scroll_by(
-                    Some(current_offset),
-                    BATCH_SIZE + 1,
-                    &false.into(),
-                    &false.into(),
-                    Some(&filter),
-                    &self.search_runtime,
-                    None,
-                    None,
-                    hw_measurement_acc.clone(),
-                )
-                .await?;
-
-            if points.len() > BATCH_SIZE {
-                next_offset = points.pop().map(|points| points.id);
-            } else {
-                next_offset = None;
-            }
-
-            ids.extend(points.into_iter().map(|points| points.id));
-        }
+    pub(crate) fn get_snapshots_storage_manager(&self) -> CollectionResult<SnapshotStorageManager> {
+        SnapshotStorageManager::new(&self.shared_storage_config.snapshots_config)
+    }
 
-        if ids.is_empty() {
-            return Ok(UpdateResult {
-                operation_id: None,
-                status: UpdateStatus::Completed,
-                clock_tag: None,
-            });
+    /// Trigger optimizers manually after config updates or snapshot restores.
+    pub(crate) async fn trigger_optimizers(&self) -> bool {
+        let guard = self.local.read().await;
+        if let Some(sh) = guard.as_ref() {
+            sh.trigger_optimizers().await;
+            true
+        } else {
+            false
         }
+    }
 
-        drop(local_shard_guard);
-
-        let op =
-            CollectionUpdateOperations::PointOperation(point_ops::PointOperations::DeletePoints {
-                ids,
-            });
-
-        // TODO(resharding): Assign clock tag to the operation!? ğŸ¤”
-        let result = self
-            .update_local(op.into(), true, hw_measurement_acc, force)
-            .await?
-            .ok_or_else(|| {
-                CollectionError::bad_request(format!(
-                    "local shard {}:{} does not exist or is unavailable",
-                    self.collection_id, self.shard_id,
-                ))
-            })?;
-
-        Ok(result)
+    /// Returns estimated local shard stats. Cache in hot paths.
+    pub(crate) async fn calculate_local_shard_stats(&self) -> Option<CollectionSizeStats> {
+        self.local
+            .read()
+            .await
+            .as_ref()
+            .and_then(|sh| match sh {
+                Shard::Local(local) => {
+                    let mut vec_size = 0;
+                    let mut pay_size = 0;
+                    let mut pts = 0;
+                    for seg in local.segments.read().iter() {
+                        let info = seg.1.get().read().size_info();
+                        vec_size += info.vectors_size_bytes;
+                        pay_size += info.payloads_size_bytes;
+                        pts += info.num_points;
+                    }
+                    Some(CollectionSizeStats {
+                        vector_storage_size: vec_size,
+                        payload_storage_size: pay_size,
+                        points_count: pts,
+                    })
+                }
+                _ => None,
+            })
     }
 
     fn init_remote_shards(
@@ -958,105 +572,79 @@ impl ShardReplicaSet {
         state
             .peers()
             .iter()
-            .filter(|(peer, _)| **peer != state.this_peer_id)
-            .map(|(peer_id, _is_active)| {
-                RemoteShard::new(
-                    shard_id,
-                    collection_id.clone(),
-                    *peer_id,
-                    channel_service.clone(),
-                )
+            .filter(|(p, _)| *p != &state.this_peer_id)
+            .map(|(&p, _)| {
+                RemoteShard::new(shard_id, collection_id.clone(), p, channel_service.clone())
             })
             .collect()
     }
 
-    /// Check whether a peer is registered as `active`.
-    /// Unknown peers are not active.
     fn peer_is_active(&self, peer_id: PeerId) -> bool {
-        // This is used *exclusively* during `execute_*_read_operation`, and so it *should* consider
-        // `ReshardingScaleDown` replicas
-        let is_active = matches!(
+        let is = matches!(
             self.peer_state(peer_id),
             Some(ReplicaState::Active | ReplicaState::ReshardingScaleDown)
         );
-
-        is_active && !self.is_locally_disabled(peer_id)
+        is && !self.is_locally_disabled(peer_id)
     }
 
     fn peer_is_active_or_resharding(&self, peer_id: PeerId) -> bool {
-        let is_active_or_resharding = matches!(
+        let ok = matches!(
             self.peer_state(peer_id),
             Some(
-                ReplicaState::Active | ReplicaState::Resharding | ReplicaState::ReshardingScaleDown
+                ReplicaState::Active
+                    | ReplicaState::Resharding
+                    | ReplicaState::ReshardingScaleDown
             )
         );
-
-        let is_locally_disabled = self.is_locally_disabled(peer_id);
-
-        is_active_or_resharding && !is_locally_disabled
+        ok && !self.is_locally_disabled(peer_id)
     }
 
     fn is_locally_disabled(&self, peer_id: PeerId) -> bool {
         self.locally_disabled_peers.read().is_disabled(peer_id)
     }
 
-    /// Locally disable given peer
-    ///
-    /// Disables the peer and notifies consensus periodically.
+    /// Disable a peer locally and notify consensus periodically.
     ///
-    /// Prevents disabling the last peer (according to consensus).
-    ///
-    /// If `from_state` is given, the peer will only be disabled if the given state matches
-    /// consensus.
+    /// If `from_state` is Some(s), only disable if the current consensus state matches s.
     fn add_locally_disabled(
         &self,
         state: &ReplicaSetState,
         peer_id: PeerId,
         from_state: Option<ReplicaState>,
     ) {
-        let other_peers = state
+        let other = state
             .active_or_resharding_peers()
-            .filter(|id| id != &peer_id);
-
-        let mut locally_disabled_peers_guard = self.locally_disabled_peers.upgradable_read();
+            .filter(|&x| x != peer_id);
+        let mut guard = self.locally_disabled_peers.upgradable_read();
 
-        // Prevent disabling last peer in consensus
-        {
-            if !locally_disabled_peers_guard.is_disabled(peer_id)
-                && locally_disabled_peers_guard.is_all_disabled(other_peers)
-            {
-                log::warn!("Cannot locally disable last active peer {peer_id} for replica");
-                return;
-            }
+        // Prevent disabling last active
+        if !guard.is_disabled(peer_id) && guard.is_all_disabled(other) {
+            log::warn!("Cannot locally disable last active peer {peer_id}");
+            return;
         }
 
-        locally_disabled_peers_guard.with_upgraded(|locally_disabled_peers| {
-            if locally_disabled_peers.disable_peer_and_notify_if_elapsed(peer_id, from_state) {
+        guard.with_upgraded(|reg| {
+            if reg.disable_peer_and_notify_if_elapsed(peer_id, from_state) {
                 self.notify_peer_failure(peer_id, from_state);
             }
         });
     }
 
-    /// Make sure that locally disabled peers do not contradict the consensus
-    fn update_locally_disabled(&self, peer_id_to_remove: PeerId) {
-        let mut locally_disabled_peers = self.locally_disabled_peers.write();
-
-        // Check that we are not trying to disable the last active peer
-        if locally_disabled_peers
-            .is_all_disabled(self.replica_state.read().active_or_resharding_peers())
-        {
+    /// Ensure local vs consensus don't conflict.
+    fn update_locally_disabled(&self, peer_id: PeerId, from_state: Option<ReplicaState>) {
+        let state = self.replica_state.read();
+        let mut reg = self.locally_disabled_peers.write();
+        if reg.is_all_disabled(state.active_peers().into_iter()) {
             log::warn!("Resolving consensus/local state inconsistency");
-            locally_disabled_peers.clear();
+            reg.clear();
         } else {
-            locally_disabled_peers.enable_peer(peer_id_to_remove);
+            if reg.disable_peer_and_notify_if_elapsed(peer_id, from_state) {
+                self.notify_peer_failure(peer_id, from_state);
+            }
         }
     }
 
-    fn notify_peer_failure(&self, peer_id: PeerId, from_state: Option<ReplicaState>) {
-        log::debug!("Notify peer failure: {peer_id}");
-        self.notify_peer_failure_cb.deref()(peer_id, self.shard_id, from_state)
-    }
-
+    /// Abort shard transfer callback.
     fn abort_shard_transfer(&self, transfer: ShardTransfer, reason: &str) {
         log::debug!(
             "Abort {}:{} / {} -> {} shard transfer",
@@ -1065,82 +653,33 @@ impl ShardReplicaSet {
             transfer.from,
             transfer.to,
         );
-
-        self.abort_shard_transfer_cb.deref()(transfer, reason)
+        (self.abort_shard_transfer_cb)(transfer, reason)
     }
 
-    /// Get shard recovery point for WAL.
-    pub(crate) async fn shard_recovery_point(&self) -> CollectionResult<RecoveryPoint> {
-        let local_shard = self.local.read().await;
-        let Some(local_shard) = local_shard.as_ref() else {
-            return Err(CollectionError::NotFound {
-                what: "Peer does not have local shard".into(),
-            });
-        };
-
-        local_shard.shard_recovery_point().await
+    fn notify_peer_failure(&self, peer_id: PeerId, from_state: Option<ReplicaState>) {
+        log::debug!("Notify peer failure: {peer_id}");
+        (self.notify_peer_failure_cb)(peer_id, self.shard_id, from_state)
     }
 
-    /// Update the cutoff point for the local shard.
-    pub(crate) async fn update_shard_cutoff_point(
+    /// Rate-limiter check for writes.
+    fn check_write_rate_limiter<F>(
         &self,
-        cutoff: &RecoveryPoint,
-    ) -> CollectionResult<()> {
-        let local_shard = self.local.read().await;
-        let Some(local_shard) = local_shard.as_ref() else {
-            return Err(CollectionError::NotFound {
-                what: "Peer does not have local shard".into(),
-            });
-        };
-
-        local_shard.update_cutoff(cutoff).await
-    }
-
-    pub(crate) fn get_snapshots_storage_manager(&self) -> CollectionResult<SnapshotStorageManager> {
-        SnapshotStorageManager::new(&self.shared_storage_config.snapshots_config)
-    }
-
-    pub(crate) async fn trigger_optimizers(&self) -> bool {
-        let shard = self.local.read().await;
-        let Some(shard) = shard.as_ref() else {
-            return false;
-        };
-        shard.trigger_optimizers();
-        true
-    }
-
-    /// Returns the estimated size of all local segments.
-    /// Since this locks all segments you should cache this value in performance critical scenarios!
-    pub(crate) async fn calculate_local_shard_stats(&self) -> Option<CollectionSizeStats> {
-        self.local
-            .read()
-            .await
-            .as_ref()
-            .map(|i| match i {
-                Shard::Local(local) => {
-                    let mut total_vector_size = 0;
-                    let mut total_payload_size = 0;
-                    let mut total_points = 0;
-
-                    for segment in local.segments.read().iter() {
-                        let size_info = segment.1.get().read().size_info();
-                        total_vector_size += size_info.vectors_size_bytes;
-                        total_payload_size += size_info.payloads_size_bytes;
-                        total_points += size_info.num_points;
-                    }
-
-                    Some(CollectionSizeStats {
-                        vector_storage_size: total_vector_size,
-                        payload_storage_size: total_payload_size,
-                        points_count: total_points,
-                    })
-                }
-                Shard::Proxy(_)
-                | Shard::ForwardProxy(_)
-                | Shard::QueueProxy(_)
-                | Shard::Dummy(_) => None,
-            })
-            .unwrap_or_default()
+        hw: &HwMeasurementAcc,
+        cost_fn: F,
+    ) -> CollectionResult<()>
+    where
+        F: FnOnce() -> usize,
+    {
+        if hw.is_disposable() {
+            return Ok(());
+        }
+        if let Some(rl) = &self.write_rate_limiter {
+            let cost = cost_fn();
+            rl.lock()
+                .try_consume(cost as f64)
+                .map_err(|err| CollectionError::rate_limit_error(err, cost, true))?;
+        }
+        Ok(())
     }
 }
 
@@ -1172,134 +711,87 @@ impl ReplicaSetState {
     pub fn active_peers(&self) -> Vec<PeerId> {
         self.peers
             .iter()
-            .filter_map(|(peer_id, state)| {
-                // We consider `ReshardingScaleDown` to be `Active`!
-                matches!(
-                    state,
-                    ReplicaState::Active | ReplicaState::ReshardingScaleDown
-                )
-                .then_some(*peer_id)
+            .filter_map(|(&pid, &st)| {
+                if matches!(st, ReplicaState::Active | ReplicaState::ReshardingScaleDown) {
+                    Some(pid)
+                } else {
+                    None
+                }
             })
             .collect()
     }
 
     pub fn active_or_resharding_peers(&self) -> impl Iterator<Item = PeerId> + '_ {
-        self.peers.iter().filter_map(|(peer_id, state)| {
-            matches!(
-                state,
-                ReplicaState::Active | ReplicaState::Resharding | ReplicaState::ReshardingScaleDown
-            )
-            .then_some(*peer_id)
+        self.peers.iter().filter_map(|(&pid, &st)| {
+            if matches!(
+                st,
+                ReplicaState::Active
+                    | ReplicaState::Resharding
+                    | ReplicaState::ReshardingScaleDown
+            ) {
+                Some(pid)
+            } else {
+                None
+            }
         })
     }
-
-    pub fn set_peers(&mut self, peers: HashMap<PeerId, ReplicaState>) {
-        self.peers = peers;
-    }
 }
 
 /// State of the single shard within a replica set.
-#[derive(
-    Debug, Deserialize, Serialize, JsonSchema, Default, PartialEq, Eq, Hash, Clone, Copy, Anonymize,
-)]
+#[derive(Debug, Deserialize, Serialize, JsonSchema, Default, PartialEq, Eq, Hash, Clone, Copy, Anonymize)]
 pub enum ReplicaState {
     // Active and sound
     #[default]
     Active,
     // Failed for some reason
     Dead,
-    // The shard is partially loaded and is currently receiving data from other shards
+    // Partially loaded, receiving data
     Partial,
     // Collection is being created
     Initializing,
-    // A shard which receives data, but is not used for search
-    // Useful for backup shards
+    // Receives data but not used for search
     Listener,
-    // Deprecated since Qdrant 1.9.0, used in Qdrant 1.7.0 and 1.8.0
-    //
-    // Snapshot shard transfer is in progress, updates aren't sent to the shard
-    // Normally rejects updates. Since 1.8 it allows updates if force is true.
+    // Snapshot transfer in progress, rejects updates normally
     PartialSnapshot,
-    // Shard is undergoing recovery by an external node
-    // Normally rejects updates, accepts updates if force is true
+    // Undergoing external recovery
     Recovery,
-    // Points are being migrated to this shard as part of resharding up
+    // Points migrating in resharding up
     Resharding,
-    // Points are being migrated to this shard as part of resharding down
+    // Points migrating in resharding down
     ReshardingScaleDown,
 }
 
 impl ReplicaState {
-    /// Check if replica state is active
+    /// Is Active?
     pub fn is_active(self) -> bool {
-        match self {
-            ReplicaState::Active => true,
-            ReplicaState::ReshardingScaleDown => true,
-
-            ReplicaState::Dead => false,
-            ReplicaState::Partial => false,
-            ReplicaState::Initializing => false,
-            ReplicaState::Listener => false,
-            ReplicaState::PartialSnapshot => false,
-            ReplicaState::Recovery => false,
-            ReplicaState::Resharding => false,
-        }
+        matches!(self, ReplicaState::Active | ReplicaState::ReshardingScaleDown)
     }
 
-    /// Check whether the replica state is active or listener or resharding.
+    /// Accept updates for read proxies?
     pub fn is_active_or_listener_or_resharding(self) -> bool {
-        match self {
+        matches!(
+            self,
             ReplicaState::Active
-            | ReplicaState::Listener
-            | ReplicaState::Resharding
-            | ReplicaState::ReshardingScaleDown => true,
-
-            ReplicaState::Dead
-            | ReplicaState::Initializing
-            | ReplicaState::Partial
-            | ReplicaState::PartialSnapshot
-            | ReplicaState::Recovery => false,
-        }
+                | ReplicaState::Listener
+                | ReplicaState::Resharding
+                | ReplicaState::ReshardingScaleDown
+        )
     }
 
-    /// Check whether the replica state is partial or partial-like.
-    ///
-    /// In other words: is the state related to shard transfers?
-    //
-    // TODO(resharding): What's the best way to handle `ReshardingScaleDown` properly!?
+    /// Is partial or part of a transfer?
     pub fn is_partial_or_recovery(self) -> bool {
-        match self {
+        matches!(
+            self,
             ReplicaState::Partial
-            | ReplicaState::PartialSnapshot
-            | ReplicaState::Recovery
-            | ReplicaState::Resharding
-            | ReplicaState::ReshardingScaleDown => true,
-
-            ReplicaState::Active
-            | ReplicaState::Dead
-            | ReplicaState::Initializing
-            | ReplicaState::Listener => false,
-        }
+                | ReplicaState::PartialSnapshot
+                | ReplicaState::Recovery
+                | ReplicaState::Resharding
+                | ReplicaState::ReshardingScaleDown
+        )
     }
 
-    /// Returns `true` if the replica state is resharding, either up or down.
+    /// Is currently resharding (up or down)?
     pub fn is_resharding(&self) -> bool {
-        match self {
-            ReplicaState::Resharding | ReplicaState::ReshardingScaleDown => true,
-
-            ReplicaState::Partial
-            | ReplicaState::PartialSnapshot
-            | ReplicaState::Recovery
-            | ReplicaState::Active
-            | ReplicaState::Dead
-            | ReplicaState::Initializing
-            | ReplicaState::Listener => false,
-        }
+        matches!(self, ReplicaState::Resharding | ReplicaState::ReshardingScaleDown)
     }
-}
-
-/// Represents a change in replica set, due to scaling of `replication_factor`
-#[derive(Debug, Deserialize, Serialize, PartialEq, Eq, Hash, Clone)]
-pub enum Change {
-    Remove(ShardId, PeerId),
 }
\ No newline at end of file
