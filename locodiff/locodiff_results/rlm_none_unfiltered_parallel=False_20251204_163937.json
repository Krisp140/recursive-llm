[
  {
    "success": true,
    "id": "aider_aider___init__.py",
    "exact_match": false,
    "similarity": 0.9491173416407062,
    "diff_lines": 15,
    "predicted_output": "__version__ = \"0.82.3.dev\"\n\nsafe_version = __version__\n\ntry:\n    from aider.__version__ import __version__\nexcept Exception:\n    __version__ = safe_version + \"+import\"\n\nif type(__version__) is not str:\n    __version__ = safe_version + \"+type\"\nelse:\n    try:\n        if version.parse(__version__) < version.parse(safe_version):\n            __version__ = safe_version + \"+less\"\n    except Exception:\n        __version__ = safe_version + \"+parse\"\n\n__all__ = [__version__]",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9491173416407062,
      "diff_lines": 15,
      "added_lines": 2,
      "removed_lines": 3,
      "diff_preview": "--- expected+++ predicted@@ -1,10 +1,9 @@-from packaging import version\n+__version__ = \"0.82.3.dev\"\n \n-__version__ = \"0.82.3.dev\"\n safe_version = __version__\n \n try:\n-    from aider._version import __version__\n+    from aider.__version__ import __version__\n except Exception:\n     __version__ = safe_version + \"+import\"\n \n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 18.859992265701294,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 59517,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_aider_analytics.py",
    "exact_match": false,
    "similarity": 0.8351154501782116,
    "diff_lines": 176,
    "predicted_output": "import json\nimport platform\nimport sys\nimport time\nimport uuid\nfrom pathlib import Path\nfrom posthog import Posthog\n\nfrom aider import __version__\nfrom aider.dump import dump  # noqa: F401\n\nPERCENT = 10\n\ndef compute_hex_threshold(percent):\n    \"\"\"Convert percentage to 6-digit hex threshold.\n    \n    Args:\n        percent: Percentage threshold (0-100)\n        \n    Returns:\n        str: 6-digit hex threshold\n    \"\"\"\n    return format(int(0xFFFFFF * percent / 100), \"06x\")\n\ndef is_uuid_in_percentage(uuid_str, percent):\n    \"\"\"Check if a UUID string falls within the first X percent of the UUID space.\n\n    Args:\n        uuid_str: UUID string to test\n        percent: Percentage threshold (0-100)\n\n    Returns:\n        bool: True if UUID falls within the first X percent\n    \"\"\"\n    if not (0 <= percent <= 100):\n        raise ValueError(\"Percentage must be between 0 and 100\")\n\n    if not uuid_str:\n        return False\n\n    # Convert percentage to hex threshold (1% = \"04...\", 10% = \"1a...\", etc)\n    # Using first 6 hex digits\n    if percent == 0:\n        return False\n\n    threshold = compute_hex_threshold(percent)\n    return uuid_str[:6] <= threshold\n\nclass Analytics:\n    # providers\n    mp = None\n    ph = None\n\n    # saved\n    user_id = None\n    permanently_disable = None\n    asked_opt_in = None\n\n    # ephemeral\n    logfile = None\n\n    def __init__(self, logfile=None, permanently_disable=False):\n        self.logfile = logfile\n        self.get_or_create_uuid()\n\n        if self.permanently_disable or permanently_disable or not self.asked_opt_in:\n            self.disable(permanently_disable)\n\n    def enable(self):\n        if not self.user_id:\n            self.disable(False)\n            return\n\n        if self.permanently_disable:\n            self.disable(False)\n            return\n\n        if not self.asked_opt_in:\n            self.disable(False)\n            return\n\n        self.mp = Mixpanel(mixpanel_project_token)\n        self.ph = Posthog(project_api_key=posthog_project_api_key, host=posthog_host)\n\n    def disable(self, permanently):\n        self.mp = None\n        self.ph = None\n        if permanently:\n            self.asked_opt_in = True\n            self.permanently_disable = True\n            self.save_data()\n\n    def get_data_file_path(self):\n        try:\n            data_file = Path.home() / \".aider\" / \"analytics.json\"\n            data_file.parent.mkdir(parents=True, exist_ok=True)\n            return data_file\n        except OSError:\n            # If we can't create/access the directory, just disable analytics\n            self.disable(permanently=False)\n            return None\n\n    def get_or_create_uuid(self):\n        self.load_data()\n        if self.user_id:\n            return\n\n        self.user_id = str(uuid.uuid4())\n        self.save_data()\n\n    def load_data(self):\n        data_file = self.get_data_file_path()\n        if not data_file:\n            return\n            \n        if data_file.exists():\n            try:\n                data = json.loads(data_file.read_text())\n                self.permanently_disable = data.get(\"permanently_disable\")\n                self.user_id = data.get(\"uuid\")\n                self.asked_opt_in = data.get(\"asked_opt_in\", False)\n            except (json.decoder.JSONDecodeError, OSError):\n                self.disable(permanently=False)\n\n    def save_data(self):\n        data_file = self.get_data_file_path()\n        if not data_file:\n            return\n            \n        data = dict(\n            uuid=self.user_id,\n            permanently_disable=self.permanently_disable,\n            asked_opt_in=self.asked_opt_in,\n        )\n\n        # Allow exceptions; crash if we can't record permanently_disabled=True, etc\n        data_file.write_text(json.dumps(data, indent=4))\n\n    def get_system_info(self):\n        return {\n            \"python_version\": sys.version.split()[0],\n            \"os_platform\": platform.system(),\n            \"os_release\": platform.release(),\n            \"machine\": platform.machine(),\n            \"aider_version\": __version__,\n        }\n\n    def _redact_model_name(self, model):\n        if not model:\n            return None\n            \n        info = model_info_manager.get_model_from_cached_json_db(model.name)\n        if info:\n            return model.name\n        elif \"/\" in model.name:\n            return model.name.split(\"/\")[0] + \"/REDACTED\"\n        return None\n\n    def event(self, event_name, main_model=None, **kwargs):\n        if not (self.mp or self.ph) and not self.logfile:\n            return\n\n        properties = {}\n\n        if main_model:\n            properties[\"main_model\"] = self._redact_model_name(main_model)\n            properties[\"weak_model\"] = self._redact_model_name(main_model.weak_model)\n            properties[\"editor_model\"] = self._redact_model_name(main_model.editor_model)\n\n        properties.update(kwargs)\n\n        if self.mp:\n            try:\n                self.mp.track(self.user_id, event_name, dict(properties))\n            except MixpanelException:\n                self.mp = None  # Disable mixpanel on connection errors\n\n        if self.ph:\n            self.ph.capture(self.user_id, event_name, dict(properties))\n\n        if self.logfile:\n            log_entry = {\n                \"event\": event_name,\n                \"properties\": properties,\n                \"user_id\": self.user_id,\n                \"time\": int(time.time()),\n            }\n            try:\n                with open(self.logfile, \"a\") as f:\n                    json.dump(log_entry, f)\n                    f.write(\"\\n\")\n            except OSError:\n                pass  # Ignore OS errors when writing to logfile\n\nif __name__ == \"__main__\":\n    dump(compute_hex_threshold(PERCENT))",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.8351154501782116,
      "diff_lines": 176,
      "added_lines": 11,
      "removed_lines": 65,
      "diff_preview": "--- expected+++ predicted@@ -4,28 +4,23 @@ import time\n import uuid\n from pathlib import Path\n-\n-from mixpanel import MixpanelException\n from posthog import Posthog\n \n from aider import __version__\n from aider.dump import dump  # noqa: F401\n-from aider.models import model_info_manager\n \n PERCENT = 10\n \n-\n def compute_hex_threshold(percent):\n     \"\"\"Convert percentage to 6-digit hex threshold.\n-\n+    \n     Args:\n         percent: Percentage threshold (0-100)\n-\n+        \n     Returns:\n         str: 6-digit hex threshold\n     \"\"\"\n     return format(int(0xFFFFFF * percent / 100), \"06x\")\n-\n \n def is_uuid_in_percentage(uuid_str, percent):\n     \"\"\"Check if a UUID string falls within the first X percent of the UUID space.\n@@ -50,12 +45,6 @@ \n     threshold = compute_hex_threshold(percent)\n     return uuid_str[:6] <= threshold\n-\n-\n-mixpanel_project_token = \"6da9a43058a5d1b9f3353153921fb04d\"\n-posthog_project_api_key = \"phc_99T7muzafUMMZX15H8XePbMSreEUzahHbtWjy3l5Qbv\"\n-posthog_host = \"https://us.i.posthog.com\"\n-\n \n class Analytics:\n     # providers\n@@ -83,48 +72,23 @@             return\n \n         if self.permanently_disable:\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 31.889384984970093,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 24542,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_aider_args.py",
    "exact_match": false,
    "similarity": 0.27334863555215505,
    "diff_lines": 713,
    "predicted_output": "import argparse\nimport os\nimport sys\nfrom pathlib import Path\nimport configargparse\n\nfrom aider import __version__\nfrom aider.args_formatter import (\n    DotEnvFormatter,\n    MarkdownHelpFormatter,\n    YamlHelpFormatter,\n)\nfrom aider.deprecated import add_deprecated_model_args\nfrom .dump import dump  # noqa: F401\n\ndef resolve_aiderignore_path(path_str, git_root=None):\n    path = Path(path_str)\n    if path.is_absolute():\n        return str(path)\n    elif git_root:\n        return str(Path(git_root) / path)\n    return str(path)\n\ndef default_env_file(git_root):\n    return os.path.join(git_root, \".env\") if git_root else \".env\"\n\ndef get_parser(default_config_files, git_root):\n    parser = configargparse.ArgumentParser(\n        description=\"aider is AI pair programming in your terminal\",\n        add_config_file_help=True,\n        default_config_files=default_config_files,\n        auto_env_var_prefix=\"AIDER_\",\n    )\n    group = parser.add_argument_group(\"Main model\")\n    group.add_argument(\n        \"files\", metavar=\"FILE\", nargs=\"*\", help=\"files to edit with an LLM (optional)\"\n    )\n    group.add_argument(\n        \"--model\",\n        metavar=\"MODEL\",\n        default=None,\n        help=\"Specify the model to use for the main chat\",\n    )\n    \n    # Add deprecated model shortcut arguments\n    add_deprecated_model_args(parser, group)\n\n    ##########\n    group = parser.add_argument_group(\"API Keys and settings\")\n    group.add_argument(\n        \"--openai-api-key\",\n        help=\"Specify the OpenAI API key\",\n    )\n    group.add_argument(\n        \"--anthropic-api-key\",\n        help=\"Specify the Anthropic API key\",\n    )\n    group.add_argument(\n        \"--openai-api-base\",\n        help=\"Specify the api base url\",\n    )\n    group.add_argument(\n        \"--openai-api-type\",\n        help=\"Specify the api_type\",\n    )\n    group.add_argument(\n        \"--openai-api-version\",\n        help=\"Specify the api_version\",\n    )\n    group.add_argument(\n        \"--openai-api-deployment-id\",\n        help=\"Specify the deployment_id\",\n    )\n    group.add_argument(\n        \"--openai-organization-id\",\n        help=\"Specify the OpenAI organization ID\",\n    )\n    group.add_argument(\n        \"--set-env\",\n        action=\"append\",\n        metavar=\"ENV_VAR_NAME=value\",\n        help=\"Set an environment variable (to control API settings, can be used multiple times)\",\n        default=[],\n    )\n    group.add_argument(\n        \"--api-key\",\n        action=\"append\",\n        metavar=\"PROVIDER=KEY\",\n        help=\"Set an API key for a provider (eg: --api-key provider=<key> sets PROVIDER_API_KEY=<key>)\",\n        default=[],\n    )\n    \n    ##########\n    group = parser.add_argument_group(\"Model settings\")\n    group.add_argument(\n        \"--list-models\",\n        \"--models\",\n        metavar=\"MODEL\",\n        help=\"List known models which match the (partial) MODEL name\",\n    )\n    group.add_argument(\n        \"--model-settings-file\",\n        metavar=\"MODEL_SETTINGS_FILE\",\n        default=\".aider.model.settings.yml\",\n        help=\"Specify a file with aider model settings for unknown models\",\n    )\n    group.add_argument(\n        \"--model-metadata-file\",\n        metavar=\"MODEL_METADATA_FILE\",\n        default=\".aider.model.metadata.json\",\n        help=\"Specify a file with context window and costs for unknown models\",\n    )\n\n    ##########\n    group = parser.add_argument_group(\"Cache settings\")\n    group.add_argument(\n        \"--cache-prompts\",\n        action=argparse.BooleanOptionalAction,\n        default=False,\n        help=\"Enable caching of prompts (default: False)\",\n    )\n    group.add_argument(\n        \"--cache-keepalive-pings\",\n        type=int,\n        default=0,\n        help=\"Number of times to ping at 5min intervals to keep prompt cache warm (default: 0)\",\n    )\n\n    ##########\n    group = parser.add_argument_group(\"Repomap settings\")\n    group.add_argument(\n        \"--map-tokens\",\n        type=int,\n        default=None,\n        help=\"Suggested number of tokens to use for repo map, use 0 to disable\",\n    )\n    group.add_argument(\n        \"--map-refresh\",\n        choices=[\"auto\", \"always\", \"files\", \"manual\"],\n        default=\"auto\",\n        help=(\n            \"Control how often the repo map is refreshed. Options: auto, always, files, manual\"\n            \" (default: auto)\"\n        ),\n    )\n    group.add_argument(\n        \"--map-multiplier-no-files\",\n        type=float,\n        default=2,\n        help=\"Multiplier for map tokens when no files are specified (default: 2)\",\n    )\n\n    ##########\n    group = parser.add_argument_group(\"History Files\")\n    default_input_history_file = (\n        os.path.join(git_root, \".aider.input.history\") if git_root else \".aider.input.history\"\n    )\n    default_chat_history_file = (\n        os.path.join(git_root, \".aider.chat.history.md\") if git_root else \".aider.chat.history.md\"\n    )\n    group.add_argument(\n        \"--input-history-file\",\n        metavar=\"INPUT_HISTORY_FILE\",\n        default=default_input_history_file,\n        help=f\"Specify the chat input history file (default: {default_input_history_file})\",\n    )\n    group.add_argument(\n        \"--chat-history-file\",\n        metavar=\"CHAT_HISTORY_FILE\",\n        default=default_chat_history_file,\n        help=f\"Specify the chat history file (default: {default_chat_history_file})\",\n    )\n\n    ##########\n    group = parser.add_argument_group(\"Output settings\")\n    group.add_argument(\n        \"--dark-mode\",\n        action=\"store_true\",\n        help=\"Use colors suitable for a dark terminal background (default: False)\",\n        default=False,\n    )\n    group.add_argument(\n        \"--light-mode\",\n        action=\"store_true\",\n        help=\"Use colors suitable for a light terminal background (default: False)\",\n        default=False,\n    )\n    group.add_argument(\n        \"--pretty\",\n        action=argparse.BooleanOptionalAction,\n        default=True,\n        help=\"Enable/disable pretty, colorized output (default: True)\",\n    )\n    group.add_argument(\n        \"--stream\",\n        action=argparse.BooleanOptionalAction,\n        default=True,\n        help=\"Enable/disable streaming responses (default: True)\",\n    )\n    group.add_argument(\n        \"--user-input-color\",\n        default=\"#00cc00\",\n        help=\"Set the color for user input (default: #00cc00)\",\n    )\n    group.add_argument(\n        \"--tool-output-color\",\n        default=None,\n        help=\"Set the color for tool output (default: None)\",\n    )\n    group.add_argument(\n        \"--tool-error-color\",\n        default=\"#FF2222\",\n        help=\"Set the color for tool error messages (default: red)\",\n    )\n    group.add_argument(\n        \"--assistant-output-color\",\n        default=\"#0088ff\",\n        help=\"Set the color for assistant output (default: #0088ff)\",\n    )\n    group.add_argument(\n        \"--completion-menu-color\",\n        metavar=\"COLOR\",\n        default=None,\n        help=\"Set the color for the completion menu (default: terminal's default text color)\",\n    )\n    group.add_argument(\n        \"--completion-menu-bg-color\",\n        metavar=\"COLOR\",\n        default=None,\n        help=(\n            \"Set the background color for the completion menu (default: terminal's default\"\n            \" background color)\"\n        ),\n    )\n    group.add_argument(\n        \"--completion-menu-current-color\",\n        metavar=\"COLOR\",\n        default=None,\n        help=(\n            \"Set the color for the current item in the completion menu (default: terminal's default\"\n            \" background color)\"\n        ),\n    )\n    group.add_argument(\n        \"--completion-menu-current-bg-color\",\n        metavar=\"COLOR\",\n        default=None,\n        help=(\n            \"Set the background color for the current item in the completion menu (default:\"\n            \" terminal's default text color)\"\n        ),\n    )\n    group.add_argument(\n        \"--code-theme\",\n        default=\"default\",\n        help=(\n            \"Set the markdown code theme (default: default, other options include monokai,\"\n            \" solarized-dark, solarized-light, or a Pygments builtin style,\"\n            \" see https://pygments.org/styles for available themes)\"\n        ),\n    )\n    group.add_argument(\n        \"--show-diffs\",\n        action=\"store_true\",\n        help=\"Show diffs when committing changes (default: False)\",\n        default=False,\n    )\n\n    ##########\n    group = parser.add_argument_group(\"Git settings\")\n    group.add_argument(\n        \"--git\",\n        action=argparse.BooleanOptionalAction,\n        default=True,\n        help=\"Enable/disable looking for a git repo (default: True)\",\n    )\n    group.add_argument(\n        \"--gitignore\",\n        action=argparse.BooleanOptionalAction,\n        default=True,\n        help=\"Enable/disable adding .aider* to .gitignore (default: True)\",\n    )\n    default_aiderignore_file = (\n        os.path.join(git_root, \".aiderignore\") if git_root else \".aiderignore\"\n    )\n    group.add_argument(\n        \"--aiderignore\",\n        metavar=\"AIDERIGNORE\",\n        type=lambda path_str: resolve_aiderignore_path(path_str, git_root),\n        default=default_aiderignore_file,\n        help=\"Specify the aider ignore file (default: .aiderignore in git root)\",\n    )\n    group.add_argument(\n        \"--auto-commits\",\n        action=argparse.BooleanOptionalAction,\n        default=True,\n        help=\"Enable/disable auto commit of LLM changes (default: True)\",\n    )\n    group.add_argument(\n        \"--dirty-commits\",\n        action=argparse.BooleanOptionalAction,\n        default=True,\n        help=\"Enable/disable commits when repo is found dirty (default: True)\",\n    )\n    group.add_argument(\n        \"--dry-run\",\n        action=argparse.BooleanOptionalAction,\n        default=False,\n        help=\"Perform a dry run without modifying files (default: False)\",\n    )\n    group.add_argument(\n        \"--commit\",\n        action=\"store_true\",\n        help=\"Commit all pending changes with a suitable commit message, then exit\",\n        default=False,\n    )\n\n    ##########\n    group = parser.add_argument_group(\"Fixing and committing\")\n    group.add_argument(\n        \"--lint\",\n        action=\"store_true\",\n        help=\"Run the linter on all dirty files, fix problems and commit\",\n        default=False,\n    )\n    group.add_argument(\n        \"--test\",\n        action=\"store_true\",\n        help=\"Run tests, fix problems found and then exit\",\n        default=False,\n    )\n\n    ##########\n    group = parser.add_argument_group(\"Other settings\")\n    group.add_argument(\n        \"--file\",\n        action=\"append\",\n        metavar=\"FILE\",\n        help=\"specify a file to edit (can be used multiple times)\",\n    )\n    group.add_argument(\n        \"--read\",\n        action=\"append\",\n        metavar=\"FILE\",\n        help=\"specify a read-only file (can be used multiple times)\",\n    )\n    group.add_argument(\n        \"--vim\",\n        action=\"store_true\",\n        help=\"Use VI editing mode in the terminal (default: False)\",\n        default=False,\n    )\n    group.add_argument(\n        \"--chat-language\",\n        metavar=\"CHAT_LANGUAGE\",\n        default=None,\n        help=\"Specify the language to use in the chat (default: None, uses system settings)\",\n    )\n    group.add_argument(\n        \"--yes-always\",\n        action=\"store_true\",\n        help=\"Always say yes to every confirmation\",\n        default=None,\n    )\n    group.add_argument(\n        \"-v\",\n        \"--verbose\",\n        action=\"store_true\",\n        help=\"Enable verbose output\",\n        default=False,\n    )\n    group.add_argument(\n        \"--load\",\n        metavar=\"LOAD_FILE\",\n        help=\"Load and execute commands from a file on launch\",\n    )\n    group.add_argument(\n        \"--just-check-update\",\n        action=\"store_true\",\n        help=\"Check for updates and return status in the exit code\",\n        default=False,\n    )\n    group.add_argument(\n        \"--install-main-branch\",\n        action=\"store_true\",\n        help=\"Install the latest version from the main branch\",\n        default=False,\n    )\n    group.add_argument(\n        \"--upgrade\",\n        action=\"store_true\",\n        help=\"Upgrade aider to the latest version from PyPI\",\n        default=False,\n    )\n    group.add_argument(\n        \"--version\",\n        action=\"version\",\n        version=f\"%(prog)s {__version__}\",\n        help=\"Show the version number and exit\",\n    )\n\n    ##########\n    group = parser.add_argument_group(\"Voice settings\")\n    group.add_argument(\n        \"--voice-format\",\n        metavar=\"VOICE_FORMAT\",\n        default=\"wav\",\n        choices=[\"wav\", \"mp3\", \"webm\"],\n        help=\"Audio format for voice recording (default: wav). webm and mp3 require ffmpeg\",\n    )\n    group.add_argument(\n        \"--voice-language\",\n        metavar=\"VOICE_LANGUAGE\",\n        default=\"en\",\n        help=\"Specify the language for voice using ISO 639-1 code (default: auto)\",\n    )\n    group.add_argument(\n        \"--voice-input-device\",\n        metavar=\"VOICE_INPUT_DEVICE\",\n        default=None,\n        help=\"Specify the input device name for voice recording\",\n    )\n\n    return parser",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.27334863555215505,
      "diff_lines": 713,
      "added_lines": 71,
      "removed_lines": 498,
      "diff_preview": "--- expected+++ predicted@@ -1,10 +1,7 @@-#!/usr/bin/env python\n-\n import argparse\n import os\n import sys\n from pathlib import Path\n-\n import configargparse\n \n from aider import __version__\n@@ -14,9 +11,7 @@     YamlHelpFormatter,\n )\n from aider.deprecated import add_deprecated_model_args\n-\n from .dump import dump  # noqa: F401\n-\n \n def resolve_aiderignore_path(path_str, git_root=None):\n     path = Path(path_str)\n@@ -26,17 +21,14 @@         return str(Path(git_root) / path)\n     return str(path)\n \n-\n def default_env_file(git_root):\n     return os.path.join(git_root, \".env\") if git_root else \".env\"\n-\n \n def get_parser(default_config_files, git_root):\n     parser = configargparse.ArgumentParser(\n         description=\"aider is AI pair programming in your terminal\",\n         add_config_file_help=True,\n         default_config_files=default_config_files,\n-        config_file_parser_class=configargparse.YAMLConfigFileParser,\n         auto_env_var_prefix=\"AIDER_\",\n     )\n     group = parser.add_argument_group(\"Main model\")\n@@ -49,6 +41,9 @@         default=None,\n         help=\"Specify the model to use for the main chat\",\n     )\n+    \n+    # Add deprecated model shortcut arguments\n+    add_deprecated_model_args(parser, group)\n \n     ##########\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 83.31801795959473,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 61543,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_aider_coders_editblock_coder.py",
    "exact_match": false,
    "similarity": 0.002847264592231035,
    "diff_lines": 666,
    "predicted_output": "foo.txt\n<<<<<<< HEAD\nTwo\n=======\nTooooo\n>>>>>>> REPLACE",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.002847264592231035,
      "diff_lines": 666,
      "added_lines": 6,
      "removed_lines": 657,
      "diff_preview": "--- expected+++ predicted@@ -1,657 +1,6 @@-import difflib\n-import math\n-import re\n-import sys\n-from difflib import SequenceMatcher\n-from pathlib import Path\n-\n-from aider import utils\n-\n-from ..dump import dump  # noqa: F401\n-from .base_coder import Coder\n-from .editblock_prompts import EditBlockPrompts\n-\n-\n-class EditBlockCoder(Coder):\n-    \"\"\"A coder that uses search/replace blocks for code modifications.\"\"\"\n-\n-    edit_format = \"diff\"\n-    gpt_prompts = EditBlockPrompts()\n-\n-    def get_edits(self):\n-        content = self.partial_response_content\n-\n-        # might raise ValueError for malformed ORIG/UPD blocks\n-        edits = list(\n-            find_original_update_blocks(\n-                content,\n-                self.fence,\n-                self.get_inchat_relative_files(),\n-            )\n-        )\n-\n-        self.shell_commands += [edit[1] for edit in edits if edit[0] is None]\n-        edits = [edit for edit in edits if edit[0] is not None]\n-\n-        return edits\n-\n-    def apply_edits_dry_run(self, edits):\n-        return self.apply_edits(edits, dry_run=True)\n-\n-    def apply_edits(self, edits, dry_run=False):\n-        failed = []\n-        passed = []\n-        updated_edits = []\n-\n-        for edit in edits:\n-            path, original, updated = edit\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 90.90202307701111,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 56338,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_aider_coders_patch_coder.py",
    "exact_match": false,
    "similarity": 0.9697905477980666,
    "diff_lines": 282,
    "predicted_output": "import pathlib\nimport re\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Dict, List, Optional, Tuple\n\nfrom ..dump import dump  # noqa: F401\nfrom .base_coder import Coder\nfrom .patch_prompts import PatchPrompts\n\n# --------------------------------------------------------------------------- #\n#  Domain objects & Exceptions (Adapted from apply_patch.py)\n# --------------------------------------------------------------------------- #\nclass DiffError(ValueError):\n    \"\"\"Any problem detected while parsing or applying a patch.\"\"\"\n\nclass ActionType(str, Enum):\n    ADD = \"Add\"\n    DELETE = \"Delete\"\n    UPDATE = \"Update\"\n\n@dataclass\nclass Chunk:\n    orig_index: int = -1  # Line number in the *original* file block where the change starts\n    del_lines: List[str] = field(default_factory=list)\n    ins_lines: List[str] = field(default_factory=list)\n\n@dataclass\nclass PatchAction:\n    type: ActionType\n    path: str\n    new_content: Optional[str] = None  # For Add\n    chunks: List[Chunk] = field(default_factory=list)  # For Update\n    move_path: Optional[str] = None  # For Update\n\n@dataclass\nclass Patch:\n    actions: Dict[str, PatchAction] = field(default_factory=dict)\n    fuzz: int = 0  # Track fuzziness used during parsing\n\n# --------------------------------------------------------------------------- #\n#  Helper functions (Adapted from apply_patch.py)\n# --------------------------------------------------------------------------- #\ndef _norm(line: str) -> str:\n    \"\"\"Strip CR so comparisons work for both LF and CRLF input.\"\"\"\n    return line.rstrip(\"\\r\")\n\ndef find_context_core(lines: List[str], context: List[str], start: int) -> Tuple[int, int]:\n    \"\"\"Finds context block, returns start index and fuzz level.\"\"\"\n    if not context:\n        return start, 0\n\n    # Exact match\n    for i in range(start, len(lines) - len(context) + 1):\n        if lines[i : i + len(context)] == context:\n            return i, 0\n    # Rstrip match\n    norm_context = [s.rstrip() for s in context]\n    for i in range(start, len(lines) - len(context) + 1):\n        if [s.rstrip() for s in lines[i : i + len(context)]] == norm_context:\n            return i, 1  # Fuzz level 1\n    # Strip match\n    norm_context_strip = [s.strip() for s in context]\n    for i in range(start, len(lines) - len(context) + 1):\n        if [s.strip() for s in lines[i : i + len(context)]] == norm_context_strip:\n            return i, 100  # Fuzz level 100\n    return -1, 0\n\ndef find_context(lines: List[str], context: List[str], start: int, eof: bool) -> Tuple[int, int]:\n    \"\"\"Finds context, handling EOF marker.\"\"\"\n    if eof:\n        # If EOF marker, first try matching at the very end\n        if len(lines) >= len(context):\n            new_index, fuzz = find_context_core(lines, context, len(lines) - len(context))\n            if new_index != -1:\n                return new_index, fuzz\n        # If not found at end, search from `start` as fallback\n        new_index, fuzz = find_context_core(lines, context, start)\n        return new_index, fuzz + 10_000  # Add large fuzz penalty if EOF wasn't at end\n    # Normal case: search from `start`\n    return find_context_core(lines, context, start)\n\ndef peek_next_section(lines: List[str], index: int) -> Tuple[List[str], List[Chunk], int, bool]:\n    \"\"\"\n    Parses one section (context, -, + lines) of an Update block.\n    Returns: (context_lines, chunks_in_section, next_index, is_eof)\n    \"\"\"\n    context_lines: List[str] = []\n    del_lines: List[str] = []\n    ins_lines: List[str] = []\n    chunks: List[Chunk] = []\n    mode = \"keep\"  # Start by expecting context lines\n    start_index = index\n\n    while index < len(lines):\n        line = lines[index]\n        norm_line = _norm(line)\n\n        # Check for section terminators\n        if norm_line.startswith(\n            (\n                \"@@\",\n                \"*** End Patch\",\n                \"*** Update File:\",\n                \"*** Delete File:\",\n                \"*** Add File:\",\n                \"*** End of File\",  # Special terminator\n            )\n        ):\n            break\n        if norm_line == \"***\":  # Legacy/alternative terminator? Handle just in case.\n            break\n        if norm_line.startswith(\"***\"):  # Invalid line\n            raise DiffError(f\"Invalid patch line found in update section: {line}\")\n\n        index += 1\n        last_mode = mode\n\n        # Determine line type and strip prefix\n        if line.startswith(\"+\"):\n            mode = \"add\"\n            line_content = line[1:]\n        elif line.startswith(\"-\"):\n            mode = \"delete\"\n            line_content = line[1:]\n        elif line.startswith(\" \"):\n            mode = \"keep\"\n            line_content = line[1:]\n        elif line.strip() == \"\":  # Treat blank lines in patch as context ' '\n            mode = \"keep\"\n            line_content = \"\"  # Keep it as a blank line\n        else:\n            # Assume lines without prefix are context if format is loose,\n            # but strict format requires ' '. Raise error for strictness.\n            raise DiffError(f\"Invalid line prefix in update section: {line}\")\n\n        # If mode changes from add/delete back to keep, finalize the previous chunk\n        if mode == \"keep\" and last_mode != \"keep\":\n            if del_lines or ins_lines:\n                chunks.append(\n                    Chunk(\n                        # orig_index is relative to the start of the *context* block found\n                        orig_index=len(context_lines) - len(del_lines),\n                        del_lines=del_lines,\n                        ins_lines=ins_lines,\n                    )\n                )\n            del_lines, ins_lines = [], []\n\n        # Collect lines based on mode\n        if mode == \"delete\":\n            del_lines.append(line_content)\n            context_lines.append(line_content)  # Deleted lines are part of the original context\n        elif mode == \"add\":\n            ins_lines.append(line_content)\n        elif mode == \"keep\":\n            context_lines.append(line_content)\n\n    # Finalize any pending chunk at the end of the section\n    if del_lines or ins_lines:\n        chunks.append(\n            Chunk(\n                orig_index=len(context_lines) - len(del_lines),\n                del_lines=del_lines,\n                ins_lines=ins_lines,\n            )\n        )\n\n    # Check for EOF marker\n    is_eof = False\n    if index < len(lines) and _norm(lines[index]) == \"*** End of File\":\n        index += 1\n        is_eof = True\n\n    if index == start_index and not is_eof:  # Should not happen if patch is well-formed\n        raise DiffError(\"Empty patch section found.\")\n\n    return context_lines, chunks, index, is_eof\n\ndef identify_files_needed(text: str) -> List[str]:\n    \"\"\"Extracts file paths from Update and Delete actions.\"\"\"\n    lines = text.splitlines()\n    paths = set()\n    for line in lines:\n        norm_line = _norm(line)\n        if norm_line.startswith(\"*** Update File: \"):\n            paths.add(norm_line[len(\"*** Update File: \") :].strip())\n        elif norm_line.startswith(\"*** Delete File: \"):\n            paths.add(norm_line[len(\"*** Delete File: \") :].strip())\n    return list(paths)\n\n# --------------------------------------------------------------------------- #\n#  PatchCoder Class Implementation\n# --------------------------------------------------------------------------- #\nclass PatchCoder(Coder):\n    \"\"\"\n    A coder that uses a custom patch format for code modifications,\n    inspired by the format described in tmp.gpt41edits.txt.\n    Applies patches using logic adapted from the reference apply_patch.py script.\n    \"\"\"\n\n    edit_format = \"patch\"\n    gpt_prompts = PatchPrompts()\n\n    def get_edits(self) -> List[Tuple[str, PatchAction]]:\n        \"\"\"\n        Parses the LLM response content (containing the patch) into a list of\n        tuples, where each tuple contains the file path and the PatchAction object.\n        \"\"\"\n        content = self.partial_response_content\n        if not content or not content.strip():\n            return []\n\n        # Check for patch sentinels\n        lines = content.splitlines()\n        if (\n            len(lines) < 2\n            or not _norm(lines[0]).startswith(\"*** Begin Patch\")\n        ):\n            # Tolerate missing sentinels if content looks like a patch action\n            is_patch_like = any(_norm(line).startswith(\n                (\"@@\", \"*** Update File:\", \"*** Add File:\", \"*** Delete File:\")\n            ) for line in lines)\n            if not is_patch_like:\n                # If it doesn't even look like a patch, return empty\n                self.io.tool_warning(\"Response does not appear to be in patch format.\")\n                return []\n            # If it looks like a patch but lacks sentinels, try parsing anyway but warn.\n            self.io.tool_warning(\"Patch format warning: Missing '*** Begin Patch'/'*** End Patch' sentinels.\")\n            start_index = 0\n        else:\n            start_index = 1  # Skip \"*** Begin Patch\"\n\n        # Identify files needed for context lookups during parsing\n        needed_paths = identify_files_needed(content)\n        current_files: Dict[str, str] = {}\n        for rel_path in needed_paths:\n            abs_path = self.abs_root_path(rel_path)\n            try:\n                # Use io.read_text to handle potential errors/encodings\n                file_content = self.io.read_text(abs_path)\n                if file_content is None:\n                    raise DiffError(f\"File referenced in patch not found or could not be read: {rel_path}\")\n                current_files[rel_path] = file_content\n            except FileNotFoundError:\n                raise DiffError(f\"File referenced in patch not found: {rel_path}\")\n            except IOError as e:\n                raise DiffError(f\"Error reading file {rel_path}: {e}\")\n\n        try:\n            # Parse the patch text using adapted logic\n            patch_obj = self._parse_patch_text(lines, start_index, current_files)\n            # Convert Patch object actions dict to a list of tuples (path, action)\n            # for compatibility with the base Coder's prepare_to_edit method.\n            results = []\n            for path, action in patch_obj.actions.items():\n                results.append((path, action))\n            return results\n        except DiffError as e:\n            # Raise as ValueError for consistency with other coders' error handling\n            raise ValueError(f\"Error parsing patch content: {e}\")\n        except Exception as e:\n            # Catch unexpected errors during parsing\n            raise ValueError(f\"Unexpected error parsing patch: {e}\")\n\n    def _parse_patch_text(\n        self, lines: List[str], start_index: int, current_files: Dict[str, str]\n    ) -> Patch:\n        \"\"\"\n        Parses patch content lines into a Patch object.\n        Adapted from the Parser class in apply_patch.py.\n        \"\"\"\n        patch = Patch()\n        index = start_index\n        fuzz_accumulator = 0\n\n        while index < len(lines):\n            line = lines[index]\n            norm_line = _norm(line)\n\n            if norm_line == \"*** End Patch\":\n                index += 1\n                break  # Successfully reached end\n\n            # ---------- UPDATE ---------- #\n            if norm_line.startswith(\"*** Update File: \"):\n                path = norm_line[len(\"*** Update File: \") :].strip()\n                index += 1\n                if not path:\n                    raise DiffError(\"Update File action missing path.\")\n\n                # Optional move target\n                move_to = None\n                if index < len(lines) and _norm(lines[index]).startswith(\"*** Move to: \"):\n                    move_to = _norm(lines[index])[len(\"*** Move to: \") :].strip()\n                    index += 1\n                    if not move_to:\n                        raise DiffError(\"Move to action missing path.\")\n\n                if path not in current_files:\n                    raise DiffError(f\"Update File Error - missing file content for: {path}\")\n\n                file_content = current_files[path]\n                existing_action = patch.actions.get(path)\n                if existing_action is not None:\n                    # Merge additional UPDATE block into the existing one\n                    if existing_action.type != ActionType.UPDATE:\n                        raise DiffError(f\"Conflicting actions for file: {path}\")\n\n                    new_action, index, fuzz = self._parse_update_file_sections(\n                        lines, index, file_content\n                    )\n                    existing_action.chunks.extend(new_action.chunks)\n\n                    if move_to:\n                        if existing_action.move_path and existing_action.move_path != move_to:\n                            raise DiffError(f\"Conflicting move targets for file: {path}\")\n                        existing_action.move_path = move_to\n                    fuzz_accumulator += fuzz\n                else:\n                    # First UPDATE block for this file\n                    action, index, fuzz = self._parse_update_file_sections(\n                        lines, index, file_content\n                    )\n                    action.path = path\n                    action.move_path = move_to\n                    patch.actions[path] = action\n                    fuzz_accumulator += fuzz\n                continue\n\n            # ---------- DELETE ---------- #\n            elif norm_line.startswith(\"*** Delete File: \"):\n                path = norm_line[len(\"*** Delete File: \") :].strip()\n                index += 1\n                if not path:\n                    raise DiffError(\"Delete File action missing path.\")\n                existing_action = patch.actions.get(path)\n                if existing_action:\n                    if existing_action.type == ActionType.DELETE:\n                        # Duplicate delete \u2013 ignore the extra block\n                        self.io.tool_warning(f\"Duplicate delete action for file: {path} ignored.\")\n                        continue\n                    else:\n                        raise DiffError(f\"Conflicting actions for file: {path}\")\n                if path not in current_files:\n                    raise DiffError(f\"Delete File Error - file not found: {path}\")  # Check against known files\n\n                patch.actions[path] = PatchAction(type=ActionType.DELETE, path=path)\n                continue\n\n            # ---------- ADD ---------- #\n            elif norm_line.startswith(\"*** Add File: \"):\n                path = norm_line[len(\"*** Add File: \") :].strip()\n                index += 1\n                if not path:\n                    raise DiffError(\"Add File action missing path.\")\n                if path in patch.actions:\n                    raise DiffError(f\"Duplicate action for file: {path}\")\n                # Check if file exists in the context provided (should not for Add).\n                # Note: We only have needed files, a full check requires FS access.\n                # if path in current_files:\n                #     raise DiffError(f\"Add File Error - file already exists: {path}\")\n\n                action, index = self._parse_add_file_content(lines, index)\n                action.path = path  # Ensure path is set\n                patch.actions[path] = action\n                continue\n\n            # If we are here, the line is unexpected\n            # Allow blank lines between actions\n            if not norm_line.strip():\n                index += 1\n                continue\n\n            raise DiffError(f\"Unknown or misplaced line while parsing patch: {line}\")\n\n        # Check if we consumed the whole input or stopped early\n        # Tolerate missing \"*** End Patch\" if we processed actions\n        # if index < len(lines) and _norm(lines[index-1]) != \"*** End Patch\":\n        #    raise DiffError(\"Patch parsing finished unexpectedly before end of input.\")\n\n        patch.fuzz = fuzz_accumulator\n        return patch\n\n    def _parse_update_file_sections(\n        self, lines: List[str], index: int, file_content: str\n    ) -> Tuple[PatchAction, int, int]:\n        \"\"\"Parses all sections (@@, context, -, +) for a single Update File action.\"\"\"\n        action = PatchAction(type=ActionType.UPDATE, path=\"\")  # Path set by caller\n        orig_lines = file_content.splitlines()  # Use splitlines for consistency\n        current_file_index = 0  # Track position in original file content\n        total_fuzz = 0\n\n        while index < len(lines):\n            norm_line = _norm(lines[index])\n            # Check for terminators for *this* file update\n            if norm_line.startswith(\n                (\n                    \"*** End Patch\",\n                    \"*** Update File:\",\n                    \"*** Delete File:\",\n                    \"*** Add File:\",\n                )\n            ):\n                break  # End of this file's update section\n\n            # Handle @@ scope lines (optional)\n            scope_lines = []\n            while index < len(lines) and _norm(lines[index]).startswith(\"@@\"):\n                scope_line_content = lines[index][len(\"@@\") :].strip()\n                if scope_line_content:  # Ignore empty @@ lines?\n                    scope_lines.append(scope_line_content)\n                index += 1\n\n            # Find the scope in the original file if specified\n            if scope_lines:\n                # Simple scope finding: search from current position\n                # A more robust finder could handle nested scopes like the reference @@ @@\n                found_scope = False\n                temp_index = current_file_index\n                while temp_index < len(orig_lines):\n                    # Check if all scope lines match sequentially from temp_index\n                    match = True\n                    for i, scope in enumerate(scope_lines):\n                        if temp_index + i >= len(orig_lines) or _norm(orig_lines[temp_index + i]).strip() != scope:\n                            match = False\n                            break\n                    if match:\n                        current_file_index = temp_index + len(scope_lines)\n                        found_scope = True\n                        break\n                    temp_index += 1\n\n                if not found_scope:\n                    # Try fuzzy scope matching (strip whitespace)\n                    temp_index = current_file_index\n                    while temp_index < len(orig_lines):\n                        match = True\n                        for i, scope in enumerate(scope_lines):\n                            if temp_index + i >= len(orig_lines) or _norm(orig_lines[temp_index + i]).strip() != scope.strip():\n                                match = False\n                                break\n                        if match:\n                            current_file_index = temp_index + len(scope_lines)\n                            found_scope = True\n                            total_fuzz += 1  # Add fuzz for scope match difference\n                            break\n                        temp_index += 1\n\n                if not found_scope:\n                    scope_txt = \"\\n\".join(scope_lines)\n                    raise DiffError(f\"Could not find scope context:\\n{scope_txt}\")\n\n            # Peek and parse the next context/change section\n            context_block, chunks_in_section, next_index, is_eof = peek_next_section(lines, index)\n\n            # Find where this context block appears in the original file\n            found_index, fuzz = find_context(orig_lines, context_block, current_file_index, is_eof)\n            total_fuzz += fuzz\n\n            if found_index == -1:\n                ctx_txt = \"\\n\".join(context_block)\n                marker = \"*** End of File\" if is_eof else \"\"\n                raise DiffError(\n                    f\"Could not find patch context {marker} starting near line\"\n                    f\" {current_file_index}:\\n{ctx_txt}\"\n                )\n\n            # Adjust chunk original indices to be absolute within the file\n            for chunk in chunks_in_section:\n                # chunk.orig_index from peek is relative to context_block start\n                # We need it relative to the file start\n                chunk.orig_index += found_index\n                action.chunks.append(chunk)\n\n            # Advance file index past the matched context block\n            current_file_index = found_index + len(context_block)\n            # Advance line index past the processed section in the patch\n            index = next_index\n\n        return action, index, total_fuzz\n\n    def _parse_add_file_content(self, lines: List[str], index: int) -> Tuple[PatchAction, int]:\n        \"\"\"Parses the content (+) lines for an Add File action.\"\"\"\n        added_lines: List[str] = []\n        while index < len(lines):\n            line = lines[index]\n            norm_line = _norm(line)\n            # Stop if we hit another action or end marker\n            if norm_line.startswith(\n                (\n                    \"*** End Patch\",\n                    \"*** Update File:\",\n                    \"*** Delete File:\",\n                    \"*** Add File:\",\n                )\n            ):\n                break\n\n            # Expect lines to start with '+'\n            if not line.startswith(\"+\"):\n                # Tolerate blank lines? Or require '+'? Reference implies '+' required.\n                if norm_line.strip() == \"\":\n                    # Treat blank line as adding a blank line\n                    added_lines.append(\"\")\n                else:\n                    raise DiffError(f\"Invalid Add File line (missing '+'): {line}\")\n            else:\n                added_lines.append(line[1:])  # Strip leading '+'\n\n            index += 1\n\n        action = PatchAction(type=ActionType.ADD, path=\"\", new_content=\"\\n\".join(added_lines))\n        return action, index\n\n    def apply_edits(self, edits: List[Tuple[str, PatchAction]]):\n        \"\"\"\n        Applies the parsed PatchActions to the corresponding files.\n        \"\"\"\n        if not edits:\n            return\n\n        # Group edits by original path? Not strictly needed if processed sequentially.\n\n        for _path_tuple_element, action in edits:\n            # action is the PatchAction object\n            # action.path is the canonical path within the action logic\n            full_path = self.abs_root_path(action.path)\n            path_obj = pathlib.Path(full_path)\n\n            try:\n                if action.type == ActionType.ADD:\n                    # Check existence *before* writing\n                    if path_obj.exists():\n                        raise DiffError(f\"ADD Error: File already exists: {action.path}\")\n                    if action.new_content is None:\n                        raise DiffError(f\"ADD change for {action.path} has no content\")\n\n                    self.io.tool_output(f\"Adding {action.path}\")\n                    path_obj.parent.mkdir(parents=True, exist_ok=True)\n                    # Ensure single trailing newline, matching reference behavior\n                    content_to_write = action.new_content\n                    if not content_to_write.endswith(\"\\n\"):\n                        content_to_write += \"\\n\"\n                    self.io.write_text(full_path, content_to_write)\n\n                elif action.type == ActionType.DELETE:\n                    self.io.tool_output(f\"Deleting {action.path}\")\n                    if not path_obj.exists():\n                        self.io.tool_warning(f\"DELETE Warning: File not found, skipping: {action.path}\")\n                    else:\n                        path_obj.unlink()\n\n                elif action.type == ActionType.UPDATE:\n                    if not path_obj.exists():\n                        raise DiffError(f\"UPDATE Error: File does not exist: {action.path}\")\n\n                    current_content = self.io.read_text(full_path)\n                    if current_content is None:\n                        raise DiffError(f\"Could not read file for UPDATE: {action.path}\")\n\n                    # Apply the update logic using the parsed chunks\n                    new_content = self._apply_update(current_content, action, action.path)\n\n                    target_full_path = (\n                        self.abs_root_path(action.move_path) if action.move_path else full_path\n                    )\n                    target_path_obj = pathlib.Path(target_full_path)\n\n                    if action.move_path:\n                        self.io.tool_output(f\"Updating and moving {action.path} to {action.move_path}\")\n                        # Check if target exists before overwriting/moving\n                        if target_path_obj.exists() and full_path != target_full_path:\n                            self.io.tool_warning(f\"UPDATE Warning: Target file for move already exists, overwriting: {action.move_path}\")\n                    else:\n                        self.io.tool_output(f\"Updating {action.path}\")\n\n                    # Ensure parent directory exists for target\n                    target_path_obj.parent.mkdir(parents=True, exist_ok=True)\n                    self.io.write_text(target_full_path, new_content)\n\n                    # Remove original file *after* successful write to new location if moved\n                    if action.move_path and full_path != target_full_path:\n                        path_obj.unlink()\n\n                else:\n                    raise DiffError(f\"Unknown action type encountered: {action.type}\")\n\n            except (DiffError, FileNotFoundError, IOError, OSError) as e:\n                # Raise a ValueError to signal failure, consistent with other coders.\n                raise ValueError(f\"Error applying action '{action.type}' to {action.path}: {e}\")\n            except Exception as e:\n                # Catch unexpected errors during application\n                raise ValueError(f\"Unexpected error applying action '{action.type}' to {action.path}: {e}\")\n\n    def _apply_update(self, text: str, action: PatchAction, path: str) -> str:\n        \"\"\"\n        Applies UPDATE chunks to the given text content.\n        Adapted from _get_updated_file in apply_patch.py.\n        \"\"\"\n        if action.type is not ActionType.UPDATE:\n            # Should not be called otherwise, but check for safety\n            raise DiffError(\"_apply_update called with non-update action\")\n\n        orig_lines = text.splitlines()  # Use splitlines to handle endings consistently\n        dest_lines: List[str] = []\n        current_orig_line_idx = 0  # Tracks index in orig_lines processed so far\n\n        # Sort chunks by their original index to apply them sequentially\n        sorted_chunks = sorted(action.chunks, key=lambda c: c.orig_index)\n\n        for chunk in sorted_chunks:\n            # chunk.orig_index is the absolute line number where the change starts\n            # (where the first deleted line was, or where inserted lines go if no deletes)\n            chunk_start_index = chunk.orig_index\n\n            if chunk_start_index < current_orig_line_idx:\n                # This indicates overlapping chunks or incorrect indices from parsing\n                raise DiffError(\n                    f\"{path}: Overlapping or out-of-order chunk detected.\"\n                    f\" Current index {current_orig_line_idx}, chunk starts at {chunk_start_index}.\"\n                )\n\n            # Add lines from original file between the last chunk and this one\n            dest_lines.extend(orig_lines[current_orig_line_idx:chunk_start_index])\n\n            # Verify that the lines to be deleted actually match the original file content\n            # (The parser should have used find_context, but double-check here)\n            num_del = len(chunk.del_lines)\n            actual_deleted_lines = orig_lines[chunk_start_index:chunk_start_index + num_del]\n\n            # Use the same normalization as find_context_core for comparison robustness\n            norm_chunk_del = [_norm(s).strip() for s in chunk.del_lines]\n            norm_actual_del = [_norm(s).strip() for s in actual_deleted_lines]\n\n            if norm_chunk_del != norm_actual_del:\n                # This indicates the context matching failed or the file changed since parsing\n                # Provide detailed error message\n                expected_str = \"\\n\".join(f\"- {s}\" for s in chunk.del_lines)\n                actual_str = \"\\n\".join(f\"  {s}\" for s in actual_deleted_lines)\n                raise DiffError(\n                    f\"{path}: Mismatch applying patch near line {chunk_start_index + 1}.\\n\"\n                    f\"Expected lines to remove:\\n{expected_str}\\n\"\n                    f\"Found lines in file:\\n{actual_str}\"\n                )\n\n            # Add the inserted lines from the chunk\n            dest_lines.extend(chunk.ins_lines)\n\n            # Advance the original line index past the lines processed (deleted lines)\n            current_orig_line_idx = chunk_start_index + num_del\n\n        # Add any remaining lines from the original file after the last chunk\n        dest_lines.extend(orig_lines[current_orig_line_idx:])\n\n        # Join lines and ensure a single trailing newline\n        result = \"\\n\".join(dest_lines)\n        if result or orig_lines:  # Add newline unless result is empty and original was empty\n            result += \"\\n\"\n        return result",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9697905477980666,
      "diff_lines": 282,
      "added_lines": 20,
      "removed_lines": 66,
      "diff_preview": "--- expected+++ predicted@@ -1,11 +1,12 @@ import pathlib\n+import re\n from dataclasses import dataclass, field\n from enum import Enum\n from typing import Dict, List, Optional, Tuple\n \n+from ..dump import dump  # noqa: F401\n from .base_coder import Coder\n from .patch_prompts import PatchPrompts\n-\n \n # --------------------------------------------------------------------------- #\n #  Domain objects & Exceptions (Adapted from apply_patch.py)\n@@ -13,12 +14,10 @@ class DiffError(ValueError):\n     \"\"\"Any problem detected while parsing or applying a patch.\"\"\"\n \n-\n class ActionType(str, Enum):\n     ADD = \"Add\"\n     DELETE = \"Delete\"\n     UPDATE = \"Update\"\n-\n \n @dataclass\n class Chunk:\n@@ -26,27 +25,18 @@     del_lines: List[str] = field(default_factory=list)\n     ins_lines: List[str] = field(default_factory=list)\n \n-\n @dataclass\n class PatchAction:\n     type: ActionType\n     path: str\n-    # For ADD:\n-    new_content: Optional[str] = None\n-    # For UPDATE:\n-    chunks: List[Chunk] = field(default_factory=list)\n-    move_path: Optional[str] = None\n-\n-\n-# Type alias for the return type of get_edits\n-EditResult = Tuple[str, PatchAction]\n-\n+    new_content: Optional[str] = None  # For Add\n+    chunks: List[Chunk] = field(default_factory=list)  # For Update\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 145.29382467269897,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 22441,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_aider_coders_wholefile_coder.py",
    "exact_match": false,
    "similarity": 0.6453032332121675,
    "diff_lines": 93,
    "predicted_output": "from pathlib import Path\n\nfrom aider import diffs\nfrom ..dump import dump  # noqa: F401\nfrom .base_coder import Coder\nfrom .wholefile_prompts import WholeFilePrompts\n\n\nclass WholeFileCoder(Coder):\n    \"\"\"A coder that operates on entire files for code modifications.\"\"\"\n    edit_format = \"whole\"\n    gpt_prompts = WholeFilePrompts()\n\n    def update_cur_messages(self, edited):\n        if edited:\n            self.cur_messages += [\n                dict(role=\"assistant\", content=self.gpt_prompts.redacted_edit_message)\n            ]\n        else:\n            self.cur_messages += [dict(role=\"assistant\", content=self.partial_response_content)]\n\n    def render_incremental_response(self, final):\n        try:\n            return self.get_edits(mode=\"diff\")\n        except ValueError:\n            return self.get_multi_response_content_in_progress()\n\n    def get_edits(self, mode=\"update\"):\n        content = self.get_multi_response_content_in_progress()\n\n        chat_files = self.get_inchat_relative_files()\n\n        output = []\n        lines = content.splitlines(keepends=True)\n\n        edits = []\n        saw_fname = None\n        fname = None\n        fname_source = None\n        new_lines = []\n        for i, line in enumerate(lines):\n            if line.startswith(self.fence[0]) or line.startswith(self.fence[1]):\n                if fname is not None:\n                    # ending an existing block\n                    saw_fname = None\n\n                    full_path = self.abs_root_path(fname)\n\n                    if mode == \"diff\":\n                        output += self.do_live_diff(full_path, new_lines, True)\n                    else:\n                        edits.append((fname, fname_source, new_lines))\n\n                    fname = None\n                    new_lines = []\n                    continue\n\n                # fname==None ... starting a new block\n                if i > 0:\n                    fname_source = \"block\"\n                    fname = lines[i - 1].strip()\n                    fname = fname.strip(\"*\")  # handle **filename.py**\n                    fname = fname.rstrip(\":\")\n                    fname = fname.strip(\"`\")\n                    fname = fname.lstrip(\"#\")\n                    fname = fname.strip()\n\n                    # Issue #1232\n                    if len(fname) > 250:\n                        fname = \"\"\n\n                    # Did gpt prepend a bogus dir? It especially likes to\n                    # include the path/to prefix from the one-shot example in\n                    # the prompt.\n                    if fname and fname not in chat_files and Path(fname).name in chat_files:\n                        fname = Path(fname).name\n                if not fname:  # blank line? or ``` was on first line i==0\n                    if saw_fname:\n                        fname = saw_fname\n                    elif len(chat_files) == 1:\n                        fname = chat_files[0]\n                    else:\n                        # TODO: sense which file it is by diff size\n                        raise ValueError(\n                            \"No filename provided before {} in file listing\".format(self.fence[0])\n                        )\n\n            elif fname is not None:\n                new_lines.append(line)\n            else:\n                output.append(line)\n\n        if mode == \"diff\":\n            if fname:\n                # ending an existing block\n                full_path = self.abs_root_path(fname)\n\n                if full_path.exists():\n                    orig_lines = self.io.read_text(full_path).splitlines(keepends=True)\n\n                    show_diff = diffs.diff_partial_update(\n                        orig_lines,\n                        new_lines,\n                    ).splitlines()\n                    output += show_diff\n                else:\n                    output += [\"```\"] + new_lines + [\"```\"]\n\n            return \"\\n\".join(output)\n\n        if fname:\n            edits.append((fname, fname_source, new_lines))\n\n        seen = set()\n        refined_edits = []\n        # process from most reliable filename, to least reliable\n        for source in (\"block\", \"saw\", \"chat\"):\n            for fname, fname_source, new_lines in edits:\n                if fname_source != source:\n                    continue\n                # if a higher priority source already edited the file, skip\n                if fname in seen:\n                    continue\n\n                seen.add(fname)\n                refined_edits.append((fname, fname_source, new_lines))\n\n        return refined_edits\n\n    def apply_edits(self, edits):\n        for path, fname_source, new_lines in edits:\n            full_path = self.abs_root_path(path)\n            new_lines = \"\".join(new_lines)\n            self.io.write_text(full_path, new_lines)\n\n    def do_live_diff(self, full_path, new_lines, final):\n        if Path(full_path).exists():\n            orig_lines = self.io.read_text(full_path)\n            if orig_lines is not None:\n                orig_lines = orig_lines.splitlines(keepends=True)\n\n                show_diff = diffs.diff_partial_update(\n                    orig_lines,\n                    new_lines,\n                    final=final,\n                ).splitlines()\n                return show_diff\n\n        output = [\"```\"] + new_lines + [\"```\"]\n        return output",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.6453032332121675,
      "diff_lines": 93,
      "added_lines": 23,
      "removed_lines": 17,
      "diff_preview": "--- expected+++ predicted@@ -1,7 +1,6 @@ from pathlib import Path\n \n from aider import diffs\n-\n from ..dump import dump  # noqa: F401\n from .base_coder import Coder\n from .wholefile_prompts import WholeFilePrompts\n@@ -9,9 +8,16 @@ \n class WholeFileCoder(Coder):\n     \"\"\"A coder that operates on entire files for code modifications.\"\"\"\n-\n     edit_format = \"whole\"\n     gpt_prompts = WholeFilePrompts()\n+\n+    def update_cur_messages(self, edited):\n+        if edited:\n+            self.cur_messages += [\n+                dict(role=\"assistant\", content=self.gpt_prompts.redacted_edit_message)\n+            ]\n+        else:\n+            self.cur_messages += [dict(role=\"assistant\", content=self.partial_response_content)]\n \n     def render_incremental_response(self, final):\n         try:\n@@ -28,7 +34,6 @@         lines = content.splitlines(keepends=True)\n \n         edits = []\n-\n         saw_fname = None\n         fname = None\n         fname_source = None\n@@ -47,7 +52,6 @@                         edits.append((fname, fname_source, new_lines))\n \n                     fname = None\n-                    fname_source = None\n                     new_lines = []\n                     continue\n \n@@ -73,33 +77,35 @@                 if not fname:  # blank line? or ``` was on first line i==0\n                     if saw_fname:\n                         fname = saw_fname\n-                        fname_source = \"saw\"\n                     elif len(chat_files) == 1:\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 27.15246605873108,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 20299,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_aider_exceptions.py",
    "exact_match": false,
    "similarity": 0.9402861860209136,
    "diff_lines": 90,
    "predicted_output": "from dataclasses import dataclass\n\n\n@dataclass\nclass ExInfo:\n    name: str\n    retry: bool\n    description: str\n\n\nEXCEPTIONS = [\n    ExInfo(\"APIConnectionError\", True, None),\n    ExInfo(\"APIError\", True, None),\n    ExInfo(\"APIResponseValidationError\", True, None),\n    ExInfo(\"AuthenticationError\", False, \"The API provider is not able to authenticate you. Check your API key.\"),\n    ExInfo(\"AzureOpenAIError\", True, None),\n    ExInfo(\"BadRequestError\", False, None),\n    ExInfo(\"BudgetExceededError\", True, None),\n    ExInfo(\"ContentPolicyViolationError\", True, \"The API provider has refused the request due to a safety policy about the content.\"),\n    ExInfo(\"ContextWindowExceededError\", False, None),  # special case handled in base_coder\n    ExInfo(\"InternalServerError\", True, \"The API provider's servers are down or overloaded.\"),\n    ExInfo(\"InvalidRequestError\", True, None),\n    ExInfo(\"JSONSchemaValidationError\", True, None),\n    ExInfo(\"NotFoundError\", False, None),\n    ExInfo(\"OpenAIError\", True, None),\n    ExInfo(\"RateLimitError\", True, \"The API provider has rate limited you. Try again later or check your quotas.\"),\n    ExInfo(\"RouterRateLimitError\", True, None),\n    ExInfo(\"ServiceUnavailableError\", True, \"The API provider's servers are down or overloaded.\"),\n    ExInfo(\"UnprocessableEntityError\", True, None),\n    ExInfo(\"UnsupportedParamsError\", True, None),\n    ExInfo(\"Timeout\", True, \"The API provider timed out without returning a response. They may be down or overloaded.\"),\n]\n\n\nclass LiteLLMExceptions:\n    exceptions = dict()\n    exception_info = {exi.name: exi for exi in EXCEPTIONS}\n\n    def __init__(self):\n        self._load()\n\n    def _load(self, strict=False):\n        import litellm\n\n        for var in dir(litellm):\n            if var.endswith(\"Error\"):\n                if var not in self.exception_info:\n                    raise ValueError(f\"{var} is in litellm but not in aider's exceptions list\")\n\n                ex = getattr(litellm, var)\n                self.exceptions[ex] = self.exception_info[var]\n\n    def exceptions_tuple(self):\n        return tuple(self.exceptions)\n\n    def get_ex_info(self, ex):\n        \"\"\"Return the ExInfo for a given exception instance\"\"\"\n        import litellm\n\n        if ex.__class__ is litellm.APIConnectionError:\n            if \"google.auth\" in str(ex):\n                return ExInfo(\n                    \"APIConnectionError\", False, \"You need to: pip install google-generativeai\"\n                )\n            if \"boto3\" in str(ex):\n                return ExInfo(\"APIConnectionError\", False, \"You need to: pip install boto3\")\n            if \"OpenrouterException\" in str(ex) and \"'choices'\" in str(ex):\n                return ExInfo(\n                    \"APIConnectionError\", True, \"The OpenRouter API provider is down or overloaded.\"\n                )\n        # Check for specific non-retryable APIError cases like insufficient credits\n        if ex.__class__ is litellm.APIError:\n            err_str = str(ex).lower()\n            if \"insufficient credits\" in err_str and '\"code\":402' in err_str:\n                return ExInfo(\n                    \"APIError\",\n                    False,\n                    \"Insufficient credits with the API provider. Please add credits.\",\n                )\n            # Fall through to default APIError handling if not the specific credits error\n\n        return self.exceptions.get(ex.__class__, ExInfo(None, None, None))\n\n\nlitellm_ex = LiteLLMExceptions()",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9402861860209136,
      "diff_lines": 90,
      "added_lines": 11,
      "removed_lines": 33,
      "diff_preview": "--- expected+++ predicted@@ -1,6 +1,4 @@ from dataclasses import dataclass\n-\n-from aider.dump import dump  # noqa: F401\n \n \n @dataclass\n@@ -14,39 +12,23 @@     ExInfo(\"APIConnectionError\", True, None),\n     ExInfo(\"APIError\", True, None),\n     ExInfo(\"APIResponseValidationError\", True, None),\n-    ExInfo(\n-        \"AuthenticationError\",\n-        False,\n-        \"The API provider is not able to authenticate you. Check your API key.\",\n-    ),\n+    ExInfo(\"AuthenticationError\", False, \"The API provider is not able to authenticate you. Check your API key.\"),\n     ExInfo(\"AzureOpenAIError\", True, None),\n     ExInfo(\"BadRequestError\", False, None),\n     ExInfo(\"BudgetExceededError\", True, None),\n-    ExInfo(\n-        \"ContentPolicyViolationError\",\n-        True,\n-        \"The API provider has refused the request due to a safety policy about the content.\",\n-    ),\n+    ExInfo(\"ContentPolicyViolationError\", True, \"The API provider has refused the request due to a safety policy about the content.\"),\n     ExInfo(\"ContextWindowExceededError\", False, None),  # special case handled in base_coder\n     ExInfo(\"InternalServerError\", True, \"The API provider's servers are down or overloaded.\"),\n     ExInfo(\"InvalidRequestError\", True, None),\n     ExInfo(\"JSONSchemaValidationError\", True, None),\n     ExInfo(\"NotFoundError\", False, None),\n     ExInfo(\"OpenAIError\", True, None),\n-    ExInfo(\n-        \"RateLimitError\",\n-        True,\n-        \"The API provider has rate limited you. Try again later or check your quotas.\",\n-    ),\n+    ExInfo(\"RateLimitError\", True, \"The API provider has rate limited you. Try again later or check your quotas.\"),\n     ExInfo(\"RouterRateLimitError\", True, None),\n     ExInfo(\"ServiceUnavailableError\", True, \"The API provider's servers are down or overloaded.\"),\n     ExInfo(\"UnprocessableEntityError\", True, None),\n     ExInfo(\"UnsupportedParamsError\", True, None),\n-    ExInfo(\n-        \"Timeout\",\n-        True,\n-        \"The API provider timed out without returning a response. They may be down or overloaded.\",\n-    ),\n+    ExInfo(\"Timeout\", True, \"The API provider timed out without returning a response. They may be down or overloaded.\"),\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 26.182698011398315,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 7265,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_aider_history.py",
    "exact_match": false,
    "similarity": 0.8483359941053665,
    "diff_lines": 105,
    "predicted_output": "import argparse\n\nfrom aider import models, prompts\nfrom aider.dump import dump  # noqa: F401\n\nclass ChatSummary:\n    def __init__(self, models=None, max_tokens=1024):\n        if not models:\n            raise ValueError(\"At least one model must be provided\")\n        self.models = models if isinstance(models, list) else [models]\n        self.max_tokens = max_tokens\n        self.token_count = self.models[0].token_count\n\n    def too_big(self, messages):\n        sized = self.tokenize(messages)\n        total = sum(tokens for tokens, _msg in sized)\n        return total > self.max_tokens\n\n    def tokenize(self, messages):\n        sized = []\n        for msg in messages:\n            tokens = len(self.token_count(msg))\n            sized.append((tokens, msg))\n        return sized\n\n    def summarize(self, messages, depth=0):\n        messages = self.summarize_real(messages)\n        if messages and messages[-1][\"role\"] != \"assistant\":\n            messages.append(dict(role=\"assistant\", content=\"Ok.\"))\n        return messages\n\n    def summarize_real(self, messages, depth=0):\n        if not self.models:\n            raise ValueError(\"No models available for summarization\")\n\n        sized = self.tokenize(messages)\n        total = sum(tokens for tokens, _msg in sized)\n        if total <= self.max_tokens and depth == 0:\n            return messages\n\n        tail_tokens = 0\n        split_index = len(messages)\n        model_max_input_tokens = self.models[0].info.get(\"max_input_tokens\") or 4096\n        model_max_input_tokens -= 512\n\n        for i in range(split_index):\n            tail_tokens += sized[i][0]\n            if tail_tokens > model_max_input_tokens:\n                split_index = i\n                break\n\n        head = messages[:split_index]\n        tail = messages[split_index:]\n\n        summary = self.summarize_all(head)\n\n        tail_tokens = sum(tokens for tokens, msg in sized[split_index:])\n        summary_tokens = self.token_count(summary)\n\n        result = summary + tail\n        if summary_tokens + tail_tokens < self.max_tokens:\n            return result\n\n        return self.summarize_real(result, depth + 1)\n\n    def summarize_all(self, messages):\n        content = \"\"\n        for msg in messages:\n            role = msg[\"role\"].upper()\n            if role not in (\"USER\", \"ASSISTANT\"):\n                continue\n            content += f\"# {role}\\n\"\n            content += msg[\"content\"]\n            if not content.endswith(\"\\n\"):\n                content += \"\\n\"\n\n        messages = [\n            dict(role=\"system\", content=prompts.summarize),\n            dict(role=\"user\", content=content),\n        ]\n\n        for model in self.models:\n            try:\n                summary = model.simple_send_with_retries(messages)\n                if summary is not None:\n                    summary = prompts.summary_prefix + summary\n                    return [dict(role=\"user\", content=summary)]\n            except Exception as e:\n                print(f\"Summarization failed for model {model.name}: {str(e)}\")\n\n        raise ValueError(\"summarizer unexpectedly failed for all models\")\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"filename\", help=\"Markdown file to parse\")\n    args = parser.parse_args()\n\n    model_names = [\"gpt-3.5-turbo\", \"gpt-4\"]  # Add more model names as needed\n    model_list = [models.Model(name) for name in model_names]\n    summarizer = ChatSummary(model_list)\n\n    with open(args.filename, \"r\") as f:\n        text = f.read()\n\n    summary = summarizer.summarize_chat_history_markdown(text)\n    dump(summary)",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.8483359941053665,
      "diff_lines": 105,
      "added_lines": 10,
      "removed_lines": 46,
      "diff_preview": "--- expected+++ predicted@@ -2,7 +2,6 @@ \n from aider import models, prompts\n from aider.dump import dump  # noqa: F401\n-\n \n class ChatSummary:\n     def __init__(self, models=None, max_tokens=1024):\n@@ -20,7 +19,7 @@     def tokenize(self, messages):\n         sized = []\n         for msg in messages:\n-            tokens = self.token_count(msg)\n+            tokens = len(self.token_count(msg))\n             sized.append((tokens, msg))\n         return sized\n \n@@ -39,52 +38,21 @@         if total <= self.max_tokens and depth == 0:\n             return messages\n \n-        min_split = 4\n-        if len(messages) <= min_split or depth > 3:\n-            return self.summarize_all(messages)\n-\n         tail_tokens = 0\n         split_index = len(messages)\n-        half_max_tokens = self.max_tokens // 2\n+        model_max_input_tokens = self.models[0].info.get(\"max_input_tokens\") or 4096\n+        model_max_input_tokens -= 512\n \n-        # Iterate over the messages in reverse order\n-        for i in range(len(sized) - 1, -1, -1):\n-            tokens, _msg = sized[i]\n-            if tail_tokens + tokens < half_max_tokens:\n-                tail_tokens += tokens\n+        for i in range(split_index):\n+            tail_tokens += sized[i][0]\n+            if tail_tokens > model_max_input_tokens:\n                 split_index = i\n-            else:\n                 break\n-\n-        # Ensure the head ends with an assistant message\n-        while messages[split_index - 1][\"role\"] != \"assistant\" and split_index > 1:\n-            split_index -= 1\n-\n-        if split_index <= min_split:\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 23.5360050201416,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 18915,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_aider_linter.py",
    "exact_match": false,
    "similarity": 0.7429464856797668,
    "diff_lines": 224,
    "predicted_output": "import os\nimport re\nimport subprocess\nimport sys\nimport traceback\nimport warnings\nimport shlex\nfrom dataclasses import dataclass\nfrom pathlib import Path\n\nfrom grep_ast import TreeContext, filename_to_lang\nfrom tree_sitter_language_pack import get_parser  # noqa: E402\n\nfrom aider.dump import dump  # noqa: F401\nfrom aider.run_cmd import run_cmd_subprocess  # noqa: F401\n\n# tree_sitter is throwing a FutureWarning\nwarnings.simplefilter(\"ignore\", category=FutureWarning)\n\nclass Linter:\n    def __init__(self, encoding=\"utf-8\", root=None):\n        self.encoding = encoding\n        self.root = root\n\n        self.languages = dict(\n            python=self.py_lint,\n        )\n        self.all_lint_cmd = None\n\n    def set_linter(self, lang, cmd):\n        if lang:\n            self.languages[lang] = cmd\n            return\n\n        self.all_lint_cmd = cmd\n\n    def get_rel_fname(self, fname):\n        if self.root:\n            try:\n                return os.path.relpath(fname, self.root)\n            except ValueError:\n                return fname\n        else:\n            return fname\n\n    def run_cmd(self, cmd, rel_fname, code):\n        cmd += \" \" + shlex.quote(rel_fname)\n\n        returncode = 0\n        stdout = \"\"\n        try:\n            process = subprocess.Popen(\n                cmd,\n                cwd=self.root,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.STDOUT,\n                encoding=self.encoding,\n                errors=\"replace\",\n            )\n        except OSError as err:\n            print(f\"Unable to execute lint command: {err}\")\n            return\n        stdout, _ = process.communicate()\n        errors = stdout\n        if process.returncode == 0:\n            return  # zero exit status\n\n        res = f\"## Running: {cmd}\\n\\n\"\n        res += errors\n\n        return self.errors_to_lint_result(rel_fname, res)\n\n    def errors_to_lint_result(self, rel_fname, errors):\n        if not errors:\n            return\n\n        linenums = []\n        filenames_linenums = find_filenames_and_linenums(errors, [rel_fname])\n        if filenames_linenums:\n            filename, linenums = next(iter(filenames_linenums.items()))\n            linenums = [num - 1 for num in linenums]\n\n        return LintResult(text=errors, lines=linenums)\n\n    def lint(self, fname, cmd=None):\n        rel_fname = self.get_rel_fname(fname)\n        code = Path(fname).read_text(encoding=self.encoding, errors=\"replace\")\n\n        if cmd:\n            cmd = cmd.strip()\n        if not cmd:\n            lang = filename_to_lang(fname)\n            if not lang:\n                return\n            cmd = self.languages.get(lang)\n\n        if callable(cmd):\n            lintres = cmd(fname, rel_fname, code)\n        elif cmd:\n            lintres = self.run_cmd(cmd, rel_fname, code)\n        else:\n            lintres = basic_lint(rel_fname, code)\n\n        if not lintres:\n            return\n\n        res = \"# Fix any errors below, if possible.\\n\\n\"\n        res += lintres.text\n        res += \"\\n\"\n        res += tree_context(rel_fname, code, lintres.lines)\n\n        return res\n\n    def py_lint(self, fname, rel_fname, code):\n        basic_res = basic_lint(rel_fname, code)\n        compile_res = lint_python_compile(fname, code)\n\n        fatal = \"E9,F821,F823,F831,F406,F407,F701,F702,F704,F706\"\n        flake8 = f\"flake8 --select={fatal} --show-source --isolated\"\n\n        try:\n            flake_res = self.run_cmd(flake8, rel_fname, code)\n        except FileNotFoundError:\n            flake_res = None\n\n        text = \"\"\n        lines = set()\n        for res in [basic_res, compile_res, flake_res]:\n            if not res:\n                continue\n            if text:\n                text += \"\\n\"\n            text += res.text\n            lines.update(res.lines)\n\n        if text or lines:\n            return LintResult(text, lines)\n\n@dataclass\nclass LintResult:\n    text: str\n    lines: list\n\ndef lint_python_compile(fname, code):\n    try:\n        compile(code, fname, \"exec\")  # USE TRACEBACK BELOW HERE\n        return\n    except Exception as err:\n        end_lineno = getattr(err, \"end_lineno\", err.lineno)\n        line_numbers = list(range(err.lineno - 1, end_lineno))\n\n        tb_lines = traceback.format_exception(type(err), err, err.__traceback__)\n        return LintResult(text=\"\".join(tb_lines), lines=line_numbers)\n\ndef basic_lint(fname, code):\n    lang = filename_to_lang(fname)\n    if lang == \"typescript\":\n        return\n\n    try:\n        parser = get_parser(lang)\n    except Exception as err:\n        print(f\"Unable to load parser: {err}\")\n        return\n\n    tree = parser.parse(bytes(code, \"utf-8\"))\n\n    try:\n        errors = traverse_tree(tree.root_node)\n    except RecursionError:\n        print(f\"Unable to lint {fname} due to RecursionError\")\n        return\n\n    if not errors:\n        return\n\n    return LintResult(text=\"\", lines=errors)\n\ndef traverse_tree(node):\n    errors = []\n    if node.type == \"ERROR\" or node.is_missing:\n        line_no = node.start_point[0]\n        errors.append(line_no)\n\n    for child in node.children:\n        errors += traverse_tree(child)\n\n    return errors\n\ndef find_filenames_and_linenums(text, fnames):\n    \"\"\"\n    Search text for all occurrences of <filename>:\\d+ and make a list of them\n    where <filename> is one of the filenames in the list `fnames`.\n    \"\"\"\n    pattern = re.compile(r\"(\\b(?:\" + \"|\".join(re.escape(fname) for fname in fnames) + r\"):\\d+\\b)\")\n    matches = pattern.findall(text)\n    result = {}\n    for match in matches:\n        fname, linenum = match.rsplit(\":\", 1)\n        if fname not in result:\n            result[fname] = set()\n        result[fname].add(int(linenum))\n    return result\n\ndef main():\n    \"\"\"\n    Main function to parse files provided as command line arguments.\n    \"\"\"\n    if len(sys.argv) < 2:\n        print(\"Usage: python linter.py <file1> <file2> ...\")\n        sys.exit(1)\n\n    linter = Linter(root=os.getcwd())\n    for file_path in sys.argv[1:]:\n        errors = linter.lint(file_path)\n        if errors:\n            print(errors)\n\nif __name__ == \"__main__\":\n    main()",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.7429464856797668,
      "diff_lines": 224,
      "added_lines": 19,
      "removed_lines": 103,
      "diff_preview": "--- expected+++ predicted@@ -9,14 +9,13 @@ from pathlib import Path\n \n from grep_ast import TreeContext, filename_to_lang\n-from grep_ast.tsl import get_parser  # noqa: E402\n+from tree_sitter_language_pack import get_parser  # noqa: E402\n \n from aider.dump import dump  # noqa: F401\n from aider.run_cmd import run_cmd_subprocess  # noqa: F401\n \n # tree_sitter is throwing a FutureWarning\n warnings.simplefilter(\"ignore\", category=FutureWarning)\n-\n \n class Linter:\n     def __init__(self, encoding=\"utf-8\", root=None):\n@@ -50,16 +49,20 @@         returncode = 0\n         stdout = \"\"\n         try:\n-            returncode, stdout = run_cmd_subprocess(\n+            process = subprocess.Popen(\n                 cmd,\n                 cwd=self.root,\n+                stdout=subprocess.PIPE,\n+                stderr=subprocess.STDOUT,\n                 encoding=self.encoding,\n+                errors=\"replace\",\n             )\n         except OSError as err:\n             print(f\"Unable to execute lint command: {err}\")\n             return\n+        stdout, _ = process.communicate()\n         errors = stdout\n-        if returncode == 0:\n+        if process.returncode == 0:\n             return  # zero exit status\n \n         res = f\"## Running: {cmd}\\n\\n\"\n@@ -81,11 +84,7 @@ \n     def lint(self, fname, cmd=None):\n         rel_fname = self.get_rel_fname(fname)\n-        try:\n-            code = Path(fname).read_text(encoding=self.encoding, errors=\"replace\")\n-        except OSError as err:\n-            print(f\"Unable to read {fname}: {err}\")\n-            return\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 75.74684500694275,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 35338,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_aider_mdstream.py",
    "exact_match": false,
    "similarity": 0.004054054054054054,
    "diff_lines": 242,
    "predicted_output": "def greeting():\n    print(\"Hello world!\")",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.004054054054054054,
      "diff_lines": 242,
      "added_lines": 2,
      "removed_lines": 237,
      "diff_preview": "--- expected+++ predicted@@ -1,237 +1,2 @@-#!/usr/bin/env python\n-\n-import io\n-import time\n-\n-from rich import box\n-from rich.console import Console\n-from rich.live import Live\n-from rich.markdown import CodeBlock, Heading, Markdown\n-from rich.panel import Panel\n-from rich.syntax import Syntax\n-from rich.text import Text\n-\n-from aider.dump import dump  # noqa: F401\n-\n-_text_prefix = \"\"\"\n-# Header\n-\n-Lorem Ipsum is simply dummy text of the printing and typesetting industry.\n-Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\n-when an unknown printer took a galley of type and scrambled it to make a type\n-specimen book. It has survived not only five centuries, but also the leap into\n-electronic typesetting, remaining essentially unchanged. It was popularised in\n-the 1960s with the release of Letraset sheets containing Lorem Ipsum passages,\n-and more recently with desktop publishing software like Aldus PageMaker\n-including versions of Lorem Ipsum.\n-\n-\n-\n-## Sub header\n-\n-- List 1\n-- List 2\n-- List me\n-- List you\n-\n-\n-\n-```python\n-\"\"\"\n-\n-_text_suffix = \"\"\"\n-```\n-\n-## Sub header too\n-\n-The end.\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 37.764432191848755,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 14962,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_aider_onboarding.py",
    "exact_match": false,
    "similarity": 0.7004851598173516,
    "diff_lines": 406,
    "predicted_output": "import os\nimport requests\nimport webbrowser\nimport http.server\nimport socketserver\nimport threading\nimport time\nfrom urllib.parse import parse_qs, urlparse\nfrom aider import urls\nfrom aider.io import InputOutput\n\ndef check_openrouter_tier(api_key):\n    \"\"\"\n    Checks if the user is on a free tier for OpenRouter.\n    \n    Args:\n        api_key: The OpenRouter API key to check.\n    \n    Returns:\n        A boolean indicating if the user is on a free tier (True) or paid tier (False).\n        Returns True if the check fails.\n    \"\"\"\n    try:\n        response = requests.get(\n            \"https://openrouter.ai/api/v1/auth/key\",\n            headers={\"Authorization\": f\"Bearer {api_key}\"},\n            timeout=5  # Add a reasonable timeout\n        )\n        response.raise_for_status()\n        data = response.json()\n        return data.get(\"data\", {}).get(\"is_free_tier\", True)  # Default to True if not found\n    except Exception:\n        return True  # Default to free tier if there's an error\n\ndef try_to_select_default_model():\n    \"\"\"\n    Attempts to select a default model based on available API keys.\n    Checks OpenRouter tier status to select appropriate model.\n\n    Returns:\n        The name of the selected model, or None if no suitable default is found.\n    \"\"\"\n    openrouter_key = os.environ.get(\"OPENROUTER_API_KEY\")\n    if openrouter_key:\n        is_free_tier = check_openrouter_tier(openrouter_key)\n        if is_free_tier:\n            return \"openrouter/google/gemini-2.5-pro-exp-03-25:free\"\n        else:\n            return \"openrouter/anthropic/claude-3.7-sonnet\"\n    \n    model_key_pairs = [\n        (\"ANTHROPIC_API_KEY\", \"sonnet\"),\n        (\"DEEPSEEK_API_KEY\", \"deepseek\"),\n        (\"OPENAI_API_KEY\", \"gpt-4o\"),\n        (\"GEMINI_API_KEY\", \"gemini/gemini-2.5-pro-exp-03-25\"),\n        (\"VERTEXAI_PROJECT\", \"vertex_ai/gemini-2.5-pro-exp-03-25\"),\n    ]\n\n    for env_key, model_name in model_key_pairs:\n        api_key_value = os.environ.get(env_key)\n        if api_key_value:\n            return model_name\n\n    return None\n\ndef offer_openrouter_oauth(io, analytics):\n    \"\"\"\n    Offers OpenRouter OAuth flow to the user if no API keys are found.\n    \n    Args:\n        io: The InputOutput object for user interaction.\n        analytics: The Analytics object for tracking events.\n        \n    Returns:\n        True if authentication was successful, False otherwise.\n    \"\"\"\n    io.tool_warning(\"No model was specified and no API keys were provided.\")\n    io.tool_output(\"OpenRouter provides free and paid access to many LLMs.\")\n    if io.confirm_ask(\"Login to OpenRouter or create a free account?\", default=\"y\"):\n        analytics.event(\"oauth_flow_initiated\", provider=\"openrouter\")\n        openrouter_key = start_openrouter_oauth_flow(io, analytics)\n        if openrouter_key:\n            analytics.event(\"oauth_flow_success\")\n            return True\n        analytics.event(\"oauth_flow_failure\")\n        io.tool_error(\"OpenRouter authentication did not complete successfully.\")\n    \n    return False\n\ndef select_default_model(args, io, analytics):\n    \"\"\"\n    Selects a default model based on available API keys if no model is specified.\n    Offers OAuth flow for OpenRouter if no keys are found.\n\n    Args:\n        args: The command line arguments object.\n        io: The InputOutput object for user interaction.\n        analytics: The Analytics object for tracking events.\n\n    Returns:\n        The name of the selected model, or None if no suitable default is found.\n    \"\"\"\n    if args.model:\n        return args.model  # Model already specified\n\n    model = try_to_select_default_model()\n    if model:\n        io.tool_warning(f\"Using {model} model with API key from environment.\")\n        analytics.event(\"auto_model_selection\", model=model)\n        return model\n\n    no_model_msg = \"No LLM model was specified and no API keys were provided.\"\n    io.tool_warning(no_model_msg)\n\n    offer_openrouter_oauth(io, analytics)\n    \n    model = try_to_select_default_model()\n    if model:\n        return model\n\n    io.offer_url(urls.models_and_keys, \"Open documentation URL for more info?\")\n    return None\n\ndef find_available_port(start_port=8484, end_port=8584):\n    for port in range(start_port, end_port + 1):\n        try:\n            with socketserver.TCPServer((\"localhost\", port), None):\n                return port\n        except OSError:\n            continue\n    return None\n\ndef exchange_code_for_key(code, code_verifier, io):\n    try:\n        response = requests.post(\n            \"https://openrouter.ai/api/v1/auth/keys\",\n            headers={\"Content-Type\": \"application/json\"},\n            json={\n                \"code\": code,\n                \"code_verifier\": code_verifier,\n                \"code_challenge_method\": \"S256\",\n            },\n            timeout=30,\n        )\n        response.raise_for_status()\n        data = response.json()\n        api_key = data.get(\"key\")\n        if not api_key:\n            io.tool_error(\"Error: 'key' not found in OpenRouter response.\")\n            io.tool_error(f\"Response: {response.text}\")\n            return None\n        return api_key\n    except requests.exceptions.Timeout:\n        io.tool_error(\"Error: Request to OpenRouter timed out during code exchange.\")\n        return None\n    except requests.exceptions.HTTPError as e:\n        io.tool_error(\n            f\"Error exchanging code for OpenRouter key: {e.response.status_code} {e.response.reason}\"\n        )\n        io.tool_error(f\"Response: {e.response.text}\")\n        return None\n    except requests.exceptions.RequestException as e:\n        io.tool_error(f\"Error exchanging code for OpenRouter key: {e}\")\n        return None\n    except Exception as e:\n        io.tool_error(f\"Unexpected error during code exchange: {e}\")\n        return None\n\ndef start_openrouter_oauth_flow(io, analytics):\n    \"\"\"Initiates the OpenRouter OAuth PKCE flow using a local server.\"\"\"\n    port = find_available_port()\n    if not port:\n        io.tool_error(\"Could not find an available port between 8484 and 8584.\")\n        return None\n\n    callback_url = f\"http://localhost:{port}/callback/aider\"\n    auth_code = None\n    server_error = None\n    server_started = threading.Event()\n    shutdown_server = threading.Event()\n\n    class OAuthCallbackHandler(http.server.SimpleHTTPRequestHandler):\n        def do_GET(self):\n            nonlocal auth_code, server_error\n            parsed_path = urlparse(self.path)\n            if parsed_path.path == \"/callback/aider\":\n                query_params = parse_qs(parsed_path.query)\n                if \"code\" in query_params:\n                    auth_code = query_params[\"code\"][0]\n                    self.send_response(200)\n                    self.send_header(\"Content-type\", \"text/html\")\n                    self.end_headers()\n                    self.wfile.write(\n                        b\"<html><body><h1>Success!</h1>\"\n                        b\"<p>Aider has received the authentication code. \"\n                        b\"You can close this browser tab.</p></body></html>\"\n                    )\n                    shutdown_server.set()\n                else:\n                    self.send_response(302)  # Redirect to the main website\n                    self.send_header(\"Location\", urls.website)\n                    self.end_headers()\n                    shutdown_server.set()\n            else:\n                self.send_response(302)  # Redirect anything else to the main website\n                self.send_header(\"Location\", urls.website)\n                self.end_headers()\n\n    def run_server():\n        try:\n            with socketserver.TCPServer((\"localhost\", port), OAuthCallbackHandler) as httpd:\n                io.tool_output(f\"Temporary server listening on {callback_url}\", log_only=True)\n                server_started.set()\n                while not shutdown_server.is_set():\n                    httpd.handle_request()\n                    time.sleep(0.1)\n                io.tool_output(\"Shutting down temporary server.\", log_only=True)\n        except Exception as e:\n            server_error = f\"Failed to start or run temporary server: {e}\"\n            server_started.set()\n            shutdown_server.set()\n\n    server_thread = threading.Thread(target=run_server, daemon=True)\n    server_thread.start()\n\n    if not server_started.wait(timeout=5):\n        io.tool_error(\"Temporary authentication server failed to start in time.\")\n        shutdown_server.set()\n        server_thread.join(timeout=1)\n        return None\n\n    if server_error:\n        io.tool_error(server_error)\n        shutdown_server.set()\n        server_thread.join(timeout=1)\n        return None\n\n    code_verifier, code_challenge = generate_pkce_codes()\n    auth_url_base = \"https://openrouter.ai/auth\"\n    auth_params = {\n        \"callback_url\": callback_url,\n        \"code_challenge\": code_challenge,\n        \"code_challenge_method\": \"S256\",\n    }\n    auth_url = f\"{auth_url_base}?{'&'.join(f'{k}={v}' for k, v in auth_params.items())}\"\n\n    io.tool_output(\"\\nPlease open this URL in your browser to connect Aider with OpenRouter:\")\n    io.tool_output()\n    print(auth_url)\n    MINUTES = 5\n    io.tool_output(f\"\\nWaiting up to {MINUTES} minutes for you to finish in the browser...\")\n    io.tool_output(\"Use Control-C to interrupt.\")\n\n    try:\n        webbrowser.open(auth_url)\n    except Exception:\n        pass\n\n    interrupted = False\n    try:\n        shutdown_server.wait(timeout=MINUTES * 60)  # Convert minutes to seconds\n    except KeyboardInterrupt:\n        io.tool_warning(\"\\nOAuth flow interrupted.\")\n        analytics.event(\"oauth_flow_failed\", provider=\"openrouter\", reason=\"user_interrupt\")\n        interrupted = True\n        shutdown_server.set()\n\n    server_thread.join(timeout=1)\n\n    if not auth_code:\n        io.tool_error(\"Authentication with OpenRouter failed.\")\n        analytics.event(\"oauth_flow_failed\", provider=\"openrouter\")\n        return None\n\n    io.tool_output(\"Completing authentication...\")\n    analytics.event(\"oauth_flow_code_received\", provider=\"openrouter\")\n\n    api_key = exchange_code_for_key(auth_code, code_verifier, io)\n\n    if api_key:\n        config_dir = os.path.expanduser(\"~/.aider\")\n        os.makedirs(config_dir, exist_ok=True)\n        key_file = os.path.join(config_dir, \"oauth-keys.env\")\n        with open(key_file, \"a\", encoding=\"utf-8\") as f:\n            f.write(f'OPENROUTER_API_KEY=\"{api_key}\"\\n')\n        io.tool_warning(\"Aider will load the OpenRouter key automatically in future sessions.\")\n        io.tool_output()\n        analytics.event(\"oauth_flow_success\", provider=\"openrouter\")\n        return api_key\n    else:\n        io.tool_error(\"Authentication with OpenRouter failed.\")\n        analytics.event(\"oauth_flow_failed\", provider=\"openrouter\", reason=\"code_exchange_failed\")\n        return None\n\ndef generate_pkce_codes():\n    code_verifier = secrets.token_urlsafe(64)\n    hasher = hashlib.sha256()\n    hasher.update(code_verifier.encode(\"utf-8\"))\n    code_challenge = base64.urlsafe_b64encode(hasher.digest()).rstrip(b\"=\").decode(\"utf-8\")\n    return code_verifier, code_challenge\n\ndef main():\n    \"\"\"Main function to test the OpenRouter OAuth flow.\"\"\"\n    print(\"Starting OpenRouter OAuth flow test...\")\n\n    io = InputOutput(\n        pretty=True,\n        yes=False,\n        input_history_file=None,\n        chat_history_file=None,\n        tool_output_color=\"BLUE\",\n        tool_error_color=\"RED\",\n    )\n    analytics = DummyAnalytics()\n\n    if \"OPENROUTER_API_KEY\" in os.environ:\n        print(\"Warning: OPENROUTER_API_KEY is already set in environment.\")\n\n    api_key = start_openrouter_oauth_flow(io, analytics)\n\n    if api_key:\n        print(\"\\nOAuth flow completed successfully!\")\n        print(f\"Obtained API Key (first 5 chars): {api_key[:5]}...\")\n    else:\n        print(\"\\nOAuth flow failed or was cancelled.\")\n\n    print(\"\\nOpenRouter OAuth flow test finished.\")\n\nif __name__ == \"__main__\":\n    main()",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.7004851598173516,
      "diff_lines": 406,
      "added_lines": 43,
      "removed_lines": 141,
      "diff_preview": "--- expected+++ predicted@@ -1,27 +1,21 @@-import base64\n-import hashlib\n+import os\n+import requests\n+import webbrowser\n import http.server\n-import os\n-import secrets\n import socketserver\n import threading\n import time\n-import webbrowser\n from urllib.parse import parse_qs, urlparse\n-\n-import requests\n-\n from aider import urls\n from aider.io import InputOutput\n \n-\n def check_openrouter_tier(api_key):\n     \"\"\"\n     Checks if the user is on a free tier for OpenRouter.\n-\n+    \n     Args:\n         api_key: The OpenRouter API key to check.\n-\n+    \n     Returns:\n         A boolean indicating if the user is on a free tier (True) or paid tier (False).\n         Returns True if the check fails.\n@@ -30,16 +24,13 @@         response = requests.get(\n             \"https://openrouter.ai/api/v1/auth/key\",\n             headers={\"Authorization\": f\"Bearer {api_key}\"},\n-            timeout=5,  # Add a reasonable timeout\n+            timeout=5  # Add a reasonable timeout\n         )\n         response.raise_for_status()\n         data = response.json()\n-        # According to the documentation, 'is_free_tier' will be true if the user has never paid\n         return data.get(\"data\", {}).get(\"is_free_tier\", True)  # Default to True if not found\n     except Exception:\n-        # If there's any error, we'll default to assuming free tier\n-        return True\n-\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 72.82336091995239,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 17935,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_aider_repo.py",
    "exact_match": false,
    "similarity": 0.14401390370107042,
    "diff_lines": 629,
    "predicted_output": "import os\nimport time\nfrom pathlib import Path, PurePosixPath\n\ntry:\n    import git\n    ANY_GIT_ERROR = [\n        git.exc.ODBError,\n        git.exc.GitError,\n        git.exc.InvalidGitRepositoryError,\n        git.exc.GitCommandNotFound,\n    ]\nexcept ImportError:\n    git = None\n    ANY_GIT_ERROR = []\n\nimport pathspec\n\nfrom aider import prompts, utils\nfrom aider.sendchat import simple_send_with_retries\n\nfrom .dump import dump  # noqa: F401\n\nclass UnableToCountRepoFiles(Exception):\n    pass\n\nclass GitRepo:\n    repo = None\n    aider_ignore_file = None\n    aider_ignore_spec = None\n    aider_ignore_ts = 0\n    subtree_only = False\n    ignore_file_cache = {}\n    aider_ignore_last_check = 0\n\n    def __init__(\n        self,\n        io,\n        fnames,\n        git_dname,\n        aider_ignore_file=None,\n        models=None,\n        attribute_author=True,\n        attribute_committer=True,\n        attribute_commit_message_author=False,\n        attribute_commit_message_committer=False,\n        commit_prompt=None,\n        subtree_only=False,\n        git_commit_verify=True,\n    ):\n        self.io = io\n        self.models = models\n        self.attribute_author = attribute_author\n        self.attribute_committer = attribute_committer\n        self.attribute_commit_message_author = attribute_commit_message_author\n        self.attribute_commit_message_committer = attribute_commit_message_committer\n        self.commit_prompt = commit_prompt\n        self.subtree_only = subtree_only\n        self.git_commit_verify = git_commit_verify\n        self.ignore_file_cache = {}\n\n        if git_dname:\n            check_fnames = [git_dname]\n        elif fnames:\n            check_fnames = fnames\n        else:\n            check_fnames = [\".\"]\n        \n        repo_paths = []\n        for fname in check_fnames:\n            fname = Path(fname)\n            fname = fname.resolve()\n\n            try:\n                repo_path = git.Repo(fname, search_parent_directories=True).working_dir\n                repo_path = utils.safe_abs_path(repo_path)\n                repo_paths.append(repo_path)\n            except (git.exc.InvalidGitRepositoryError, git.exc.ODBError):\n                pass\n\n        num_repos = len(set(repo_paths))\n\n        if num_repos == 0:\n            raise FileNotFoundError(\"No valid git repository found.\")\n        if num_repos > 1:\n            self.io.tool_error(\"Files are in different git repos.\")\n            raise FileNotFoundError(\"Files are in different git repos.\")\n\n        self.repo = git.Repo(repo_paths.pop(), odbt=git.GitDB)\n        self.root = utils.safe_abs_path(self.repo.working_tree_dir)\n\n    def get_commit_message(self, diffs, context):\n        diffs = \"# Diffs:\\n\" + diffs\n\n        content = \"\"\n        if context:\n            content += context + \"\\n\"\n\n        messages = [\n            dict(role=\"system\", content=self.commit_prompt or prompts.commit_system),\n            dict(role=\"user\", content=content),\n        ]\n\n        commit_message = None\n        for model in self.models:\n            num_tokens = model.token_count(messages)\n            max_tokens = model.info.get(\"max_input_tokens\") or 0\n            if max_tokens and num_tokens > max_tokens:\n                continue\n            commit_message = simple_send_with_retries(model.name, messages)\n            if commit_message:\n                break\n\n        return commit_message\n\n    def get_tracked_files(self):\n        if not self.repo:\n            return []\n\n        try:\n            commit = self.repo.head.commit\n        except ValueError:\n            commit = None\n\n        files = set()\n        if commit:\n            if commit in self.tree_files:\n                files = self.tree_files[commit]\n            else:\n                try:\n                    for blob in commit.tree.traverse():\n                        if blob.type == \"blob\":  # blob is a file\n                            files.add(blob.path)\n                    files = set(self.normalize_path(path) for path in files)\n                    self.tree_files[commit] = set(files)\n                except ANY_GIT_ERROR as err:\n                    self.io.tool_error(f\"Unable to list files in git repo: {err}\")\n                    return []\n\n        # Add staged files\n        index = self.repo.index\n        staged_files = [path for path, _ in index.entries.keys()]\n        files.update(self.normalize_path(path) for path in staged_files)\n\n        res = [fname for fname in files if not self.ignored_file(fname)]\n        return res\n\n    def normalize_path(self, path):\n        orig_path = path\n        res = self.normalized_path.get(orig_path)\n        if res:\n            return res\n\n        path = str(Path(PurePosixPath((Path(self.root) / path).relative_to(self.root))))\n        self.normalized_path[orig_path] = path\n        return path\n\n    def ignored_file(self, fname):\n        self.refresh_aider_ignore()\n\n        if fname in self.ignore_file_cache:\n            return self.ignore_file_cache[fname]\n\n        result = self.ignored_file_raw(fname)\n        self.ignore_file_cache[fname] = result\n        return result\n\n    def ignored_file_raw(self, fname):\n        if self.subtree_only:\n            try:\n                fname_path = Path(self.normalize_path(fname))\n                cwd_path = Path.cwd().resolve().relative_to(Path(self.root).resolve())\n            except ValueError:\n                return True\n\n            if cwd_path not in fname_path.parents and fname_path != cwd_path:\n                return True\n\n        if not self.aider_ignore_file or not self.aider_ignore_file.is_file():\n            return False\n\n        try:\n            fname = self.normalize_path(fname)\n        except ValueError:\n            return True\n\n        mtime = self.aider_ignore_file.stat().st_mtime\n        if mtime != self.aider_ignore_ts:\n            self.aider_ignore_ts = mtime\n            self.ignore_file_cache = {}\n            lines = self.aider_ignore_file.read_text().splitlines()\n            self.aider_ignore_spec = pathspec.PathSpec.from_lines(\n                pathspec.patterns.GitWildMatchPattern,\n                lines,\n            )\n\n        return self.aider_ignore_spec.match_file(fname)\n\n    def refresh_aider_ignore(self):\n        if not self.aider_ignore_file:\n            return\n\n        current_time = time.time()\n        if current_time - self.aider_ignore_last_check < 1:\n            return\n\n        self.aider_ignore_last_check = current_time\n\n        if not self.aider_ignore_file.is_file():\n            return\n\n        mtime = self.aider_ignore_file.stat().st_mtime\n        if mtime != self.aider_ignore_ts:\n            self.aider_ignore_ts = mtime\n            self.ignore_file_cache = {}\n            lines = self.aider_ignore_file.read_text().splitlines()\n            self.aider_ignore_spec = pathspec.PathSpec.from_lines(\n                pathspec.patterns.GitWildMatchPattern,\n                lines,\n            )\n\n    def get_dirty_files(self):\n        \"\"\"\n        Returns a list of all files which are dirty (not committed), either staged or in the working directory.\n        \"\"\"\n        dirty_files = set()\n\n        # Get staged files\n        staged_files = self.repo.git.diff('--name-only', '--cached').splitlines()\n        dirty_files.update(staged_files)\n\n        # Get unstaged files\n        unstaged_files = self.repo.git.diff('--name-only').splitlines()\n        dirty_files.update(unstaged_files)\n\n        return list(dirty_files)\n\n    def is_dirty(self, path=None):\n        if path and not self.path_in_repo(path):\n            return True\n\n        return self.repo.is_dirty(path=path)\n\n    def path_in_repo(self, path):\n        if not self.repo:\n            return\n        if not path:\n            return\n\n        tracked_files = set(self.get_tracked_files())\n        return self.normalize_path(path) in tracked_files\n\n    def get_head_commit(self):\n        try:\n            return self.repo.head.commit\n        except (ValueError, git.exc.ODBError, git.exc.GitError):\n            return None\n\n    def get_head_commit_sha(self, short=False):\n        commit = self.get_head_commit()\n        if not commit:\n            return\n        if short:\n            return commit.hexsha[:7]\n        return commit.hexsha\n\n    def get_head_commit_message(self, default=None):\n        commit = self.get_head_commit()\n        if not commit:\n            return default\n        return commit.message\n\n    def git_ignored_file(self, path):\n        if not self.repo:\n            return\n        try:\n            if self.repo.ignored(path):\n                return True\n        except ANY_GIT_ERROR:\n            return False\n\n    def commit(self, fnames=None, context=None, aider_edits=False):\n        if not fnames and not self.repo.is_dirty():\n            return\n\n        diffs = self.get_diffs(fnames)\n        if not diffs:\n            return\n\n        commit_message = \"(no commit message provided)\"\n        if context:\n            commit_message += \"\\n\\n# Aider chat conversation:\\n\\n\" + context\n\n        cmd = [\"-m\", commit_message]\n        if not self.git_commit_verify:\n            cmd.append(\"--no-verify\")\n        if fnames:\n            fnames = [str(self.abs_root_path(fn)) for fn in fnames]\n            for fname in fnames:\n                try:\n                    self.repo.git.add(fname)\n                except ANY_GIT_ERROR as err:\n                    self.io.tool_error(f\"Unable to add {fname}: {err}\")\n            cmd += [\"--\"] + fnames\n        else:\n            cmd += [\"-a\"]\n\n        try:\n            self.repo.git.commit(cmd)\n            commit_hash = self.get_head_commit_sha(short=True)\n            self.io.tool_output(f\"Commit {commit_hash} {commit_message}\", bold=True)\n            return commit_hash, commit_message\n        except ANY_GIT_ERROR as err:\n            self.io.tool_error(f\"Unable to commit: {err}\")\n        finally:\n            if self.attribute_committer:\n                if original_committer_name_env is not None:\n                    os.environ[\"GIT_COMMITTER_NAME\"] = original_committer_name_env\n                else:\n                    del os.environ[\"GIT_COMMITTER_NAME\"]\n\n            if aider_edits and self.attribute_author:\n                if original_author_name_env is not None:\n                    os.environ[\"GIT_AUTHOR_NAME\"] = original_author_name_env\n                else:\n                    del os.environ[\"GIT_AUTHOR_NAME\"]\n\n    def get_rel_repo_dir(self):\n        try:\n            return os.path.relpath(self.repo.git_dir, os.getcwd())\n        except (ValueError, OSError):\n            return self.repo.git_dir",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.14401390370107042,
      "diff_lines": 629,
      "added_lines": 206,
      "removed_lines": 338,
      "diff_preview": "--- expected+++ predicted@@ -4,7 +4,6 @@ \n try:\n     import git\n-\n     ANY_GIT_ERROR = [\n         git.exc.ODBError,\n         git.exc.GitError,\n@@ -18,31 +17,21 @@ import pathspec\n \n from aider import prompts, utils\n+from aider.sendchat import simple_send_with_retries\n \n from .dump import dump  # noqa: F401\n \n-ANY_GIT_ERROR += [\n-    OSError,\n-    IndexError,\n-    BufferError,\n-    TypeError,\n-    ValueError,\n-    AttributeError,\n-    AssertionError,\n-    TimeoutError,\n-]\n-ANY_GIT_ERROR = tuple(ANY_GIT_ERROR)\n-\n+class UnableToCountRepoFiles(Exception):\n+    pass\n \n class GitRepo:\n     repo = None\n     aider_ignore_file = None\n     aider_ignore_spec = None\n     aider_ignore_ts = 0\n-    aider_ignore_last_check = 0\n     subtree_only = False\n     ignore_file_cache = {}\n-    git_repo_error = None\n+    aider_ignore_last_check = 0\n \n     def __init__(\n         self,\n@@ -61,10 +50,6 @@     ):\n         self.io = io\n         self.models = models\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 73.85213017463684,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 51983,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_aider_run_cmd.py",
    "exact_match": false,
    "similarity": 0.8593827775012444,
    "diff_lines": 67,
    "predicted_output": "import os\nimport platform\nimport subprocess\nimport sys\nfrom io import BytesIO\nimport pexpect\nimport psutil\n\n\ndef run_cmd(command, verbose=False, error_print=None, cwd=None):\n    try:\n        if sys.stdin.isatty() and hasattr(pexpect, \"spawn\") and platform.system() != \"Windows\":\n            return run_cmd_pexpect(command, verbose, cwd)\n\n        return run_cmd_subprocess(command, verbose, cwd)\n    except OSError as e:\n        error_message = f\"Error occurred while running command '{command}': {str(e)}\"\n        if error_print is None:\n            print(error_message)\n        else:\n            error_print(error_message)\n        return 1, error_message\n\n\ndef get_windows_parent_process_name():\n    try:\n        current_process = psutil.Process()\n        while True:\n            parent = current_process.parent()\n            if parent is None:\n                break\n            parent_name = parent.name().lower()\n            if parent_name in [\"powershell.exe\", \"cmd.exe\"]:\n                return parent_name\n            current_process = parent\n        return None\n    except Exception:\n        return None\n\n\ndef run_cmd_subprocess(command, verbose=False, cwd=None):\n    if verbose:\n        print(\"Using run_cmd_subprocess:\", command)\n\n    try:\n        shell = os.environ.get(\"SHELL\", \"/bin/sh\")\n        parent_process = None\n\n        # Determine the appropriate shell\n        if platform.system() == \"Windows\":\n            parent_process = get_windows_parent_process_name()\n            if parent_process == \"powershell.exe\":\n                command = f\"powershell -Command {command}\"\n\n        if verbose:\n            print(\"Running command:\", command)\n            print(\"SHELL:\", shell)\n            if platform.system() == \"Windows\":\n                print(\"Parent process:\", parent_process)\n\n        process = subprocess.Popen(\n            command,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            text=True,\n            shell=True,\n            encoding=sys.stdout.encoding,\n            errors=\"replace\",\n            bufsize=0,  # Set bufsize to 0 for unbuffered output\n            universal_newlines=True,\n            cwd=cwd,\n        )\n\n        output = []\n        while True:\n            chunk = process.stdout.read(1)\n            if not chunk:\n                break\n            print(chunk, end=\"\", flush=True)  # Print the chunk in real-time\n            output.append(chunk)  # Store the chunk for later use\n\n        process.wait()\n        return process.returncode, \"\".join(output)\n    except Exception as e:\n        return 1, str(e)\n\n\ndef run_cmd_pexpect(command, verbose=False, cwd=None):\n    \"\"\"\n    Run a shell command interactively using pexpect, capturing all output.\n\n    :param command: The command to run as a string.\n    :param verbose: If True, print output in real-time.\n    :return: A tuple containing (exit_status, output)\n    \"\"\"\n    output = BytesIO()\n\n    def output_callback(b):\n        output.write(b)\n        if verbose:\n            print(b.decode(\"utf-8\", errors=\"replace\"), end=\"\", flush=True)\n        return b\n\n    try:\n        # Use the shell from SHELL environment variable\n        shell = os.environ.get(\"SHELL\", \"/bin/sh\")\n        if verbose:\n            print(\"Running pexpect.spawn with shell:\", shell)\n        child = pexpect.spawn(shell, args=[\"-c\", command], encoding=\"utf-8\", cwd=cwd)\n        \n        # Transfer control to the user, capturing output\n        child.interact(output_filter=output_callback)\n\n        # Wait for the command to finish and get the exit status\n        child.close()\n        return child.exitstatus, output.getvalue().decode(\"utf-8\", errors=\"replace\")\n\n    except (pexpect.ExceptionPexpect, TypeError, ValueError) as e:\n        error_msg = f\"Error running command {command}: {e}\"\n        return 1, error_msg",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.8593827775012444,
      "diff_lines": 67,
      "added_lines": 8,
      "removed_lines": 20,
      "diff_preview": "--- expected+++ predicted@@ -3,7 +3,6 @@ import subprocess\n import sys\n from io import BytesIO\n-\n import pexpect\n import psutil\n \n@@ -39,7 +38,7 @@         return None\n \n \n-def run_cmd_subprocess(command, verbose=False, cwd=None, encoding=sys.stdout.encoding):\n+def run_cmd_subprocess(command, verbose=False, cwd=None):\n     if verbose:\n         print(\"Using run_cmd_subprocess:\", command)\n \n@@ -65,7 +64,7 @@             stderr=subprocess.STDOUT,\n             text=True,\n             shell=True,\n-            encoding=encoding,\n+            encoding=sys.stdout.encoding,\n             errors=\"replace\",\n             bufsize=0,  # Set bufsize to 0 for unbuffered output\n             universal_newlines=True,\n@@ -94,32 +93,21 @@     :param verbose: If True, print output in real-time.\n     :return: A tuple containing (exit_status, output)\n     \"\"\"\n-    if verbose:\n-        print(\"Using run_cmd_pexpect:\", command)\n-\n     output = BytesIO()\n \n     def output_callback(b):\n         output.write(b)\n+        if verbose:\n+            print(b.decode(\"utf-8\", errors=\"replace\"), end=\"\", flush=True)\n         return b\n \n     try:\n-        # Use the SHELL environment variable, falling back to /bin/sh if not set\n+        # Use the shell from SHELL environment variable\n         shell = os.environ.get(\"SHELL\", \"/bin/sh\")\n         if verbose:\n-            print(\"With shell:\", shell)\n-\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 28.72355580329895,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 11571,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_aider_scrape.py",
    "exact_match": false,
    "similarity": 0.908830424589737,
    "diff_lines": 154,
    "predicted_output": "#!/usr/bin/env python\n\nimport re\nimport sys\nimport pypandoc\nfrom playwright.sync_api import sync_playwright\nfrom aider import __version__, urls, utils\nfrom aider.dump import dump  # noqa: F401\n\naider_user_agent = f\"Aider/{__version__} +{urls.website}\"\n\n# Playwright is nice because it has a simple way to install dependencies on most\n# platforms.\nPLAYWRIGHT_INFO = \"\"\"\nFor better web scraping, install Playwright chromium:\n\n{cmds}\n\nSee {urls.enable_playwright} for more info.\n\"\"\"\n\ndef install_playwright(io):\n    try:\n        from playwright.sync_api import sync_playwright\n\n        has_pip = True\n    except ImportError:\n        has_pip = False\n\n    try:\n        with sync_playwright() as p:\n            p.chromium.launch()\n            has_chromium = True\n    except Exception:\n        has_chromium = False\n\n    if has_pip and has_chromium:\n        return True\n\n    pip_cmd = utils.get_pip_install([\"aider-chat[playwright]\"])\n    chromium_cmd = \"-m playwright install --with-deps chromium\"\n    chromium_cmd = [sys.executable] + chromium_cmd.split()\n\n    cmds = \"\"\n    if not has_pip:\n        cmds += \" \".join(pip_cmd) + \"\\n\"\n    if not has_chromium:\n        cmds += \" \".join(chromium_cmd) + \"\\n\"\n\n    text = f\"\"\"For the best web scraping, install Playwright:\n\n{cmds}\nSee {urls.enable_playwright} for more info.\n\"\"\"\n\n    io.tool_output(text)\n    if not io.confirm_ask(\"Install playwright?\", default=\"y\"):\n        return\n\n    if not has_pip:\n        success, output = utils.run_install(pip_cmd)\n        if not success:\n            io.tool_error(output)\n            return\n\n    success, output = utils.run_install(chromium_cmd)\n    if not success:\n        io.tool_error(output)\n        return\n\n    return True\n\nclass Scraper:\n    pandoc_available = None\n    playwright_available = None\n    playwright_instructions_shown = False\n\n    # Public API...\n    def __init__(self, print_error=None, playwright_available=None, verify_ssl=True):\n        \"\"\"\n        `print_error` - a function to call to print error/debug info.\n        `verify_ssl` - if False, disable SSL certificate verification when scraping.\n        \"\"\"\n        if print_error:\n            self.print_error = print_error\n        else:\n            self.print_error = print\n\n        self.playwright_available = playwright_available\n        self.verify_ssl = verify_ssl\n\n    def scrape(self, url):\n        \"\"\"\n        Scrape a url and turn it into readable markdown if it's HTML.\n        If it's plain text or non-HTML, return it as-is.\n\n        `url` - the URL to scrape.\n        \"\"\"\n        if self.playwright_available:\n            content, mime_type = self.scrape_with_playwright(url)\n        else:\n            content, mime_type = self.scrape_with_httpx(url)\n\n        if not content:\n            self.print_error(f\"Failed to retrieve content from {url}\")\n            return None\n\n        # Check if the content is HTML based on MIME type or content\n        if (mime_type and mime_type.startswith(\"text/html\")) or (mime_type is None and self.looks_like_html(content)):\n            self.try_pandoc()\n            content = self.html_to_markdown(content)\n\n        return content\n\n    def looks_like_html(self, content):\n        \"\"\"\n        Check if the content looks like HTML.\n        \"\"\"\n        if isinstance(content, str):\n            # Check for common HTML tags\n            html_patterns = [\n                r\"<!DOCTYPE\\s+html\",\n                r\"<html\",\n                r\"<head\",\n                r\"<body\",\n                r\"<div\",\n                r\"<p>\",\n                r\"<a\\s+href=\",\n            ]\n            return any(re.search(pattern, content, re.IGNORECASE) for pattern in html_patterns)\n        return False\n\n    # Internals...\n    def scrape_with_playwright(self, url):\n        import playwright  # noqa: F401\n        from playwright.sync_api import sync_playwright\n        from playwright.sync_api import Error as PlaywrightError\n        from playwright.sync_api import TimeoutError as PlaywrightTimeoutError\n\n        with sync_playwright() as p:\n            try:\n                browser = p.chromium.launch()\n            except Exception as e:\n                self.playwright_available = False\n                self.print_error(str(e))\n                return None, None\n\n            try:\n                context = browser.new_context(ignore_https_errors=not self.verify_ssl)\n                page = context.new_page()\n\n                user_agent = page.evaluate(\"navigator.userAgent\")\n                user_agent = user_agent.replace(\"Headless\", \"\")\n                user_agent = user_agent.replace(\"headless\", \"\")\n                user_agent += \" \" + aider_user_agent\n\n                page.set_extra_http_headers({\"User-Agent\": user_agent})\n\n                response = None\n                try:\n                    response = page.goto(url, wait_until=\"networkidle\", timeout=5000)\n                except PlaywrightTimeoutError:\n                    print(f\"Page didn't quiesce, scraping content anyway: {url}\")\n                    response = None\n                except PlaywrightError as e:\n                    self.print_error(f\"Error navigating to {url}: {str(e)}\")\n                    return None, None\n\n                try:\n                    content = page.content()\n                    mime_type = response.header_value(\"content-type\").split(\";\")[0] if response else None\n                except PlaywrightError as e:\n                    self.print_error(f\"Error retrieving page content: {str(e)}\")\n                    content = None\n                    mime_type = None\n            finally:\n                browser.close()\n\n        return content, mime_type\n\n    def scrape_with_httpx(self, url):\n        import httpx\n\n        headers = {\"User-Agent\": f\"Mozilla./5.0 ({aider_user_agent})\"}\n        try:\n            with httpx.Client(headers=headers, verify=self.verify_ssl, follow_redirects=True) as client:\n                response = client.get(url)\n                response.raise_for_status()\n                return response.text, response.headers.get(\"content-type\", \"\").split(\";\")[0]\n        except httpx.HTTPError as http_err:\n            self.print_error(f\"HTTP error occurred: {http_err}\")\n        except Exception as err:\n            self.print_error(f\"An error occurred: {err}\")\n        return None, None\n\n    def try_pandoc(self):\n        if self.pandoc_available:\n            return\n\n        try:\n            pypandoc.get_pandoc_version()\n        except OSError:\n            pass\n\n        try:\n            pypandoc.download_pandoc(delete_installer=True)\n        except Exception as err:\n            self.print_error(f\"Unable to install pandoc: {err}\")\n            return\n\n        self.pandoc_available = True\n\n    def html_to_markdown(self, page_source):\n        if not self.pandoc_available:\n            return page_source\n\n        try:\n            md = pypandoc.convert_text(page_source, \"markdown\", format=\"html\")\n        except OSError:\n            return page_source\n\n        md = re.sub(r\"</div>\", \"      \", md)\n        md = re.sub(r\"<div>\", \"     \", md)\n\n        return md\n\ndef main(url):\n    scraper = Scraper()\n    content = scraper.scrape(url)\n    print(content)\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python playw.py <URL>\")",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.908830424589737,
      "diff_lines": 154,
      "added_lines": 13,
      "removed_lines": 53,
      "diff_preview": "--- expected+++ predicted@@ -2,9 +2,8 @@ \n import re\n import sys\n-\n import pypandoc\n-\n+from playwright.sync_api import sync_playwright\n from aider import __version__, urls, utils\n from aider.dump import dump  # noqa: F401\n \n@@ -12,7 +11,13 @@ \n # Playwright is nice because it has a simple way to install dependencies on most\n # platforms.\n-\n+PLAYWRIGHT_INFO = \"\"\"\n+For better web scraping, install Playwright chromium:\n+\n+{cmds}\n+\n+See {urls.enable_playwright} for more info.\n+\"\"\"\n \n def install_playwright(io):\n     try:\n@@ -65,7 +70,6 @@ \n     return True\n \n-\n class Scraper:\n     pandoc_available = None\n     playwright_available = None\n@@ -92,7 +96,6 @@ \n         `url` - the URL to scrape.\n         \"\"\"\n-\n         if self.playwright_available:\n             content, mime_type = self.scrape_with_playwright(url)\n         else:\n@@ -103,9 +106,7 @@             return None\n \n         # Check if the content is HTML based on MIME type or content\n-        if (mime_type and mime_type.startswith(\"text/html\")) or (\n-            mime_type is None and self.looks_like_html(content)\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 32.353535890579224,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 23410,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_aider_sendchat.py",
    "exact_match": false,
    "similarity": 0.5520419513745431,
    "diff_lines": 141,
    "predicted_output": "import hashlib\nimport json\nimport time\n\nfrom aider.dump import dump  # noqa: F401\nfrom aider.exceptions import LiteLLMExceptions\nfrom aider.llm import litellm\nfrom aider.utils import format_messages\n\ndef sanity_check_messages(messages):\n    \"\"\"Check if messages alternate between user and assistant roles.\n    System messages can be interspersed anywhere.\n    Also verifies the last non-system message is from the user.\n    Returns True if valid, False otherwise.\"\"\"\n    last_role = None\n    last_non_system_role = None\n    \n    for msg in messages:\n        role = msg.get(\"role\")\n        if role == \"system\":\n            continue\n            \n        if last_role and role == last_role:\n            turns = format_messages(messages)\n            raise ValueError(\"Messages don't properly alternate user/assistant:\\n\\n\" + turns)\n        \n        last_role = role\n        last_non_system_role = role\n        \n    return last_non_system_role == \"user\"\n\ndef ensure_alternating_roles(messages):\n    \"\"\"Ensure messages alternate between 'assistant' and 'user' roles.\n    \n    Inserts empty messages of the opposite role when consecutive messages\n    of the same role are found.\n\n    Args:\n        messages: List of message dictionaries with 'role' and 'content' keys.\n\n    Returns:\n        List of messages with alternating roles.\n    \"\"\"\n    if not messages:\n        return messages\n\n    fixed_messages = []\n    prev_role = None\n\n    for msg in messages:\n        current_role = msg.get(\"role\")  # Get 'role', None if missing\n\n        # If current role same as previous, insert empty message\n        # of the opposite role\n        if current_role == prev_role:\n            if current_role == \"user\":\n                fixed_messages.append({\"role\": \"assistant\", \"content\": \"\"})\n            else:\n                fixed_messages.append({\"role\": \"user\", \"content\": \"\"})\n\n        fixed_messages.append(msg)\n        prev_role = current_role\n\n    return fixed_messages\n\ndef send_completion(\n    model_name,\n    messages,\n    functions,\n    stream,\n    temperature=0,\n    extra_params=None,\n):\n    #\n    #\n    if os.environ.get(\"AIDER_SANITY_CHECK_TURNS\"):\n        sanity_check_messages(messages)\n    #\n    #\n\n    if \"deepseek-reasoner\" in model_name:\n        messages = ensure_alternating_roles(messages)\n\n    kwargs = dict(\n        model=model_name,\n        messages=messages,\n        stream=stream,\n    )\n    if temperature is not None:\n        kwargs[\"temperature\"] = temperature\n\n    if functions is not None:\n        function = functions[0]\n        kwargs[\"tools\"] = [dict(type=\"function\", function=function)]\n        kwargs[\"tool_choice\"] = {\"type\": \"function\", \"function\": {\"name\": function[\"name\"]}}\n\n    if extra_params is not None:\n        kwargs.update(extra_params)\n\n    key = json.dumps(kwargs, sort_keys=True).encode()\n    # dump(kwargs)\n\n    # Generate SHA1 hash of kwargs and append it to chat_completion_call_hashes\n    hash_object = hashlib.sha1(key)\n\n    res = litellm.completion(**kwargs)\n\n    return hash_object, res\n\ndef simple_send_with_retries(model, messages):\n    litellm_ex = LiteLLMExceptions()\n\n    if \"deepseek-reasoner\" in model.name:\n        messages = ensure_alternating_roles(messages)\n\n    retry_delay = 0.125\n    while True:\n        try:\n            kwargs = {\n                \"model_name\": model.name,\n                \"messages\": messages,\n                \"functions\": None,\n                \"stream\": False,\n                \"temperature\": None if not model.use_temperature else 0,\n                \"extra_params\": model.extra_params,\n            }\n\n            _hash, response = send_completion(**kwargs)\n            if not response or not hasattr(response, \"choices\") or not response.choices:\n                return None\n            return response.choices[0].message.content\n        except litellm_ex.exceptions_tuple() as err:\n            ex_info = litellm_ex.get_ex_info(err)\n\n            print(str(err))\n            if ex_info.description:\n                print(ex_info.description)\n\n            should_retry = ex_info.retry\n            if should_retry:\n                retry_delay *= 2\n                if retry_delay > RETRY_TIMEOUT:\n                    should_retry = False\n\n            if not should_retry:\n                return None\n\n            print(f\"Retrying in {retry_delay:.1f} seconds...\")\n            time.sleep(retry_delay)\n            continue\n        except AttributeError:\n            return None",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.5520419513745431,
      "diff_lines": 141,
      "added_lines": 100,
      "removed_lines": 9,
      "diff_preview": "--- expected+++ predicted@@ -1,6 +1,11 @@+import hashlib\n+import json\n+import time\n+\n from aider.dump import dump  # noqa: F401\n+from aider.exceptions import LiteLLMExceptions\n+from aider.llm import litellm\n from aider.utils import format_messages\n-\n \n def sanity_check_messages(messages):\n     \"\"\"Check if messages alternate between user and assistant roles.\n@@ -9,26 +14,24 @@     Returns True if valid, False otherwise.\"\"\"\n     last_role = None\n     last_non_system_role = None\n-\n+    \n     for msg in messages:\n         role = msg.get(\"role\")\n         if role == \"system\":\n             continue\n-\n+            \n         if last_role and role == last_role:\n             turns = format_messages(messages)\n             raise ValueError(\"Messages don't properly alternate user/assistant:\\n\\n\" + turns)\n-\n+        \n         last_role = role\n         last_non_system_role = role\n-\n-    # Ensure last non-system message is from user\n+        \n     return last_non_system_role == \"user\"\n-\n \n def ensure_alternating_roles(messages):\n     \"\"\"Ensure messages alternate between 'assistant' and 'user' roles.\n-\n+    \n     Inserts empty messages of the opposite role when consecutive messages\n     of the same role are found.\n \n@@ -58,4 +61,92 @@         fixed_messages.append(msg)\n         prev_role = current_role\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 25.910881996154785,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 36327,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_aider_special.py",
    "exact_match": false,
    "similarity": 0.9965901341213912,
    "diff_lines": 21,
    "predicted_output": "import os\n\nROOT_IMPORTANT_FILES = [\n    # Version Control\n    \".gitignore\",\n    \".gitattributes\",\n    # Documentation\n    \"README\",\n    \"README.md\",\n    \"README.txt\",\n    \"README.rst\",\n    \"CONTRIBUTING\",\n    \"CONTRIBUTING.md\",\n    \"CONTRIBUTING.txt\",\n    \"CONTRIBUTING.rst\",\n    \"LICENSE\",\n    \"LICENSE.md\",\n    \"LICENSE.txt\",\n    \"CHANGELOG\",\n    \"CHANGELOG.md\",\n    \"CHANGELOG.txt\",\n    \"CHANGELOG.rst\",\n    \"SECURITY\",\n    \"SECURITY.md\",\n    \"SECURITY.txt\",\n    \"CODEOWNERS\",\n    # Package Management and Dependencies\n    \"requirements.txt\",\n    \"Pipfile\",\n    \"Pipfile.lock\",\n    \"pyproject.toml\",\n    \"setup.py\",\n    \"setup.cfg\",\n    \"package.json\",\n    \"package-lock.json\",\n    \"yarn.lock\",\n    \"npm-shrinkwrap.json\",\n    \"Gemfile\",\n    \"Gemfile.lock\",\n    \"composer.json\",\n    \"composer.lock\",\n    \"pom.xml\",\n    \"build.gradle\",\n    \"build.gradle.kts\",\n    \"build.sbt\",\n    \"go.mod\",\n    \"go.sum\",\n    \"Cargo.toml\",\n    \"Cargo.lock\",\n    \"mix.exs\",\n    \"rebar.config\",\n    \"project.clj\",\n    \"Podfile\",\n    \"Cartfile\",\n    \"dub.json\",\n    \"dub.sdl\",\n    # Configuration and Settings\n    \".env\",\n    \".env.example\",\n    \".editorconfig\",\n    \"tsconfig.json\",\n    \"jsconfig.json\",\n    \".babelrc\",\n    \"babel.config.js\",\n    \".eslintrc\",\n    \".eslintignore\",\n    \".prettierrc\",\n    \".stylelintrc\",\n    \"tslint.json\",\n    \".pylintrc\",\n    \".flake8\",\n    \".rubocop.yml\",\n    \".scalafmt.conf\",\n    \".dockerignore\",\n    \".gitpod.yml\",\n    \"sonar-project.properties\",\n    \"renovate.json\",\n    \"dependabot.yml\",\n    \".pre-commit-config.yaml\",\n    \"mypy.ini\",\n    \"tox.ini\",\n    \".yamllint\",\n    \"pyrightconfig.json\",\n    # Build and Compilation\n    \"webpack.config.js\",\n    \"rollup.config.js\",\n    \"parcel.config.js\",\n    \"gulpfile.js\",\n    \"Gruntfile.js\",\n    \"build.xml\",\n    \"build.boot\",\n    \"project.json\",\n    \"build.cake\",\n    \"MANIFEST.in\",\n    # Testing\n    \"pytest.ini\",\n    \"phpunit.xml\",\n    \"karma.conf.js\",\n    \"jest.config.js\",\n    \"cypress.json\",\n    \".nycrc\",\n    \".nycrc.json\",\n    # CI/CD\n    \".travis.yml\",\n    \".gitlab-ci.yml\",\n    \"Jenkinsfile\",\n    \"azure-pipelines.yml\",\n    \"bitbucket-pipelines.yml\",\n    \"appveyor.yml\",\n    \"circle.yml\",\n    \".circleci/config.yml\",\n    \".github/dependabot.yml\",\n    \"codecov.yml\",\n    \".coveragerc\",\n    # Docker and Containers\n    \"Dockerfile\",\n    \"docker-compose.yml\",\n    \"docker-compose.override.yml\",\n    # Cloud and Serverless\n    \"serverless.yml\",\n    \"firebase.json\",\n    \"now.json\",\n    \"netlify.toml\",\n    \"vercel.json\",\n    \"app.yaml\",\n    \"terraform.tf\",\n    \"main.tf\",\n    \"cloudformation.yaml\",\n    \"cloudformation.json\",\n    \"ansible.cfg\",\n    \"kubernetes.yaml\",\n    \"k8s.yaml\",\n    # Database\n    \"schema.sql\",\n    \"liquibase.properties\",\n    \"flyway.conf\",\n    # Framework-specific\n    \"next.config.js\",\n    \"nuxt.config.js\",\n    \"vue.config.js\",\n    \"angular.json\",\n    \"gatsby-config.js\",\n    \"gridsome.config.js\",\n    # API Documentation\n    \"swagger.yaml\",\n    \"swagger.json\",\n    \"openapi.yaml\",\n    \"openapi.json\",\n    # Development environment\n    \".nvmrc\",\n    \".ruby-version\",\n    \".python-version\",\n    \"Vagrantfile\",\n    # Quality and metrics\n    \".codeclimate.yml\",\n    \"codecov.yml\",\n    # Documentation\n    \"mkdocs.yml\",\n    \"_config.yml\",\n    \"book.toml\",\n    \"readthedocs.yml\",\n    \".readthedocs.yaml\",\n    # Package registries\n    \".npmrc\",\n    \".yarnrc\",\n    # Linting and formatting\n    \".isort.cfg\",\n    \".markdownlint.json\",\n    \".markdownlint.yaml\",\n    # Security\n    \".bandit\",\n    \".secrets.baseline\",\n    # Misc\n    \".pypirc\",\n    \".gitkeep\",\n    \".npmignore\",\n]\n\nNORMALIZED_ROOT_IMPORTANT_FILES = set(os.path.normpath(path) for path in ROOT_IMPORTANT_FILES)\n\ndef is_important(file_path):\n    file_name = os.path.basename(file_path)\n    dir_name = os.path.normpath(os.path.dirname(file_path))\n    normalized_path = os.path.normpath(file_path)\n\n    # Check for GitHub Actions workflow files\n    if dir_name == os.path.normpath(\".github/workflows\") and file_name.endswith(\".yml\"):\n        return True\n\n    return normalized_path in NORMALIZED_ROOT_IMPORTANT_FILES\n\ndef filter_important_files(file_paths):\n    \"\"\"\n    Filter a list of file paths to return only those that are commonly important in codebases.\n\n    :param file_paths: List of file paths to check\n    :return: List of file paths that match important file patterns\n    \"\"\"\n    return list(filter(is_important, file_paths))",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9965901341213912,
      "diff_lines": 21,
      "added_lines": 0,
      "removed_lines": 4,
      "diff_preview": "--- expected+++ predicted@@ -176,10 +176,7 @@     \".npmignore\",\n ]\n \n-\n-# Normalize the lists once\n NORMALIZED_ROOT_IMPORTANT_FILES = set(os.path.normpath(path) for path in ROOT_IMPORTANT_FILES)\n-\n \n def is_important(file_path):\n     file_name = os.path.basename(file_path)\n@@ -192,7 +189,6 @@ \n     return normalized_path in NORMALIZED_ROOT_IMPORTANT_FILES\n \n-\n def filter_important_files(file_paths):\n     \"\"\"\n     Filter a list of file paths to return only those that are commonly important in codebases.\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 32.07409596443176,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 23015,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_aider_utils.py",
    "exact_match": false,
    "similarity": 0.40740939597315434,
    "diff_lines": 529,
    "predicted_output": "import itertools\nimport os\nimport platform\nimport shlex\nimport subprocess\nimport sys\nimport tempfile\nimport time\nfrom pathlib import Path\n\nimport git\nfrom aider.dump import dump  # noqa: F401\n\nIMAGE_EXTENSIONS = {\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\", \".tiff\", \".webp\", \".pdf\"}\n\nclass IgnorantTemporaryDirectory:\n    def __init__(self):\n        if sys.version_info >= (3, 10):\n            self.temp_dir = tempfile.TemporaryDirectory(ignore_cleanup_errors=True)\n        else:\n            self.temp_dir = tempfile.TemporaryDirectory()\n\n    def __enter__(self):\n        return self.temp_dir.__enter__()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.cleanup()\n\n    def cleanup(self):\n        try:\n            self.temp_dir.cleanup()\n        except (OSError, PermissionError, RecursionError):\n            pass  # Ignore errors (Windows and potential recursion)\n\n    def __getattr__(self, item):\n        return getattr(self.temp_dir, item)\n\nclass ChdirTemporaryDirectory(IgnorantTemporaryDirectory):\n    def __init__(self):\n        super().__init__()\n        try:\n            self.cwd = os.getcwd()\n        except FileNotFoundError:\n            self.cwd = None\n\n    def __enter__(self):\n        res = super().__enter__()\n        os.chdir(Path(self.temp_dir.name).resolve())\n        return res\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.cwd:\n            try:\n                os.chdir(self.cwd)\n            except FileNotFoundError:\n                pass\n        super().__exit__(exc_type, exc_val, exc_tb)\n\nclass GitTemporaryDirectory(ChdirTemporaryDirectory):\n    def __enter__(self):\n        dname = super().__enter__()\n        self.repo = make_repo(dname)\n        return dname\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        del self.repo\n        super().__exit__(exc_type, exc_val, exc_tb)\n\ndef make_repo(path=None):\n    if not path:\n        path = \".\"\n    repo = git.Repo.init(path)\n    repo.config_writer().set_value(\"user\", \"name\", \"Test User\").release()\n    repo.config_writer().set_value(\"user\", \"email\", \"testuser@example.com\").release()\n    return repo\n\ndef run_interactive_command(command):\n    try:\n        import pexpect\n        return run_interactive_command_pexpect(command)\n    except ImportError:\n        return run_interactive_command_subprocess(command)\n\ndef run_interactive_command_subprocess(command):\n    try:\n        result = subprocess.run(\n            command,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            text=True,\n            shell=True,\n            encoding=sys.stdout.encoding,\n            errors=\"replace\",\n        )\n        return result.returncode, result.stdout\n    except Exception as e:\n        return 1, str(e)\n\ndef run_interactive_command_pexpect(command):\n    output = BytesIO()\n\n    def output_callback(b):\n        output.write(b)\n        return b\n\n    try:\n        shell = os.environ.get(\"SHELL\", \"/bin/sh\")\n        if os.path.exists(shell):\n            child = pexpect.spawn(shell, args=[\"-c\", command], encoding=\"utf-8\")\n        else:\n            child = pexpect.spawn(command, encoding=\"utf-8\")\n\n        child.interact(output_filter=output_callback)\n        child.close()\n        return child.exitstatus, output.getvalue().decode(\"utf-8\", errors=\"replace\")\n\n    except pexpect.ExceptionPexpect as e:\n        error_msg = f\"Error running command: {e}\"\n        return 1, error_msg\n\ndef safe_abs_path(res):\n    \"Gives an abs path, which safely returns a full (not 8.3) windows path\"\n    res = Path(res).resolve()\n    return str(res)\n\ndef format_tokens(count):\n    if count < 1000:\n        return f\"{count}\"\n    elif count < 10000:\n        return f\"{count / 1000:.1f}k\"\n    else:\n        return f\"{round(count / 1000)}k\"\n\ndef touch_file(fname):\n    fname = Path(fname)\n    try:\n        fname.parent.mkdir(parents=True, exist_ok=True)\n        fname.touch()\n        return True\n    except OSError:\n        return False\n\ndef check_pip_install_extra(io, module, prompt, pip_install_cmd, self_update=False):\n    if module:\n        try:\n            __import__(module)\n            return True\n        except (ImportError, ModuleNotFoundError):\n            pass\n\n    cmd = get_pip_install(pip_install_cmd)\n\n    if prompt:\n        io.tool_warning(prompt)\n\n    if self_update and platform.system() == \"Windows\":\n        io.tool_output(\"Run this command to update:\")\n        print(printable_shell_command(cmd))\n        return\n\n    if not io.confirm_ask(\"Run pip install?\", default=\"y\", subject=printable_shell_command(cmd)):\n        return\n\n    success, output = run_install(cmd)\n    if success:\n        if not module:\n            return True\n        try:\n            __import__(module)\n            return True\n        except (ImportError, ModuleNotFoundError, RuntimeError) as err:\n            io.tool_error(str(err))\n            pass\n\n    io.tool_error(output)\n\ndef get_pip_install(args):\n    cmd = [\n        sys.executable,\n        \"-m\",\n        \"pip\",\n        \"install\",\n        \"--upgrade\",\n        \"--upgrade-strategy\",\n        \"only-if-needed\",\n    ]\n    cmd += args\n    return cmd\n\ndef printable_shell_command(cmd_list):\n    if platform.system() == \"Windows\":\n        return subprocess.list2cmdline(cmd_list)\n    else:\n        return shlex.join(cmd_list)\n\ndef find_common_root(abs_fnames):\n    try:\n        if len(abs_fnames) == 1:\n            return safe_abs_path(os.path.dirname(list(abs_fnames)[0]))\n        elif abs_fnames:\n            return safe_abs_path(os.path.commonpath(list(abs_fnames)))\n    except OSError:\n        pass\n\n    try:\n        return safe_abs_path(os.getcwd())\n    except FileNotFoundError:\n        return \".\"\n\nclass Spinner:\n    spinner_chars = itertools.cycle([\"\u280b\", \"\u2819\", \"\u2839\", \"\u2838\", \"\u283c\", \"\u2834\", \"\u2826\", \"\u2827\", \"\u2807\", \"\u280f\"])\n\n    def __init__(self, text):\n        self.text = text\n        self.start_time = time.time()\n        self.last_update = 0\n        self.visible = False\n        self.is_tty = sys.stdout.isatty()\n        self.tested = False\n\n    def test_charset(self):\n        if self.tested:\n            return\n        self.tested = True\n        try:\n            print(self.spinner_chars[0], end=\"\", flush=True)\n            print(\"\\r\", end=\"\", flush=True)\n        except UnicodeEncodeError:\n            self.spinner_chars = itertools.cycle([\"|\", \"/\", \"-\", \"\\\\\"])\n\n    def step(self):\n        if not self.is_tty:\n            return\n        self.test_charset()\n        if not self.visible:\n            return\n\n        print(f\"\\r{self.text} {next(self.spinner_chars)}\\r{self.text} \", end=\"\", flush=True)\n\n    def end(self):\n        if self.visible and self.is_tty:\n            print(\"\\r\" + \" \" * (len(self.text) + 3))\n\ndef run_install(cmd):\n    print()\n    print(\"Installing:\", printable_shell_command(cmd))\n\n    try:\n        output = []\n        process = subprocess.Popen(\n            cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            text=True,\n            shell=True,\n            encoding=sys.stdout.encoding,\n            errors=\"replace\",\n        )\n        spinner = Spinner(\"Installing...\")\n\n        while True:\n            char = process.stdout.read(1)\n            if not char:\n                break\n\n            output.append(char)\n            spinner.step()\n\n        return_code = process.wait()\n        output = \"\".join(output)\n\n        if return_code == 0:\n            print(\"Installation complete.\")\n            print()\n            return True, output\n\n    except Exception as e:\n        print(f\"\\nError running pip install: {e}\")\n\n    print(\"\\nInstallation failed.\\n\")\n    return False, output\n\nif __name__ == \"__main__\":\n    import sys\n\n    if len(sys.argv) > 1:\n        command = \" \".join(sys.argv[1:])\n        exit_status, output = run_interactive_command(command)\n        print(\"Exit status:\")\n        dump(exit_status)\n        print(\"\\nFull output:\")\n        dump(output)\n    else:\n        print(\"Usage: python -m aider.utils <command>\")\n        sys.exit(1)",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.40740939597315434,
      "diff_lines": 529,
      "added_lines": 170,
      "removed_lines": 277,
      "diff_preview": "--- expected+++ predicted@@ -8,10 +8,10 @@ import time\n from pathlib import Path\n \n+import git\n from aider.dump import dump  # noqa: F401\n \n IMAGE_EXTENSIONS = {\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\", \".tiff\", \".webp\", \".pdf\"}\n-\n \n class IgnorantTemporaryDirectory:\n     def __init__(self):\n@@ -35,15 +35,13 @@     def __getattr__(self, item):\n         return getattr(self.temp_dir, item)\n \n-\n class ChdirTemporaryDirectory(IgnorantTemporaryDirectory):\n     def __init__(self):\n+        super().__init__()\n         try:\n             self.cwd = os.getcwd()\n         except FileNotFoundError:\n             self.cwd = None\n-\n-        super().__init__()\n \n     def __enter__(self):\n         res = super().__enter__()\n@@ -58,7 +56,6 @@                 pass\n         super().__exit__(exc_type, exc_val, exc_tb)\n \n-\n class GitTemporaryDirectory(ChdirTemporaryDirectory):\n     def __enter__(self):\n         dname = super().__enter__()\n@@ -69,129 +66,113 @@         del self.repo\n         super().__exit__(exc_type, exc_val, exc_tb)\n \n-\n def make_repo(path=None):\n-    import git\n-\n     if not path:\n         path = \".\"\n     repo = git.Repo.init(path)\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 51.8607542514801,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 57219,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_aider_voice.py",
    "exact_match": false,
    "similarity": 0.9181281688970161,
    "diff_lines": 129,
    "predicted_output": "import os\nimport queue\nimport tempfile\nimport time\nimport warnings\n\nfrom prompt_toolkit.shortcuts import prompt\nfrom pydub import AudioSegment\n\nfrom aider.llm import litellm\n\nwarnings.filterwarnings(\n    \"ignore\", message=\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\"\n)\nwarnings.filterwarnings(\"ignore\", category=SyntaxWarning)\n\n\nclass SoundDeviceError(Exception):\n    pass\n\n\nclass Voice:\n    threshold = 0.15\n\n    def __init__(self, audio_format=\"wav\", device_name=None):\n        if sf is None:\n            raise SoundDeviceError\n        try:\n            import sounddevice as sd\n\n            self.sd = sd\n\n            devices = sd.query_devices()\n\n            if device_name:\n                # Find the device with matching name\n                device_id = None\n                for i, device in enumerate(devices):\n                    if device_name in device[\"name\"]:\n                        device_id = i\n                        break\n                if device_id is None:\n                    available_inputs = [d[\"name\"] for d in devices if d[\"max_input_channels\"] > 0]\n                    raise ValueError(\n                        f\"Device '{device_name}' not found. Available input devices: {available_inputs}\"\n                    )\n\n                print(f\"Using input device: {device_name} (ID: {device_id})\")\n\n                self.device_id = device_id\n            else:\n                self.device_id = None\n\n        except (OSError, ModuleNotFoundError):\n            raise SoundDeviceError\n        if audio_format not in [\"wav\", \"mp3\", \"webm\"]:\n            raise ValueError(f\"Unsupported audio format: {audio_format}\")\n        self.audio_format = audio_format\n\n    def callback(self, indata, frames, time, status):\n        \"\"\"This is called (from a separate thread) for each audio block.\"\"\"\n        rms = np.sqrt(np.mean(indata**2))\n        self.max_rms = max(self.max_rms, rms)\n        self.min_rms = min(self.min_rms, rms)\n        rng = self.max_rms - self.min_rms\n        if rng > 0.001:\n            self.pct = (rms - self.min_rms) / rng\n        else:\n            self.pct = 0.5\n        self.q.put(indata.copy())\n\n    def get_prompt(self):\n        num = 10\n        if math.isnan(self.pct) or self.pct < self.threshold:\n            cnt = 0\n        else:\n            cnt = int(self.pct * 10)\n\n        bar = \"\u2591\" * cnt + \"\u2588\" * (num - cnt)\n        bar = bar[:num]\n\n        dur = time.time() - self.start_time\n        return f\"Recording, press ENTER when done... {dur:.1f}sec {bar}\"\n\n    def record_and_transcribe(self, history=None, language=None):\n        try:\n            return self.raw_record_and_transcribe(history, language)\n        except KeyboardInterrupt:\n            return\n        except SoundDeviceError as e:\n            print(f\"Error: {e}\")\n            print(\"Please ensure you have a working audio input device connected and try again.\")\n            return\n\n    def raw_record_and_transcribe(self, history, language):\n        self.q = queue.Queue()\n\n        temp_wav = tempfile.mktemp(suffix=\".wav\")\n\n        try:\n            sample_rate = int(self.sd.query_devices(self.device_id, \"input\")[\"default_samplerate\"])\n        except (TypeError, ValueError):\n            sample_rate = 16000  # fallback to 16kHz if unable to query device\n        except self.sd.PortAudioError:\n            raise SoundDeviceError(f\"Error accessing audio input device: {err}\")\n\n        self.start_time = time.time()\n\n        try:\n            with self.sd.InputStream(samplerate=sample_rate, channels=1, callback=self.callback, device=self.device_id):\n                prompt(self.get_prompt, refresh_interval=0.1)\n        except self.sd.PortAudioError as err:\n            raise SoundDeviceError(f\"Error accessing audio input device: {err}\")\n\n        with sf.SoundFile(temp_wav, mode=\"x\", samplerate=sample_rate, channels=1) as file:\n            while not self.q.empty():\n                file.write(self.q.get())\n\n        # Check file size and offer to convert to mp3 if too large\n        file_size = os.path.getsize(temp_wav)\n        if file_size > 24.9 * 1024 * 1024 and self.audio_format == \"wav\":\n            print(\"\\nWarning: {temp_wav} is too large, switching to mp3 format.\")\n            use_audio_format = \"mp3\"\n        else:\n            use_audio_format = self.audio_format\n\n        filename = temp_wav\n        if use_audio_format != \"wav\":\n            try:\n                new_filename = tempfile.mktemp(suffix=f\".{use_audio_format}\")\n                audio = AudioSegment.from_wav(temp_wav)\n                audio.export(new_filename, format=use_audio_format)\n                os.remove(temp_wav)\n                filename = new_filename\n            except (CouldntDecodeError, CouldntEncodeError) as e:\n                print(f\"Error converting audio: {e}\")\n                filename = temp_wav  # fall back to original file\n            except (OSError, FileNotFoundError) as e:\n                print(f\"File system error during conversion: {e}\")\n                filename = temp_wav  # fall back to original file\n            except Exception as e:\n                print(f\"Unexpected error during audio conversion: {e}\")\n                filename = temp_wav  # fall back to original file\n\n        with open(filename, \"rb\") as fh:\n            try:\n                transcript = litellm.transcription(\n                    model=\"whisper-1\", file=fh, prompt=history, language=language\n                )\n            except Exception as err:\n                print(f\"Unable to transcribe {filename}: {err}\")\n                return\n\n        if filename != temp_wav:\n            os.remove(filename)\n\n        text = transcript.text\n        return text\n\n\nif __name__ == \"__main__\":\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        raise ValueError(\"Please set the OPENAI_API_KEY environment variable.\")\n    print(Voice().record_and_transcribe())",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9181281688970161,
      "diff_lines": 129,
      "added_lines": 9,
      "removed_lines": 31,
      "diff_preview": "--- expected+++ predicted@@ -1,4 +1,3 @@-import math\n import os\n import queue\n import tempfile\n@@ -6,10 +5,9 @@ import warnings\n \n from prompt_toolkit.shortcuts import prompt\n+from pydub import AudioSegment\n \n from aider.llm import litellm\n-\n-from .dump import dump  # noqa: F401\n \n warnings.filterwarnings(\n     \"ignore\", message=\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\"\n@@ -17,31 +15,17 @@ warnings.filterwarnings(\"ignore\", category=SyntaxWarning)\n \n \n-from pydub import AudioSegment  # noqa\n-from pydub.exceptions import CouldntDecodeError, CouldntEncodeError  # noqa\n-\n-try:\n-    import soundfile as sf\n-except (OSError, ModuleNotFoundError):\n-    sf = None\n-\n-\n class SoundDeviceError(Exception):\n     pass\n \n \n class Voice:\n-    max_rms = 0\n-    min_rms = 1e5\n-    pct = 0\n-\n     threshold = 0.15\n \n     def __init__(self, audio_format=\"wav\", device_name=None):\n         if sf is None:\n             raise SoundDeviceError\n         try:\n-            print(\"Initializing sound device...\")\n             import sounddevice as sd\n \n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 38.78833985328674,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 23587,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_aider_watch.py",
    "exact_match": false,
    "similarity": 0.7062554345046289,
    "diff_lines": 492,
    "predicted_output": "from pathlib import Path\nimport re\nimport threading\nfrom typing import Optional\nfrom pathspec import PathSpec\nfrom pathspec.patterns import GitWildMatchPattern\nfrom watchfiles import watch\nfrom aider.dump import dump  # noqa\nfrom aider.watch_prompts import watch_ask_prompt, watch_code_prompt\n\n\nclass FileWatcher:\n    \"\"\"Watches source files for changes and AI comments\"\"\"\n\n    # Compiled regex pattern for AI comments\n    ai_comment_pattern = re.compile(r\"(?:#|//|--|;+) *(ai\\b.*|ai\\b.*|.*\\bai[?!]?) *$\", re.IGNORECASE)\n\n    def __init__(self, coder, gitignores=None, verbose=False, analytics=None, root=None):\n        self.coder = coder\n        self.io = coder.io\n        self.root = Path(root) if root else Path(coder.root)\n        self.verbose = verbose\n        self.analytics = analytics\n        self.stop_event = None\n        self.watcher_thread = None\n        self.changed_files = set()\n        self.gitignore_spec = load_gitignores(\n            [Path(g) for g in self.gitignores] if self.gitignores else []\n        )\n\n        coder.io.file_watcher = self\n\n    def filter_func(self, change_type, path):\n        \"\"\"Filter function for the file watcher\"\"\"\n        path_obj = Path(path)\n        path_abs = path_obj.absolute()\n\n        if not path_abs.is_relative_to(self.root.absolute()):\n            return False\n\n        rel_path = path_abs.relative_to(self.root)\n        if self.verbose:\n            dump(rel_path)\n\n        if self.gitignore_spec and self.gitignore_spec.match_file(\n            rel_path.as_posix() + (\"/\" if path_abs.is_dir() else \"\")\n        ):\n            return False\n\n        if self.verbose:\n            dump(\"ok\", rel_path)\n\n        # Check if file contains AI markers\n        try:\n            content = self.io.read_text(str(path_abs))\n            return self.ai_comment_pattern.search(content)\n        except Exception:\n            return\n\n    def get_roots_to_watch(self):\n        \"\"\"Determine which root paths to watch based on gitignore rules\"\"\"\n        if self.gitignore_spec:\n            roots = [\n                str(path)\n                for path in self.root.iterdir()\n                if not self.gitignore_spec.match_file(\n                    path.relative_to(self.root).as_posix() + (\"/\" if path.is_dir() else \"\")\n                )\n            ]\n            # Fallback to watching root if all top-level items are filtered out\n            return roots if roots else [str(self.root)]\n        return [str(self.root)]\n\n    def handle_changes(self, changes):\n        \"\"\"Process the detected changes and update state\"\"\"\n        if not changes:\n            return False\n\n        changed_files = {str(Path(change[1])) for change in changes}\n        self.changed_files.update(changed_files)\n        self.io.interrupt_input()\n        return True\n\n    def watch_files(self):\n        \"\"\"Watch for file changes and process them\"\"\"\n        try:\n            roots_to_watch = self.get_roots_to_watch()\n\n            for changes in watch(\n                *roots_to_watch,\n                watch_filter=self.filter_func,\n                stop_event=self.stop_event,\n                ignore_permission_denied=True,\n            ):\n                if self.handle_changes(changes):\n                    return\n\n        except Exception as e:\n            if self.verbose:\n                dump(f\"File watcher error: {e}\")\n            raise e\n\n    def start(self):\n        \"\"\"Start watching for file changes\"\"\"\n        self.stop_event = threading.Event()\n        self.changed_files = set()\n\n        self.watcher_thread = threading.Thread(target=self.watch_files, daemon=True)\n        self.watcher_thread.start()\n\n    def stop(self):\n        \"\"\"Stop watching for file changes\"\"\"\n        if self.stop_event:\n            self.stop_event.set()\n        if self.watcher_thread:\n            self.watcher_thread.join()\n            self.watcher_thread = None\n            self.stop_event = None\n\n    def process_changes(self):\n        \"\"\"Get any detected file changes\"\"\"\n        has_action = None\n        added = False\n        for fname in self.changed_files:\n            _, _, action = self.get_ai_comments(fname)\n            if action in (\"!\", \"?\"):\n                has_action = action\n\n            if fname in self.coder.abs_fnames:\n                continue\n            self.coder.abs_fnames.add(fname)\n            rel_fname = self.coder.get_rel_fname(fname)\n            if not added:\n                self.io.tool_output()\n                added = True\n            self.io.tool_output(f\"Added {rel_fname} to the chat\")\n\n        if not has_action:\n            if added:\n                self.io.tool_output(\n                    \"End your comment with AI! to request changes or AI? to ask questions\"\n                )\n            return \"\"\n\n        if self.analytics:\n            self.analytics.event(\"ai-comments execute\")\n        self.io.tool_output(\"Processing your request...\")\n\n        # Refresh all AI comments from tracked files\n        ai_comments = {}\n        for fname in self.coder.abs_fnames:\n            line_nums, comments, _action = self.get_ai_comments(fname)\n            if not line_nums:\n                continue\n            ai_comments[fname] = comments\n\n        res = \"\"\"The \"AI\" comments below can be found in the code files I've shared with you.\nThey contain your instructions.\nMake the requested changes.\nBe sure to remove all these \"AI\" comments from the code!\n\n\"\"\"\n        for fname, comments in ai_comments.items():\n            if not comments:\n                continue\n\n            code = self.io.read_text(fname)\n            if not code:\n                continue\n\n            rel_fname = self.coder.get_rel_fname(fname)\n            res += f\"\\n{rel_fname}:\\n\"\n\n            # Convert comment line numbers to line indices (0-based)\n            lois = [ln - 1 for ln, _ in zip(line_nums, comments) if ln > 0]\n\n            context = TreeContext(\n                rel_fname,\n                code,\n                color=False,\n                line_number=False,\n                child_context=False,\n                last_line=False,\n                margin=0,\n                mark_lois=True,\n                loi_pad=3,\n                show_top_of_file_parent_scope=False,\n            )\n            context.lines_of_interest = set()\n            context.add_lines_of_interest(lois)\n            context.add_context()\n            res += context.format()\n\n        return res\n\n    def get_ai_comments(self, filepath):\n        \"\"\"Extract AI comment line numbers, comments and action status from a file\"\"\"\n        line_nums = []\n        comments = []\n        has_action = None  # None, \"!\" or \"?\"\n        content = self.io.read_text(filepath, silent=True)\n        for i, line in enumerate(content.splitlines(), 1):\n            if match := self.ai_comment_pattern.search(line):\n                comment = match.group(0).strip()\n                if comment:\n                    line_nums.append(i)\n                    comments.append(comment)\n                    comment = comment.lower()\n                    comment = comment.lstrip(\"/#-;\")  # Added semicolon for Lisp comments\n                    comment = comment.strip()\n                    if comment.startswith(\"ai!\") or comment.endswith(\"ai!\"):\n                        has_action = \"!\"\n                    elif comment.startswith(\"ai?\") or comment.endswith(\"ai?\"):\n                        has_action = \"?\"\n        if not line_nums:\n            return None, None, None\n        return line_nums, comments, has_action\n\n\ndef load_gitignores(gitignore_paths: list[Path]) -> Optional[PathSpec]:\n    \"\"\"Load and parse multiple .gitignore files into a single PathSpec\"\"\"\n    if not gitignore_paths:\n        return None\n\n    patterns = [\n        \".aider*\",\n        \".git\",\n        # Common editor backup/temp files\n        \"*~\",  # Emacs/vim backup\n        \"*.bak\",  # Generic backup\n        \"*.swp\",  # Vim swap\n        \"*.swo\",  # Vim swap\n        \"#*#\",  # Emacs auto-save\n        \".#*\",  # Emacs lock files\n        \"*.tmp\",  # Generic temp files\n        \"*.temp\",  # Generic temp files\n        \"*.orig\",  # Merge conflict originals\n        \"*.pyc\",  # Python bytecode\n        \"__pycache__/\",  # Python cache dir\n        \".DS_Store\",  # macOS metadata\n        \"Thumbs.db\",  # Windows thumbnail cache\n        # IDE files\n        \".idea/\",  # JetBrains IDEs\n        \".vscode/\",  # VS Code\n        \"*.sublime-*\",  # Sublime Text\n        \".project\",  # Eclipse\n        \".settings/\",  # Eclipse\n        \"*.code-workspace\",  # VS Code workspace\n        # Environment files\n        \".env\",  # Environment variables\n        \".venv/\",  # Python virtual environments\n        \"node_modules/\",  # Node.js dependencies\n        \"vendor/\",  # Various dependencies\n        # Logs and caches\n        \"*.log\",  # Log files\n        \".cache/\",  # Cache directories\n        \".pytest_cache/\",  # Python test cache\n        \"coverage/\",  # Code coverage reports\n    ]  # Always ignore\n    for path in gitignore_paths:\n        if path.exists():\n            with open(path) as f:\n                patterns.extend(f.readlines())\n\n    return PathSpec.from_lines(GitWildMatchPattern, patterns) if patterns else None",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.7062554345046289,
      "diff_lines": 492,
      "added_lines": 211,
      "removed_lines": 256,
      "diff_preview": "--- expected+++ predicted@@ -1,15 +1,220 @@+from pathlib import Path\n import re\n import threading\n-from pathlib import Path\n from typing import Optional\n-\n-from grep_ast import TreeContext\n from pathspec import PathSpec\n from pathspec.patterns import GitWildMatchPattern\n from watchfiles import watch\n-\n from aider.dump import dump  # noqa\n from aider.watch_prompts import watch_ask_prompt, watch_code_prompt\n+\n+\n+class FileWatcher:\n+    \"\"\"Watches source files for changes and AI comments\"\"\"\n+\n+    # Compiled regex pattern for AI comments\n+    ai_comment_pattern = re.compile(r\"(?:#|//|--|;+) *(ai\\b.*|ai\\b.*|.*\\bai[?!]?) *$\", re.IGNORECASE)\n+\n+    def __init__(self, coder, gitignores=None, verbose=False, analytics=None, root=None):\n+        self.coder = coder\n+        self.io = coder.io\n+        self.root = Path(root) if root else Path(coder.root)\n+        self.verbose = verbose\n+        self.analytics = analytics\n+        self.stop_event = None\n+        self.watcher_thread = None\n+        self.changed_files = set()\n+        self.gitignore_spec = load_gitignores(\n+            [Path(g) for g in self.gitignores] if self.gitignores else []\n+        )\n+\n+        coder.io.file_watcher = self\n+\n+    def filter_func(self, change_type, path):\n+        \"\"\"Filter function for the file watcher\"\"\"\n+        path_obj = Path(path)\n+        path_abs = path_obj.absolute()\n+\n+        if not path_abs.is_relative_to(self.root.absolute()):\n+            return False\n+\n+        rel_path = path_abs.relative_to(self.root)\n+        if self.verbose:\n+            dump(rel_path)\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 66.94012379646301,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 56211,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_aider_website__includes_leaderboard_table.js",
    "exact_match": false,
    "similarity": 0.8071052768661771,
    "diff_lines": 356,
    "predicted_output": "document.addEventListener('DOMContentLoaded', function() {\n  let currentMode = 'view'; // 'view', 'select', 'detail'\n  let selectedRows = new Set(); // Store indices of selected rows\n  const MAX_DISPLAY_COST_CAP = 75; // Define the constant here\n\n  const allMainRows = document.querySelectorAll('tr[id^=\"main-row-\"]');\n  const allDetailsRows = document.querySelectorAll('tr[id^=\"details-\"]');\n  const searchInput = document.getElementById('editSearchInput');\n  const modeViewButton = document.getElementById('mode-view-btn');\n  const modeDetailButton = document.getElementById('mode-detail-btn');\n  const modeSelectButton = document.getElementById('mode-select-btn');\n  const modeButtons = [modeViewButton, modeSelectButton, modeDetailButton];\n  const selectAllCheckbox = document.getElementById('select-all-checkbox');\n  const leaderboardTitle = document.getElementById('leaderboard-title'); // Get title element\n  const defaultTitle = \"Aider polyglot coding leaderboard\";\n  const filteredTitle = \"Aider polyglot coding benchmark results (selected)\";\n\n  function applySearchFilter() {\n    const searchTerm = searchInput.value.toLowerCase();\n    allMainRows.forEach(row => {\n      const textContent = row.textContent.toLowerCase();\n      const detailsRow = document.getElementById(row.id.replace('main-row-', 'details-'));\n      const matchesSearch = textContent.includes(searchTerm);\n\n      if (matchesSearch) {\n        row.classList.remove('hidden-by-search');\n        if (detailsRow) detailsRow.classList.remove('hidden-by-search');\n      } else {\n        row.classList.add('hidden-by-search');\n        if (detailsRow) detailsRow.classList.add('hidden-by-search');\n      }\n    });\n    // After applying search filter, re-apply view mode filter and update select-all state\n    updateTableView(currentMode);\n    if (currentMode === 'select') {\n        updateSelectAllCheckboxState();\n    }\n  }\n\n  function getVisibleMainRows() {\n      // Helper to get rows currently visible (not hidden by search or mode)\n      return Array.from(allMainRows).filter(row =>\n          !row.classList.contains('hidden-by-search') && !row.classList.contains('hidden-by-mode')\n      );\n  }\n\n  function updateSelectAllCheckboxState() {\n      // Update the header checkbox based on the selection state of *visible* rows\n      if (currentMode !== 'select') return; // Only relevant in select mode\n\n      const visibleRows = getVisibleMainRows();\n      const visibleRowCount = visibleRows.length;\n      const selectedVisibleRowCount = visibleRows.filter(row => selectedRows.has(row.querySelector('.row-selector')?.dataset.rowIndex)).length;\n\n      if (visibleRowCount === 0) {\n          selectAllCheckbox.checked = false;\n          selectAllCheckbox.indeterminate = false;\n      } else if (selectedVisibleRowCount === visibleRowCount) {\n          selectAllCheckbox.checked = true;\n          selectAllCheckbox.indeterminate = false;\n      } else if (selectedVisibleRowCount > 0) {\n          selectAllCheckbox.checked = false;\n          selectAllCheckbox.indeterminate = true;\n      } else {\n          selectAllCheckbox.checked = false;\n          selectAllCheckbox.indeterminate = false;\n      }\n  }\n\n  function updateTableView(mode) {\n    currentMode = mode; // Update global state ('view', 'select', 'detail')\n\n    // Update button styles first\n    modeButtons.forEach(btn => {\n        btn.classList.remove('active');\n        // Reset specific styles potentially added by .active\n        btn.style.backgroundColor = '';\n        btn.style.color = '';\n    });\n    let activeButton;\n    if (mode === 'view') activeButton = modeViewButton;\n    else if (mode === 'select') activeButton = modeSelectButton;\n    else if (mode === 'detail') activeButton = modeDetailButton;\n\n    activeButton.classList.add('active');\n    activeButton.style.backgroundColor = '#e7f3ff'; // Use selected row highlight blue\n    activeButton.style.color = '#495057'; // Use dark text for contrast on light blue\n\n    // Get the first header cell (for the toggle/checkbox column)\n    const firstHeaderCell = document.querySelector('table thead th:first-child');\n\n    // Show/hide header checkbox based on mode\n    selectAllCheckbox.style.display = mode === 'select' ? 'inline-block' : 'none';\n\n    allMainRows.forEach(row => {\n      const rowIndex = row.querySelector('.row-selector')?.dataset.rowIndex;\n      const toggleButton = row.querySelector('.toggle-details');\n      const selectorCheckbox = row.querySelector('.row-selector');\n      const detailsRow = document.getElementById(`details-${rowIndex}`);\n      const isSelected = selectedRows.has(rowIndex);\n\n      row.classList.remove('hidden-by-mode');\n      if (detailsRow) detailsRow.classList.remove('hidden-by-mode');\n\n      // Show/hide the first column (header and data cells) based on mode\n      if (firstHeaderCell) {\n          firstHeaderCell.style.display = mode === 'view' ? 'none' : '';\n      }\n      if (row.querySelector('td:first-child')) {\n          row.querySelector('td:first-child').style.display = mode === 'view' ? 'none' : '';\n      }\n\n      // Apply mode-specific logic\n      if (mode === 'view') { // --- VIEW MODE ---\n          toggleButton.style.display = 'none'; // Hide toggle in view mode\n          selectorCheckbox.style.display = 'none';\n          row.classList.remove('row-selected'); // Ensure no selection highlight\n\n          // In 'view' mode, hide row if selections exist AND this row is NOT selected\n          if (selectedRows.size > 0 && !isSelected) {\n              row.classList.add('hidden-by-mode');\n              if (detailsRow) detailsRow.classList.add('hidden-by-mode');\n          } else {\n              // Ensure row is not hidden by mode if it's selected or no selections exist\n              row.classList.remove('hidden-by-mode');\n              if (detailsRow) detailsRow.classList.remove('hidden-by-mode');\n          }\n          // Always hide details row content in view mode regardless of visibility class\n          if (detailsRow) {\n              detailsRow.style.display = 'none';\n          }\n      } else if (mode === 'select') { // --- SELECT MODE ---\n          toggleButton.style.display = 'none';\n          selectorCheckbox.style.display = 'inline-block';\n          selectorCheckbox.checked = isSelected;\n          row.classList.toggle('row-selected', isSelected);\n          row.classList.remove('view-highlighted'); // Clear view highlight\n          // Always hide details row in select mode\n          if (detailsRow) detailsRow.style.display = 'none';\n\n          // In 'select' mode, no rows should be hidden based on selection status\n          row.classList.remove('hidden-by-mode');\n          if (detailsRow) detailsRow.classList.remove('hidden-by-mode');\n      } else { // --- DETAIL MODE --- (mode === 'detail')\n          toggleButton.style.display = 'inline-block'; // Show toggle\n          selectorCheckbox.style.display = 'none';\n          row.classList.remove('row-selected'); // Clear selection highlight\n          row.classList.remove('view-highlighted'); // Clear view highlight\n          // Details row visibility is controlled by the toggle button state, don't force hide/show here\n          // Ensure main row is visible if not hidden by search\n          row.classList.remove('hidden-by-mode');\n          if (detailsRow) {\n              detailsRow.classList.remove('hidden-by-mode');\n              // Preserve existing display state (controlled by toggle) unless hidden by search\n              if (detailsRow.classList.contains('hidden-by-search')) {\n                  detailsRow.style.display = 'none';\n              }\n          }\n      }\n\n      // Ensure rows hidden by search remain hidden regardless of mode\n      if (row.classList.contains('hidden-by-search')) {\n          row.style.display = 'none';\n          if (detailsRow) detailsRow.style.display = 'none';\n      } else if (!row.classList.contains('hidden-by-mode')) {\n          // Make row visible if not hidden by search or mode\n          row.style.display = ''; // Or 'table-row' if needed, but '' usually works\n      } else {\n          // Row is hidden by mode, ensure it's hidden\n          row.style.display = 'none';\n          if (detailsRow) detailsRow.style.display = 'none';\n      }\n    });\n\n    // Update the select-all checkbox state after updating the view\n    updateSelectAllCheckboxState();\n    \n    // Update leaderboard title based on mode and selection\n    if (leaderboardTitle) {\n      if (currentMode === 'view' && selectedRows.size > 0) {\n        leaderboardTitle.textContent = filteredTitle;\n      } else {\n        leaderboardTitle.textContent = defaultTitle;\n      }\n    }\n  }\n\n  // --- Existing Initializations ---\n  // Add percentage ticks\n  const percentCells = document.querySelectorAll('.bar-cell:not(.cost-bar-cell)');\n  percentCells.forEach(cell => {\n    // Add ticks at 0%, 10%, 20%, ..., 100%\n    for (let i = 0; i <= 100; i += 10) {\n      const tick = document.createElement('div');\n      tick.className = 'percent-tick';\n      tick.style.left = `${i}%`;\n      cell.appendChild(tick);\n    }\n  });\n\n  // Process cost bars\n  const costBars = document.querySelectorAll('.cost-bar');\n  \n  costBars.forEach(bar => {\n    const cost = parseFloat(bar.dataset.cost);\n    const maxCost = parseFloat(bar.dataset.maxCost);\n\n    if (cost > 0 && maxCost > 0) {\n      // Calculate percentage based on the display max\n      const percent = Math.min(cost, MAX_DISPLAY_COST_CAP) / MAX_DISPLAY_COST_CAP * 100;\n      // Clamp percentage between 0 and 100\n      bar.style.width = Math.max(0, Math.min(100, percent)) + '%';\n    } else {\n      // Set width to 0 if cost is 0 or negative\n      bar.style.width = '0%';\n    }\n  });\n\n  // Calculate and add cost ticks dynamically\n  const costCells = document.querySelectorAll('.cost-bar-cell');\n  if (costCells.length > 0) {\n    // Generate fixed tick values at $0, $10, $20, $30, $40, $50\n    const tickValues = [0, 10, 20, 30, 40, 50];\n    \n    // Calculate percentage positions for each tick\n    const tickPercentages = tickValues.map(tickCost => {\n      return (tickCost / MAX_DISPLAY_COST_CAP) * 100;\n    });\n\n    // Add tick divs to each cost cell\n    costCells.forEach(cell => {\n      const costBar = cell.querySelector('.cost-bar');\n      // Use optional chaining and provide '0' as fallback if costBar or dataset.cost is missing\n      const cost = parseFloat(costBar?.dataset?.cost || '0');\n\n      // Only add ticks if the cost is actually greater than 0\n      if (cost > 0) {\n        tickPercentages.forEach(percent => {\n          // Ensure percentage is within valid range\n          if (percent >= 0 && percent <= 100) {\n            const tick = document.createElement('div');\n            tick.className = 'cost-tick';\n            tick.style.left = `${percent}%`;\n            cell.appendChild(tick);\n          }\n        });\n      }\n    });\n  }\n\n  // --- New Event Listeners ---\n  // Listener for mode toggle buttons\n  modeButtons.forEach(button => {\n    button.addEventListener('click', function(event) {\n      const newMode = this.dataset.mode;\n      if (newMode !== currentMode) {\n        // Update active button style\n        modeButtons.forEach(btn => {\n            btn.classList.remove('active');\n            // Reset specific styles potentially added by .active\n            btn.style.backgroundColor = '';\n            btn.style.color = '';\n        });\n        this.classList.add('active');\n        // Apply active styles directly as inline styles might interfere\n        this.style.backgroundColor = '#e7f3ff'; // Use selected row highlight blue\n        this.style.color = '#495057'; // Use dark text for contrast on light blue\n\n        // Update table view and apply filters\n        updateTableView(newMode);\n        applySearchFilter(); // Re-apply search filter when mode changes\n      }\n    });\n  });\n\n  // Listener for row selector checkboxes (using event delegation on table body)\n  const tableBody = document.querySelector('table tbody');\n  tableBody.addEventListener('change', function(event) {\n    if (event.target.classList.contains('row-selector') && currentMode === 'select') {\n      const checkbox = event.target;\n      const rowIndex = checkbox.dataset.rowIndex;\n      const mainRow = checkbox.closest('tr');\n\n      if (checkbox.checked) {\n        selectedRows.add(rowIndex);\n        mainRow.classList.add('row-selected');\n      } else {\n        selectedRows.delete(rowIndex);\n        mainRow.classList.remove('row-selected');\n      }\n      // Update select-all checkbox state\n      updateSelectAllCheckboxState();\n    }\n  }); // End of tableBody listener\n\n  // Listener for Select All checkbox\n  selectAllCheckbox.addEventListener('change', function() {\n      if (currentMode !== 'select') return;\n\n      const isChecked = selectAllCheckbox.checked;\n      // Select/deselect only the rows that are currently visible\n      const visibleRows = getVisibleMainRows();\n\n      visibleRows.forEach(row => {\n          const checkbox = row.querySelector('.row-selector');\n          const rowIndex = checkbox?.dataset.rowIndex;\n          if (!checkbox || !rowIndex) return; // Skip if no checkbox/index found\n\n          // Only change state if it differs from target state\n          if (checkbox.checked !== isChecked) {\n              checkbox.checked = isChecked;\n              row.classList.toggle('row-selected', isChecked);\n              if (isChecked) {\n                  selectedRows.add(rowIndex);\n              } else {\n                  selectedRows.delete(rowIndex);\n              }\n          }\n      });\n      // After bulk change, ensure the selectAll checkbox state is correct (not indeterminate)\n      updateSelectAllCheckboxState();\n  });\n\n  // Listener for search input\n  searchInput.addEventListener('input', applySearchFilter);\n\n  // Add toggle functionality for details (Modified to respect modes)\n  const toggleButtons = document.querySelectorAll('.toggle-details');\n  toggleButtons.forEach(button => {\n    button.addEventListener('click', function() {\n      // Only allow toggling in 'detail' mode\n      if (currentMode !== 'detail') return;\n\n      const targetId = this.getAttribute('data-target');\n      const targetRow = document.getElementById(targetId);\n      const mainRow = this.closest('tr'); // Get the main row associated with this button\n\n      if (targetRow && !mainRow.classList.contains('hidden-by-mode') && !mainRow.classList.contains('hidden-by-search')) {\n        const isVisible = targetRow.style.display !== 'none';\n        targetRow.style.display = isVisible ? 'none' : 'table-row';\n        this.textContent = isVisible ? '\u25b6' : '\u25bc';\n      }\n    });\n  });\n\n  // Listener for clicking anywhere on a row\n  tableBody.addEventListener('click', function(event) {\n    const clickedRow = event.target.closest('tr');\n\n    // Ensure it's a main row and not a details row or header/footer\n    if (!clickedRow || !clickedRow.id.startsWith('main-row-')) return;\n\n    // --- START conditional logic ---\n    if (currentMode === 'select') {\n        // --- SELECT MODE LOGIC (Existing) ---\n        // Find the checkbox within this row\n        const checkbox = clickedRow.querySelector('.row-selector');\n        if (!checkbox) return; // No checkbox found in this row\n\n        // If the click was directly on the checkbox or its label (if any),\n        // let the default behavior and the 'change' event listener handle it.\n        // Otherwise, toggle the checkbox state programmatically.\n        if (event.target !== checkbox && event.target.tagName !== 'LABEL' /* Add if you use labels */) {\n            checkbox.checked = !checkbox.checked;\n            // Manually trigger the change event to update state and UI\n            checkbox.dispatchEvent(new Event('change', { bubbles: true }));\n        }\n        // --- END SELECT MODE LOGIC ---\n\n    } else if (currentMode === 'view') {\n        // --- VIEW MODE LOGIC (New) ---\n        // Don't highlight if the click was on the details toggle button\n        if (event.target.classList.contains('toggle-details')) {\n            return;\n        }\n        // Toggle the highlight class on the clicked row\n        clickedRow.classList.toggle('view-highlighted');\n        // --- END VIEW MODE LOGIC ---\n    }\n    // --- END conditional logic ---\n  });\n\n  // --- Initial Setup ---\n  updateTableView('view'); // Initialize view to 'view' mode\n  applySearchFilter(); // Apply initial search filter (if any text is pre-filled or just to set initial state)\n\n  // Close button functionality\n  const closeControlsBtn = document.getElementById('close-controls-btn');\n  if (closeControlsBtn) {\n    closeControlsBtn.addEventListener('click', function() {\n      const controlsContainer = document.getElementById('controls-container');\n      if (controlsContainer) {\n        controlsContainer.style.display = 'none';\n      }\n    });\n  }\n\n});",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.8071052768661771,
      "diff_lines": 356,
      "added_lines": 50,
      "removed_lines": 167,
      "diff_preview": "--- expected+++ predicted@@ -35,10 +35,6 @@     if (currentMode === 'select') {\n         updateSelectAllCheckboxState();\n     }\n-    \n-    // Update cost bars and ticks since visible rows may have changed\n-    updateCostBars();\n-    updateCostTicks();\n   }\n \n   function getVisibleMainRows() {\n@@ -71,7 +67,6 @@       }\n   }\n \n-\n   function updateTableView(mode) {\n     currentMode = mode; // Update global state ('view', 'select', 'detail')\n \n@@ -101,11 +96,9 @@       const rowIndex = row.querySelector('.row-selector')?.dataset.rowIndex;\n       const toggleButton = row.querySelector('.toggle-details');\n       const selectorCheckbox = row.querySelector('.row-selector');\n-      const firstCell = row.querySelector('td:first-child'); // Get the first cell of the main row\n       const detailsRow = document.getElementById(`details-${rowIndex}`);\n       const isSelected = selectedRows.has(rowIndex);\n \n-      // Reset visibility classes before applying mode logic\n       row.classList.remove('hidden-by-mode');\n       if (detailsRow) detailsRow.classList.remove('hidden-by-mode');\n \n@@ -113,8 +106,8 @@       if (firstHeaderCell) {\n           firstHeaderCell.style.display = mode === 'view' ? 'none' : '';\n       }\n-      if (firstCell) {\n-          firstCell.style.display = mode === 'view' ? 'none' : '';\n+      if (row.querySelector('td:first-child')) {\n+          row.querySelector('td:first-child').style.display = mode === 'view' ? 'none' : '';\n       }\n \n       // Apply mode-specific logic\n@@ -122,7 +115,6 @@           toggleButton.style.display = 'none'; // Hide toggle in view mode\n           selectorCheckbox.style.display = 'none';\n           row.classList.remove('row-selected'); // Ensure no selection highlight\n-          // view-highlighted is handled by row click listener\n \n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 68.05085492134094,
    "repo": "aider",
    "language": "JavaScript",
    "prompt_tokens": 18588,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_aider_website__includes_recording.js",
    "exact_match": false,
    "similarity": 0.8822894570354658,
    "diff_lines": 242,
    "predicted_output": "document.addEventListener('DOMContentLoaded', function() {\n  let player; // Store player reference to make it accessible to click handlers\n  let globalAudio; // Global audio element to be reused\n  \n  // Detect if device likely has no physical keyboard\n  function detectNoKeyboard() {\n    // Check if it's a touch device (most mobile devices)\n    const isTouchDevice = ('ontouchstart' in window) || \n                         (navigator.maxTouchPoints > 0) ||\n                         (navigator.msMaxTouchPoints > 0);\n                         \n    // Check common mobile user agents as additional signal\n    const isMobileUA = /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);\n    \n    // If it's a touch device and has a mobile user agent, likely has no physical keyboard\n    if (isTouchDevice && isMobileUA) {\n      document.body.classList.add('no-physical-keyboard');\n    }\n  }\n  \n  // Run detection\n  detectNoKeyboard();\n  \n  // Parse the transcript section to create markers and convert timestamps to links\n  function parseTranscript() {\n    const markers = [];\n    // Find the Commentary heading\n    const transcriptHeading = Array.from(document.querySelectorAll('h2')).find(el => el.textContent.trim() === 'Commentary');\n    \n    if (transcriptHeading) {\n      // Get all list items after the transcript heading\n      let currentElement = transcriptHeading.nextElementSibling;\n      \n      while (currentElement && currentElement.tagName === 'UL') {\n        const listItems = currentElement.querySelectorAll('li');\n        \n        listItems.forEach(item => {\n          const text = item.textContent.trim();\n          const match = text.match(/(\\d+):(\\d+)\\s+(.*)/);\n          \n          if (match) {\n            const minutes = parseInt(match[1], 10);\n            const seconds = parseInt(match[2], 10);\n            const timeInSeconds = minutes * 60 + seconds;\n            const message = match[3].trim();\n            \n            // Create link for the timestamp\n            const timeLink = document.createElement('a');\n            timeLink.href = '#';\n            timeLink.textContent = `${minutes}:${seconds.toString().padStart(2, '0')}`;\n            timeLink.className = 'timestamp-link';\n            timeLink.dataset.time = timeInSeconds;\n            timeLink.dataset.message = message;\n            \n            // Add click event to seek the player\n            timeLink.addEventListener('click', function(e) {\n              e.preventDefault();\n              if (player && typeof player.seek === 'function') {\n                player.seek(timeInSeconds);\n                player.play();\n                \n                // Also trigger toast and speech\n                showToast(message);\n                speakText(message);\n              }\n            });\n            \n            // Replace text with the link + message\n            item.textContent = '';\n            item.appendChild(timeLink);\n            item.appendChild(document.createTextNode(' ' + message));\n            \n            // Add class and click handler to the entire list item\n            item.classList.add('transcript-item');\n            item.dataset.time = timeInSeconds;\n            item.dataset.message = message;\n            \n            item.addEventListener('click', function(e) {\n              // Prevent click event if the user clicked directly on the timestamp link\n              // This prevents double-firing of the event\n              if (e.target !== timeLink) {\n                e.preventDefault();\n                if (player && typeof player.seek === 'function') {\n                  player.seek(timeInSeconds);\n                  player.play();\n                  \n                  // Also trigger toast and speech\n                  showToast(message);\n                  speakText(message);\n                  \n                  // Highlight this timestamp\n                  highlightTimestamp(timeInSeconds);\n                }\n              }\n            });\n            \n            markers.push([timeInSeconds, message]);\n          }\n        });\n        \n        currentElement = currentElement.nextElementSibling;\n      }\n    }\n    \n    return markers;\n  }\n\n  const url = \"https://gist.githubusercontent.com/paul-gauthier/3011ab9455c2d28c0e5a60947202752f/raw/5a5b3dbf68a9c2b22b4954af287efedecdf79d52/tmp.redacted.cast\";\n  \n  // Parse transcript and create markers\n  const markers = parseTranscript();\n  \n  // Create player with a single call\n  player = AsciinemaPlayer.create(\n    url,\n    document.getElementById('demo'),\n    {\n      speed: 1.25,\n      idleTimeLimit: 1,\n      theme: \"aider\",\n      poster: \"npt:0:01\",\n      markers: markers,\n      controls: true\n    }\n  );\n\n  // Focus on the player element so keyboard shortcuts work immediately\n  setTimeout(() => {\n    // Use setTimeout to ensure the player is fully initialized\n    if (player && typeof player.focus === 'function') {\n      player.focus();\n    } else {\n      // If player doesn't have a focus method, try to find and focus the terminal element\n      const playerElement = document.querySelector('.asciinema-terminal');\n      if (playerElement) {\n        playerElement.focus();\n      } else {\n        // Last resort - try to find element with tabindex\n        const tabbableElement = document.querySelector('[tabindex]');\n        if (tabbableElement) {\n          tabbableElement.focus();\n        }\n      }\n    }\n  }, 100);\n  \n  // Function to display toast notification\n  function showToast(text) {\n    // Get the appropriate container based on fullscreen state\n    let container = document.getElementById('toast-container');\n    const isFullscreen = document.fullscreenElement || \n                         document.webkitFullscreenElement || \n                         document.mozFullScreenElement || \n                         document.msFullscreenElement;\n    \n    // If in fullscreen, check if we need to create a fullscreen toast container\n    if (isFullscreen) {\n      // Target the fullscreen element as the container parent\n      const fullscreenElement = document.fullscreenElement || \n                               document.webkitFullscreenElement || \n                               document.mozFullScreenElement || \n                               document.msFullscreenElement;\n      \n      // Look for an existing fullscreen toast container\n      let fsContainer = fullscreenElement.querySelector('.fs-toast-container');\n      \n      if (!fsContainer) {\n        // Create a new container for fullscreen mode\n        fsContainer = document.createElement('div');\n        fsContainer.className = 'toast-container fs-toast-container';\n        fsContainer.id = 'fs-toast-container';\n        fullscreenElement.appendChild(fsContainer);\n      }\n      \n      container = fsContainer;\n    }\n    \n    // Create toast element\n    const toast = document.createElement('div');\n    toast.className = 'toast-notification';\n    toast.textContent = text;\n    \n    // Add to container\n    container.appendChild(toast);\n    \n    // Trigger animation\n    setTimeout(() => {\n      toast.style.opacity = '1';\n    }, 10);\n    \n    // Remove after audio/speech completes\n    return toast;\n  }\n  \n  // Function to hide a toast\n  function hideToast(toast) {\n    if (!toast) return;\n    \n    toast.style.opacity = '0';\n    setTimeout(() => {\n      if (toast.parentNode) {\n        toast.parentNode.removeChild(toast);\n      }\n    }, 300); // Wait for fade out animation\n  }\n  \n  // Track if TTS is currently in progress to prevent duplicates\n  let ttsInProgress = false;\n  \n  // Improved browser TTS function\n  function useBrowserTTS(text) {\n    // Don't start new speech if already in progress\n    if (ttsInProgress) {\n      console.log('Speech synthesis already in progress, skipping');\n      return false;\n    }\n    \n    if ('speechSynthesis' in window) {\n      console.log('Using browser TTS fallback');\n      \n      // Set flag to prevent duplicate speech\n      ttsInProgress = true;\n      \n      // Cancel any ongoing speech\n      window.speechSynthesis.cancel();\n      \n      const utterance = new SpeechSynthesisUtterance(text);\n      utterance.rate = 1.0;\n      utterance.pitch = 1.0;\n      utterance.volume = 1.0;\n      \n      utterance.onstart = () => console.log('Speech started');\n      utterance.onend = () => {\n        console.log('Speech ended');\n        ttsInProgress = false; // Reset flag when speech completes\n      };\n      utterance.onerror = (e) => {\n        console.warn('Speech error:', e);\n        ttsInProgress = false; // Reset flag on error\n      };\n      \n      window.speechSynthesis.speak(utterance);\n      return true;\n    }\n    console.warn('SpeechSynthesis not supported');\n    return false;\n  }\n  \n  // Function to play pre-generated TTS audio files\n  function speakText(text, timeInSeconds) {\n    // Show the toast and keep reference\n    const currentToast = showToast(text);\n    \n    // Format time for filename (MM-SS)\n    const minutes = Math.floor(timeInSeconds / 60);\n    const seconds = timeInSeconds % 60;\n    const formattedTime = `${minutes.toString().padStart(2, '0')}-${seconds.toString().padStart(2, '0')}`;\n    \n    // Get recording_id from the page or use default from the URL\n    const recordingId = typeof recording_id !== 'undefined' ? recording_id : \n                       window.location.pathname.split('/').pop().replace('.html', '');\n                       \n    // Construct audio file path\n    const audioPath = `/assets/audio/${recordingId}/${formattedTime}.mp3`;\n    \n    // Log for debugging\n    console.log(`Attempting to play audio: ${audioPath}`);\n    \n    // Detect iOS\n    const isIOS = /iPad|iPhone|iPod/.test(navigator.userAgent) && !window.MSStream;\n    console.log(`Device is iOS: ${isIOS}`);\n    \n    try {\n      // Create or reuse audio element\n      if (!globalAudio) {\n        globalAudio = new Audio();\n        console.log(\"Created new global Audio element\");\n      }\n      \n      // Set up event handlers\n      globalAudio.onerror = (e) => {\n        console.warn(`Audio error: ${e.type}`, e);\n        useBrowserTTS(text);\n      };\n      \n      // For iOS, preload might help with subsequent plays\n      if (isIOS) {\n        globalAudio.preload = \"auto\";\n      }\n      \n      // Set the new source\n      globalAudio.src = audioPath;\n      \n      // Play with proper error handling\n      const playPromise = globalAudio.play();\n      \n      if (playPromise !== undefined) {\n        playPromise.catch(error => {\n          console.warn(`Play error: ${error.message}`);\n          \n          // On iOS, a user gesture might be required\n          if (isIOS) {\n            console.log(\"iOS playback failed, trying SpeechSynthesis\");\n          }\n          \n          useBrowserTTS(text);\n        });\n      }\n    } catch (e) {\n      console.error(`Exception in audio playback: ${e.message}`);\n      useBrowserTTS(text);\n    }\n  }\n  \n  // Function to highlight the active timestamp in the transcript\n  function highlightTimestamp(timeInSeconds) {\n    // Remove previous highlights\n    document.querySelectorAll('.timestamp-active').forEach(el => {\n      el.classList.remove('timestamp-active');\n    });\n    \n    document.querySelectorAll('.active-marker').forEach(el => {\n      el.classList.remove('active-marker');\n    });\n    \n    // Find the timestamp link with matching time\n    const timestampLinks = document.querySelectorAll('.timestamp-link');\n    let activeLink = null;\n    \n    for (const link of timestampLinks) {\n      if (parseInt(link.dataset.time) === timeInSeconds) {\n        activeLink = link;\n        break;\n      }\n    }\n    \n    if (activeLink) {\n      // Add highlight class to the link\n      activeLink.classList.add('timestamp-active');\n      \n      // Also highlight the parent list item\n      const listItem = activeLink.closest('li');\n      if (listItem) {\n        listItem.classList.add('active-marker');\n      }\n    }\n  }\n\n  // Add event listener with safety checks\n  if (player && typeof player.addEventListener === 'function') {\n    player.addEventListener('marker', function(event) {\n      try {\n        const { index, time, label } = event;\n        console.log(`marker! ${index} - ${time} - ${label}`);\n        \n        // Speak the marker label and show toast\n        speakText(label, time);\n        \n        // Highlight the corresponding timestamp in the transcript\n        highlightTimestamp(time);\n      } catch (error) {\n        console.error('Error in marker event handler:', error);\n      }\n    });\n  }\n});",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.8822894570354658,
      "diff_lines": 242,
      "added_lines": 19,
      "removed_lines": 81,
      "diff_preview": "--- expected+++ predicted@@ -42,13 +42,12 @@             const minutes = parseInt(match[1], 10);\n             const seconds = parseInt(match[2], 10);\n             const timeInSeconds = minutes * 60 + seconds;\n-            const formattedTime = `${minutes}:${seconds.toString().padStart(2, '0')}`;\n             const message = match[3].trim();\n             \n             // Create link for the timestamp\n             const timeLink = document.createElement('a');\n             timeLink.href = '#';\n-            timeLink.textContent = formattedTime;\n+            timeLink.textContent = `${minutes}:${seconds.toString().padStart(2, '0')}`;\n             timeLink.className = 'timestamp-link';\n             timeLink.dataset.time = timeInSeconds;\n             timeLink.dataset.message = message;\n@@ -62,10 +61,7 @@                 \n                 // Also trigger toast and speech\n                 showToast(message);\n-                speakText(message, timeInSeconds);\n-                \n-                // Highlight this timestamp\n-                highlightTimestamp(timeInSeconds);\n+                speakText(message);\n               }\n             });\n             \n@@ -90,7 +86,7 @@                   \n                   // Also trigger toast and speech\n                   showToast(message);\n-                  speakText(message, timeInSeconds);\n+                  speakText(message);\n                   \n                   // Highlight this timestamp\n                   highlightTimestamp(timeInSeconds);\n@@ -109,12 +105,14 @@     return markers;\n   }\n \n+  const url = \"https://gist.githubusercontent.com/paul-gauthier/3011ab9455c2d28c0e5a60947202752f/raw/5a5b3dbf68a9c2b22b4954af287efedecdf79d52/tmp.redacted.cast\";\n+  \n   // Parse transcript and create markers\n   const markers = parseTranscript();\n   \n   // Create player with a single call\n   player = AsciinemaPlayer.create(\n-    recording_url,\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 67.50160717964172,
    "repo": "aider",
    "language": "JavaScript",
    "prompt_tokens": 10768,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_benchmark_over_time.py",
    "exact_match": false,
    "similarity": 0.9331919406150583,
    "diff_lines": 113,
    "predicted_output": "import matplotlib.pyplot as plt\nimport yaml\nfrom imgcat import imgcat\nfrom matplotlib import rc\nfrom dataclasses import dataclass\nfrom datetime import date\nfrom typing import Dict, List\n\n@dataclass\nclass ModelData:\n    name: str\n    release_date: date\n    pass_rate: float\n\n    @property\n    def color(self) -> str:\n        model = self.name.lower()\n        if \"gemini\" in model and \"pro\" in model:\n            return \"magenta\"\n        if \"qwen\" in model:\n            return \"darkblue\"\n        if \"mistral\" in model:\n            return \"cyan\"\n        if \"haiku\" in model:\n            return \"pink\"\n        if \"deepseek\" in model:\n            return \"brown\"\n        if \"sonnet\" in model:\n            return \"orange\"\n        if \"-4o\" in model:\n            return \"purple\"\n        if \"gpt-4\" in model:\n            return \"red\"\n        if \"gpt-3.5\" in model:\n            return \"green\"\n        return \"lightblue\"\n\n    @property\n    def legend_label(self) -> str:\n        model = self.name.lower()\n        if \"claude-3-sonnet\" in model:\n            return \"Sonnet\"\n        if \"o1-preview\" in model:\n            return \"O1 Preview\"\n        if \"gpt-3.5\" in model:\n            return \"GPT-3.5 Turbo\"\n        if \"gpt-4-\" in model and \"-4o\" not in model:\n            return \"GPT-4\"\n        if \"qwen\" in model:\n            return \"Qwen\"\n        if \"-4o\" in model:\n            return \"GPT-4o\"\n        if \"haiku\" in model:\n            return \"Haiku\"\n        if \"deepseek\" in model:\n            return \"DeepSeek\"\n        if \"mistral\" in model:\n            return \"Mistral\"\n        if \"gemini\" in model and \"pro\" in model:\n            return \"Gemini 1.5 Pro\"\n        return model\n\nclass BenchmarkPlotter:\n    LABEL_FONT_SIZE = 16\n\n    def __init__(self):\n        self.setup_plot_style()\n\n    def setup_plot_style(self):\n        plt.rcParams[\"hatch.linewidth\"] = 0.5\n        plt.rcParams[\"hatch.color\"] = \"#444444\"\n        rc(\"font\", **{\"family\": \"sans-serif\", \"sans-serif\": [\"Helvetica\"], \"size\": 10})\n        plt.rcParams[\"text.color\"] = \"#444444\"\n\n    def load_data(self, yaml_file: str) -> List[ModelData]:\n        with open(yaml_file, \"r\") as file:\n            data = yaml.safe_load(file)\n\n        models = []\n        for entry in data:\n            if \"released\" in entry and \"pass_rate_2\" in entry:\n                model = ModelData(\n                    name=entry[\"model\"].split(\"(\")[0].strip(),\n                    release_date=entry[\"released\"],\n                    pass_rate=entry[\"pass_rate_2\"],\n                )\n                models.append(model)\n        return models\n\n    def create_figure(self):\n        fig, ax = plt.subplots(figsize=(12, 8))\n        ax.grid(axis=\"y\", zorder=0, lw=0.2)\n        for spine in ax.spines.values():\n            spine.set_edgecolor(\"#DDDDDD\")\n            spine.set_linewidth(0.5)\n        return fig, ax\n\n    def plot_model_series(self, ax, models):\n        color_groups = {}\n        for model in models:\n            if model.color not in color_groups:\n                color_groups[model.color] = []\n            color_groups[model.color].append(model)\n\n        for color, group in color_groups.items():\n            sorted_group = sorted(group, key=lambda x: x.release_date)\n            dates = [m.release_date for m in sorted_group]\n            rates = [m.pass_rate for m in sorted_group]\n\n            ax.plot(dates, rates, c=color, alpha=0.5, linewidth=1)\n            ax.scatter(dates, rates, c=color, alpha=0.5, s=120)\n\n            first_model = sorted_group[0]\n            ax.annotate(\n                first_model.legend_label,\n                (first_model.release_date, first_model.pass_rate),\n                xytext=(10, 5),\n                textcoords=\"offset points\",\n                color=color,\n                alpha=0.8,\n                fontsize=self.LABEL_FONT_SIZE,\n            )\n\n    def set_labels_and_style(self, ax):\n        ax.set_xlabel(\"Model release date\", fontsize=18, color=\"#555\")\n        ax.set_ylabel(\n            \"Aider code editing benchmark,\\npercent completed correctly\", fontsize=18, color=\"#555\"\n        )\n        ax.set_title(\"LLM code editing skill by model release date\", fontsize=20)\n        ax.set_ylim(30, 90)\n        plt.xticks(fontsize=14, rotation=45, ha=\"right\")\n        plt.tight_layout(pad=1.0)\n\n    def save_and_display(self, fig):\n        plt.savefig(\"aider/website/assets/models-over-time.png\")\n        plt.savefig(\"aider/website/assets/models-over-time.svg\")\n        imgcat(fig)\n\n    def plot(self, yaml_file: str):\n        models = self.load_data(yaml_file)\n        fig, ax = self.create_figure()\n        self.plot_model_series(ax, models)\n        self.set_labels_and_style(ax)\n        self.save_and_display(fig)\n\ndef main():\n    plotter = BenchmarkPlotter()\n    models = plotter.load_data(\"aider/website/_data/edit_leaderboard.yml\")\n\n    for model in sorted(models, key=lambda x: x.release_date):\n        print(f\"{model.release_date}: {model.name}\")\n\n    plotter.plot(\"aider/website/_data/edit_leaderboard.yml\")\n\nif __name__ == \"__main__\":\n    main()",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9331919406150583,
      "diff_lines": 113,
      "added_lines": 10,
      "removed_lines": 22,
      "diff_preview": "--- expected+++ predicted@@ -1,12 +1,10 @@-from dataclasses import dataclass\n-from datetime import date\n-from typing import Dict, List, Tuple\n-\n import matplotlib.pyplot as plt\n import yaml\n from imgcat import imgcat\n from matplotlib import rc\n-\n+from dataclasses import dataclass\n+from datetime import date\n+from typing import Dict, List\n \n @dataclass\n class ModelData:\n@@ -40,8 +38,6 @@     @property\n     def legend_label(self) -> str:\n         model = self.name.lower()\n-        if \"gemini\" in model and \"pro\" in model:\n-            return \"Gemini 1.5 Pro\"\n         if \"claude-3-sonnet\" in model:\n             return \"Sonnet\"\n         if \"o1-preview\" in model:\n@@ -60,8 +56,9 @@             return \"DeepSeek\"\n         if \"mistral\" in model:\n             return \"Mistral\"\n+        if \"gemini\" in model and \"pro\" in model:\n+            return \"Gemini 1.5 Pro\"\n         return model\n-\n \n class BenchmarkPlotter:\n     LABEL_FONT_SIZE = 16\n@@ -90,7 +87,7 @@                 models.append(model)\n         return models\n \n-    def create_figure(self) -> Tuple[plt.Figure, plt.Axes]:\n+    def create_figure(self):\n         fig, ax = plt.subplots(figsize=(12, 8))\n         ax.grid(axis=\"y\", zorder=0, lw=0.2)\n         for spine in ax.spines.values():\n@@ -98,27 +95,21 @@             spine.set_linewidth(0.5)\n         return fig, ax\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 34.30988597869873,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 35454,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_benchmark_problem_stats.py",
    "exact_match": false,
    "similarity": 0.7662883754577926,
    "diff_lines": 277,
    "predicted_output": "#!/usr/bin/env python\n\nimport argparse\nimport json\nfrom collections import defaultdict\nfrom pathlib import Path\nimport yaml\n\nHARD_SET_NUM = 3  # Number of models that defines the hard set threshold\n\ndef get_dirs_from_leaderboard():\n    # Load the leaderboard data\n    with open(\"aider/website/_data/polyglot_leaderboard.yml\") as f:\n        leaderboard = yaml.safe_load(f)\n    return [(entry[\"dirname\"], entry[\"model\"]) for entry in leaderboard]\n\ndef load_results(dirname):\n    \"\"\"Load all result files from a benchmark directory\"\"\"\n    dirname = Path(dirname)\n    benchmark_dir = dirname\n    if not benchmark_dir.exists():\n        benchmark_dir = Path(\"tmp.benchmarks\") / dirname\n        if not benchmark_dir.exists():\n            return None\n\n    all_results = []\n    parse_errors = []  # Track which exercises had parse errors for this model\n    \n    # Look in language subdirectories under exercises/practice\n    for fname in benchmark_dir.glob(\"*/exercises/practice/*/.aider.results.json\"):\n        error = False\n        try:\n            results = json.loads(fname.read_text())\n            error = \"testcase\" not in results\n            if not error:\n                # Add language info to results\n                lang = fname.parts[-5]  # Get language from path\n                results[\"language\"] = lang\n                all_results.append(results)\n\n        except json.JSONDecodeError:\n            error = True\n\n        if error:\n            # Track the parse error for this exercise/model combination\n            lang = fname.parts[-5]\n            exercise = f\"{fname.parts[-2]}/{lang}\"  # Use directory name as testcase\n            parse_errors.append(exercise)\n            print(f\"Bad results file {fname}\")\n            continue\n\n    return all_results, parse_errors\n\ndef analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):\n    PARSE_ERROR_M = 4  # Threshold for number of parse errors to DQ an exercise\n\n    if dirs is None:\n        # Use leaderboard data if no directories specified\n        dir_entries = get_dirs_from_leaderboard()\n    else:\n        # Use provided directories, with dirname as model name\n        dir_entries = [(d, d) for d in dirs]\n\n    # Filter out entries that don't load and sort by pass rate\n    valid_entries = []\n    parse_errors_by_model = {}  # Track which exercises had parse errors for each model\n    \n    for dirname, model in dir_entries:\n        results_data = load_results(dirname)\n        if results_data:\n            results, model_parse_errors = results_data\n            parse_errors_by_model[model] = set(model_parse_errors)\n            # Calculate pass rate for sorting when using custom dirs\n            if dirs is not None:\n                pass_rate = sum(\n                    1 for r in results if r.get(\"tests_outcomes\", []) and r[\"tests_outcomes\"][-1]\n                ) / len(results)\n            else:\n                # Use existing pass rate from leaderboard\n                pass_rate = next(\n                    (\n                        entry[\"pass_rate_2\"]\n                        for entry in yaml.safe_load(\n                            open(\"aider/website/_data/polyglot_leaderboard.yml\")\n                        )\n                        if entry[\"dirname\"] == dirname\n                    ),\n                    0,\n                )\n            valid_entries.append((dirname, model, results, pass_rate))\n\n    # Count parse errors per exercise\n    parse_error_counts = defaultdict(int)\n    for model_errors in parse_errors_by_model.values():\n        for exercise in model_errors:\n            parse_error_counts[exercise] += 1\n\n    # Find exercises to disqualify based on parse error threshold\n    disqualified_exercises = {\n        exercise for exercise, count in parse_error_counts.items() if count >= PARSE_ERROR_M\n    }\n\n    if disqualified_exercises:\n        print(f\"\\nDisqualified {len(disqualified_exercises)} exercises with {PARSE_ERROR_M}+ parse errors:\")\n        for ex in sorted(disqualified_exercises):\n            print(f\"  {ex} ({parse_error_counts[ex]} parse errors)\")\n\n    # Collect the hard set (exercises solved by HARD_SET_NUM or fewer models)\n    print(f\"\\nHard Set Analysis (exercises solved by \u2264{HARD_SET_NUM} models):\")\n    print(\"-\" * 60)\n    hard_set = {\n        ex for ex, models in exercise_solutions.items()\n        if len(models) <= HARD_SET_NUM and ex not in disqualified_exercises\n    }\n    print(f\"Total hard set exercises: {len(hard_set)}\")\n\n    # Count total problems, unsolved problems, and hard set problems by language\n    lang_totals = defaultdict(int)\n    lang_unsolved = defaultdict(int)\n    lang_hard_set = defaultdict(int)\n\n    for exercise in all_exercises:\n        lang = exercise.split(\"/\")[1]  # Get language from path\n        lang_totals[lang] += 1\n        if not exercise_solutions[exercise]:  # No models solved this exercise\n            lang_unsolved[lang] += 1\n        if exercise in hard_set:  # Exercise is in the hard set\n            lang_hard_set[lang] += 1\n\n    print(\"\\nUnsolved and hard set problems by language:\")\n    print(f\"{'Language':<12} {'Unsolved':>8} {'Hard Set':>9} {'Total':>7} {'%hardUnsolved':>8}\")\n    print(\"-\" * 47)\n    for lang in sorted(lang_totals.keys()):\n        count = lang_unsolved[lang]\n        hard = lang_hard_set[lang]\n        total = lang_totals[lang]\n        pct = (count / hard) * 100 if hard else -1\n        print(f\"{lang:<12} {count:>8} {hard:>9} {total:>7} {pct:>7.1f}%\")\n    print()\n\n    # For each model, compute performance on hard set\n    model_hard_stats = []\n    for (dirname, model), results, _ in valid_entries:\n        if not results:\n            continue\n        \n        solved_hard = 0\n        for result in results:\n            testcase = result.get(\"testcase\")\n            if not testcase:\n                continue\n            lang = result.get(\"language\")\n            if not lang:\n                continue\n            \n            testcase = f\"{testcase}/{lang}\"\n            if testcase in hard_set:\n                tests_outcomes = result.get(\"tests_outcomes\", [])\n                if tests_outcomes and tests_outcomes[-1]:\n                    solved_hard += 1\n        \n        pct = (solved_hard / len(hard_set)) * 100\n        model_hard_stats.append((model, solved_hard, pct))\n\n    # Sort by number solved\n    model_hard_stats.sort(key=lambda x: x[1], reverse=True)\n    \n    print(\"\\nModel performance on hard set:\")\n    print(f\"{'Model':<55} {'Solved':<8} {'Percent':>7}\")\n    print(\"-\" * 50)\n    for model, solved, pct in model_hard_stats:\n        print(f\"{model:<55} {solved:>6d}   {pct:>6.1f}%\")\n\n    if copy_hard_set:\n        # Create hard set directory\n        src_dir = Path(\"tmp.benchmarks/exercism\")\n        dst_dir = Path(\"tmp.benchmarks/exercism-polyglot\")\n\n        if dst_dir.exists():\n            print(f\"\\nError: Destination directory {dst_dir} already exists\")\n            return\n\n        print(f\"\\nCopying hard set problems to {dst_dir}...\")\n        # Create a set of (exercise, language) pairs from hard_set\n        hard_set_pairs = {tuple(exercise.split(\"/\")) for exercise in hard_set}\n\n        # Copy each hard set problem's directory\n        copied_by_lang = defaultdict(int)\n        for lang_dir in src_dir.glob(\"*/exercises/practice\"):\n            if not lang_dir.is_dir():\n                continue\n            \n            lang = lang_dir.parts[-3]  # Get language from path\n            for problem_dir in lang_dir.glob(\"*\"):\n                if (problem_dir.name, lang) in hard_set_pairs:\n                    rel_path = problem_dir.relative_to(src_dir)\n                    dst_path = dst_dir / rel_path\n                    dst_path.parent.mkdir(parents=True, exist_ok=True)\n                    shutil.copytree(problem_dir, dst_path)\n                    copied_by_lang[lang] += 1\n\n        total_copied = sum(copied_by_lang.values())\n        print(f\"\\nCopied {total_copied} hard set problems:\")\n        for lang in sorted(copied_by_lang):\n            print(f\"  {lang}: {copied_by_lang[lang]}\")\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--topn\", type=int, help=\"Only consider top N models by pass rate\")\n    parser.add_argument(\n        \"dirs\", nargs=\"*\", help=\"Directories to analyze (optional, defaults to leaderboard entries)\"\n    )\n    parser.add_argument(\n        \"--copy-hard-set\",\n        action=\"store_true\",\n        help=\"Copy hard set problems to tmp.benchmarks/exercism-polyglot\",\n    )\n    args = parser.parse_args()\n\n    analyze_exercise_solutions(args.dirs if args.dirs else None, args.topn, args.copy_hard_set)",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.7662883754577926,
      "diff_lines": 277,
      "added_lines": 11,
      "removed_lines": 146,
      "diff_preview": "--- expected+++ predicted@@ -2,16 +2,11 @@ \n import argparse\n import json\n-import shutil\n from collections import defaultdict\n from pathlib import Path\n-\n import yaml\n \n-from aider.dump import dump  # noqa\n-\n HARD_SET_NUM = 3  # Number of models that defines the hard set threshold\n-\n \n def get_dirs_from_leaderboard():\n     # Load the leaderboard data\n@@ -19,11 +14,9 @@         leaderboard = yaml.safe_load(f)\n     return [(entry[\"dirname\"], entry[\"model\"]) for entry in leaderboard]\n \n-\n def load_results(dirname):\n     \"\"\"Load all result files from a benchmark directory\"\"\"\n     dirname = Path(dirname)\n-\n     benchmark_dir = dirname\n     if not benchmark_dir.exists():\n         benchmark_dir = Path(\"tmp.benchmarks\") / dirname\n@@ -32,7 +25,7 @@ \n     all_results = []\n     parse_errors = []  # Track which exercises had parse errors for this model\n-\n+    \n     # Look in language subdirectories under exercises/practice\n     for fname in benchmark_dir.glob(\"*/exercises/practice/*/.aider.results.json\"):\n         error = False\n@@ -58,7 +51,6 @@ \n     return all_results, parse_errors\n \n-\n def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):\n     PARSE_ERROR_M = 4  # Threshold for number of parse errors to DQ an exercise\n \n@@ -72,12 +64,9 @@     # Filter out entries that don't load and sort by pass rate\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 43.40673494338989,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 29665,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_scripts_blame.py",
    "exact_match": false,
    "similarity": 0.8073099589261266,
    "diff_lines": 238,
    "predicted_output": "#!/usr/bin/env python3\n\nimport argparse\nimport subprocess\nimport sys\nfrom collections import defaultdict\nfrom datetime import datetime\nfrom operator import itemgetter\nimport semver\nimport yaml\nfrom tqdm import tqdm\n\nwebsite_files = [\n    \"aider/website/index.html\",\n    \"aider/website/share/index.md\",\n    \"aider/website/_includes/head_custom.html\",\n    \"aider/website/_includes/home.css\",\n    \"aider/website/docs/leaderboards/index.md\",\n]\n\nexclude_files = [\n    \"aider/website/install.ps1\",\n    \"aider/website/install.sh\",\n]\n\ndef blame(start_tag, end_tag=None):\n    commits = get_all_commit_hashes_between_tags(start_tag, end_tag)\n\n    authors = get_commit_authors(commits)\n\n    revision = end_tag if end_tag else \"HEAD\"\n    files = run([\"git\", \"ls-tree\", \"-r\", \"--name-only\", revision]).strip().split(\"\\n\")\n    files = [\n        f\n        for f in files\n        if f.endswith((\".js\", \".py\", \".scm\", \".sh\", \"Dockerfile\", \"Gemfile\"))\n        or (f.startswith(\".github/workflows/\") and f.endswith(\".yml\"))\n        or f in website_files\n        or f in test_files\n    ]\n    files = [f for f in files if not f.endswith(\"prompts.py\")]\n    files = [f for f in files if not f.startswith(\"tests/fixtures/watch\")]\n    files = [f for f in files if f not in exclude_files]\n\n    all_file_counts = {}\n    grand_total = defaultdict(int)\n    aider_total = 0\n    for file in files:\n        file_counts = get_counts_for_file(start_tag, end_tag, authors, file)\n        if file_counts:\n            all_file_counts[file] = file_counts\n            for author, count in file_counts.items():\n                grand_total[author] += count\n                if \"(aider)\" in author.lower():\n                    aider_total += count\n\n    total_lines = sum(grand_total.values())\n    aider_percentage = (aider_total / total_lines) * 100 if total_lines > 0 else 0\n\n    end_date = get_tag_date(end_tag if end_tag else \"HEAD\")\n\n    return all_file_counts, grand_total, total_lines, aider_total, aider_percentage, end_date\n\ndef get_all_commit_hashes_between_tags(start_tag, end_tag=None):\n    if end_tag:\n        res = run([\"git\", \"rev-list\", f\"{start_tag}..{end_tag}\"])\n    else:\n        res = run([\"git\", \"rev-list\", f\"{start_tag}..HEAD\"])\n\n    if res:\n        commit_hashes = res.strip().split(\"\\n\")\n        return commit_hashes\n\ndef run(cmd):\n    result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n    return result.stdout\n\ndef get_commit_authors(commits):\n    commit_to_author = dict()\n    for commit in commits:\n        author = run([\"git\", \"show\", \"-s\", \"--format=%an\", commit]).strip()\n        commit_message = run([\"git\", \"show\", \"-s\", \"--format=%s\", commit]).strip()\n        if commit_message.lower().startswith(\"aider:\"):\n            author += \" (aider)\"\n        commit_to_author[commit] = author\n    return commit_to_author\n\nhash_len = len(\"44e6fefc2\")\n\ndef process_all_tags_since(start_tag):\n    tags = get_all_tags_since(start_tag)\n    results = []\n    for i in tqdm(range(len(tags) - 1), desc=\"Processing tags\"):\n        start_tag, end_tag = tags[i], tags[i + 1]\n        all_file_counts, grand_total, total_lines, aider_total, aider_percentage, end_date = blame(start_tag, end_tag)\n        results.append(\n            {\n                \"start_tag\": start_tag,\n                \"end_tag\": end_tag,\n                \"end_date\": end_date.strftime(\"%Y-%m-%d\"),\n                \"file_counts\": all_file_counts,\n                \"grand_total\": {\n                    author: count\n                    for author, count in sorted(grand_total.items(), key=itemgetter(1), reverse=True)\n                },\n                \"total_lines\": total_lines,\n                \"aider_total\": aider_total,\n                \"aider_percentage\": round(aider_percentage, 2),\n            }\n        )\n    return results\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Get aider/non-aider blame stats\")\n    parser.add_argument(\"start_tag\", help=\"The tag to start from\", nargs=\"?\")\n    parser.add_argument(\"--end-tag\", help=\"The tag to end at (default: HEAD)\", default=None)\n    parser.add_argument(\n        \"--all-since\",\n        action=\"store_true\",\n        help=\"Find all tags since the specified tag and print aider percentage between each pair of successive tags\"\n    )\n    parser.add_argument(\n        \"--output\",\n        help=\"Output file to save the YAML results\",\n        type=str,\n        default=None\n    )\n    args = parser.parse_args()\n\n    if not args.start_tag:\n        args.start_tag = get_latest_version_tag()\n        if not args.start_tag:\n            print(\"Error: No valid vX.Y.0 tag found.\")\n            return\n\n    if args.all_since:\n        results = process_all_tags_since(args.start_tag)\n        yaml_output = yaml.dump(results, sort_keys=True)\n    else:\n        all_file_counts, grand_total, total_lines, aider_total, aider_percentage, end_date = blame(\n            args.start_tag, args.end_tag\n        )\n\n        result = {\n            \"start_tag\": args.start_tag,\n            \"end_tag\": args.end_tag or \"HEAD\",\n            \"end_date\": end_date.strftime(\"%Y-%m-%d\"),\n            \"file_counts\": all_file_counts,\n            \"grand_total\": {\n                author: count\n                for author, count in sorted(grand_total.items(), key=itemgetter(1), reverse=True)\n            },\n            \"total_lines\": total_lines,\n            \"aider_total\": aider_total,\n            \"aider_percentage\": round(aider_percentage, 2),\n        }\n\n        yaml_output = yaml.dump(result, sort_keys=True)\n\n    if args.output:\n        with open(args.output, \"w\") as f:\n            f.write(yaml_output)\n    else:\n        print(yaml_output)\n\ndef get_counts_for_file(start_tag, end_tag, authors, fname):\n    try:\n        if end_tag:\n            text = run([\"git\", \"blame\", f\"{start_tag}..{end_tag}\", \"--\", fname])\n        else:\n            text = run([\"git\", \"blame\", f\"{start_tag}..HEAD\", \"--\", fname])\n        if not text:\n            return None\n        text = text.splitlines()\n        line_counts = defaultdict(int)\n        for line in text:\n            if line.startswith(\"^\"):\n                continue\n            hsh = line[:hash_len]\n            author = authors.get(hsh, \"Unknown\")\n            line_counts[author] += 1\n\n        return dict(line_counts)\n    except subprocess.CalledProcessError:\n        return None\n\ndef get_all_tags_since(start_tag):\n    all_tags = run([\"git\", \"tag\", \"--sort=v:refname\"]).strip().split(\"\\n\")\n    start_version = semver.Version.parse(start_tag[1:])  # Remove 'v' prefix\n    filtered_tags = [\n        tag\n        for tag in all_tags\n        if semver.Version.is_valid(tag[1:]) and semver.Version.parse(tag[1:]) >= start_version\n    ]\n    return [tag for tag in filtered_tags if tag.endswith(\".0\")]\n\ndef get_tag_date(tag):\n    date_str = run([\"git\", \"log\", \"-1\", \"--format=%ai\", tag]).strip()\n    return datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S %z\")\n\nif __name__ == \"__main__\":\n    main()",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.8073099589261266,
      "diff_lines": 238,
      "added_lines": 14,
      "removed_lines": 103,
      "diff_preview": "--- expected+++ predicted@@ -1,13 +1,11 @@ #!/usr/bin/env python3\n \n import argparse\n-import os\n import subprocess\n import sys\n from collections import defaultdict\n from datetime import datetime\n from operator import itemgetter\n-\n import semver\n import yaml\n from tqdm import tqdm\n@@ -25,22 +23,18 @@     \"aider/website/install.sh\",\n ]\n \n-\n def blame(start_tag, end_tag=None):\n     commits = get_all_commit_hashes_between_tags(start_tag, end_tag)\n-    commits = [commit[:hash_len] for commit in commits]\n \n     authors = get_commit_authors(commits)\n \n     revision = end_tag if end_tag else \"HEAD\"\n     files = run([\"git\", \"ls-tree\", \"-r\", \"--name-only\", revision]).strip().split(\"\\n\")\n-    test_files = [f for f in files if f.startswith(\"tests/fixtures/languages/\") and \"/test.\" in f]\n     files = [\n         f\n         for f in files\n         if f.endswith((\".js\", \".py\", \".scm\", \".sh\", \"Dockerfile\", \"Gemfile\"))\n         or (f.startswith(\".github/workflows/\") and f.endswith(\".yml\"))\n-        or (f.startswith(\"aider/resources/\") and f.endswith(\".yml\"))\n         or f in website_files\n         or f in test_files\n     ]\n@@ -67,7 +61,6 @@ \n     return all_file_counts, grand_total, total_lines, aider_total, aider_percentage, end_date\n \n-\n def get_all_commit_hashes_between_tags(start_tag, end_tag=None):\n     if end_tag:\n         res = run([\"git\", \"rev-list\", f\"{start_tag}..{end_tag}\"])\n@@ -78,12 +71,9 @@         commit_hashes = res.strip().split(\"\\n\")\n         return commit_hashes\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 38.9901659488678,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 47383,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_scripts_issues.py",
    "exact_match": false,
    "similarity": 0.922737691124844,
    "diff_lines": 358,
    "predicted_output": "#!/usr/bin/env python3\n\nimport os\nimport re\nfrom collections import defaultdict\nfrom datetime import datetime\nfrom dotenv import load_dotenv\nimport requests\nfrom tqdm import tqdm\n\ndef has_been_reopened(issue_number):\n    timeline_url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue_number}/timeline\"\n    response = requests.get(timeline_url, headers=headers)\n    response.raise_for_status()\n    events = response.json()\n    return any(event[\"event\"] == \"reopened\" for event in events if \"event\" in event)\n\n# Load environment variables from .env file\nload_dotenv()\n\nBOT_SUFFIX = \"\"\"\nNote: A [bot script](https://github.com/Aider-AI/aider/blob/main/scripts/issues.py) made these updates to the issue.\n\"\"\"  # noqa\n\nDUPLICATE_COMMENT = (\n    \"\"\"Thanks for trying aider and filing this issue.\n\nThis looks like a duplicate of #{oldest_issue_number}. Please see the comments there for more information, and feel free to continue the discussion within that issue.\n\nI'm going to close this issue for now. But please let me know if you think this is actually a distinct issue and I will reopen this issue.\"\"\"  # noqa\n    + BOT_SUFFIX\n)\n\nSTALE_COMMENT = (\n    \"\"\"I'm labeling this issue as stale because it has been open for 2 weeks with no activity. If there are no additional comments, I will close this in 7 days.\"\"\"  # noqa\n    + BOT_SUFFIX\n)\n\nCLOSE_STALE_COMMENT = (\n    \"\"\"I'm closing this issue because it has been stalled for 3 weeks with no activity. Feel free to add a comment here and we can re-open this issue or create a new one.\"\"\"  # noqa\n    + BOT_SUFFIX\n)\n\nCLOSE_FIXED_ENHANCEMENT_COMMENT = (\n    \"\"\"I'm closing this enhancement request since it has been marked as 'fixed' for over 3 weeks. The requested feature should now be available in recent versions of aider.\n\nIf you find that this enhancement is still needed, please feel free to reopen this issue or create a new one.\"\"\"  # noqa\n    + BOT_SUFFIX\n)\n\nCLOSE_FIXED_BUG_COMMENT = (\n    \"\"\"I'm closing this bug report since it has been marked as 'fixed' for over 3 weeks. This issue should be resolved in recent versions of aider.\n\nIf you find that this bug is still present, please feel free to reopen this issue or create a new one with steps to reproduce.\"\"\"  # noqa\n    + BOT_SUFFIX\n)\n\n# GitHub API configuration\nGITHUB_API_URL = \"https://api.github.com\"\nREPO_OWNER = \"Aider-AI\"\nREPO_NAME = \"aider\"\nTOKEN = os.getenv(\"GITHUB_TOKEN\")\n\nheaders = {\"Authorization\": f\"token {TOKEN}\", \"Accept\": \"application/vnd.github.v3+json\"}\n\ndef get_issues(state=\"open\"):\n    issues = []\n    page = 1\n    per_page = 100\n\n    # First, get the total count of issues\n    response = requests.get(\n        f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues\",\n        headers=headers,\n        params={\"state\": state, \"per_page\": 1},\n    )\n    response.raise_for_status()\n    total_count = int(response.headers.get(\"Link\", \"\").split(\"page=\")[-1].split(\">\")[0])\n    total_pages = (total_count + per_page - 1) // per_page\n\n    with tqdm(total=total_pages, desc=\"Collecting issues\", unit=\"page\") as pbar:\n        while True:\n            response = requests.get(\n                f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues\",\n                headers=headers,\n                params={\"state\": state, \"page\": page, \"per_page\": per_page},\n            )\n            response.raise_for_status()\n            page_issues = response.json()\n            if not page_issues:\n                break\n            issues.extend(page_issues)\n            page += 1\n            pbar.update(1)\n    return issues\n\ndef group_issues_by_subject(issues):\n    grouped_issues = defaultdict(list)\n    pattern = r\"Uncaught .+ in .+ line \\d+\"\n    for issue in issues:\n        if re.search(pattern, issue[\"title\"]) and not has_been_reopened(issue[\"number\"]):\n            subject = issue[\"title\"]\n            grouped_issues[subject].append(issue)\n    return grouped_issues\n\ndef find_oldest_issue(subject, all_issues):\n    oldest_issue = None\n    oldest_date = datetime.now()\n\n    for issue in all_issues:\n        if issue[\"title\"] == subject and not has_been_reopened(issue[\"number\"]):\n            created_at = datetime.strptime(issue[\"created_at\"], \"%Y-%m-%dT%H:%M:%SZ\")\n            if created_at < oldest_date:\n                oldest_date = created_at\n                oldest_issue = issue\n\n    return oldest_issue\n\ndef comment_and_close_duplicate(issue, oldest_issue):\n    # Skip if issue is labeled as priority\n    if \"priority\" in [label[\"name\"] for label in issue[\"labels\"]]:\n        print(f\"  - Skipping priority issue #{issue['number']}\")\n        return\n\n    comment_url = (\n        f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}/comments\"\n    )\n    close_url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}\"\n    \n    comment_body = DUPLICATE_COMMENT.format(oldest_issue_number=oldest_issue['number'])\n    \n    # Post comment\n    response = requests.post(comment_url, headers=headers, json={\"body\": comment_body})\n    response.raise_for_status()\n    \n    # Close issue\n    response = requests.patch(close_url, headers=headers, json={\"state\": \"closed\"})\n    response.raise_for_status()\n    \n    print(f\"  - Commented and closed issue #{issue['number']}\")\n\ndef find_unlabeled_with_paul_comments(issues):\n    unlabeled_issues = []\n    for issue in issues:\n        # Skip pull requests\n        if \"pull_request\" in issue:\n            continue\n            \n        if not issue[\"labels\"] and issue[\"state\"] == \"open\":\n            # Get comments for this issue\n            comments_url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}/comments\"\n            response = requests.get(comments_url, headers=headers)\n            response.raise_for_status()\n            comments = response.json()\n            \n            # Check if paul-gauthier has commented\n            if any(comment[\"user\"][\"login\"] == \"paul-gauthier\" for comment in comments):\n                unlabeled_issues.append(issue)\n    return unlabeled_issues\n\ndef handle_unlabeled_issues(all_issues, auto_yes):\n    print(\"\\nFinding unlabeled issues with paul-gauthier comments...\")\n    unlabeled_issues = [\n        issue for issue in find_unlabeled_with_paul_comments(all_issues)\n        if \"priority\" not in [label[\"name\"] for label in issue[\"labels\"]]\n    ]\n\n    if not unlabeled_issues:\n        print(\"No unlabeled issues with paul-gauthier comments found.\")\n        return\n\n    print(f\"\\nFound {len(unlabeled_issues)} unlabeled issues with paul-gauthier comments:\")\n    for issue in unlabeled_issues:\n        print(f\"  - #{issue['number']}: {issue['title']} {issue['html_url']}\")\n\n    if not auto_yes:\n        confirm = input(\"\\nDo you want to add the 'question' label to these issues? (y/n): \")\n        if confirm.lower() != \"y\":\n            print(\"Skipping labeling.\")\n            return\n\n    print(\"\\nAdding 'question' label to issues...\")\n    for issue in unlabeled_issues:\n        url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}\"\n        response = requests.patch(url, headers=headers, json={\"labels\": [\"question\"]})\n        response.raise_for_status()\n        print(f\"  - Added 'question' label to #{issue['number']}\")\n\ndef handle_stale_issues(all_issues, auto_yes):\n    print(\"\\nChecking for stale question issues...\")\n    \n    for issue in all_issues:\n        # Skip if not open, not a question, already stale, or has been reopened\n        labels = [label[\"name\"] for label in issue[\"labels\"]]\n        if (\n            issue[\"state\"] != \"open\"\n            or \"question\" not in labels\n            or \"stale\" in labels\n            or \"priority\" in labels\n            or has_been_reopened(issue[\"number\"])\n        ):\n            continue\n\n        # Get latest activity timestamp from issue or its comments\n        latest_activity = datetime.strptime(issue[\"updated_at\"], \"%Y-%m-%dT%H:%M:%SZ\")\n        \n        # Check if issue is stale (no activity for 14 days)\n        days_inactive = (datetime.now() - latest_activity).days\n        if days_inactive >= 14:\n            print(f\"\\nStale issue found: #{issue['number']}: {issue['title']}\")\n            print(f\"  No activity for {days_inactive} days\")\n            \n            if not auto_yes:\n                confirm = input(\"Add stale label and comment? (y/n): \")\n                if confirm.lower() != \"y\":\n                    print(\"Skipping this issue.\")\n                    continue\n\n            # Add stale label\n            url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}\"\n            response = requests.patch(url, headers=headers, json={\"labels\": [\"question\", \"stale\"]})\n            response.raise_for_status()\n\n            # Add comment\n            comment_url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}/comments\"\n            response = requests.post(comment_url, headers=headers, json={\"body\": STALE_COMMENT})\n            response.raise_for_status()\n            \n            print(f\"  Added stale label and comment to #{issue['number']}\")\n\ndef handle_stale_closing(all_issues, auto_yes):\n    print(\"\\nChecking for issues to close or unstale...\")\n\n    for issue in all_issues:\n        # Skip if not open, not stale, or is priority\n        labels = [label[\"name\"] for label in issue[\"labels\"]]\n        if issue[\"state\"] != \"open\" or \"stale\" not in labels or \"priority\" in labels:\n            continue\n\n        # Get the timeline to find when the stale label was last added\n        timeline_url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}/timeline\"\n        response = requests.get(timeline_url, headers=headers)\n        response.raise_for_status()\n        events = response.json()\n\n        # Find the most recent stale label addition\n        stale_events = [\n            event\n            for event in events\n            if event.get(\"event\") == \"labeled\" and event.get(\"label\", {}).get(\"name\") == \"stale\"\n        ]\n        \n        if not stale_events:\n            continue\n\n        latest_stale = datetime.strptime(stale_events[-1][\"created_at\"], \"%Y-%m-%dT%H:%M:%SZ\")\n\n        # Get comments since the stale label\n        comments_url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}/comments\"\n        response = requests.get(comments_url, headers=headers)\n        response.raise_for_status()\n        comments = response.json()\n\n        # Check for comments newer than the stale label\n        new_comments = [\n            comment for comment in comments\n            if datetime.strptime(comment[\"created_at\"], \"%Y-%m-%dT%H:%M:%SZ\") > latest_stale\n        ]\n\n        if new_comments:\n            print(f\"\\nFound new activity on stale issue #{issue['number']}: {issue['title']}\")\n            print(f\"  {len(new_comments)} new comments since stale label\")\n            \n            if not auto_yes:\n                confirm = input(\"Remove stale label? (y/n): \")\n                if confirm.lower() != \"y\":\n                    print(\"Skipping this issue.\")\n                    continue\n\n            # Remove stale label but keep question label\n            url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}\"\n            response = requests.patch(url, headers=headers, json={\"labels\": [\"question\"]})\n            response.raise_for_status()\n            print(f\"  Removed stale label from #{issue['number']}\")\n        else:\n            # Check if it's been 7 days since stale label\n            days_stale = (datetime.now() - latest_stale).days\n            if days_stale >= 7:\n                print(f\"\\nStale issue ready for closing #{issue['number']}: {issue['title']}\")\n                print(f\"  No activity for {days_stale} days since stale label\")\n\n                if not auto_yes:\n                    confirm = input(\"Close this issue? (y/n): \")\n                    if confirm.lower() != \"y\":\n                        print(\"Skipping this issue.\")\n                        continue\n\n                # Add closing comment\n                comment_url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}/comments\"\n                response = requests.post(comment_url, headers=headers, json={\"body\": CLOSE_STALE_COMMENT})\n                response.raise_for_status()\n\n                # Close the issue\n                url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}\"\n                response = requests.patch(url, headers=headers, json={\"state\": \"closed\"})\n                response.raise_for_status()\n                print(f\"  Closed issue #{issue['number']}\")\n\ndef handle_fixed_issues(all_issues, auto_yes):\n    print(\"\\nChecking for fixed enhancement and bug issues to close...\")\n\n    for issue in all_issues:\n        # Skip if not open, doesn't have fixed label, or is priority\n        labels = [label[\"name\"] for label in issue[\"labels\"]]\n        if issue[\"state\"] != \"open\" or \"fixed\" not in labels or \"priority\" in labels:\n            continue\n\n        # Check if it's an enhancement or bug\n        is_enhancement = \"enhancement\" in labels\n        is_bug = \"bug\" in labels\n        if not (is_enhancement or is_bug):\n            continue\n\n        # Find when the fixed label was added\n        timeline_url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}/timeline\"\n        response = requests.get(timeline_url, headers=headers)\n        response.raise_for_status()\n        events = response.json()\n\n        # Find the most recent fixed label addition\n        fixed_events = [\n            event\n            for event in events\n            if event.get(\"event\") == \"labeled\" and event.get(\"label\", {}).get(\"name\") == \"fixed\"\n        ]\n\n        if not fixed_events:\n            continue\n\n        latest_fixed = datetime.strptime(fixed_events[-1][\"created_at\"], \"%Y-%m-%dT%H:%M:%SZ\")\n        days_fixed = (datetime.now() - latest_fixed).days\n\n        if days_fixed >= 21:\n            issue_type = \"enhancement\" if is_enhancement else \"bug\"\n            print(f\"\\nFixed {issue_type} ready for closing #{issue['number']}: {issue['title']}\")\n            print(f\"  Has been marked fixed for {days_fixed} days\")\n\n            if not auto_yes:\n                confirm = input(\"Close this issue? (y/n): \")\n                if confirm.lower() != \"y\":\n                    print(\"Skipping this issue.\")\n                    continue\n\n            # Add closing comment\n            comment_url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}/comments\"\n            comment = CLOSE_FIXED_ENHANCEMENT_COMMENT if is_enhancement else CLOSE_FIXED_BUG_COMMENT\n            response = requests.post(comment_url, headers=headers, json={\"body\": comment})\n            response.raise_for_status()\n\n            # Close the issue\n            url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}\"\n            response = requests.patch(url, headers=headers, json={\"state\": \"closed\"})\n            response.raise_for_status()\n            print(f\"  Closed issue #{issue['number']}\")\n\ndef handle_duplicate_issues(all_issues, auto_yes):\n    open_issues = [issue for issue in all_issues if issue[\"state\"] == \"open\"]\n    grouped_open_issues = group_issues_by_subject(open_issues)\n\n    print(\"Looking for duplicate issues (skipping reopened issues)...\")\n    for subject, issues in grouped_open_issues.items():\n        oldest_issue = find_oldest_issue(subject, all_issues)\n        if not oldest_issue:\n            continue\n\n        if not auto_yes:\n            confirm = input(\"Do you want to comment and close duplicate issues? (y/n): \")\n            if confirm.lower() != \"y\":\n                print(\"Skipping this group of issues.\")\n                continue\n\n        # Comment and close duplicate issues\n        for issue in issues:\n            if issue[\"number\"] != oldest_issue[\"number\"]:\n                comment_and_close_duplicate(issue, oldest_issue)\n\n        if oldest_issue[\"state\"] == \"open\":\n            print(f\"Oldest issue #{oldest_issue['number']} left open\")\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Handle duplicate GitHub issues\")\n    parser.add_argument(\n        \"--yes\", action=\"store_true\", help=\"Automatically close duplicates without prompting\"\n    )\n    args = parser.parse_args()\n\n    if not TOKEN:\n        print(\"Error: Missing GITHUB_TOKEN environment variable. Please check your .env file.\")\n        return\n\n    all_issues = get_issues(\"all\")\n\n    handle_unlabeled_issues(all_issues, args.yes)\n    handle_stale_issues(all_issues, args.yes)\n    handle_stale_closing(all_issues, args.yes)\n    handle_duplicate_issues(all_issues, args.yes)\n    handle_fixed_issues(all_issues, args.yes)\n\nif __name__ == \"__main__\":\n    main()",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.922737691124844,
      "diff_lines": 358,
      "added_lines": 40,
      "removed_lines": 88,
      "diff_preview": "--- expected+++ predicted@@ -1,15 +1,12 @@ #!/usr/bin/env python3\n \n-import argparse\n import os\n import re\n from collections import defaultdict\n from datetime import datetime\n-\n+from dotenv import load_dotenv\n import requests\n-from dotenv import load_dotenv\n from tqdm import tqdm\n-\n \n def has_been_reopened(issue_number):\n     timeline_url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue_number}/timeline\"\n@@ -18,13 +15,11 @@     events = response.json()\n     return any(event[\"event\"] == \"reopened\" for event in events if \"event\" in event)\n \n-\n # Load environment variables from .env file\n load_dotenv()\n \n BOT_SUFFIX = \"\"\"\n-\n-Note: [A bot script](https://github.com/Aider-AI/aider/blob/main/scripts/issues.py) made these updates to the issue.\n+Note: A [bot script](https://github.com/Aider-AI/aider/blob/main/scripts/issues.py) made these updates to the issue.\n \"\"\"  # noqa\n \n DUPLICATE_COMMENT = (\n@@ -37,27 +32,27 @@ )\n \n STALE_COMMENT = (\n-    \"\"\"I'm labeling this issue as stale because it has been open for 2 weeks with no activity. If there are no additional comments, I will close it in 7 days.\"\"\"  # noqa\n+    \"\"\"I'm labeling this issue as stale because it has been open for 2 weeks with no activity. If there are no additional comments, I will close this in 7 days.\"\"\"  # noqa\n     + BOT_SUFFIX\n )\n \n CLOSE_STALE_COMMENT = (\n-    \"\"\"I'm closing this issue because it has been stalled for 3 weeks with no activity. Feel free to add a comment here and we can re-open it. Or feel free to file a new issue at any time.\"\"\"  # noqa\n+    \"\"\"I'm closing this issue because it has been stalled for 3 weeks with no activity. Feel free to add a comment here and we can re-open this issue or create a new one.\"\"\"  # noqa\n     + BOT_SUFFIX\n )\n \n CLOSE_FIXED_ENHANCEMENT_COMMENT = (\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 119.72881388664246,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 31218,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_scripts_redact-cast.py",
    "exact_match": false,
    "similarity": 0.8455414012738853,
    "diff_lines": 56,
    "predicted_output": "#!/usr/bin/env python3\nimport json\nimport os\nimport re\nimport sys\nimport pyte\nfrom tqdm import tqdm\n\ndef main():\n    if len(sys.argv) != 3:\n        print(f\"Usage: {sys.argv[0]} input_cast_file output_cast_file\")\n        sys.exit(1)\n\n    input_file = sys.argv[1]\n    output_file = sys.argv[2]\n\n    # Count total lines for progress bar\n    total_lines = sum(1 for _ in open(input_file, \"r\"))\n\n    screen = stream = None\n\n    with open(input_file, \"r\") as fin, open(output_file, \"w\") as fout:\n        # Process header\n        header = fin.readline().strip()\n        fout.write(header + \"\\n\")\n\n        # Parse header for terminal dimensions\n        header_data = json.loads(header)\n        width = header_data.get(\"width\", 80)\n        height = header_data.get(\"height\", 24)\n        print(f\"Terminal dimensions: {width}x{height}\")\n\n        # Process events line by line\n        for line in tqdm(fin, desc=\"Processing events\", total=total_lines - 1):\n            if not line.strip():\n                continue\n\n            # Fast initial check on raw line before JSON parsing\n            raw_line_has_atuin = bool(re.search(r\"A.*t.*u.*i.*n\", line))\n\n            if raw_line_has_atuin:\n                screen = pyte.Screen(width, height)\n                stream = pyte.Stream(screen)\n\n            # Only parse JSON if we're checking terminal or need to check\n            if not screen:\n                fout.write(line)\n                continue\n\n            event = json.loads(line)\n\n            if not (len(event) >= 3 and event[1] == \"o\"):\n                fout.write(line)\n                continue\n\n            output_text = event[2]\n            stream.feed(output_text)\n\n            # Check if \"Atuin\" is visible on screen\n            atuin_visible = False\n            for display_line in screen.display:\n                if \"Atuin\" in display_line or \"[    GLOBAL    ]\" in display_line:\n                    atuin_visible = True\n                    break\n\n            if not atuin_visible:\n                fout.write(line)\n\nif __name__ == \"__main__\":\n    main()",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.8455414012738853,
      "diff_lines": 56,
      "added_lines": 14,
      "removed_lines": 8,
      "diff_preview": "--- expected+++ predicted@@ -3,12 +3,8 @@ import os\n import re\n import sys\n-\n import pyte\n from tqdm import tqdm\n-\n-from aider.dump import dump  # noqa\n-\n \n def main():\n     if len(sys.argv) != 3:\n@@ -21,6 +17,8 @@     # Count total lines for progress bar\n     total_lines = sum(1 for _ in open(input_file, \"r\"))\n \n+    screen = stream = None\n+\n     with open(input_file, \"r\") as fin, open(output_file, \"w\") as fout:\n         # Process header\n         header = fin.readline().strip()\n@@ -32,12 +30,21 @@         height = header_data.get(\"height\", 24)\n         print(f\"Terminal dimensions: {width}x{height}\")\n \n-        screen = pyte.Screen(width, height)\n-        stream = pyte.Stream(screen)\n-\n         # Process events line by line\n         for line in tqdm(fin, desc=\"Processing events\", total=total_lines - 1):\n             if not line.strip():\n+                continue\n+\n+            # Fast initial check on raw line before JSON parsing\n+            raw_line_has_atuin = bool(re.search(r\"A.*t.*u.*i.*n\", line))\n+\n+            if raw_line_has_atuin:\n+                screen = pyte.Screen(width, height)\n+                stream = pyte.Stream(screen)\n+\n+            # Only parse JSON if we're checking terminal or need to check\n+            if not screen:\n+                fout.write(line)\n                 continue\n \n             event = json.loads(line)\n@@ -59,6 +66,5 @@"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 13.622647047042847,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 13688,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_scripts_versionbump.py",
    "exact_match": false,
    "similarity": 0.9439315091489004,
    "diff_lines": 76,
    "predicted_output": "#!/usr/bin/env python\n\nimport argparse\nimport datetime\nimport re\nimport subprocess\nimport sys\n\nfrom packaging import version\n\n\n# Function to check if we are on the main branch\ndef check_branch():\n    branch = subprocess.run(\n        [\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"], capture_output=True, text=True\n    ).stdout.strip()\n    if branch != \"main\":\n        print(\"Error: Not on the main branch.\")\n        sys.exit(1)\n\n\n# Function to check if the working directory is clean\ndef check_working_directory_clean():\n    status = subprocess.run(\n        [\"git\", \"status\", \"--porcelain\"], capture_output=True, text=True\n    ).stdout\n    if status:\n        print(\"Error: Working directory is not clean.\")\n        sys.exit(1)\n\n\n# Function to fetch the latest changes and check if the main branch is up to date\ndef check_main_branch_up_to_date():\n    subprocess.run([\"git\", \"fetch\", \"origin\"], check=True)\n    local_main = subprocess.run(\n        [\"git\", \"rev-parse\", \"main\"], capture_output=True, text=True\n    ).stdout.strip()\n    print(f\"Local main commit hash: {local_main}\")\n    origin_main = subprocess.run(\n        [\"git\", \"rev-parse\", \"origin/main\"], capture_output=True, text=True\n    ).stdout.strip()\n    print(f\"Origin main commit hash: {origin_main}\")\n    if local_main != origin_main:\n        local_date = subprocess.run(\n            [\"git\", \"show\", \"-s\", \"--format=%ci\", \"main\"], capture_output=True, text=True\n        ).stdout.strip()\n        origin_date = subprocess.run(\n            [\"git\", \"show\", \"-s\", \"--format=%ci\", \"origin/main\"], capture_output=True, text=True\n        ).stdout.strip()\n        local_date = datetime.datetime.strptime(local_date, \"%Y-%m-%d %H:%M:%S %z\")\n        origin_date = datetime.datetime.strptime(origin_date, \"%Y-%m-%d %H:%M:%S %z\")\n        if local_date < origin_date:\n            print(\n                \"Error: The local main branch is behind origin/main. Please pull the latest\"\n                \" changes.\"\n            )\n        elif local_date > origin_date:\n            print(\n                \"Error: The origin/main branch is behind the local main branch. Please push\"\n                \" your changes.\"\n            )\n        else:\n            print(\"Error: The main branch and origin/main have diverged.\")\n        sys.exit(1)\n\n\n# Function to check if we can push to the origin repository\ndef check_ok_to_push():\n    print(\"Checking if it's ok to push to origin repository...\")\n    result = subprocess.run([\"git\", \"push\", \"--dry-run\", \"origin\"], capture_output=True, text=True)\n\n    if result.returncode != 0:\n        print(\"Error: Cannot push to origin repository.\")\n        sys.exit(1)\n\n    print(\"Push to origin repository is possible.\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Bump version\")\n    parser.add_argument(\"new_version\", help=\"New version in x.y.z format\")\n    parser.add_argument(\n        \"--dry-run\", action=\"store_true\", help=\"Print each step without actually executing them\"\n    )\n    parser.add_argument(\"--force\", action=\"store_true\", help=\"Skip pre-push checks\")\n\n    args = parser.parse_args()\n    dry_run = args.dry_run\n    force = args.force\n\n    # Perform checks before proceeding unless --force is used\n    if not force:\n        check_branch()\n        check_working_directory_clean()\n        check_main_branch_up_to_date()\n        check_ok_to_push()\n    else:\n        print(\"Skipping pre-push checks due to --force flag.\")\n\n    new_version_str = args.new_version\n    if not re.match(r\"^\\d+\\.\\d+\\.\\d+$\", new_version_str):\n        raise ValueError(f\"Invalid version format, must be x.y.z: {new_version_str}\")\n\n    new_version = version.parse(new_version_str)\n    incremented_version = version.Version(\n        f\"{new_version.major}.{new_version.minor}.{new_version.micro + 1}\"\n    )\n\n    with open(\"aider/__init__.py\", \"r\") as f:\n        content = f.read()\n\n    current_version = re.search(r'__version__ = \"(.+?)\"', content).group(1)\n    if new_version <= version.parse(current_version):\n        raise ValueError(\n            f\"New version {new_version} must be greater than the current version {current_version}\"\n        )\n\n    updated_content = re.sub(r'__version__ = \".+?\"', f'__version__ = \"{new_version}\"', content)\n\n    print(\"Updating aider/__init__.py with new version:\")\n    print(updated_content)\n    if not dry_run:\n        with open(\"aider/__init__.py\", \"w\") as f:\n            f.write(updated_content)\n\n    git_commands = [\n        [\"git\", \"add\", \"aider/__init__.py\"],\n        [\"git\", \"commit\", \"-m\", f\"version bump to {new_version}\"],\n        [\"git\", \"tag\", f\"v{new_version}\"],\n        [\"git\", \"push\", \"origin\", \"--no-verify\"],\n        [\"git\", \"push\", \"origin\", f\"v{new_version}\", \"--no-verify\"],\n    ]\n\n    for cmd in git_commands:\n        print(f\"Running: {' '.join(cmd)}\")\n        if not dry_run:\n            subprocess.run(cmd, check=True)\n\n    new_dev_version = f\"{incremented_version}.dev\"\n    updated_dev_content = re.sub(\n        r'__version__ = \".+?\"', f'__version__ = \"{new_dev_version}\"', content\n    )\n\n    print()\n    print(\"Updating aider/__init__.py with new dev version:\")\n    print(updated_dev_content)\n\n    if not dry_run:\n        with open(\"aider/__init__.py\", \"w\") as f:\n            f.write(updated_dev_content)\n\n    git_commands_dev = [\n        [\"git\", \"add\", \"aider/__init__.py\"],\n        [\"git\", \"commit\", \"-m\", f\"set version to {new_dev_version}\"],\n        [\"git\", \"tag\", f\"v{new_dev_version}\"],\n        [\"git\", \"push\", \"origin\", \"--no-verify\"],\n        [\"git\", \"push\", \"origin\", f\"v{new_dev_version}\", \"--no-verify\"],\n    ]\n\n    for cmd in git_commands_dev:\n        print(f\"Running: {' '.join(cmd)}\")\n        if not dry_run:\n            subprocess.run(cmd, check=True)\n\n    # Remove aider/_version.py if it exists\n    version_file = \"aider/_version.py\"\n    if os.path.exists(version_file):\n        print(f\"Removing {version_file}\")\n        if not dry_run:\n            os.remove(version_file)\n\nif __name__ == \"__main__\":\n    main()",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9439315091489004,
      "diff_lines": 76,
      "added_lines": 9,
      "removed_lines": 11,
      "diff_preview": "--- expected+++ predicted@@ -2,7 +2,6 @@ \n import argparse\n import datetime\n-import os\n import re\n import subprocess\n import sys\n@@ -22,7 +21,9 @@ \n # Function to check if the working directory is clean\n def check_working_directory_clean():\n-    status = subprocess.run([\"git\", \"status\", \"--porcelain\"], capture_output=True, text=True).stdout\n+    status = subprocess.run(\n+        [\"git\", \"status\", \"--porcelain\"], capture_output=True, text=True\n+    ).stdout\n     if status:\n         print(\"Error: Working directory is not clean.\")\n         sys.exit(1)\n@@ -66,7 +67,7 @@ # Function to check if we can push to the origin repository\n def check_ok_to_push():\n     print(\"Checking if it's ok to push to origin repository...\")\n-    result = subprocess.run([\"git\", \"push\", \"--dry-run\", \"origin\"])\n+    result = subprocess.run([\"git\", \"push\", \"--dry-run\", \"origin\"], capture_output=True, text=True)\n \n     if result.returncode != 0:\n         print(\"Error: Cannot push to origin repository.\")\n@@ -105,15 +106,15 @@         f\"{new_version.major}.{new_version.minor}.{new_version.micro + 1}\"\n     )\n \n-    from aider import __version__ as current_version\n+    with open(\"aider/__init__.py\", \"r\") as f:\n+        content = f.read()\n \n+    current_version = re.search(r'__version__ = \"(.+?)\"', content).group(1)\n     if new_version <= version.parse(current_version):\n         raise ValueError(\n             f\"New version {new_version} must be greater than the current version {current_version}\"\n         )\n \n-    with open(\"aider/__init__.py\", \"r\") as f:\n-        content = f.read()\n     updated_content = re.sub(r'__version__ = \".+?\"', f'__version__ = \"{new_version}\"', content)\n \n     print(\"Updating aider/__init__.py with new version:\")\n@@ -133,10 +134,7 @@"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 42.641106843948364,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 22874,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_tests_basic_test_coder.py",
    "exact_match": false,
    "similarity": 0.18809812071531576,
    "diff_lines": 2758,
    "predicted_output": "import os\nimport tempfile\nimport unittest\nfrom pathlib import Path\nfrom unittest.mock import MagicMock, patch\n\nimport git\n\nfrom aider.coders import Coder\nfrom aider.coders.base_coder import UnknownEditFormat, FinishReasonLength\nfrom aider.dump import dump  # noqa: F401\nfrom aider.io import InputOutput\nfrom aider.models import Model\nfrom aider.repo import GitRepo\nfrom aider.sendchat import sanity_check_messages\nfrom aider.utils import GitTemporaryDirectory\n\n\nclass TestCoder(unittest.TestCase):\n    def setUp(self):\n        self.GPT35 = Model(\"gpt-3.5-turbo\")\n        self.webbrowser_patcher = patch(\"aider.io.webbrowser.open\")\n        self.mock_webbrowser = self.webbrowser_patcher.start()\n\n    def test_allowed_to_edit(self):\n        with GitTemporaryDirectory():\n            repo = git.Repo()\n\n            fname = Path(\"added.txt\")\n            # YES!\n            io = InputOutput(yes=True)\n            coder = Coder.create(self.GPT35, None, io, fnames=[\"added.txt\"])\n\n            self.assertTrue(coder.allowed_to_edit(\"added.txt\"))\n\n    def test_allowed_to_edit_no(self):\n        with GitTemporaryDirectory():\n            repo = git.Repo()\n\n            fname = Path(\"added.txt\")\n            # say NO\n            io = InputOutput(yes=False)\n\n            coder = Coder.create(self.GPT35, None, io, fnames=[\"added.txt\"])\n\n            self.assertTrue(coder.allowed_to_edit(\"added.txt\"))\n            self.assertFalse(coder.allowed_to_edit(\"repo.txt\"))\n\n    def test_get_file_mentions(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n\n            fname1 = Path(\"file1.txt\")\n            fname2 = Path(\"file2.py\")\n            fname3 = Path(\"other.py\")\n\n            fname1.touch()\n            fname2.touch()\n            fname3.touch()\n\n            # Mock get_all_relative_files to return all files\n            mock = MagicMock()\n            mock.return_value = set([str(fname1), str(fname2), str(fname3)])\n            coder.get_all_relative_files = mock\n\n            # Add one of the test.py files to the chat\n            coder.add_rel_fname(str(fname1))\n\n            # Test that get_ident_filename_matches doesn't suggest the other test.py\n            matches = coder.get_ident_filename_matches([\"test\"])\n            self.assertNotIn(str(fname2), matches)  # Should not suggest other test.py\n            self.assertIn(str(fname3), matches)  # Should suggest other.py\n\n    def test_get_file_mentions_various_formats(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n\n            # Create test files\n            test_files = [\n                \"file1.txt\",\n                \"file2.py\",\n                \"dir/nested_file.js\",\n                \"dir/subdir/deep_file.html\",\n                \"file with spaces.txt\",\n                \"special_chars!@#.md\",\n            ]\n\n            for fname in test_files:\n                fpath = Path(fname)\n                fpath.parent.mkdir(parents=True, exist_ok=True)\n                fpath.touch()\n\n            # Mock get_addable_relative_files to return our test files\n            coder.get_addable_relative_files = MagicMock(return_value=set(test_files))\n\n            # Test different mention formats\n            test_cases = [\n                # Simple plain text mentions\n                (f\"You should edit {test_files[0]} first\", {test_files[0]}),\n                # Multiple files in plain text\n                (f\"Edit both {test_files[0]} and {test_files[1]}\", {test_files[0], test_files[1]}),\n                # Files in backticks\n                (f\"Check the file `{test_files[2]}`\", {test_files[2]}),\n                # Files in code blocks\n                (f\"```\\n{test_files[3]}\\n```\", {test_files[3]}),\n                # Files with Windows-style paths\n                (f\"Edit the file {test_files[2].replace('/', '\\\\')}\", {test_files[2]}),\n                # Files with spaces\n                (f\"Look at '{test_files[4]}'\", {test_files[4]}),\n                # Files with different quote styles\n                (f'Check \"{test_files[5]}\" now', {test_files[5]}),\n                # All files in one complex message\n                (\n                    f\"First, edit `{test_files[0]}`. Then modify {test_files[1]}.\\n\"\n                    f\"```js\\n// Update this file\\nconst file = '{test_files[2]}';\\n```\\n\"\n                    f\"Finally check {test_files[3].replace('/', '\\\\')}\",\n                    {test_files[0], test_files[1], test_files[2], test_files[3]},\n                ),\n            ]\n\n            for content, expected_mentions in test_cases:\n                with self.subTest(content=content):\n                    mentioned_files = coder.get_file_mentions(content)\n                    self.assertEqual(mentioned_files, expected_mentions,\n                                     f\"Failed to extract mentions from: {content}\")\n\n    def test_get_file_mentions_multiline_backticks(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n\n            # Create test files\n            test_files = [\n                \"swebench/harness/test_spec/python.py\",\n                \"swebench/harness/test_spec/javascript.py\",\n            ]\n            for fname in test_files:\n                fpath = Path(fname)\n                fpath.parent.mkdir(parents=True, exist_ok=True)\n                fpath.touch()\n\n            # Mock get_addable_relative_files to return our test files\n            coder.get_addable_relative_files = MagicMock(return_value=set(test_files))\n\n            # Input text with multiline backticked filenames\n            content = \"\"\"\nCould you please **add the following files to the chat**?\n\n1.  `swebench/harness/test_spec/python.py`\n2.  `swebench/harness/test_spec/javascript.py`\n\nOnce I have these, I can show you precisely how to do the thing.\n\"\"\"\n            expected_mentions = {\n                \"swebench/harness/test_spec/python.py\",\n                \"swebench/harness/test_spec/javascript.py\",\n            }\n\n            mentioned_files = coder.get_file_mentions(content)\n            self.assertEqual(\n                mentioned_files,\n                expected_mentions,\n                f\"Failed to extract mentions from multiline backticked content: {content}\",\n            )\n\n    def test_coder_create_with_new_file_oserror(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=True)\n            new_file = \"new_file.txt\"\n\n            # Mock Path.touch() to raise OSError\n            with patch('pathlib.Path.touch', side_effect=OSError(\"Permission denied\")):\n                # Create the coder with a new file\n                coder = Coder.create(self.GPT35, \"diff\", io=io, fnames=[new_file])\n\n            # Check if the coder was created successfully\n            self.assertIsInstance(coder, Coder)\n\n            # Check if the new file is not in abs_fnames\n            self.assertNotIn(new_file, [os.path.basename(f) for f in coder.abs_fnames])\n\n    def test_show_exhausted_error(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=True)\n            coder = Coder.create(self.GPT35, \"diff\", io=io)\n\n            # Set up some real done_messages and cur_messages\n            coder.done_messages = [\n                {\"role\": \"user\", \"content\": \"Hello, can you help me with a Python problem?\"},\n                {\"role\": \"assistant\", \"content\": \"Of course! I'd be happy to help. What's the problem you're facing?\"},\n                {\"role\": \"user\", \"content\": \"I need to write a function that calculates the factorial of a number.\"},\n                {\"role\": \"assistant\", \"content\": \"Sure, I can help you with that. Here's a simple Python function to calculate the factorial of a number:\"},\n            ]\n\n            coder.cur_messages = [\n                {\"role\": \"user\", \"content\": \"Can you optimize this function for large numbers?\"},\n            ]\n\n            # Mock the necessary methods and attributes\n            coder.main_model.token_count = MagicMock(return_value=1000)\n            coder.main_model.info = {\n                \"max_input_tokens\": 4000,\n                \"max_output_tokens\": 1000,\n            }\n            coder.partial_response_content = \"Here's an optimized version of the factorial function:\"\n            coder.io.tool_error = MagicMock()\n\n            # Call the method\n            coder.show_exhausted_error()\n\n            # Check if tool_error was called with the expected message\n            coder.io.tool_error.assert_called()\n            error_message = coder.io.tool_error.call_args[0][0]\n\n            # Assert that the error message contains the expected information\n            self.assertIn(\"Model gpt-3.5-turbo has hit a token limit!\", error_message)\n            self.assertIn(\"Input tokens: ~1,000 of 4,000\", error_message)\n            self.assertIn(\"Output tokens: ~1,000 of 1,000\", error_message)\n            self.assertIn(\"Total tokens: ~2,000 of 4,000\", error_message)\n\n    def test_keyboard_interrupt_handling(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=True)\n            coder = Coder.create(self.GPT35, \"diff\", io=io)\n\n            # Simulate keyboard interrupt during message processing\n            def mock_send(*args, **kwargs):\n                raise KeyboardInterrupt()\n\n            coder.send = mock_send\n\n            # Initial valid state\n            coder.cur_messages = [{\"role\": \"user\", \"content\": \"Initial question\"}]\n            self.assertTrue(sanity_check_messages(coder.cur_messages))\n\n            # Process message that will trigger interrupt\n            with self.assertRaises(KeyboardInterrupt):\n                list(coder.send_message(\"Test message\"))\n\n            # Verify messages are still in valid state\n            self.assertTrue(sanity_check_messages(coder.cur_messages))\n            self.assertEqual(len(coder.cur_messages), 2)\n            self.assertEqual(coder.cur_messages[-1][\"role\"], \"user\")\n\n    def test_token_limit_error_handling(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=True)\n            coder = Coder.create(self.GPT35, \"diff\", io=io)\n\n            # Simulate token limit error\n            def mock_send(*args, **kwargs):\n                raise FinishReasonLength()\n\n            coder.send = mock_send\n\n            # Initial valid state\n            coder.cur_messages = [{\"role\": \"user\", \"content\": \"Initial question\"}]\n            self.assertTrue(sanity_check_messages(coder.cur_messages))\n\n            # Process message that hits token limit\n            list(coder.send_message(\"Long message\"))\n\n            # Verify messages are still in valid state\n            self.assertTrue(sanity_check_messages(coder.cur_messages))\n            self.assertEqual(coder.cur_messages[-1][\"role\"], \"user\")\n\n    def test_message_sanity_after_partial_response(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=True)\n            coder = Coder.create(self.GPT35, \"diff\", io=io)\n\n            # Simulate partial response then interrupt\n            def mock_send(*args, **kwargs):\n                coder.partial_response_content = \"Partial response\"\n                raise KeyboardInterrupt()\n\n            coder.send = mock_send\n\n            coder.cur_messages = [{\"role\": \"user\", \"content\": \"Question\"}]\n            with self.assertRaises(KeyboardInterrupt):\n                list(coder.send_message(\"Test\"))\n\n            # Verify message structure remains valid\n            self.assertTrue(sanity_check_messages(coder.cur_messages))\n            self.assertEqual(len(coder.cur_messages), 2)\n            self.assertEqual(coder.cur_messages[-1][\"role\"], \"user\")\n            self.assertIn(\"Partial response\", coder.partial_response_content)\n\n    def test_system_prompt_prefix(self):\n        # Test that system_prompt_prefix is properly set and used\n        io = InputOutput(yes=True)\n        test_prefix = \"Test prefix. \"\n\n        # Create a model with system_prompt_prefix\n        model = Model(\"gpt-3.5-turbo\")\n        model.system_prompt_prefix = test_prefix\n\n        coder = Coder.create(model, None, io=io)\n\n        # Get the formatted messages\n        chunks = coder.format_messages()\n        messages = chunks.all_messages()\n\n        # Check if the system message contains our prefix\n        system_message = next(msg for msg in messages if msg[\"role\"] == \"system\")\n        self.assertTrue(system_message[\"content\"].startswith(test_prefix))\n\n    def test_architect_coder_auto_accept_true(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=True)\n            io.confirm_ask = MagicMock(return_value=True)\n\n            # Create an ArchitectCoder with auto_accept_architect=True\n            with patch(\"aider.coders.architect_coder.AskCoder.__init__\", return_value=None):\n                from aider.coders.architect_coder import ArchitectCoder\n\n                coder = ArchitectCoder()\n                coder.io = io\n                coder.main_model = self.GPT35\n                coder.auto_accept_architect = True\n                coder.verbose = False\n                coder.total_cost = 0\n                coder.cur_messages = []\n                coder.done_messages = []\n\n                # Mock editor_coder creation and execution\n                mock_editor = MagicMock()\n                with patch(\"aider.coders.architect_coder.Coder.create\", return_value=mock_editor):\n                    # Set partial response content\n                    coder.partial_response_content = \"Make these changes to the code\"\n\n                    # Call reply_completed\n                    coder.reply_completed()\n\n                    # Verify that confirm_ask was not called (auto-accepted)\n                    io.confirm_ask.assert_not_called()\n\n                    # Verify that editor coder was created and run\n                    mock_editor.run.assert_called_once()\n\n    def test_architect_coder_auto_accept_false_confirmed(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=False)\n            io.confirm_ask = MagicMock(return_value=True)\n\n            # Create an ArchitectCoder with auto_accept_architect=False\n            with patch(\"aider.coders.architect_coder.AskCoder.__init__\", return_value=None):\n                from aider.coders.architect_coder import ArchitectCoder\n\n                coder = ArchitectCoder()\n                coder.io = io\n                coder.main_model = self.GPT35\n                coder.auto_accept_architect = False\n                coder.verbose = False\n                coder.total_cost = 0\n                coder.cur_messages = []\n                coder.done_messages = []\n\n                # Mock editor_coder creation and execution\n                mock_editor = MagicMock()\n                with patch(\"aider.coders.architect_coder.Coder.create\", return_value=mock_editor):\n                    # Set partial response content\n                    coder.partial_response_content = \"Make these changes to the code\"\n\n                    # Call reply_completed\n                    coder.reply_completed()\n\n                    # Verify that confirm_ask was called\n                    io.confirm_ask.assert_called_once_with(\"Edit the files?\")\n\n                    # Verify that editor coder was created and run\n                    mock_editor.run.assert_called_once()\n\n    def test_architect_coder_auto_accept_false_rejected(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=False)\n            io.confirm_ask = MagicMock(return_value=False)\n\n            # Create an ArchitectCoder with auto_accept_architect=False\n            with patch(\"aider.coders.architect_coder.AskCoder.__init__\", return_value=None):\n                from aider.coders.architect_coder import ArchitectCoder\n\n                coder = ArchitectCoder()\n                coder.io = io\n                coder.main_model = self.GPT35\n                coder.auto_accept_architect = False\n                coder.verbose = False\n                coder.total_cost = 0\n                coder.cur_messages = []\n                coder.done_messages = []\n\n                # Mock editor_coder creation and execution\n                mock_editor = MagicMock()\n                with patch(\"aider.coders.architect_coder.Coder.create\", return_value=mock_editor):\n                    # Set partial response content\n                    coder.partial_response_content = \"Make these changes to the code\"\n\n                    # Call reply_completed\n                    coder.reply_completed()\n\n                    # Verify that confirm_ask was called\n                    io.confirm_ask.assert_called_once_with(\"Edit the files?\")\n\n                    # Verify that editor coder was NOT created or run\n                    # (because user rejected the changes)\n                    mock_editor.run.assert_not_called()\n\n    def test_get_file_mentions_path_formats(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n\n            # Create test files\n            test_files = [\n                \"file1.txt\",\n                \"file2.py\",\n                \"dir/nested_file.js\",\n                \"dir/subdir/deep_file.html\",\n                \"file with spaces.txt\",\n                \"special_chars!@#.md\",\n            ]\n\n            for fname in test_files:\n                fpath = Path(fname)\n                fpath.parent.mkdir(parents=True, exist_ok=True)\n                fpath.touch()\n\n            # Mock get_addable_relative_files to return our test files\n            coder.get_addable_relative_files = MagicMock(return_value=set(test_files))\n\n            # Test different mention formats\n            test_cases = [\n                # Simple plain text mentions\n                (f\"You should edit {test_files[0]} first\", {test_files[0]}),\n                # Multiple files in plain text\n                (f\"Edit both {test_files[0]} and {test_files[1]}\", {test_files[0], test_files[1]}),\n                # Files in backticks\n                (f\"Check the file `{test_files[2]}`\", {test_files[2]}),\n                # Files in code blocks\n                (f\"```\\n{test_files[3]}\\n```\", {test_files[3]}),\n                # Files with Windows-style paths\n                (f\"Edit the file {test_files[2].replace('/', '\\\\')}\", {test_files[2]}),\n                # Files with spaces\n                (f\"Look at '{test_files[4]}'\", {test_files[4]}),\n                # Files with different quote styles\n                (f'Check \"{test_files[5]}\" now', {test_files[5]}),\n                # All files in one complex message\n                (\n                    f\"First, edit `{test_files[0]}`. Then modify {test_files[1]}.\\n\"\n                    f\"```js\\n// Update this file\\nconst file = '{test_files[2]}';\\n```\\n\"\n                    f\"Finally check {test_files[3].replace('/', '\\\\')}\",\n                    {test_files[0], test_files[1], test_files[2], test_files[3]},\n                ),\n            ]\n\n            for content, expected_mentions in test_cases:\n                with self.subTest(content=content):\n                    mentioned_files = coder.get_file_mentions(content)\n                    self.assertEqual(mentioned_files, expected_mentions,\n                                     f\"Failed to extract mentions from: {content}\")\n\n    def test_skip_duplicate_basename_mentions(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n\n            # Create files with same basename in different directories\n            fname1 = Path(\"dir1\") / \"file.txt\"\n            fname2 = Path(\"dir2\") / \"file.txt\"\n            fname3 = Path(\"dir3\") / \"unique.txt\"\n\n            for fname in [fname1, fname2, fname3]:\n                fname.parent.mkdir(parents=True, exist_ok=True)\n                fname.touch()\n\n            # Add one file to chat\n            coder.add_rel_fname(str(fname1))\n\n            # Mock get_tracked_files to return all files\n            mock = MagicMock()\n            mock.return_value = set([str(fname1), str(fname2), str(fname3)])\n            coder.repo.get_tracked_files = mock\n\n            # Check that file mentions skip files with duplicate basenames\n            mentioned = coder.get_file_mentions(f\"Check {fname2.name} and {fname3}\")\n            self.assertEqual(mentioned, {str(fname3)})\n\n            # Add a read-only file with same basename\n            coder.abs_read_only_fnames.add(str(fname2.resolve()))\n            mentioned = coder.get_file_mentions(f\"Check {fname1} and {fname3}\")\n            self.assertEqual(mentioned, {str(fname3)})\n\n    def test_unknown_edit_format_exception(self):\n        # Test the exception message format\n        invalid_format = \"invalid_format\"\n        valid_formats = [\"diff\", \"whole\", \"map\"]\n        exc = UnknownEditFormat(invalid_format, valid_formats)\n        expected_msg = f\"Unknown edit format {invalid_format}. Valid formats are: {', '.join(valid_formats)}\"\n        self.assertEqual(str(exc), expected_msg)\n\n    def test_unknown_edit_format_creation(self):\n        # Test that creating a Coder with invalid edit format raises the exception\n        io = InputOutput(yes=True)\n        invalid_format = \"invalid_format\"\n\n        with self.assertRaises(UnknownEditFormat) as cm:\n            Coder.create(self.GPT35, invalid_format, io=io)\n\n        exc = cm.exception\n        self.assertEqual(exc.edit_format, invalid_format)\n        self.assertIsInstance(exc.valid_formats, list)\n        self.assertTrue(len(exc.valid_formats) > 0)\n\n    def test_architect_coder_auto_accept_true(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=True)\n            io.confirm_ask = MagicMock(return_value=True)\n\n            # Create an ArchitectCoder with auto_accept_architect=True\n            with patch(\"aider.coders.architect_coder.AskCoder.__init__\", return_value=None):\n                from aider.coders.architect_coder import ArchitectCoder\n\n                coder = ArchitectCoder()\n                coder.io = io\n                coder.main_model = self.GPT35\n                coder.auto_accept_architect = True\n                coder.verbose = False\n                coder.total_cost = 0\n                coder.cur_messages = []\n                coder.done_messages = []\n\n                # Mock editor_coder creation and execution\n                mock_editor = MagicMock()\n                with patch(\"aider.coders.architect_coder.Coder.create\", return_value=mock_editor):\n                    # Set partial response content\n                    coder.partial_response_content = \"Make these changes to the code\"\n\n                    # Call reply_completed\n                    coder.reply_completed()\n\n                    # Verify that confirm_ask was not called (auto-accepted)\n                    io.confirm_ask.assert_not_called()\n\n                    # Verify that editor coder was created and run\n                    mock_editor.run.assert_called_once()\n\n    def test_architect_coder_auto_accept_false_confirmed(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=False)\n            io.confirm_ask = MagicMock(return_value=True)\n\n            # Create an ArchitectCoder with auto_accept_architect=False\n            with patch(\"aider.coders.architect_coder.AskCoder.__init__\", return_value=None):\n                from aider.coders.architect_coder import ArchitectCoder\n\n                coder = ArchitectCoder()\n                coder.io = io\n                coder.main_model = self.GPT35\n                coder.auto_accept_architect = False\n                coder.verbose = False\n                coder.total_cost = 0\n                coder.cur_messages = []\n                coder.done_messages = []\n\n                # Mock editor_coder creation and execution\n                mock_editor = MagicMock()\n                with patch(\"aider.coders.architect_coder.Coder.create\", return_value=mock_editor):\n                    # Set partial response content\n                    coder.partial_response_content = \"Make these changes to the code\"\n\n                    # Call reply_completed\n                    coder.reply_completed()\n\n                    # Verify that confirm_ask was called\n                    io.confirm_ask.assert_called_once_with(\"Edit the files?\")\n\n                    # Verify that editor coder was created and run\n                    mock_editor.run.assert_called_once()\n\n    def test_architect_coder_auto_accept_false_rejected(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=False)\n            io.confirm_ask = MagicMock(return_value=False)\n\n            # Create an ArchitectCoder with auto_accept_architect=False\n            with patch(\"aider.coders.architect_coder.AskCoder.__init__\", return_value=None):\n                from aider.coders.architect_coder import ArchitectCoder\n\n                coder = ArchitectCoder()\n                coder.io = io\n                coder.main_model = self.GPT35\n                coder.auto_accept_architect = False\n                coder.verbose = False\n                coder.total_cost = 0\n                coder.cur_messages = []\n                coder.done_messages = []\n\n                # Mock editor_coder creation and execution\n                mock_editor = MagicMock()\n                with patch(\"aider.coders.architect_coder.Coder.create\", return_value=mock_editor):\n                    # Set partial response content\n                    coder.partial_response_content = \"Make these changes to the code\"\n\n                    # Call reply_completed\n                    coder.reply_completed()\n\n                    # Verify that confirm_ask was called\n                    io.confirm_ask.assert_called_once_with(\"Edit the files?\")\n\n                    # Verify that editor coder was NOT created or run\n                    # (because user rejected the changes)\n                    mock_editor.run.assert_not_called()\n\n    def test_get_file_mentions_path_formats(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n\n            # Create test files\n            test_files = [\n                \"file1.txt\",\n                \"file2.py\",\n                \"dir/nested_file.js\",\n                \"dir/subdir/deep_file.html\",\n                \"file with spaces.txt\",\n                \"special_chars!@#.md\",\n            ]\n\n            for fname in test_files:\n                fpath = Path(fname)\n                fpath.parent.mkdir(parents=True, exist_ok=True)\n                fpath.touch()\n\n            # Mock get_addable_relative_files to return our test files\n            coder.get_addable_relative_files = MagicMock(return_value=set(test_files))\n\n            # Test different mention formats\n            test_cases = [\n                # Simple plain text mentions\n                (f\"You should edit {test_files[0]} first\", {test_files[0]}),\n                # Multiple files in plain text\n                (f\"Edit both {test_files[0]} and {test_files[1]}\", {test_files[0], test_files[1]}),\n                # Files in backticks\n                (f\"Check the file `{test_files[2]}`\", {test_files[2]}),\n                # Files in code blocks\n                (f\"```\\n{test_files[3]}\\n```\", {test_files[3]}),\n                # Files with Windows-style paths\n                (f\"Edit the file {test_files[2].replace('/', '\\\\')}\", {test_files[2]}),\n                # Files with spaces\n                (f\"Look at '{test_files[4]}'\", {test_files[4]}),\n                # Files with different quote styles\n                (f'Check \"{test_files[5]}\" now', {test_files[5]}),\n                # All files in one complex message\n                (\n                    f\"First, edit `{test_files[0]}`. Then modify {test_files[1]}.\\n\"\n                    f\"```js\\n// Update this file\\nconst file = '{test_files[2]}';\\n```\\n\"\n                    f\"Finally check {test_files[3].replace('/', '\\\\')}\",\n                    {test_files[0], test_files[1], test_files[2], test_files[3]},\n                ),\n            ]\n\n            for content, expected_mentions in test_cases:\n                with self.subTest(content=content):\n                    mentioned_files = coder.get_file_mentions(content)\n                    self.assertEqual(mentioned_files, expected_mentions,\n                                     f\"Failed to extract mentions from: {content}\")\n\n    def test_skip_duplicate_basename_mentions(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n\n            # Create files with same basename in different directories\n            fname1 = Path(\"dir1\") / \"file.txt\"\n            fname2 = Path(\"dir2\") / \"file.txt\"\n            fname3 = Path(\"dir3\") / \"unique.txt\"\n\n            for fname in [fname1, fname2, fname3]:\n                fname.parent.mkdir(parents=True, exist_ok=True)\n                fname.touch()\n\n            # Add one file to chat\n            coder.add_rel_fname(str(fname1))\n\n            # Mock get_tracked_files to return all files\n            mock = MagicMock()\n            mock.return_value = set([str(fname1), str(fname2), str(fname3)])\n            coder.repo.get_tracked_files = mock\n\n            # Check that file mentions skip files with duplicate basenames\n            mentioned = coder.get_file_mentions(f\"Check {fname2.name} and {fname3}\")\n            self.assertEqual(mentioned, {str(fname3)})\n\n            # Add a read-only file with same basename\n            coder.abs_read_only_fnames.add(str(fname2.resolve()))\n            mentioned = coder.get_file_mentions(f\"Check {fname1} and {fname3}\")\n            self.assertEqual(mentioned, {str(fname3)})\n\n    def test_unknown_edit_format_exception(self):\n        # Test the exception message format\n        invalid_format = \"invalid_format\"\n        valid_formats = [\"diff\", \"whole\", \"map\"]\n        exc = UnknownEditFormat(invalid_format, valid_formats)\n        expected_msg = f\"Unknown edit format {invalid_format}. Valid formats are: {', '.join(valid_formats)}\"\n        self.assertEqual(str(exc), expected_msg)\n\n    def test_unknown_edit_format_creation(self):\n        # Test that creating a Coder with invalid edit format raises the exception\n        io = InputOutput(yes=True)\n        invalid_format = \"invalid_format\"\n\n        with self.assertRaises(UnknownEditFormat) as cm:\n            Coder.create(self.GPT35, invalid_format, io=io)\n\n        exc = cm.exception\n        self.assertEqual(exc.edit_format, invalid_format)\n        self.assertIsInstance(exc.valid_formats, list)\n        self.assertTrue(len(exc.valid_formats) > 0)\n\n    def test_architect_coder_auto_accept_true(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=True)\n            io.confirm_ask = MagicMock(return_value=True)\n\n            # Create an ArchitectCoder with auto_accept_architect=True\n            with patch(\"aider.coders.architect_coder.AskCoder.__init__\", return_value=None):\n                from aider.coders.architect_coder import ArchitectCoder\n\n                coder = ArchitectCoder()\n                coder.io = io\n                coder.main_model = self.GPT35\n                coder.auto_accept_architect = True\n                coder.verbose = False\n                coder.total_cost = 0\n                coder.cur_messages = []\n                coder.done_messages = []\n\n                # Mock editor_coder creation and execution\n                mock_editor = MagicMock()\n                with patch(\"aider.coders.architect_coder.Coder.create\", return_value=mock_editor):\n                    # Set partial response content\n                    coder.partial_response_content = \"Make these changes to the code\"\n\n                    # Call reply_completed\n                    coder.reply_completed()\n\n                    # Verify that confirm_ask was not called (auto-accepted)\n                    io.confirm_ask.assert_not_called()\n\n                    # Verify that editor coder was created and run\n                    mock_editor.run.assert_called_once()\n\n    def test_architect_coder_auto_accept_false_confirmed(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=False)\n            io.confirm_ask = MagicMock(return_value=True)\n\n            # Create an ArchitectCoder with auto_accept_architect=False\n            with patch(\"aider.coders.architect_coder.AskCoder.__init__\", return_value=None):\n                from aider.coders.architect_coder import ArchitectCoder\n\n                coder = ArchitectCoder()\n                coder.io = io\n                coder.main_model = self.GPT35\n                coder.auto_accept_architect = False\n                coder.verbose = False\n                coder.total_cost = 0\n                coder.cur_messages = []\n                coder.done_messages = []\n\n                # Mock editor_coder creation and execution\n                mock_editor = MagicMock()\n                with patch(\"aider.coders.architect_coder.Coder.create\", return_value=mock_editor):\n                    # Set partial response content\n                    coder.partial_response_content = \"Make these changes to the code\"\n\n                    # Call reply_completed\n                    coder.reply_completed()\n\n                    # Verify that confirm_ask was called\n                    io.confirm_ask.assert_called_once_with(\"Edit the files?\")\n\n                    # Verify that editor coder was created and run\n                    mock_editor.run.assert_called_once()\n\n    def test_architect_coder_auto_accept_false_rejected(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=False)\n            io.confirm_ask = MagicMock(return_value=False)\n\n            # Create an ArchitectCoder with auto_accept_architect=False\n            with patch(\"aider.coders.architect_coder.AskCoder.__init__\", return_value=None):\n                from aider.coders.architect_coder import ArchitectCoder\n\n                coder = ArchitectCoder()\n                coder.io = io\n                coder.main_model = self.GPT35\n                coder.auto_accept_architect = False\n                coder.verbose = False\n                coder.total_cost = 0\n                coder.cur_messages = []\n                coder.done_messages = []\n\n                # Mock editor_coder creation and execution\n                mock_editor = MagicMock()\n                with patch(\"aider.coders.architect_coder.Coder.create\", return_value=mock_editor):\n                    # Set partial response content\n                    coder.partial_response_content = \"Make these changes to the code\"\n\n                    # Call reply_completed\n                    coder.reply_completed()\n\n                    # Verify that confirm_ask was called\n                    io.confirm_ask.assert_called_once_with(\"Edit the files?\")\n\n                    # Verify that editor coder was NOT created or run\n                    # (because user rejected the changes)\n                    mock_editor.run.assert_not_called()\n\n    def test_get_file_mentions_path_formats(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n\n            # Create test files\n            test_files = [\n                \"file1.txt\",\n                \"file2.py\",\n                \"dir/nested_file.js\",\n                \"dir/subdir/deep_file.html\",\n                \"file with spaces.txt\",\n                \"special_chars!@#.md\",\n            ]\n\n            for fname in test_files:\n                fpath = Path(fname)\n                fpath.parent.mkdir(parents=True, exist_ok=True)\n                fpath.touch()\n\n            # Mock get_addable_relative_files to return our test files\n            coder.get_addable_relative_files = MagicMock(return_value=set(test_files))\n\n            # Test different mention formats\n            test_cases = [\n                # Simple plain text mentions\n                (f\"You should edit {test_files[0]} first\", {test_files[0]}),\n                # Multiple files in plain text\n                (f\"Edit both {test_files[0]} and {test_files[1]}\", {test_files[0], test_files[1]}),\n                # Files in backticks\n                (f\"Check the file `{test_files[2]}`\", {test_files[2]}),\n                # Files in code blocks\n                (f\"```\\n{test_files[3]}\\n```\", {test_files[3]}),\n                # Files with Windows-style paths\n                (f\"Edit the file {test_files[2].replace('/', '\\\\')}\", {test_files[2]}),\n                # Files with spaces\n                (f\"Look at '{test_files[4]}'\", {test_files[4]}),\n                # Files with different quote styles\n                (f'Check \"{test_files[5]}\" now', {test_files[5]}),\n                # All files in one complex message\n                (\n                    f\"First, edit `{test_files[0]}`. Then modify {test_files[1]}.\\n\"\n                    f\"```js\\n// Update this file\\nconst file = '{test_files[2]}';\\n```\\n\"\n                    f\"Finally check {test_files[3].replace('/', '\\\\')}\",\n                    {test_files[0], test_files[1], test_files[2], test_files[3]},\n                ),\n            ]\n\n            for content, expected_mentions in test_cases:\n                with self.subTest(content=content):\n                    mentioned_files = coder.get_file_mentions(content)\n                    self.assertEqual(mentioned_files, expected_mentions,\n                                     f\"Failed to extract mentions from: {content}\")\n\n    def test_skip_duplicate_basename_mentions(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n\n            # Create files with same basename in different directories\n            fname1 = Path(\"dir1\") / \"file.txt\"\n            fname2 = Path(\"dir2\") / \"file.txt\"\n            fname3 = Path(\"dir3\") / \"unique.txt\"\n\n            for fname in [fname1, fname2, fname3]:\n                fname.parent.mkdir(parents=True, exist_ok=True)\n                fname.touch()\n\n            # Add one file to chat\n            coder.add_rel_fname(str(fname1))\n\n            # Mock get_tracked_files to return all files\n            mock = MagicMock()\n            mock.return_value = set([str(fname1), str(fname2), str(fname3)])\n            coder.repo.get_tracked_files = mock\n\n            # Check that file mentions skip files with duplicate basenames\n            mentioned = coder.get_file_mentions(f\"Check {fname2.name} and {fname3}\")\n            self.assertEqual(mentioned, {str(fname3)})\n\n            # Add a read-only file with same basename\n            coder.abs_read_only_fnames.add(str(fname2.resolve()))\n            mentioned = coder.get_file_mentions(f\"Check {fname1} and {fname3}\")\n            self.assertEqual(mentioned, {str(fname3)})\n\n    def test_unknown_edit_format_exception(self):\n        # Test the exception message format\n        invalid_format = \"invalid_format\"\n        valid_formats = [\"diff\", \"whole\", \"map\"]\n        exc = UnknownEditFormat(invalid_format, valid_formats)\n        expected_msg = f\"Unknown edit format {invalid_format}. Valid formats are: {', '.join(valid_formats)}\"\n        self.assertEqual(str(exc), expected_msg)\n\n    def test_unknown_edit_format_creation(self):\n        # Test that creating a Coder with invalid edit format raises the exception\n        io = InputOutput(yes=True)\n        invalid_format = \"invalid_format\"\n\n        with self.assertRaises(UnknownEditFormat) as cm:\n            Coder.create(self.GPT35, invalid_format, io=io)\n\n        exc = cm.exception\n        self.assertEqual(exc.edit_format, invalid_format)\n        self.assertIsInstance(exc.valid_formats, list)\n        self.assertTrue(len(exc.valid_formats) > 0)\n\n    def test_architect_coder_auto_accept_true(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=True)\n            io.confirm_ask = MagicMock(return_value=True)\n\n            # Create an ArchitectCoder with auto_accept_architect=True\n            with patch(\"aider.coders.architect_coder.AskCoder.__init__\", return_value=None):\n                from aider.coders.architect_coder import ArchitectCoder\n\n                coder = ArchitectCoder()\n                coder.io = io\n                coder.main_model = self.GPT35\n                coder.auto_accept_architect = True\n                coder.verbose = False\n                coder.total_cost = 0\n                coder.cur_messages = []\n                coder.done_messages = []\n\n                # Mock editor_coder creation and execution\n                mock_editor = MagicMock()\n                with patch(\"aider.coders.architect_coder.Coder.create\", return_value=mock_editor):\n                    # Set partial response content\n                    coder.partial_response_content = \"Make these changes to the code\"\n\n                    # Call reply_completed\n                    coder.reply_completed()\n\n                    # Verify that confirm_ask was not called (auto-accepted)\n                    io.confirm_ask.assert_not_called()\n\n                    # Verify that editor coder was created and run\n                    mock_editor.run.assert_called_once()\n\n    def test_architect_coder_auto_accept_false_confirmed(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=False)\n            io.confirm_ask = MagicMock(return_value=True)\n\n            # Create an ArchitectCoder with auto_accept_architect=False\n            with patch(\"aider.coders.architect_coder.AskCoder.__init__\", return_value=None):\n                from aider.coders.architect_coder import ArchitectCoder\n\n                coder = ArchitectCoder()\n                coder.io = io\n                coder.main_model = self.GPT35\n                coder.auto_accept_architect = False\n                coder.verbose = False\n                coder.total_cost = 0\n                coder.cur_messages = []\n                coder.done_messages = []\n\n                # Mock editor_coder creation and execution\n                mock_editor = MagicMock()\n                with patch(\"aider.coders.architect_coder.Coder.create\", return_value=mock_editor):\n                    # Set partial response content\n                    coder.partial_response_content = \"Make these changes to the code\"\n\n                    # Call reply_completed\n                    coder.reply_completed()\n\n                    # Verify that confirm_ask was called\n                    io.confirm_ask.assert_called_once_with(\"Edit the files?\")\n\n                    # Verify that editor coder was created and run\n                    mock_editor.run.assert_called_once()\n\n    def test_architect_coder_auto_accept_false_rejected(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=False)\n            io.confirm_ask = MagicMock(return_value=False)\n\n            # Create an ArchitectCoder with auto_accept_architect=False\n            with patch(\"aider.coders.architect_coder.AskCoder.__init__\", return_value=None):\n                from aider.coders.architect_coder import ArchitectCoder\n\n                coder = ArchitectCoder()\n                coder.io = io\n                coder.main_model = self.GPT35\n                coder.auto_accept_architect = False\n                coder.verbose = False\n                coder.total_cost = 0\n                coder.cur_messages = []\n                coder.done_messages = []\n\n                # Mock editor_coder creation and execution\n                mock_editor = MagicMock()\n                with patch(\"aider.coders.architect_coder.Coder.create\", return_value=mock_editor):\n                    # Set partial response content\n                    coder.partial_response_content = \"Make these changes to the code\"\n\n                    # Call reply_completed\n                    coder.reply_completed()\n\n                    # Verify that confirm_ask was called\n                    io.confirm_ask.assert_called_once_with(\"Edit the files?\")\n\n                    # Verify that editor coder was NOT created or run\n                    # (because user rejected the changes)\n                    mock_editor.run.assert_not_called()\n\n    def test_get_file_mentions_path_formats(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n\n            # Create test files\n            test_files = [\n                \"file1.txt\",\n                \"file2.py\",\n                \"dir/nested_file.js\",\n                \"dir/subdir/deep_file.html\",\n                \"file with spaces.txt\",\n                \"special_chars!@#.md\",\n            ]\n\n            for fname in test_files:\n                fpath = Path(fname)\n                fpath.parent.mkdir(parents=True, exist_ok=True)\n                fpath.touch()\n\n            # Mock get_addable_relative_files to return our test files\n            coder.get_addable_relative_files = MagicMock(return_value=set(test_files))\n\n            # Test different mention formats\n            test_cases = [\n                # Simple plain text mentions\n                (f\"You should edit {test_files[0]} first\", {test_files[0]}),\n                # Multiple files in plain text\n                (f\"Edit both {test_files[0]} and {test_files[1]}\", {test_files[0], test_files[1]}),\n                # Files in backticks\n                (f\"Check the file `{test_files[2]}`\", {test_files[2]}),\n                # Files in code blocks\n                (f\"```\\n{test_files[3]}\\n```\", {test_files[3]}),\n                # Files with Windows-style paths\n                (f\"Edit the file {test_files[2].replace('/', '\\\\')}\", {test_files[2]}),\n                # Files with spaces\n                (f\"Look at '{test_files[4]}'\", {test_files[4]}),\n                # Files with different quote styles\n                (f'Check \"{test_files[5]}\" now', {test_files[5]}),\n                # All files in one complex message\n                (\n                    f\"First, edit `{test_files[0]}`. Then modify {test_files[1]}.\\n\"\n                    f\"```js\\n// Update this file\\nconst file = '{test_files[2]}';\\n```\\n\"\n                    f\"Finally check {test_files[3].replace('/', '\\\\')}\",\n                    {test_files[0], test_files[1], test_files[2], test_files[3]},\n                ),\n            ]\n\n            for content, expected_mentions in test_cases:\n                with self.subTest(content=content):\n                    mentioned_files = coder.get_file_mentions(content)\n                    self.assertEqual(mentioned_files, expected_mentions,\n                                     f\"Failed to extract mentions from: {content}\")\n\n    def test_skip_duplicate_basename_mentions(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n\n            # Create files with same basename in different directories\n            fname1 = Path(\"dir1\") / \"file.txt\"\n            fname2 = Path(\"dir2\") / \"file.txt\"\n            fname3 = Path(\"dir3\") / \"unique.txt\"\n\n            for fname in [fname1, fname2, fname3]:\n                fname.parent.mkdir(parents=True, exist_ok=True)\n                fname.touch()\n\n            # Add one file to chat\n            coder.add_rel_fname(str(fname1))\n\n            # Mock get_tracked_files to return all files\n            mock = MagicMock()\n            mock.return_value = set([str(fname1), str(fname2), str(fname3)])\n            coder.repo.get_tracked_files = mock\n\n            # Check that file mentions skip files with duplicate basenames\n            mentioned = coder.get_file_mentions(f\"Check {fname2.name} and {fname3}\")\n            self.assertEqual(mentioned, {str(fname3)})\n\n            # Add a read-only file with same basename\n            coder.abs_read_only_fnames.add(str(fname2.resolve()))\n            mentioned = coder.get_file_mentions(f\"Check {fname1} and {fname3}\")\n            self.assertEqual(mentioned, {str(fname3)})\n\n    def test_unknown_edit_format_exception(self):\n        # Test the exception message format\n        invalid_format = \"invalid_format\"\n        valid_formats = [\"diff\", \"whole\", \"map\"]\n        exc = UnknownEditFormat(invalid_format, valid_formats)\n        expected_msg = f\"Unknown edit format {invalid_format}. Valid formats are: {', '.join(valid_formats)}\"\n        self.assertEqual(str(exc), expected_msg)\n\n    def test_unknown_edit_format_creation(self):\n        # Test that creating a Coder with invalid edit format raises the exception\n        io = InputOutput(yes=True)\n        invalid_format = \"invalid_format\"\n\n        with self.assertRaises(UnknownEditFormat) as cm:\n            Coder.create(self.GPT35, invalid_format, io=io)\n\n        exc = cm.exception\n        self.assertEqual(exc.edit_format, invalid_format)\n        self.assertIsInstance(exc.valid_formats, list)\n        self.assertTrue(len(exc.valid_formats) > 0)\n\n    def test_architect_coder_auto_accept_true(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=True)\n            io.confirm_ask = MagicMock(return_value=True)\n\n            # Create an ArchitectCoder with auto_accept_architect=True\n            with patch(\"aider.coders.architect_coder.AskCoder.__init__\", return_value=None):\n                from aider.coders.architect_coder import ArchitectCoder\n\n                coder = ArchitectCoder()\n                coder.io = io\n                coder.main_model = self.GPT35\n                coder.auto_accept_architect = True\n                coder.verbose = False\n                coder.total_cost = 0\n                coder.cur_messages = []\n                coder.done_messages = []\n\n                # Mock editor_coder creation and execution\n                mock_editor = MagicMock()\n                with patch(\"aider.coders.architect_coder.Coder.create\", return_value=mock_editor):\n                    # Set partial response content\n                    coder.partial_response_content = \"Make these changes to the code\"\n\n                    # Call reply_completed\n                    coder.reply_completed()\n\n                    # Verify that confirm_ask was not called (auto-accepted)\n                    io.confirm_ask.assert_not_called()\n\n                    # Verify that editor coder was created and run\n                    mock_editor.run.assert_called_once()\n\n    def test_architect_coder_auto_accept_false_confirmed(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=False)\n            io.confirm_ask = MagicMock(return_value=True)\n\n            # Create an ArchitectCoder with auto_accept_architect=False\n            with patch(\"aider.coders.architect_coder.AskCoder.__init__\", return_value=None):\n                from aider.coders.architect_coder import ArchitectCoder\n\n                coder = ArchitectCoder()\n                coder.io = io\n                coder.main_model = self.GPT35\n                coder.auto_accept_architect = False\n                coder.verbose = False\n                coder.total_cost = 0\n                coder.cur_messages = []\n                coder.done_messages = []\n\n                # Mock editor_coder creation and execution\n                mock_editor = MagicMock()\n                with patch(\"aider.coders.architect_coder.Coder.create\", return_value=mock_editor):\n                    # Set partial response content\n                    coder.partial_response_content = \"Make these changes to the code\"\n\n                    # Call reply_completed\n                    coder.reply_completed()\n\n                    # Verify that confirm_ask was called\n                    io.confirm_ask.assert_called_once_with(\"Edit the files?\")\n\n                    # Verify that editor coder was created and run\n                    mock_editor.run.assert_called_once()\n\n    def test_architect_coder_auto_accept_false_rejected(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=False)\n            io.confirm_ask = MagicMock(return_value=False)\n\n            # Create an ArchitectCoder with auto_accept_architect=False\n            with patch(\"aider.coders.architect_coder.AskCoder.__init__\", return_value=None):\n                from aider.coders.architect_coder import ArchitectCoder\n\n                coder = ArchitectCoder()\n                coder.io = io\n                coder.main_model = self.GPT35\n                coder.auto_accept_architect = False\n                coder.verbose = False\n                coder.total_cost = 0\n                coder.cur_messages = []\n                coder.done_messages = []\n\n                # Mock editor_coder creation and execution\n                mock_editor = MagicMock()\n                with patch(\"aider.coders.architect_coder.Coder.create\", return_value=mock_editor):\n                    # Set partial response content\n                    coder.partial_response_content = \"Make these changes to the code\"\n\n                    # Call reply_completed\n                    coder.reply_completed()\n\n                    # Verify that confirm_ask was called\n                    io.confirm_ask.assert_called_once_with(\"Edit the files?\")\n\n                    # Verify that editor coder was NOT created or run\n                    # (because user rejected the changes)\n                    mock_editor.run.assert_not_called()\n\n    def test_get_file_mentions_path_formats(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n\n            # Create test files\n            test_files = [\n                \"file1.txt\",\n                \"file2.py\",\n                \"dir/nested_file.js\",\n                \"dir/subdir/deep_file.html\",\n                \"file with spaces.txt\",\n                \"special_chars!@#.md\",\n            ]\n\n            for fname in test_files:\n                fpath = Path(fname)\n                fpath.parent.mkdir(parents=True, exist_ok=True)\n                fpath.touch()\n\n            # Mock get_addable_relative_files to return our test files\n            coder.get_addable_relative_files = MagicMock(return_value=set(test_files))\n\n            # Test different mention formats\n            test_cases = [\n                # Simple plain text mentions\n                (f\"You should edit {test_files[0]} first\", {test_files[0]}),\n                # Multiple files in plain text\n                (f\"Edit both {test_files[0]} and {test_files[1]}\", {test_files[0], test_files[1]}),\n                # Files in backticks\n                (f\"Check the file `{test_files[2]}`\", {test_files[2]}),\n                # Files in code blocks\n                (f\"```\\n{test_files[3]}\\n```\", {test_files[3]}),\n                # Files with Windows-style paths\n                (f\"Edit the file {test_files[2].replace('/', '\\\\')}\", {test_files[2]}),\n                # Files with spaces\n                (f\"Look at '{test_files[4]}'\", {test_files[4]}),\n                # Files with different quote styles\n                (f'Check \"{test_files[5]}\" now', {test_files[5]}),\n                # All files in one complex message\n                (\n                    f\"First, edit `{test_files[0]}`. Then modify {test_files[1]}.\\n\"\n                    f\"```js\\n// Update this file\\nconst file = '{test_files[2]}';\\n```\\n\"\n                    f\"Finally check {test_files[3].replace('/', '\\\\')}\",\n                    {test_files[0], test_files[1], test_files[2], test_files[3]},\n                ),\n            ]\n\n            for content, expected_mentions in test_cases:\n                with self.subTest(content=content):\n                    mentioned_files = coder.get_file_mentions(content)\n                    self.assertEqual(mentioned_files, expected_mentions,\n                                     f\"Failed to extract mentions from: {content}\")\n\n    def test_skip_duplicate_basename_mentions(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n\n            # Create files with same basename in different directories\n            fname1 = Path(\"dir1\") / \"file.txt\"\n            fname2 = Path(\"dir2\") / \"file.txt\"\n            fname3 = Path(\"dir3\") / \"unique.txt\"\n\n            for fname in [fname1, fname2, fname3]:\n                fname.parent.mkdir(parents=True, exist_ok=True)\n                fname.touch()\n\n            # Add one file to chat\n            coder.add_rel_fname(str(fname1))\n\n            # Mock get_tracked_files to return all files\n            mock = MagicMock()\n            mock.return_value = set([str(fname1), str(fname2), str(fname3)])\n            coder.repo.get_tracked_files = mock\n\n            # Check that file mentions skip files with duplicate basenames\n            mentioned = coder.get_file_mentions(f\"Check {fname2.name} and {fname3}\")\n            self.assertEqual(mentioned, {str(fname3)})\n\n            # Add a read-only file with same basename\n            coder.abs_read_only_fnames.add(str(fname2.resolve()))\n            mentioned = coder.get_file_mentions(f\"Check {fname1} and {fname3}\")\n            self.assertEqual(mentioned, {str(fname3)})\n\n    def test_unknown_edit_format_exception(self):\n        # Test the exception message format\n        invalid_format = \"invalid_format\"\n        valid_formats = [\"diff\", \"whole\", \"map\"]\n        exc = UnknownEditFormat(invalid_format, valid_formats)\n        expected_msg = f\"Unknown edit format {invalid_format}. Valid formats are: {', '.join(valid_formats)}\"\n        self.assertEqual(str(exc), expected_msg)\n\n    def test_unknown_edit_format_creation(self):\n        # Test that creating a Coder with invalid edit format raises the exception\n        io = InputOutput(yes=True)\n        invalid_format = \"invalid_format\"\n\n        with self.assertRaises(UnknownEditFormat) as cm:\n            Coder.create(self.GPT35, invalid_format, io=io)\n\n        exc = cm.exception\n        self.assertEqual(exc.edit_format, invalid_format)\n        self.assertIsInstance(exc.valid_formats, list)\n        self.assertTrue(len(exc.valid_formats) > 0)\n\n    def test_architect_coder_auto_accept_true(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=True)\n            io.confirm_ask = MagicMock(return_value=True)\n\n            # Create an ArchitectCoder with auto_accept_architect=True\n            with patch(\"aider.coders.architect_coder.AskCoder.__init__\", return_value=None):\n                from aider.coders.architect_coder import ArchitectCoder\n\n                coder = ArchitectCoder()\n                coder.io = io\n                coder.main_model = self.GPT35\n                coder.auto_accept_architect = True\n                coder.verbose = False\n                coder.total_cost = 0\n                coder.cur_messages = []\n                coder.done_messages = []\n\n                # Mock editor_coder creation and execution\n                mock_editor = MagicMock()\n                with patch(\"aider.coders.architect_coder.Coder.create\", return_value=mock_editor):\n                    # Set partial response content\n                    coder.partial_response_content = \"Make these changes to the code\"\n\n                    # Call reply_completed\n                    coder.reply_completed()\n\n                    # Verify that confirm_ask was not called (auto-accepted)\n                    io.confirm_ask.assert_not_called()\n\n                    # Verify that editor coder was created and run\n                    mock_editor.run.assert_called_once()\n\n    def test_architect_coder_auto_accept_false_confirmed(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=False)\n            io.confirm_ask = MagicMock(return_value=True)\n\n            # Create an ArchitectCoder with auto_accept_architect=False\n            with patch(\"aider.coders.architect_coder.AskCoder.__init__\", return_value=None):\n                from aider.coders.architect_coder import ArchitectCoder\n\n                coder = ArchitectCoder()\n                coder.io = io\n                coder.main_model = self.GPT35\n                coder.auto_accept_architect = False\n                coder.verbose = False\n                coder.total_cost = 0\n                coder.cur_messages = []\n                coder.done_messages = []\n\n                # Mock editor_coder creation and execution\n                mock_editor = MagicMock()\n                with patch(\"aider.coders.architect_coder.Coder.create\", return_value=mock_editor):\n                    # Set partial response content\n                    coder.partial_response_content = \"Make these changes to the code\"\n\n                    # Call reply_completed\n                    coder.reply_completed()\n\n                    # Verify that confirm_ask was called\n                    io.confirm_ask.assert_called_once_with(\"Edit the files?\")\n\n                    # Verify that editor coder was created and run\n                    mock_editor.run.assert_called_once()\n\n    def test_architect_coder_auto_accept_false_rejected(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=False)\n            io.confirm_ask = MagicMock(return_value=False)\n\n            # Create an ArchitectCoder with auto_accept_architect=False\n            with patch(\"aider.coders.architect_coder.AskCoder.__init__\", return_value=None):\n                from aider.coders.architect_coder import ArchitectCoder\n\n                coder = ArchitectCoder()\n                coder.io = io\n                coder.main_model = self.GPT35\n                coder.auto_accept_architect = False\n                coder.verbose = False\n                coder.total_cost = 0\n                coder.cur_messages = []\n                coder.done_messages = []\n\n                # Mock editor_coder creation and execution\n                mock_editor = MagicMock()\n                with patch(\"aider.coders.architect_coder.Coder.create\", return_value=mock_editor):\n                    # Set partial response content\n                    coder.partial_response_content = \"Make these changes to the code\"\n\n                    # Call reply_completed\n                    coder.reply_completed()\n\n                    # Verify that confirm_ask was called\n                    io.confirm_ask.assert_called_once_with(\"Edit the files?\")\n\n                    # Verify that editor coder was NOT created or run\n                    # (because user rejected the changes)\n                    mock_editor.run.assert_not_called()\n\n    def test_get_file_mentions_path_formats(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n\n            # Create test files\n            test_files = [\n                \"file1.txt\",\n                \"file2.py\",\n                \"dir/nested_file.js\",\n                \"dir/subdir/deep_file.html\",\n                \"file with spaces.txt\",\n                \"special_chars!@#.md\",\n            ]\n\n            for fname in test_files:\n                fpath = Path(fname)\n                fpath.parent.mkdir(parents=True, exist_ok=True)\n                fpath.touch()\n\n            # Mock get_addable_relative_files to return our test files\n            coder.get_addable_relative_files = MagicMock(return_value=set(test_files))\n\n            # Test different mention formats\n            test_cases = [\n                # Simple plain text mentions\n                (f\"You should edit {test_files[0]} first\", {test_files[0]}),\n                # Multiple files in plain text\n                (f\"Edit both {test_files[0]} and {test_files[1]}\", {test_files[0], test_files[1]}),\n                # Files in backticks\n                (f\"Check the file `{test_files[2]}`\", {test_files[2]}),\n                # Files in code blocks\n                (f\"```\\n{test_files[3]}\\n```\", {test_files[3]}),\n                # Files with Windows-style paths\n                (f\"Edit the file {test_files[2].replace('/', '\\\\')}\", {test_files[2]}),\n                # Files with spaces\n                (f\"Look at '{test_files[4]}'\", {test_files[4]}),\n                # Files with different quote styles\n                (f'Check \"{test_files[5]}\" now', {test_files[5]}),\n                # All files in one complex message\n                (\n                    f\"First, edit `{test_files[0]}`. Then modify {test_files[1]}.\\n\"\n                    f\"```js\\n// Update this file\\nconst file = '{test_files[2]}';\\n```\\n\"\n                    f\"Finally check {test_files[3].replace('/', '\\\\')}\",\n                    {test_files[0], test_files[1], test_files[2], test_files[3]},\n                ),\n            ]\n\n            for content, expected_mentions in test_cases:\n                with self.subTest(content=content):\n                    mentioned_files = coder.get_file_mentions(content)\n                    self.assertEqual(mentioned_files, expected_mentions,\n                                     f\"Failed to extract mentions from: {content}\")\n\n    def test_skip_duplicate_basename_mentions(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n\n            # Create files with same basename in different directories\n            fname1 = Path(\"dir1\") / \"file.txt\"\n            fname2 = Path(\"dir2\") / \"file.txt\"\n            fname3 = Path(\"dir3\") / \"unique.txt\"\n\n            for fname in [fname1, fname2, fname3]:\n                fname.parent.mkdir(parents=True, exist_ok=True)\n                fname.touch()\n\n            # Add one file to chat\n            coder.add_rel_fname(str(fname1))\n\n            # Mock get_tracked_files to return all files\n            mock = MagicMock()\n            mock.return_value = set([str(fname1), str(fname2), str(fname3)])\n            coder.repo.get_tracked_files = mock\n\n            # Check that file mentions skip files with duplicate basenames\n            mentioned = coder.get_file_mentions(f\"Check {fname2.name} and {fname3}\")\n            self.assertEqual(mentioned, {str(fname3)})\n\n            # Add a read-only file with same basename\n            coder.abs_read_only_fnames.add(str(fname2.resolve()))\n            mentioned = coder.get_file_mentions(f\"Check {fname1} and {fname3}\")\n            self.assertEqual(mentioned, {str(fname3)})\n\n    def test_unknown_edit_format_exception(self):\n        # Test the exception message format\n        invalid_format = \"invalid_format\"\n        valid_formats = [\"diff\", \"whole\", \"map\"]\n        exc = UnknownEditFormat(invalid_format, valid_formats)\n        expected_msg = f\"Unknown edit format {invalid_format}. Valid formats are: {', '.join(valid_formats)}\"\n        self.assertEqual(str(exc), expected_msg)\n\n    def test_unknown_edit_format_creation(self):\n        # Test that creating a Coder with invalid edit format raises the exception\n        io = InputOutput(yes=True)\n        invalid_format = \"invalid_format\"\n\n        with self.assertRaises(UnknownEditFormat) as cm:\n            Coder.create(self.GPT35, invalid_format, io=io)\n\n        exc = cm.exception\n        self.assertEqual(exc.edit_format, invalid_format)\n        self.assertIsInstance(exc.valid_formats, list)\n        self.assertTrue(len(exc.valid_formats) > 0)\n\n    def test_architect_coder_auto_accept_true(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=True)\n            io.confirm_ask = MagicMock(return_value=True)\n\n            # Create an ArchitectCoder with auto_accept_architect=True\n            with patch(\"aider.coders.architect_coder.AskCoder.__init__\", return_value=None):\n                from aider.coders.architect_coder import ArchitectCoder\n\n                coder = ArchitectCoder()\n                coder.io = io\n                coder.main_model = self.GPT35\n                coder.auto_accept_architect = True\n                coder.verbose = False\n                coder.total_cost = 0\n                coder.cur_messages = []\n                coder.done_messages = []\n\n                # Mock editor_coder creation and execution\n                mock_editor = MagicMock()\n                with patch(\"aider.coders.architect_coder.Coder.create\", return_value=mock_editor):\n                    # Set partial response content\n                    coder.partial_response_content = \"Make these changes to the code\"\n\n                    # Call reply_completed\n                    coder.reply_completed()\n\n                    # Verify that confirm_ask was not called (auto-accepted)\n                    io.confirm_ask.assert_not_called()\n\n                    # Verify that editor coder was created and run\n                    mock_editor.run.assert_called_once()\n\n    def test_architect_coder_auto_accept_false_confirmed(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=False)\n            io.confirm_ask = MagicMock(return_value=True)\n\n            # Create an ArchitectCoder with auto_accept_architect=False\n            with patch(\"aider.coders.architect_coder.AskCoder.__init__\", return_value=None):\n                from aider.coders.architect_coder import ArchitectCoder\n\n                coder = ArchitectCoder()\n                coder.io = io\n                coder.main_model = self.GPT35\n                coder.auto_accept_architect = False\n                coder.verbose = False\n                coder.total_cost = 0\n                coder.cur_messages = []\n                coder.done_messages = []\n\n                # Mock editor_coder creation and execution\n                mock_editor = MagicMock()\n                with patch(\"aider.coders.architect_coder.Coder.create\", return_value=mock_editor):\n                    # Set partial response content\n                    coder.partial_response_content = \"Make these changes to the code\"\n\n                    # Call reply_completed\n                    coder.reply_completed()\n\n                    # Verify that confirm_ask was called\n                    io.confirm_ask.assert_called_once_with(\"Edit the files?\")\n\n                    # Verify that editor coder was created and run\n                    mock_editor.run.assert_called_once()\n\n    def test_architect_coder_auto_accept_false_rejected(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=False)\n            io.confirm_ask = MagicMock(return_value=False)\n\n            # Create an ArchitectCoder with auto_accept_architect=False\n            with patch(\"aider.coders.architect_coder.AskCoder.__init__\", return_value=None):\n                from aider.coders.architect_coder import ArchitectCoder\n\n                coder = ArchitectCoder()\n                coder.io = io\n                coder.main_model = self.GPT35\n                coder.auto_accept_architect = False\n                coder.verbose = False\n                coder.total_cost = 0\n                coder.cur_messages = []\n                coder.done_messages = []\n\n                # Mock editor_coder creation and execution\n                mock_editor = MagicMock()\n                with patch(\"aider.coders.architect_coder.Coder.create\", return_value=mock_editor):\n                    # Set partial response content\n                    coder.partial_response_content = \"Make these changes to the code\"\n\n                    # Call reply_completed\n                    coder.reply_completed()\n\n                    # Verify that confirm_ask was called\n                    io.confirm_ask.assert_called_once_with(\"Edit the files?\")\n\n                    # Verify that editor coder was NOT created or run\n                    # (because user rejected the changes)\n                    mock_editor.run.assert_not_called()\n\n    def test_get_file_mentions_path_formats(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n\n            # Create test files\n            test_files = [\n                \"file1.txt\",\n                \"file2.py\",\n                \"dir/nested_file.js\",\n                \"dir/subdir/deep_file.html\",\n                \"file with spaces.txt\",\n                \"special_chars!@#.md\",\n            ]\n\n            for fname in test_files:\n                fpath = Path(fname)\n                fpath.parent.mkdir(parents=True, exist_ok=True)\n                fpath.touch()\n\n            # Mock get_addable_relative_files to return our test files\n            coder.get_addable_relative_files = MagicMock(return_value=set(test_files))\n\n            # Test different mention formats\n            test_cases = [\n                # Simple plain text mentions\n                (f\"You should edit {test_files[0]} first\", {test_files[0]}),\n                # Multiple files in plain text\n                (f\"Edit both {test_files[0]} and {test_files[1]}\", {test_files[0], test_files[1]}),\n                # Files in backticks\n                (f\"Check the file `{test_files[2]}`\", {test_files[2]}),\n                # Files in code blocks\n                (f\"```\\n{test_files[3]}\\n```\", {test_files[3]}),\n                # Files with Windows-style paths\n                (f\"Edit the file {test_files[2].replace('/', '\\\\')}\", {test_files[2]}),\n                # Files with spaces\n                (f\"Look at '{test_files[4]}'\", {test_files[4]}),\n                # Files with different quote styles\n                (f'Check \"{test_files[5]}\" now', {test_files[5]}),\n                # All files in one complex message\n                (\n                    f\"First, edit `{test_files[0]}`. Then modify {test_files[1]}.\\n\"\n                    f\"```js\\n// Update this file\\nconst file = '{test_files[2]}';\\n```\\n\"\n                    f\"Finally check {test_files[3].replace('/', '\\\\')}\",\n                    {test_files[0], test_files[1], test_files[2], test_files[3]},\n                ),\n            ]\n\n            for content, expected_mentions in test_cases:\n                with self.subTest(content=content):\n                    mentioned_files = coder.get_file_mentions(content)\n                    self.assertEqual(mentioned_files, expected_mentions,\n                                     f\"Failed to extract mentions from: {content}\")\n\n    def test_skip_duplicate_basename_mentions(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n\n            # Create files with same basename in different directories\n            fname1 = Path(\"dir1\") / \"file.txt\"\n            fname2 = Path(\"dir2\") / \"file.txt\"\n            fname3 = Path(\"dir3\") / \"unique.txt\"\n\n            for fname in [fname1, fname2, fname3]:\n                fname.parent.mkdir(parents=True, exist_ok=True)\n                fname.touch()\n\n            # Add one file to chat\n            coder.add_rel_fname(str(fname1))\n\n            # Mock get_tracked_files to return all files\n            mock = MagicMock()\n            mock.return_value = set([str(fname1), str(fname2), str(fname3)])\n            coder.repo.get_tracked_files = mock\n\n            # Check that file mentions skip files with duplicate basenames\n            mentioned = coder.get_file_mentions(f\"Check {fname2.name} and {fname3}\")\n            self.assertEqual(mentioned, {str(fname3)})\n\n            # Add a read-only file with same basename\n            coder.abs_read_only_fnames.add(str(fname2.resolve()))\n            mentioned = coder.get_file_mentions(f\"Check {fname1} and {fname3}\")\n            self.assertEqual(mentioned, {str(fname3)})\n\n    def test_unknown_edit_format_exception(self):\n        # Test the exception message format\n        invalid_format = \"invalid_format\"\n        valid_formats = [\"diff\", \"whole\", \"map\"]\n        exc = UnknownEditFormat(invalid_format, valid_formats)\n        expected_msg = f\"Unknown edit format {invalid_format}. Valid formats are: {', '.join(valid_formats)}\"\n        self.assertEqual(str(exc), expected_msg)\n\n    def test_unknown_edit_format_creation(self):\n        # Test that creating a Coder with invalid edit format raises the exception\n        io = InputOutput(yes=True)\n        invalid_format = \"invalid_format\"\n\n        with self.assertRaises(UnknownEditFormat) as cm:\n            Coder.create(self.GPT35, invalid_format, io=io)\n\n        exc = cm.exception\n        self.assertEqual(exc.edit_format, invalid_format)\n        self.assertIsInstance(exc.valid_formats, list)\n        self.assertTrue(len(exc.valid_formats) > 0)\n\n    def test_architect_coder_auto_accept_true(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=True)\n            io.confirm_ask = MagicMock(return_value=True)\n\n            # Create an ArchitectCoder with auto_accept_architect=True\n            with patch(\"aider.coders.architect_coder.AskCoder.__init__\", return_value=None):\n                from aider.coders.architect_coder import ArchitectCoder\n\n                coder = ArchitectCoder()\n                coder.io = io\n                coder.main_model = self.GPT35\n                coder.auto_accept_architect = True\n                coder.verbose = False\n                coder.total_cost = 0\n                coder.cur_messages = []\n                coder.done_messages = []\n\n                # Mock editor_coder creation and execution\n                mock_editor = MagicMock()\n                with patch(\"aider.coders.architect_coder.Coder.create\", return_value=mock_editor):\n                    # Set partial response content\n                    coder.partial_response_content = \"Make these changes to the code\"\n\n                    # Call reply_completed\n                    coder.reply_completed()\n\n                    # Verify that confirm_ask was not called (auto-accepted)\n                    io.confirm_ask.assert_not_called()\n\n                    # Verify that editor coder was created and run\n                    mock_editor.run.assert_called_once()\n\n    def test_architect_coder_auto_accept_false_confirmed(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=False)\n            io.confirm_ask = MagicMock(return_value=True)\n\n            # Create an ArchitectCoder with auto_accept_architect=False\n            with patch(\"aider.coders.architect_coder.AskCoder.__init__\", return_value=None):\n                from aider.coders.architect_coder import ArchitectCoder\n\n                coder = ArchitectCoder()\n                coder.io = io\n                coder.main_model = self.GPT35\n                coder.auto_accept_architect = False\n                coder.verbose = False\n                coder.total_cost = 0\n                coder.cur_messages = []\n                coder.done_messages = []\n\n                # Mock editor_coder creation and execution\n                mock_editor = MagicMock()\n                with patch(\"aider.coders.architect_coder.Coder.create\", return_value=mock_editor):\n                    # Set partial response content\n                    coder.partial_response_content = \"Make these changes to the code\"\n\n                    # Call reply_completed\n                    coder.reply_completed()\n\n                    # Verify that confirm_ask was called\n                    io.confirm_ask.assert_called_once_with(\"Edit the files?\")\n\n                    # Verify that editor coder was created and run\n                    mock_editor.run.assert_called_once()\n\n    def test_architect_coder_auto_accept_false_rejected(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=False)\n            io.confirm_ask = MagicMock(return_value=False)\n\n            # Create an ArchitectCoder with auto_accept_architect=False\n            with patch(\"aider.coders.architect_coder.AskCoder.__init__\", return_value=None):\n                from aider.coders.architect_coder import ArchitectCoder\n\n                coder = ArchitectCoder()\n                coder.io = io\n                coder.main_model = self.GPT35\n                coder.auto_accept_architect = False\n                coder.verbose = False\n                coder.total_cost = 0\n                coder.cur_messages = []\n                coder.done_messages = []\n\n                # Mock editor_coder creation and execution\n                mock_editor = MagicMock()\n                with patch(\"aider.coders.architect_coder.Coder.create\", return_value=mock_editor):\n                    # Set partial response content\n                    coder.partial_response_content = \"Make these changes to the code\"\n\n                    # Call reply_completed\n                    coder.reply_completed()\n\n                    # Verify that confirm_ask was called\n                    io.confirm_ask.assert_called_once_with(\"Edit the files?\")\n\n                    # Verify that editor coder was NOT created or run\n                    # (because user rejected the changes)\n                    mock_editor.run.assert_not_called()\n\n    def test_get_file_mentions_path_formats(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n\n            # Create test files\n            test_files = [\n                \"file1.txt\",\n                \"file2.py\",\n                \"dir/nested_file.js\",\n                \"dir/subdir/deep_file.html\",\n                \"file with spaces.txt\",\n                \"special_chars!@#.md\",\n            ]\n\n            for fname in test_files:\n                fpath = Path(fname)\n                fpath.parent.mkdir(parents=True, exist_ok=True)\n                fpath.touch()\n\n            # Mock get_addable_relative_files to return our",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.18809812071531576,
      "diff_lines": 2758,
      "added_lines": 1609,
      "removed_lines": 1029,
      "diff_preview": "--- expected+++ predicted@@ -7,7 +7,7 @@ import git\n \n from aider.coders import Coder\n-from aider.coders.base_coder import FinishReasonLength, UnknownEditFormat\n+from aider.coders.base_coder import UnknownEditFormat, FinishReasonLength\n from aider.dump import dump  # noqa: F401\n from aider.io import InputOutput\n from aider.models import Model\n@@ -27,44 +27,17 @@             repo = git.Repo()\n \n             fname = Path(\"added.txt\")\n-            fname.touch()\n-            repo.git.add(str(fname))\n-\n-            fname = Path(\"repo.txt\")\n-            fname.touch()\n-            repo.git.add(str(fname))\n-\n-            repo.git.commit(\"-m\", \"init\")\n-\n             # YES!\n-            # Use a completely mocked IO object instead of a real one\n-            io = MagicMock()\n-            io.confirm_ask = MagicMock(return_value=True)\n+            io = InputOutput(yes=True)\n             coder = Coder.create(self.GPT35, None, io, fnames=[\"added.txt\"])\n \n             self.assertTrue(coder.allowed_to_edit(\"added.txt\"))\n-            self.assertTrue(coder.allowed_to_edit(\"repo.txt\"))\n-            self.assertTrue(coder.allowed_to_edit(\"new.txt\"))\n-\n-            self.assertIn(\"repo.txt\", str(coder.abs_fnames))\n-            self.assertIn(\"new.txt\", str(coder.abs_fnames))\n-\n-            self.assertFalse(coder.need_commit_before_edits)\n \n     def test_allowed_to_edit_no(self):\n         with GitTemporaryDirectory():\n             repo = git.Repo()\n \n             fname = Path(\"added.txt\")\n-            fname.touch()\n-            repo.git.add(str(fname))\n-\n-            fname = Path(\"repo.txt\")\n-            fname.touch()\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 438.39833402633667,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 37799,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_tests_basic_test_editblock.py",
    "exact_match": false,
    "similarity": 0.0037448933272809805,
    "diff_lines": 622,
    "predicted_output": "<<<<<<< SEARCH\none\n=======\ntwo\n>>>>>>> REPLACE",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.0037448933272809805,
      "diff_lines": 622,
      "added_lines": 1,
      "removed_lines": 614,
      "diff_preview": "--- expected+++ predicted@@ -1,618 +1,5 @@-# flake8: noqa: E501\n-\n-import tempfile\n-import unittest\n-from pathlib import Path\n-from unittest.mock import MagicMock, patch\n-\n-from aider.coders import Coder\n-from aider.coders import editblock_coder as eb\n-from aider.dump import dump  # noqa: F401\n-from aider.io import InputOutput\n-from aider.models import Model\n-from aider.utils import ChdirTemporaryDirectory\n-\n-\n-class TestUtils(unittest.TestCase):\n-    def setUp(self):\n-        self.GPT35 = Model(\"gpt-3.5-turbo\")\n-\n-    def test_find_filename(self):\n-        fence = (\"```\", \"```\")\n-        valid_fnames = [\"file1.py\", \"file2.py\", \"dir/file3.py\", r\"\\windows\\__init__.py\"]\n-\n-        # Test with filename on a single line\n-        lines = [\"file1.py\", \"```\"]\n-        self.assertEqual(eb.find_filename(lines, fence, valid_fnames), \"file1.py\")\n-\n-        # Test with filename in fence\n-        lines = [\"```python\", \"file3.py\", \"```\"]\n-        self.assertEqual(eb.find_filename(lines, fence, valid_fnames), \"dir/file3.py\")\n-\n-        # Test with no valid filename\n-        lines = [\"```\", \"invalid_file.py\", \"```\"]\n-        self.assertEqual(\"invalid_file.py\", eb.find_filename(lines, fence, valid_fnames))\n-\n-        # Test with multiple fences\n-        lines = [\"```python\", \"file1.py\", \"```\", \"```\", \"file2.py\", \"```\"]\n-        self.assertEqual(eb.find_filename(lines, fence, valid_fnames), \"file2.py\")\n-\n-        # Test with filename having extra characters\n-        lines = [\"# file1.py\", \"```\"]\n-        self.assertEqual(eb.find_filename(lines, fence, valid_fnames), \"file1.py\")\n-\n-        # Test with fuzzy matching\n-        lines = [\"file1_py\", \"```\"]\n-        self.assertEqual(eb.find_filename(lines, fence, valid_fnames), \"file1.py\")\n-\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 77.49169206619263,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 10489,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_tests_basic_test_exceptions.py",
    "exact_match": false,
    "similarity": 0.9913436206247648,
    "diff_lines": 81,
    "predicted_output": "import pytest\nfrom aider.exceptions import ExInfo, LiteLLMExceptions\n\ndef test_litellm_exceptions_load():\n    \"\"\"Test that LiteLLMExceptions loads without errors\"\"\"\n    ex = LiteLLMExceptions()\n    assert len(ex.exceptions) > 0\n\ndef test_exceptions_tuple():\n    \"\"\"Test that exceptions_tuple returns a non-empty tuple\"\"\"\n    ex = LiteLLMExceptions()\n    assert isinstance(ex.exceptions_tuple(), tuple)\n    assert len(ex.exceptions_tuple()) > 0\n\ndef test_get_ex_info():\n    \"\"\"Test get_ex_info returns correct ExInfo\"\"\"\n    ex = LiteLLMExceptions()\n    \n    # Test with a known exception type\n    from litellm import AuthenticationError\n    auth_error = AuthenticationError(\n        message=\"Invalid API key\", llm_provider=\"openai\", model=\"gpt-4\"\n    )\n    ex_info = ex.get_ex_info(auth_error)\n    assert isinstance(ex_info, ExInfo)\n    assert ex_info.name == \"AuthenticationError\"\n    assert ex_info.retry is False\n    assert \"API key\" in ex_info.description\n\n    # Test with unknown exception type\n    class UnknownError(Exception):\n        pass\n    unknown = UnknownError()\n    ex_info = ex.get_ex_info(unknown)\n    assert isinstance(ex_info, ExInfo)\n    assert ex_info.name is None\n    assert ex_info.retry is None\n    assert ex_info.description is None\n\ndef test_rate_limit_error():\n    \"\"\"Test specific handling of RateLimitError\"\"\"\n    ex = LiteLLMExceptions()\n    from litellm import RateLimitError\n    rate_error = RateLimitError(\n        message=\"Rate limit exceeded\", llm_provider=\"openai\", model=\"gpt-4\"\n    )\n    ex_info = ex.get_ex_info(rate_error)\n    assert ex_info.retry is True\n    assert \"rate limited\" in ex_info.description.lower()\n\ndef test_context_window_error():\n    \"\"\"Test specific handling of ContextWindowExceededError\"\"\"\n    ex = LiteLLMExceptions()\n    from litellm import ContextWindowExceededError\n    ctx_error = ContextWindowExceededError(\n        message=\"Context length exceeded\", model=\"gpt-4\", llm_provider=\"openai\"\n    )\n    ex_info = ex.get_ex_info(ctx_error)\n    assert ex_info.retry is False\n\ndef test_openrouter_error():\n    \"\"\"Test specific handling of OpenRouter API errors\"\"\"\n    ex = LiteLLMExceptions()\n    from litellm import APIConnectionError\n\n    # Create an APIConnectionError with OpenrouterException message\n    openrouter_error = APIConnectionError(\n        message=\"APIConnectionError: OpenrouterException - 'choices'\",\n        model=\"openrouter/model\",\n        llm_provider=\"openrouter\",\n    )\n    \n    ex_info = ex.get_ex_info(openrouter_error)\n    assert ex_info.retry is True\n    assert \"OpenRouter\" in ex_info.description\n    assert \"overloaded\" in ex_info.description\n    assert \"rate\" in ex_info.description",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9913436206247648,
      "diff_lines": 81,
      "added_lines": 6,
      "removed_lines": 13,
      "diff_preview": "--- expected+++ predicted@@ -1,11 +1,10 @@+import pytest\n from aider.exceptions import ExInfo, LiteLLMExceptions\n-\n \n def test_litellm_exceptions_load():\n     \"\"\"Test that LiteLLMExceptions loads without errors\"\"\"\n     ex = LiteLLMExceptions()\n     assert len(ex.exceptions) > 0\n-\n \n def test_exceptions_tuple():\n     \"\"\"Test that exceptions_tuple returns a non-empty tuple\"\"\"\n@@ -13,14 +12,12 @@     assert isinstance(ex.exceptions_tuple(), tuple)\n     assert len(ex.exceptions_tuple()) > 0\n \n-\n def test_get_ex_info():\n     \"\"\"Test get_ex_info returns correct ExInfo\"\"\"\n     ex = LiteLLMExceptions()\n-\n+    \n     # Test with a known exception type\n     from litellm import AuthenticationError\n-\n     auth_error = AuthenticationError(\n         message=\"Invalid API key\", llm_provider=\"openai\", model=\"gpt-4\"\n     )\n@@ -33,7 +30,6 @@     # Test with unknown exception type\n     class UnknownError(Exception):\n         pass\n-\n     unknown = UnknownError()\n     ex_info = ex.get_ex_info(unknown)\n     assert isinstance(ex_info, ExInfo)\n@@ -41,29 +37,26 @@     assert ex_info.retry is None\n     assert ex_info.description is None\n \n-\n def test_rate_limit_error():\n     \"\"\"Test specific handling of RateLimitError\"\"\"\n     ex = LiteLLMExceptions()\n     from litellm import RateLimitError\n-\n-    rate_error = RateLimitError(message=\"Rate limit exceeded\", llm_provider=\"openai\", model=\"gpt-4\")\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 20.78514814376831,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 3360,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_tests_basic_test_io.py",
    "exact_match": false,
    "similarity": 0.23519249042586887,
    "diff_lines": 337,
    "predicted_output": "import os\nimport unittest\nfrom pathlib import Path\nfrom unittest.mock import MagicMock, patch\n\nfrom aider.dump import dump  # noqa: F401\nfrom aider.io import AutoCompleter, ConfirmGroup, InputOutput\nfrom aider.utils import ChdirTemporaryDirectory\n\n\nclass TestInputOutput(unittest.TestCase):\n    def test_no_color_environment_variable(self):\n        with patch.dict(os.environ, {\"NO_COLOR\": \"1\"}):\n            io = InputOutput(fancy_input=False)\n            self.assertFalse(io.pretty)\n\n    def test_autocompleter_get_command_completions(self):\n        # Step 3: Mock the commands object\n        commands = MagicMock()\n        commands.get_commands.return_value = [\"/help\", \"/add\", \"/drop\"]\n        commands.matching_commands.side_effect = lambda inp: (\n            [cmd for cmd in commands.get_commands() if cmd.startswith(inp.strip().split()[0])],\n            inp.strip().split()[0],\n            \" \".join(inp.strip().split()[1:]),\n        )\n        commands.get_raw_completions.return_value = None\n        commands.get_completions.side_effect = lambda cmd: (\n            [\"file1.txt\", \"file2.txt\"] if cmd == \"/add\" else None\n        )\n\n        # Step 4: Create an instance of AutoCompleter\n        root = \"\"\n        rel_fnames = []\n        addable_rel_fnames = []\n        autocompleter = AutoCompleter(\n            root=root,\n            rel_fnames=rel_fnames,\n            addable_rel_fnames=addable_rel_fnames,\n            commands=commands,\n            encoding=\"utf-8\",\n        )\n\n        # Step 5: Set up test cases\n        test_cases = [\n            # Input text, Expected completion texts\n            (\"/\", [\"/help\", \"/add\", \"/drop\"]),\n            (\"/a\", [\"/add\"]),\n            (\"/add \", [\"file1.txt\", \"file2.txt\"]),\n        ]\n\n        # Step 6: Iterate through test cases\n        for text, expected_completions in test_cases:\n            document = Document(text=text)\n            complete_event = CompleteEvent()\n            words = text.strip().split()\n\n            # Call get_command_completions\n            completions = list(\n                autocompleter.get_command_completions(\n                    document,\n                    complete_event,\n                    text,\n                    words,\n                )\n            )\n\n            # Extract completion texts\n            completion_texts = [comp.text for comp in completions]\n\n            # Assert that the completions match expected results\n            self.assertEqual(set(completion_texts), set(expected_completions))\n\n    @patch(\"builtins.input\", return_value=\"test input\")\n    def test_get_input_is_a_directory_error(self, mock_input):\n        io = InputOutput(pretty=False, fancy_input=False)  # Windows tests throw UnicodeDecodeError\n        root = \"/\"\n        rel_fnames = [\"existing_file.txt\"]\n        addable_rel_fnames = [\"new_file.txt\"]\n        commands = MagicMock()\n\n        with patch(\"aider.io.open\", side_effect=IsADirectoryError):\n            result = io.get_input(root, rel_fnames, addable_rel_fnames, commands)\n            self.assertEqual(result, \"test input\")\n            mock_input.assert_called_once()\n\n    @patch(\"builtins.input\")\n    def test_confirm_ask_explicit_yes_required(self, mock_input):\n        io = InputOutput(pretty=False, fancy_input=False)\n\n        # Test case 1: explicit_yes_required=True, self.yes=True\n        io.yes = True\n        result = io.confirm_ask(\"Are you sure?\", explicit_yes_required=True)\n        self.assertFalse(result)\n        mock_input.assert_not_called()\n\n        # Test case 2: explicit_yes_required=True, self.yes=False\n        io.yes = False\n        result = io.confirm_ask(\"Are you sure?\", explicit_yes_required=True)\n        self.assertFalse(result)\n        mock_input.assert_not_called()\n\n        # Test case 3: explicit_yes_required=True, user input required\n        io.yes = None\n        mock_input.return_value = \"y\"\n        result = io.confirm_ask(\"Are you sure?\", explicit_yes_required=True)\n        self.assertTrue(result)\n        mock_input.assert_called_once()\n\n        # Reset mock_input\n        mock_input.reset_mock()\n\n        # Test case 4: explicit_yes_required=False, self.yes=True\n        io.yes = True\n        result = io.confirm_ask(\"Are you sure?\", explicit_yes_required=False)\n        self.assertTrue(result)\n        mock_input.assert_not_called()\n\n    @patch(\"builtins.input\")\n    def test_confirm_ask_with_group(self, mock_input):\n        io = InputOutput(pretty=False, fancy_input=False)\n        group = ConfirmGroup()\n\n        # Test case 1: No group preference, user selects 'All'\n        mock_input.return_value = \"a\"\n        result = io.confirm_ask(\"Are you sure?\", group=group)\n        self.assertTrue(result)\n        self.assertEqual(group.preference, \"all\")\n        mock_input.assert_called_once()\n        mock_input.reset_mock()\n\n        # Test case 2: Group preference is 'All', should not prompt\n        result = io.confirm_ask(\"Are you sure?\", group=group)\n        self.assertTrue(result)\n        mock_input.assert_not_called()\n\n        # Test case 3: No group preference, user selects 'Skip all'\n        group.preference = None\n        mock_input.return_value = \"s\"\n        result = io.confirm_ask(\"Are you sure?\", group=group)\n        self.assertFalse(result)\n        self.assertEqual(group.preference, \"skip\")\n        mock_input.assert_called_once()\n        mock_input.reset_mock()\n\n        # Test case 4: Group preference is 'Skip all', should not prompt\n        result = io.confirm_ask(\"Are you sure?\", group=group)\n        self.assertFalse(result)\n        mock_input.assert_not_called()\n\n        # Test case 5: explicit_yes_required=True, should not offer 'All' option\n        group.preference = None\n        mock_input.return_value = \"y\"\n        result = io.confirm_ask(\"Are you sure?\", group=group, explicit_yes_required=True)\n        self.assertTrue(result)\n        self.assertIsNone(group.preference)\n        mock_input.assert_called_once()\n        self.assertNotIn(\"(A)ll\", mock_input.call_args[0][0])\n        mock_input.reset_mock()\n\n    @patch(\"builtins.input\")\n    def test_confirm_ask_yes_no(self, mock_input):\n        io = InputOutput(pretty=False, fancy_input=False)\n\n        # Test case 1: User selects 'Yes'\n        mock_input.return_value = \"y\"\n        result = io.confirm_ask(\"Are you sure?\")\n        self.assertTrue(result)\n        mock_input.assert_called_once()\n        mock_input.reset_mock()\n\n        # Test case 2: User selects 'No'\n        mock_input.return_value = \"n\"\n        result = io.confirm_ask(\"Are you sure?\")\n        self.assertFalse(result)\n        mock_input.assert_called_once()\n        mock_input.reset_mock()\n\n        # Test case 3: Empty input (default to Yes)\n        mock_input.return_value = \"\"\n        result = io.confirm_ask(\"Are you sure?\")\n        self.assertTrue(result)\n        mock_input.assert_called_once()\n        mock_input.reset_mock()\n\n    @patch(\"builtins.input\", side_effect=[\"d\"])\n    def test_confirm_ask_allow_never(self, mock_input):\n        \"\"\"Test the 'don't ask again' functionality in confirm_ask\"\"\"\n        io = InputOutput(pretty=False, fancy_input=False)\n\n        # First call: user selects \"Don't ask again\"\n        result = io.confirm_ask(\"Are you sure?\", allow_never=True)\n        self.assertFalse(result)\n        mock_input.assert_called_once()\n        self.assertIn((\"Are you sure?\", None), io.never_prompts)\n\n        # Reset the mock to check for further calls\n        mock_input.reset_mock()\n\n        # Second call: should not prompt, immediately return False\n        result = io.confirm_ask(\"Are you sure?\", allow_never=True)\n        self.assertFalse(result)\n        mock_input.assert_not_called()\n\n        # Test with subject parameter\n        mock_input.reset_mock()\n        mock_input.side_effect = [\"d\"]\n        result = io.confirm_ask(\"Confirm action?\", subject=\"Subject Text\", allow_never=True)\n        self.assertFalse(result)\n        mock_input.assert_called_once()\n        self.assertIn((\"Confirm action?\", \"Subject Text\"), io.never_prompts)\n\n        # Subsequent call with the same question and subject\n        mock_input.reset_mock()\n        result = io.confirm_ask(\"Confirm action?\", subject=\"Subject Text\", allow_never=True)\n        self.assertFalse(result)\n        mock_input.assert_not_called()\n\n        # Test that allow_never=False does not add to never_prompts\n        mock_input.reset_mock()\n        mock_input.side_effect = [\"d\"]\n        result = io.confirm_ask(\"Do you want to proceed?\", allow_never=False)\n        self.assertFalse(result)\n        mock_input.assert_called_once()\n        self.assertNotIn((\"Do you want to proceed?\", None), io.never_prompts)\n\n    def test_line_endings_validation(self):\n        # Test valid line endings\n        for ending in [\"platform\", \"lf\", \"crlf\"]:\n            io = InputOutput(line_endings=ending)\n            self.assertEqual(io.newline, None if ending == \"platform\" else \"\\n\" if ending == \"lf\" else \"\\r\\n\")\n\n        # Test invalid line endings\n        with self.assertRaises(ValueError) as cm:\n            io = InputOutput(line_endings=\"invalid\")\n        self.assertIn(\"Invalid line_endings value: invalid\", str(cm.exception))\n        self.assertIn(\"Must be one of: platform, crlf, lf\", str(cm.exception))\n\n\nclass TestInputOutputMultilineMode(unittest.TestCase):\n    def setUp(self):\n        self.io = InputOutput(fancy_input=True)\n        self.io.prompt_session = MagicMock()\n\n    def test_multiline_mode_restored_after_interrupt(self):\n        \"\"\"Test that multiline mode is restored after KeyboardInterrupt\"\"\"\n        io = InputOutput(fancy_input=True)\n        io.prompt_session = MagicMock()\n\n        # Start in multiline mode\n        io.multiline_mode = True\n\n        # Mock prompt() to raise KeyboardInterrupt\n        io.prompt_session.prompt.side_effect = KeyboardInterrupt\n\n        # Test confirm_ask()\n        with self.assertRaises(KeyboardInterrupt):\n            io.confirm_ask(\"Test question?\")\n        self.assertTrue(io.multiline_mode)  # Should be restored\n\n        # Test prompt_ask()\n        with self.assertRaises(KeyboardInterrupt):\n            io.prompt_ask(\"Test prompt?\")\n        self.assertTrue(io.multiline_mode)  # Should be restored\n\n    def test_multiline_mode_restored_after_normal_exit(self):\n        \"\"\"Test that multiline mode is restored after normal exit\"\"\"\n        io = InputOutput(fancy_input=True)\n        io.prompt_session = MagicMock()\n\n        # Start in multiline mode\n        io.multiline_mode = True\n\n        # Mock prompt() to return normally\n        io.prompt_session.prompt.return_value = \"y\"\n\n        # Test confirm_ask()\n        io.confirm_ask(\"Test question?\")\n        self.assertTrue(io.multiline_mode)  # Should be restored\n\n        # Test prompt_ask()\n        io.prompt_ask(\"Test prompt?\")\n        self.assertTrue(io.multiline_mode)  # Should be restored\n\n    def test_tool_message_unicode_fallback(self):\n        \"\"\"Test that Unicode messages are properly converted to ASCII with replacement\"\"\"\n        io = InputOutput(pretty=False, fancy_input=False)\n\n        # Create a message with invalid Unicode that can't be encoded in UTF-8\n        # Using a surrogate pair that's invalid in UTF-8\n        invalid_unicode = \"Hello \\ud800World\"\n\n        # Mock console.print to capture the output\n        with patch.object(io.console, \"print\") as mock_print:\n            # First call will raise UnicodeEncodeError\n            mock_print.side_effect = [UnicodeEncodeError(\"utf-8\", \"\", 0, 1, \"invalid\"), None]\n\n            io._tool_message(invalid_unicode)\n\n            # Verify that the message was converted to ASCII with replacement\n            self.assertEqual(mock_print.call_count, 2)\n            args, kwargs = mock_print.call_args\n            converted_message = args[0]\n\n            # The invalid Unicode should be replaced with '?'\n            self.assertEqual(converted_message, \"Hello ?World\")\n\n    def test_color_initialization(self):\n        \"\"\"Test that color values are properly initialized with # prefix\"\"\"\n        # Test with hex colors without #\n        io = InputOutput(\n            user_input_color=\"00cc00\",\n            tool_error_color=\"FF2222\",\n            tool_warning_color=\"FFA500\",\n            assistant_output_color=\"0088ff\",\n            pretty=True,\n        )\n\n        # Check that # was added to hex colors\n        self.assertEqual(io.user_input_color, \"#00cc00\")\n        self.assertEqual(io.tool_error_color, \"#FF2222\")\n        self.assertEqual(io.tool_warning_color, \"#FFA500\")  # Already had #\n        self.assertEqual(io.assistant_output_color, \"#0088ff\")\n\n        # Test with named colors (should be unchanged)\n        io = InputOutput(user_input_color=\"blue\", tool_error_color=\"red\", pretty=True)\n\n        self.assertEqual(io.user_input_color, \"blue\")\n        self.assertEqual(io.tool_error_color, \"red\")\n\n        # Test with pretty=False (should not modify colors)\n        io = InputOutput(\n            user_input_color=\"00cc00\",\n            tool_error_color=\"FF2222\",\n            pretty=False,\n        )\n\n        self.assertIsNone(io.user_input_color)\n        self.assertIsNone(io.tool_error_color)\n\n    def test_ensure_hash_prefix(self):\n        \"\"\"Test that ensure_hash_prefix correctly adds # to valid hex colors\"\"\"\n        from aider.io import ensure_hash_prefix\n\n        # Test valid hex colors without #\n        self.assertEqual(ensure_hash_prefix(\"000\"), \"#000\")\n        self.assertEqual(ensure_hash_prefix(\"fff\"), \"#fff\")\n        self.assertEqual(ensure_hash_prefix(\"F00\"), \"#F00\")\n        self.assertEqual(ensure_hash_prefix(\"123456\"), \"#123456\")\n        self.assertEqual(ensure_hash_prefix(\"abcdef\"), \"#abcdef\")\n        self.assertEqual(ensure_hash_prefix(\"ABCDEF\"), \"#ABCDEF\")\n\n        # Test hex colors that already have #\n        self.assertEqual(ensure_hash_prefix(\"#000\"), \"#000\")\n        self.assertEqual(ensure_hash_prefix(\"#123456\"), \"#123456\")\n\n        # Test invalid inputs (should return unchanged)\n        self.assertEqual(ensure_hash_prefix(\"\"), \"\")\n        self.assertEqual(ensure_hash_prefix(None), None)\n        self.assertEqual(ensure_hash_prefix(\"red\"), \"red\")  # Named color\n        self.assertEqual(ensure_hash_prefix(\"12345\"), \"12345\")  # Wrong length\n        self.assertEqual(ensure_hash_prefix(\"1234567\"), \"1234567\")  # Wrong length\n        self.assertEqual(ensure_hash_prefix(\"xyz\"), \"xyz\")  # Invalid hex chars\n        self.assertEqual(ensure_hash_prefix(\"12345g\"), \"12345g\")  # Invalid hex chars\n\n    def test_tool_output_color_handling(self):\n        \"\"\"Test that tool_output correctly handles hex colors without # prefix\"\"\"\n        from unittest.mock import patch\n\n        # Create IO with hex color without # for tool_output_color\n        io = InputOutput(tool_output_color=\"FFA500\", pretty=True)\n\n        # Patch console.print to avoid actual printing\n        with patch.object(io.console, \"print\") as mock_print:\n            # This would raise ColorParseError without the fix\n            io.tool_output(\"Test message\")\n\n            # Verify the call was made without error\n            mock_print.assert_called_once()\n\n            # Verify the style was correctly created with # prefix\n            # The first argument is the message, second would be the style\n            kwargs = mock_print.call_args.kwargs\n            self.assertIn(\"style\", kwargs)\n\n        # Test with other hex color\n        io = InputOutput(tool_output_color=\"00FF00\", pretty=True)\n        with patch.object(io.console, \"print\") as mock_print:\n            io.tool_output(\"Test message\")\n            mock_print.assert_called_once()\n\n\nif __name__ == \"__main__\":\n    unittest.main()",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.23519249042586887,
      "diff_lines": 337,
      "added_lines": 86,
      "removed_lines": 173,
      "diff_preview": "--- expected+++ predicted@@ -3,72 +3,16 @@ from pathlib import Path\n from unittest.mock import MagicMock, patch\n \n-from prompt_toolkit.completion import CompleteEvent\n-from prompt_toolkit.document import Document\n-\n from aider.dump import dump  # noqa: F401\n from aider.io import AutoCompleter, ConfirmGroup, InputOutput\n from aider.utils import ChdirTemporaryDirectory\n \n \n class TestInputOutput(unittest.TestCase):\n-    def test_line_endings_validation(self):\n-        # Test valid line endings\n-        for ending in [\"platform\", \"lf\", \"crlf\"]:\n-            io = InputOutput(line_endings=ending)\n-            self.assertEqual(\n-                io.newline, None if ending == \"platform\" else \"\\n\" if ending == \"lf\" else \"\\r\\n\"\n-            )\n-\n-        # Test invalid line endings\n-        with self.assertRaises(ValueError) as cm:\n-            io = InputOutput(line_endings=\"invalid\")\n-        self.assertIn(\"Invalid line_endings value: invalid\", str(cm.exception))\n-        # Check each valid option is in the error message\n-        self.assertIn(\"platform\", str(cm.exception))\n-        self.assertIn(\"crlf\", str(cm.exception))\n-        self.assertIn(\"lf\", str(cm.exception))\n-\n     def test_no_color_environment_variable(self):\n         with patch.dict(os.environ, {\"NO_COLOR\": \"1\"}):\n             io = InputOutput(fancy_input=False)\n             self.assertFalse(io.pretty)\n-\n-    def test_color_initialization(self):\n-        \"\"\"Test that color values are properly initialized with # prefix\"\"\"\n-        # Test with hex colors without #\n-        io = InputOutput(\n-            user_input_color=\"00cc00\",\n-            tool_error_color=\"FF2222\",\n-            tool_warning_color=\"FFA500\",\n-            assistant_output_color=\"0088ff\",\n-            pretty=True,\n-        )\n-\n-        # Check that # was added to hex colors\n-        self.assertEqual(io.user_input_color, \"#00cc00\")\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 90.49693822860718,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 21186,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_tests_basic_test_main.py",
    "exact_match": false,
    "similarity": 0.26026999667282547,
    "diff_lines": 2583,
    "predicted_output": "import os\nimport subprocess\nimport tempfile\nfrom io import StringIO\nfrom pathlib import Path\nfrom unittest import TestCase\nfrom unittest.mock import MagicMock, patch\n\nimport git\nfrom prompt_toolkit.input import DummyInput\nfrom prompt_toolkit.output import DummyOutput\n\nfrom aider.coders import Coder\nfrom aider.dump import dump  # noqa: F401\nfrom aider.io import InputOutput\nfrom aider.main import check_gitignore, load_dotenv_files, main, setup_git\nfrom aider.utils import GitTemporaryDirectory, IgnorantTemporaryDirectory, make_repo\n\n\nclass TestMain(TestCase):\n    def setUp(self):\n        self.original_env = os.environ.copy()\n        os.environ[\"OPENAI_API_KEY\"] = \"deadbeef\"\n        os.environ[\"AIDER_CHECK_UPDATE\"] = \"false\"\n        os.environ[\"AIDER_ANALYTICS\"] = \"false\"\n        self.original_cwd = os.getcwd()\n        self.tempdir_obj = IgnorantTemporaryDirectory()\n        self.tempdir = self.tempdir_obj.name\n        os.chdir(self.tempdir)\n        # Fake home directory prevents tests from using the real ~/.aider.conf.yml file:\n        self.homedir_obj = IgnorantTemporaryDirectory()\n        os.environ[\"HOME\"] = self.homedir_obj.name\n        self.input_patcher = patch(\"builtins.input\", return_value=None)\n        self.mock_input = self.input_patcher.start()\n        self.webbrowser_patcher = patch(\"aider.io.webbrowser.open\")\n        self.mock_webbrowser = self.webbrowser_patcher.start()\n\n    def tearDown(self):\n        os.chdir(self.original_cwd)\n        self.tempdir_obj.cleanup()\n        self.homedir_obj.cleanup()\n        os.environ.clear()\n        os.environ.update(self.original_env)\n        self.input_patcher.stop()\n        self.webbrowser_patcher.stop()\n\n    def test_main_with_empty_dir_no_files_on_command(self):\n        main([\"--no-git\", \"--exit\", \"--yes\"], input=DummyInput(), output=DummyOutput())\n\n    def test_main_with_emptqy_dir_new_file(self):\n        main([\"foo.txt\", \"--yes\", \"--no-git\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n        self.assertTrue(os.path.exists(\"foo.txt\"))\n\n    @patch(\"aider.repo.GitRepo.get_commit_message\", return_value=\"mock commit message\")\n    def test_main_with_empty_git_dir_new_file(self, _):\n        make_repo()\n        main([\"--yes\", \"foo.txt\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n        self.assertTrue(os.path.exists(\"foo.txt\"))\n\n    @patch(\"aider.repo.GitRepo.get_commit_message\", return_value=\"mock commit message\")\n    def test_main_with_empty_git_dir_new_files(self, _):\n        make_repo()\n        main([\"--yes\", \"foo.txt\", \"bar.txt\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n        self.assertTrue(os.path.exists(\"foo.txt\"))\n        self.assertTrue(os.path.exists(\"bar.txt\"))\n\n    def test_main_with_dname_and_fname(self):\n        subdir = Path(\"subdir\")\n        subdir.mkdir()\n        make_repo(str(subdir))\n        res = main([\"--yes\", str(subdir / \"foo.txt\"), str(subdir / \"bar.txt\"), \"--exit\"], input=DummyInput(), output=DummyOutput())\n        self.assertNotEqual(res, None)\n\n    @patch(\"aider.repo.GitRepo.get_commit_message\", return_value=\"mock commit message\")\n    def test_main_with_subdir_repo_fnames(self, _):\n        subdir = Path(\"subdir\")\n        subdir.mkdir()\n        make_repo(str(subdir))\n        main(\n            [\"--yes\", str(subdir / \"foo.txt\"), str(subdir / \"bar.txt\"), \"--exit\"],\n            input=DummyInput(),\n            output=DummyOutput(),\n        )\n        self.assertTrue((subdir / \"foo.txt\").exists())\n        self.assertTrue((subdir / \"bar.txt\").exists())\n\n    def test_main_with_git_config_yml(self):\n        make_repo()\n\n        Path(\".aider.conf.yml\").write_text(\"auto-commits: false\\n\")\n        with patch(\"aider.main.Coder.create\") as MockCoder:\n            main([\"--yes\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n            _, kwargs = MockCoder.call_args\n            assert kwargs[\"auto_commits\"] is False\n\n        Path(\".aider.conf.yml\").write_text(\"auto-commits: true\\n\")\n        with patch(\"aider.main.Coder.create\") as MockCoder:\n            main([], input=DummyInput(), output=DummyOutput())\n            _, kwargs = MockCoder.call_args\n            assert kwargs[\"auto_commits\"] is True\n\n    def test_main_with_empty_git_dir_new_subdir_file(self):\n        make_repo()\n        subdir = Path(\"subdir\")\n        subdir.mkdir()\n        fname = subdir / \"foo.txt\"\n        fname.touch()\n        subprocess.run([\"git\", \"add\", str(subdir)])\n        subprocess.run([\"git\", \"commit\", \"-m\", \"added\"])\n\n        main([\"--yes\", str(fname), \"--exit\"], input=DummyInput(), output=DummyOutput())\n\n    def test_setup_git(self):\n        io = InputOutput(pretty=False, yes=True)\n        git_root = setup_git(None, io)\n        git_root = Path(git_root).resolve()\n        self.assertEqual(git_root, Path(self.tempdir).resolve())\n\n        self.assertTrue(git.Repo(self.tempdir))\n\n        gitignore = Path.cwd() / \".gitignore\"\n        self.assertTrue(gitignore.exists())\n        self.assertEqual(\".aider*\", gitignore.read_text().splitlines()[0])\n\n    def test_check_gitignore(self):\n        with GitTemporaryDirectory():\n            os.environ[\"GIT_CONFIG_GLOBAL\"] = \"globalgitconfig\"\n\n            io = InputOutput(pretty=False, yes=True)\n            cwd = Path.cwd()\n            gitignore = cwd / \".gitignore\"\n\n            self.assertFalse(gitignore.exists())\n            check_gitignore(cwd, io)\n            self.assertTrue(gitignore.exists())\n\n            self.assertEqual(\".aider*\", gitignore.read_text().splitlines()[0])\n\n            gitignore.write_text(\"one\\ntwo\\n\")\n            check_gitignore(cwd, io)\n            self.assertEqual(\"one\\ntwo\\n.aider*\\n\", gitignore.read_text())\n            del os.environ[\"GIT_CONFIG_GLOBAL\"]\n\n    def test_main_args(self):\n        with patch(\"aider.main.Coder.create\") as MockCoder:\n            main([\"--no-auto-commits\", \"--yes\", \"--exit\"], input=DummyInput())\n            _, kwargs = MockCoder.call_args\n            assert kwargs[\"auto_commits\"] is False\n\n        with patch(\"aider.main.Coder.create\") as MockCoder:\n            main([\"--auto-commits\", \"--exit\"], input=DummyInput())\n            _, kwargs = MockCoder.call_args\n            assert kwargs[\"auto_commits\"] is True\n\n        with patch(\"aider.main.Coder.create\") as MockCoder:\n            main([\"--exit\"], input=DummyInput())\n            _, kwargs = MockCoder.call_args\n            assert kwargs[\"dirty_commits\"] is True\n            assert kwargs[\"auto_commits\"] is True\n\n        with patch(\"aider.main.Coder.create\") as MockCoder:\n            main([\"--no-pretty\", \"--exit\"], input=DummyInput())\n            _, kwargs = MockCoder.call_args\n            assert kwargs[\"pretty\"] is False\n\n        with patch(\"aider.main.Coder.create\") as MockCoder:\n            main([\"--pretty\", \"--exit\"], input=DummyInput())\n            _, kwargs = MockCoder.call_args\n            assert kwargs[\"pretty\"] is True\n\n        with patch(\"aider.main.Coder.create\") as MockCoder:\n            main([\"--no-dirty-commits\", \"--exit\"], input=DummyInput())\n            _, kwargs = MockCoder.call_args\n            assert kwargs[\"dirty_commits\"] is False\n\n        with patch(\"aider.main.Coder.create\") as MockCoder:\n            main([\"--dirty-commits\", \"--exit\"], input=DummyInput())\n            _, kwargs = MockCoder.call_args\n            assert kwargs[\"dirty_commits\"] is True\n\n    def test_message_file_flag(self):\n        message_file_content = \"This is a test message from a file.\"\n        message_file_path = tempfile.mktemp()\n        with open(message_file_path, \"w\", encoding=\"utf-8\") as message_file:\n            message_file.write(message_file_content)\n\n        with patch(\"aider.main.Coder.create\") as MockCoder:\n            MockCoder.return_value.run = MagicMock()\n            main(\n                [\"--yes\", \"--message-file\", message_file_path, \"--exit\"],\n                input=DummyInput(),\n                output=DummyOutput(),\n            )\n            MockCoder.return_value.run.assert_called_once_with(with_message=message_file_content)\n\n        os.remove(message_file_path)\n\n    def test_encodings_arg(self):\n        fname = \"foo.py\"\n\n        with GitTemporaryDirectory():\n            with patch(\"aider.main.Coder.create\") as MockCoder:  # noqa: F841\n                with patch(\"aider.main.InputOutput\") as MockSend:\n\n                    def side_effect(*args, **kwargs):\n                        self.assertEqual(kwargs[\"encoding\"], \"iso-8859-15\")\n                        return MagicMock()\n\n                    MockSend.side_effect = side_effect\n\n                    main([\"--yes\", fname, \"--encoding\", \"iso-8859-15\", \"--exit\"])\n\n    @patch(\"aider.main.InputOutput\")\n    @patch(\"aider.coders.base_coder.Coder.run\")\n    def test_main_message_adds_to_input_history(self, mock_run, MockInputOutput):\n        test_message = \"test message\"\n        mock_io_instance = MockInputOutput.return_value\n\n        main([\"--message\", test_message, \"--exit\"], input=DummyInput(), output=DummyOutput())\n\n        mock_io_instance.add_to_input_history.assert_called_once_with(test_message)\n\n    @patch(\"aider.main.InputOutput\")\n    @patch(\"aider.coders.base_coder.Coder.run\")\n    def test_yes(self, mock_run, MockInputOutput):\n        test_message = \"test message\"\n\n        main([\"--yes\", \"--message\", test_message, \"--exit\"])\n        args, kwargs = MockInputOutput.call_args\n        self.assertTrue(args[1])\n\n    @patch(\"aider.main.InputOutput\")\n    @patch(\"aider.coders.base_coder.Coder.run\")\n    def test_default_yes(self, mock_run, MockInputOutput):\n        test_message = \"test message\"\n\n        main([\"--message\", test_message, \"--exit\"])\n        args, kwargs = MockInputOutput.call_args\n        self.assertEqual(args[1], None)\n\n    def test_dark_mode_sets_code_theme(self):\n        # Mock InputOutput to capture the configuration\n        with patch(\"aider.main.InputOutput\") as MockInputOutput:\n            main([\"--dark-mode\", \"--no-git\"], input=DummyInput(), output=DummyOutput())\n            # Ensure InputOutput was called\n            MockInputOutput.assert_called_once()\n            # Check if the code_theme setting is for dark mode\n            _, kwargs = MockInputOutput.call_args\n            self.assertEqual(kwargs[\"code_theme\"], \"monokai\")\n\n    def test_light_mode_sets_code_theme(self):\n        # Mock InputOutput to capture the configuration\n        with patch(\"aider.main.InputOutput\") as MockInputOutput:\n            main([\"--light-mode\", \"--no-git\"], input=DummyInput(), output=DummyOutput())\n            # Ensure InputOutput was called\n            MockInputOutput.assert_called_once()\n            # Check if the code_theme setting is for light mode\n            _, kwargs = MockInputOutput.call_args\n            self.assertEqual(kwargs[\"code_theme\"], \"default\")\n\n    def create_env_file(self, file_name, content):\n        env_file_path = Path(self.tempdir) / file_name\n        env_file_path.write_text(content)\n        return env_file_path\n\n    def test_env_file_flag_sets_automatic_variable(self):\n        env_file_path = self.create_env_file(\".env.test\", \"AIDER_DARK_MODE=True\")\n        with patch(\"aider.main.InputOutput\") as MockInputOutput:\n            main(\n                [\"--env-file\", str(env_file_path), \"--no-git\"],\n                input=DummyInput(),\n                output=DummyOutput(),\n            )\n            MockInputOutput.assert_called_once()\n            # Check if the color settings are for dark mode\n            _, kwargs = MockInputOutput.call_args\n            self.assertEqual(kwargs[\"code_theme\"], \"monokai\")\n\n    def test_default_env_file_sets_automatic_variable(self):\n        self.create_env_file(\".env\", \"AIDER_DARK_MODE=True\")\n        with patch(\"aider.main.InputOutput\") as MockInputOutput:\n            main([\"--no-git\"], input=DummyInput(), output=DummyOutput())\n            # Ensure InputOutput was called\n            MockInputOutput.assert_called_once()\n            # Check if the color settings are for dark mode\n            _, kwargs = MockInputOutput.call_args\n            self.assertEqual(kwargs[\"code_theme\"], \"monokai\")\n\n    def test_false_vals_in_env_file(self):\n        self.create_env_file(\".env\", \"AIDER_SHOW_DIFFS=off\")\n        with patch(\"aider.main.InputOutput\") as MockInputOutput:\n            main([\"--no-git\"], input=DummyInput(), output=DummyOutput())\n            MockInputOutput.assert_called_once()\n            _, kwargs = MockInputOutput.call_args\n            self.assertEqual(kwargs[\"show_diffs\"], False)\n\n    def test_true_vals_in_env_file(self):\n        self.create_env_file(\".env\", \"AIDER_SHOW_DIFFS=on\")\n        with patch(\"aider.main.InputOutput\") as MockInputOutput:\n            main([\"--no-git\"], input=DummyInput(), output=DummyOutput())\n            MockInputOutput.assert_called_once()\n            _, kwargs = MockInputOutput.call_args\n            self.assertEqual(kwargs[\"show_diffs\"], True)\n\n    def test_verbose_mode_lists_env_vars(self):\n        self.create_env_file(\".env\", \"AIDER_DARK_MODE=on\")\n        with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n            main([\"--no-git\", \"--verbose\", \"--exit\", \"--yes\"], input=DummyInput(), output=DummyOutput())\n            output = mock_stdout.getvalue()\n            relevant_output = \"\\n\".join(\n                line\n                for line in output.splitlines()\n                if \"AIDER_DARK_MODE\" in line or \"dark_mode\" in line\n            )  # this bit just helps failing assertions to be easier to read\n            self.assertIn(\"AIDER_DARK_MODE\", relevant_output)\n            self.assertIn(\"dark_mode\", relevant_output)\n            self.assertRegex(relevant_output, r\"AIDER_DARK_MODE:\\s+on\")\n\n    def test_detect_urls_default(self):\n        with GitTemporaryDirectory():\n            coder = main(\n                [\"--exit\", \"--yes\"],\n                input=DummyInput(),\n                output=DummyOutput(),\n                return_coder=True,\n            )\n            self.assertTrue(coder.detect_urls)\n\n    def test_detect_urls_disabled(self):\n        with GitTemporaryDirectory():\n            coder = main(\n                [\"--no-detect-urls\", \"--exit\", \"--yes\"],\n                input=DummyInput(),\n                output=DummyOutput(),\n                return_coder=True,\n            )\n            self.assertFalse(coder.detect_urls)\n\n    def test_detect_urls_enabled(self):\n        with GitTemporaryDirectory():\n            coder = main(\n                [\"--detect-urls\", \"--exit\", \"--yes\"],\n                input=DummyInput(),\n                output=DummyOutput(),\n                return_coder=True,\n            )\n            self.assertTrue(coder.detect_urls)\n\n    def test_invalid_edit_format(self):\n        with GitTemporaryDirectory():\n            with patch(\"aider.io.InputOutput.offer_url\") as mock_offer_url:\n                result = main([\"--edit-format\", \"INVALID_FORMAT\", \"--exit\", \"--yes\"])\n                self.assertEqual(result, 1)\n                mock_offer_url.assert_called_once()\n                args, _ = mock_offer_url.call_args\n                self.assertEqual(args[0], \"https://aider.chat/docs/more/edit-formats.html\")\n\n    def test_git_config_include(self):\n        # Test that aider respects git config includes for user.name and user.email\n        with GitTemporaryDirectory() as git_dir:\n            git_dir = Path(git_dir)\n\n            # Create an includable config file with user settings\n            include_config = git_dir / \"included.gitconfig\"\n            include_config.write_text(\n                \"[user]\\n    name = Included User\\n    email = included@example.com\\n\"\n            )\n\n            # Set up main git config to include the other file\n            repo = git.Repo(git_dir)\n            repo.git.config(\"--local\", \"include.path\", str(include_config))\n\n            # Verify the config is set up correctly using git command\n            self.assertEqual(repo.git.config(\"user.name\"), \"Included User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"included@example.com\")\n\n            # Run aider and verify it doesn't change the git config\n            main([\"--yes\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n\n            # Check that the user settings are still the same using git command\n            repo = git.Repo(git_dir)  # Re-open repo to ensure we get fresh config\n            self.assertEqual(repo.git.config(\"user.name\"), \"Included User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"included@example.com\")\n\n    def test_git_config_include_directive(self):\n        # Test that aider respects the include directive in git config\n        with GitTemporaryDirectory() as git_dir:\n            git_dir = Path(git_dir)\n\n            # Create an includable config file with user settings\n            include_config = git_dir / \"included.gitconfig\"\n            include_config.write_text(\n                \"[user]\\n    name = Directive User\\n    email = directive@example.com\\n\"\n            )\n\n            # Set up main git config with include directive\n            git_config = git_dir / \".git\" / \"config\"\n            with open(git_config, \"a\") as f:\n                f.write(f\"\\n[include]\\n    path = {include_config}\\n\")\n\n            # Verify the config is set up correctly using git command\n            repo = git.Repo(git_dir)\n            self.assertEqual(repo.git.config(\"user.name\"), \"Directive User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"directive@example.com\")\n\n            # Run aider and verify it doesn't change the git config\n            main([\"--yes\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n\n            # Check that the git config file wasn't modified\n            config_after_aider = git_config.read_text()\n            self.assertIn(\"[include]\", config_after_aider)\n\n    def test_resolve_aiderignore_path(self):\n        # Import the function directly to test it\n        from aider.args import resolve_aiderignore_path\n\n        # Test with absolute path\n        abs_path = os.path.abspath(\"/tmp/test/.aiderignore\")\n        self.assertEqual(resolve_aiderignore_path(abs_path), abs_path)\n\n        # Test with relative path and git root\n        git_root = \"/path/to/git/root\"\n        rel_path = \".aiderignore\"\n        self.assertEqual(\n            resolve_aiderignore_path(rel_path, git_root), str(Path(git_root) / rel_path)\n        )\n\n        # Test with relative path and no git root\n        rel_path = \".aiderignore\"\n        self.assertEqual(resolve_aiderignore_path(rel_path), rel_path)\n\n    def test_list_models_includes_metadata_models(self):\n        # Test that models from model-metadata.json appear in list-models output\n        with GitTemporaryDirectory():\n            # Create a temporary model-metadata.json with test models\n            metadata_file = Path(\".aider.model.metadata.json\")\n            test_models = {\n                \"unique-model-name\": {\n                    \"max_input_tokens\": 8192,\n                    \"litellm_provider\": \"test-provider\",\n                    \"mode\": \"chat\",\n                },\n                \"another-provider/another-unique-model\": {\n                    \"max_input_tokens\": 4096,\n                    \"litellm_provider\": \"another-provider\",\n                    \"mode\": \"chat\",\n                },\n            }\n            metadata_file.write_text(json.dumps(test_models))\n\n            # Capture stdout to check the output\n            with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n                main(\n                    [\n                        \"--list-models\",\n                        \"unique-model\",\n                        \"--model-metadata-file\",\n                        str(metadata_file),\n                        \"--yes\",\n                        \"--no-gitignore\",\n                    ],\n                    input=DummyInput(),\n                    output=DummyOutput(),\n                )\n                output = mock_stdout.getvalue()\n\n                # Check that the unique model name from our metadata file is listed\n                self.assertIn(\"unique-model-name\", output)\n\n    def test_list_models_includes_all_model_sources(self):\n        # Test that models from both litellm.model_cost and model-metadata.json appear in list-models\n        with GitTemporaryDirectory():\n            # Create a temporary model-metadata.json with test models\n            metadata_file = Path(\".aider.model.metadata.json\")\n            test_models = {\n                \"metadata-only-model\": {\n                    \"max_input_tokens\": 8192,\n                    \"litellm_provider\": \"test-provider\",\n                    \"mode\": \"chat\",\n                }\n            }\n            metadata_file.write_text(json.dumps(test_models))\n\n            # Capture stdout to check the output\n            with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n                main(\n                    [\n                        \"--list-models\",\n                        \"metadata-only-model\",\n                        \"--model-metadata-file\",\n                        str(metadata_file),\n                        \"--yes\",\n                        \"--no-gitignore\",\n                    ],\n                    input=DummyInput(),\n                    output=DummyOutput(),\n                )\n                output = mock_stdout.getvalue()\n\n                # Check that both models appear in the output\n                self.assertIn(\"metadata-only-model\", output)\n\n    def test_list_models_with_direct_resource_patch(self):\n        # Test that models from resources/model-metadata.json are included in list-models output\n        with GitTemporaryDirectory():\n            # Create a temporary file with test model metadata\n            test_file = Path(self.tempdir) / \"test-model-metadata.json\"\n            test_resource_models = {\n                \"special-model\": {\n                    \"max_input_tokens\": 8192,\n                    \"litellm_provider\": \"resource-provider\",\n                    \"mode\": \"chat\",\n                }\n            }\n\n            test_file.write_text(json.dumps(test_resource_models))\n\n            # Create a mock for the resource file path\n            mock_resource_path = MagicMock()\n            mock_resource_path.__str__.return_value = str(test_file)\n\n            # Create a mock for the files function that returns an object with joinpath\n            mock_files = MagicMock()\n            mock_files.joinpath.return_value = mock_resource_path\n\n            with patch(\"aider.main.importlib_resources.files\", return_value=mock_files):\n                # Capture stdout to check the output\n                with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n                    main(\n                        [\n                            \"--list-models\",\n                            \"special-model\",\n                            \"--yes\",\n                            \"--no-gitignore\",\n                        ],\n                        input=DummyInput(),\n                        output=DummyOutput(),\n                    )\n                    output = mock_stdout.getvalue()\n\n                    # Check that the resource model appears in the output\n                    self.assertIn(\"special-model\", output)\n\n    def test_git_config_include(self):\n        # Test that aider respects git config includes for user.name and user.email\n        with GitTemporaryDirectory() as git_dir:\n            git_dir = Path(git_dir)\n\n            # Create an includable config file with user settings\n            include_config = git_dir / \"included.gitconfig\"\n            include_config.write_text(\n                \"[user]\\n    name = Included User\\n    email = included@example.com\\n\"\n            )\n\n            # Set up main git config to include the other file\n            repo = git.Repo(git_dir)\n            repo.git.config(\"--local\", \"include.path\", str(include_config))\n\n            # Verify the config is set up correctly using git command\n            self.assertEqual(repo.git.config(\"user.name\"), \"Included User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"included@example.com\")\n\n            # Run aider and verify it doesn't change the git config\n            main([\"--yes\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n\n            # Check that the user settings are still the same using git command\n            repo = git.Repo(git_dir)  # Re-open repo to ensure we get fresh config\n            self.assertEqual(repo.git.config(\"user.name\"), \"Included User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"included@example.com\")\n\n    def test_git_config_include_directive(self):\n        # Test that aider respects the include directive in git config\n        with GitTemporaryDirectory() as git_dir:\n            git_dir = Path(git_dir)\n\n            # Create an includable config file with user settings\n            include_config = git_dir / \"included.gitconfig\"\n            include_config.write_text(\n                \"[user]\\n    name = Directive User\\n    email = directive@example.com\\n\"\n            )\n\n            # Set up main git config with include directive\n            git_config = git_dir / \".git\" / \"config\"\n            with open(git_config, \"a\") as f:\n                f.write(f\"\\n[include]\\n    path = {include_config}\\n\")\n\n            # Verify the config is set up correctly using git command\n            repo = git.Repo(git_dir)\n            self.assertEqual(repo.git.config(\"user.name\"), \"Directive User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"directive@example.com\")\n\n            # Run aider and verify it doesn't change the git config\n            main([\"--yes\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n\n            # Check that the git config file wasn't modified\n            config_after_aider = git_config.read_text()\n            self.assertIn(\"[include]\", config_after_aider)\n\n    def test_resolve_aiderignore_path(self):\n        # Import the function directly to test it\n        from aider.args import resolve_aiderignore_path\n\n        # Test with absolute path\n        abs_path = os.path.abspath(\"/tmp/test/.aiderignore\")\n        self.assertEqual(resolve_aiderignore_path(abs_path), abs_path)\n\n        # Test with relative path and git root\n        git_root = \"/path/to/git/root\"\n        rel_path = \".aiderignore\"\n        self.assertEqual(\n            resolve_aiderignore_path(rel_path, git_root), str(Path(git_root) / rel_path)\n        )\n\n        # Test with relative path and no git root\n        rel_path = \".aiderignore\"\n        self.assertEqual(resolve_aiderignore_path(rel_path), rel_path)\n\n    def test_list_models_includes_metadata_models(self):\n        # Test that models from model-metadata.json appear in list-models output\n        with GitTemporaryDirectory():\n            # Create a temporary model-metadata.json with test models\n            metadata_file = Path(\".aider.model.metadata.json\")\n            test_models = {\n                \"unique-model-name\": {\n                    \"max_input_tokens\": 8192,\n                    \"litellm_provider\": \"test-provider\",\n                    \"mode\": \"chat\",\n                },\n                \"another-provider/another-unique-model\": {\n                    \"max_input_tokens\": 4096,\n                    \"litellm_provider\": \"another-provider\",\n                    \"mode\": \"chat\",\n                },\n            }\n            metadata_file.write_text(json.dumps(test_models))\n\n            # Capture stdout to check the output\n            with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n                main(\n                    [\n                        \"--list-models\",\n                        \"unique-model\",\n                        \"--model-metadata-file\",\n                        str(metadata_file),\n                        \"--yes\",\n                        \"--no-gitignore\",\n                    ],\n                    input=DummyInput(),\n                    output=DummyOutput(),\n                )\n                output = mock_stdout.getvalue()\n\n                # Check that the unique model name from our metadata file is listed\n                self.assertIn(\"unique-model-name\", output)\n\n    def test_list_models_includes_all_model_sources(self):\n        # Test that models from both litellm.model_cost and model-metadata.json appear in list-models\n        with GitTemporaryDirectory():\n            # Create a temporary model-metadata.json with test models\n            metadata_file = Path(\".aider.model.metadata.json\")\n            test_models = {\n                \"metadata-only-model\": {\n                    \"max_input_tokens\": 8192,\n                    \"litellm_provider\": \"test-provider\",\n                    \"mode\": \"chat\",\n                }\n            }\n            metadata_file.write_text(json.dumps(test_models))\n\n            # Capture stdout to check the output\n            with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n                main(\n                    [\n                        \"--list-models\",\n                        \"metadata-only-model\",\n                        \"--model-metadata-file\",\n                        str(metadata_file),\n                        \"--yes\",\n                        \"--no-gitignore\",\n                    ],\n                    input=DummyInput(),\n                    output=DummyOutput(),\n                )\n                output = mock_stdout.getvalue()\n\n                # Check that both models appear in the output\n                self.assertIn(\"metadata-only-model\", output)\n\n    def test_list_models_with_direct_resource_patch(self):\n        # Test that models from resources/model-metadata.json are included in list-models output\n        with GitTemporaryDirectory():\n            # Create a temporary file with test model metadata\n            test_file = Path(self.tempdir) / \"test-model-metadata.json\"\n            test_resource_models = {\n                \"special-model\": {\n                    \"max_input_tokens\": 8192,\n                    \"litellm_provider\": \"resource-provider\",\n                    \"mode\": \"chat\",\n                }\n            }\n\n            test_file.write_text(json.dumps(test_resource_models))\n\n            # Create a mock for the resource file path\n            mock_resource_path = MagicMock()\n            mock_resource_path.__str__.return_value = str(test_file)\n\n            # Create a mock for the files function that returns an object with joinpath\n            mock_files = MagicMock()\n            mock_files.joinpath.return_value = mock_resource_path\n\n            with patch(\"aider.main.importlib_resources.files\", return_value=mock_files):\n                # Capture stdout to check the output\n                with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n                    main(\n                        [\n                            \"--list-models\",\n                            \"special-model\",\n                            \"--yes\",\n                            \"--no-gitignore\",\n                        ],\n                        input=DummyInput(),\n                        output=DummyOutput(),\n                    )\n                    output = mock_stdout.getvalue()\n\n                    # Check that the resource model appears in the output\n                    self.assertIn(\"special-model\", output)\n\n    def test_git_config_include(self):\n        # Test that aider respects git config includes for user.name and user.email\n        with GitTemporaryDirectory() as git_dir:\n            git_dir = Path(git_dir)\n\n            # Create an includable config file with user settings\n            include_config = git_dir / \"included.gitconfig\"\n            include_config.write_text(\n                \"[user]\\n    name = Included User\\n    email = included@example.com\\n\"\n            )\n\n            # Set up main git config to include the other file\n            repo = git.Repo(git_dir)\n            repo.git.config(\"--local\", \"include.path\", str(include_config))\n\n            # Verify the config is set up correctly using git command\n            self.assertEqual(repo.git.config(\"user.name\"), \"Included User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"included@example.com\")\n\n            # Run aider and verify it doesn't change the git config\n            main([\"--yes\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n\n            # Check that the user settings are still the same using git command\n            repo = git.Repo(git_dir)  # Re-open repo to ensure we get fresh config\n            self.assertEqual(repo.git.config(\"user.name\"), \"Included User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"included@example.com\")\n\n    def test_git_config_include_directive(self):\n        # Test that aider respects the include directive in git config\n        with GitTemporaryDirectory() as git_dir:\n            git_dir = Path(git_dir)\n\n            # Create an includable config file with user settings\n            include_config = git_dir / \"included.gitconfig\"\n            include_config.write_text(\n                \"[user]\\n    name = Directive User\\n    email = directive@example.com\\n\"\n            )\n\n            # Set up main git config with include directive\n            git_config = git_dir / \".git\" / \"config\"\n            with open(git_config, \"a\") as f:\n                f.write(f\"\\n[include]\\n    path = {include_config}\\n\")\n\n            # Verify the config is set up correctly using git command\n            repo = git.Repo(git_dir)\n            self.assertEqual(repo.git.config(\"user.name\"), \"Directive User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"directive@example.com\")\n\n            # Run aider and verify it doesn't change the git config\n            main([\"--yes\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n\n            # Check that the git config file wasn't modified\n            config_after_aider = git_config.read_text()\n            self.assertIn(\"[include]\", config_after_aider)\n\n    def test_resolve_aiderignore_path(self):\n        # Import the function directly to test it\n        from aider.args import resolve_aiderignore_path\n\n        # Test with absolute path\n        abs_path = os.path.abspath(\"/tmp/test/.aiderignore\")\n        self.assertEqual(resolve_aiderignore_path(abs_path), abs_path)\n\n        # Test with relative path and git root\n        git_root = \"/path/to/git/root\"\n        rel_path = \".aiderignore\"\n        self.assertEqual(\n            resolve_aiderignore_path(rel_path, git_root), str(Path(git_root) / rel_path)\n        )\n\n        # Test with relative path and no git root\n        rel_path = \".aiderignore\"\n        self.assertEqual(resolve_aiderignore_path(rel_path), rel_path)\n\n    def test_list_models_includes_metadata_models(self):\n        # Test that models from model-metadata.json appear in list-models output\n        with GitTemporaryDirectory():\n            # Create a temporary model-metadata.json with test models\n            metadata_file = Path(\".aider.model.metadata.json\")\n            test_models = {\n                \"unique-model-name\": {\n                    \"max_input_tokens\": 8192,\n                    \"litellm_provider\": \"test-provider\",\n                    \"mode\": \"chat\",\n                },\n                \"another-provider/another-unique-model\": {\n                    \"max_input_tokens\": 4096,\n                    \"litellm_provider\": \"another-provider\",\n                    \"mode\": \"chat\",\n                },\n            }\n            metadata_file.write_text(json.dumps(test_models))\n\n            # Capture stdout to check the output\n            with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n                main(\n                    [\n                        \"--list-models\",\n                        \"unique-model\",\n                        \"--model-metadata-file\",\n                        str(metadata_file),\n                        \"--yes\",\n                        \"--no-gitignore\",\n                    ],\n                    input=DummyInput(),\n                    output=DummyOutput(),\n                )\n                output = mock_stdout.getvalue()\n\n                # Check that the unique model name from our metadata file is listed\n                self.assertIn(\"unique-model-name\", output)\n\n    def test_list_models_includes_all_model_sources(self):\n        # Test that models from both litellm.model_cost and model-metadata.json appear in list-models\n        with GitTemporaryDirectory():\n            # Create a temporary model-metadata.json with test models\n            metadata_file = Path(\".aider.model.metadata.json\")\n            test_models = {\n                \"metadata-only-model\": {\n                    \"max_input_tokens\": 8192,\n                    \"litellm_provider\": \"test-provider\",\n                    \"mode\": \"chat\",\n                }\n            }\n            metadata_file.write_text(json.dumps(test_models))\n\n            # Capture stdout to check the output\n            with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n                main(\n                    [\n                        \"--list-models\",\n                        \"metadata-only-model\",\n                        \"--model-metadata-file\",\n                        str(metadata_file),\n                        \"--yes\",\n                        \"--no-gitignore\",\n                    ],\n                    input=DummyInput(),\n                    output=DummyOutput(),\n                )\n                output = mock_stdout.getvalue()\n\n                # Check that both models appear in the output\n                self.assertIn(\"metadata-only-model\", output)\n\n    def test_list_models_with_direct_resource_patch(self):\n        # Test that models from resources/model-metadata.json are included in list-models output\n        with GitTemporaryDirectory():\n            # Create a temporary file with test model metadata\n            test_file = Path(self.tempdir) / \"test-model-metadata.json\"\n            test_resource_models = {\n                \"special-model\": {\n                    \"max_input_tokens\": 8192,\n                    \"litellm_provider\": \"resource-provider\",\n                    \"mode\": \"chat\",\n                }\n            }\n\n            test_file.write_text(json.dumps(test_resource_models))\n\n            # Create a mock for the resource file path\n            mock_resource_path = MagicMock()\n            mock_resource_path.__str__.return_value = str(test_file)\n\n            # Create a mock for the files function that returns an object with joinpath\n            mock_files = MagicMock()\n            mock_files.joinpath.return_value = mock_resource_path\n\n            with patch(\"aider.main.importlib_resources.files\", return_value=mock_files):\n                # Capture stdout to check the output\n                with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n                    main(\n                        [\n                            \"--list-models\",\n                            \"special-model\",\n                            \"--yes\",\n                            \"--no-gitignore\",\n                        ],\n                        input=DummyInput(),\n                        output=DummyOutput(),\n                    )\n                    output = mock_stdout.getvalue()\n\n                    # Check that the resource model appears in the output\n                    self.assertIn(\"special-model\", output)\n\n    def test_git_config_include(self):\n        # Test that aider respects git config includes for user.name and user.email\n        with GitTemporaryDirectory() as git_dir:\n            git_dir = Path(git_dir)\n\n            # Create an includable config file with user settings\n            include_config = git_dir / \"included.gitconfig\"\n            include_config.write_text(\n                \"[user]\\n    name = Included User\\n    email = included@example.com\\n\"\n            )\n\n            # Set up main git config to include the other file\n            repo = git.Repo(git_dir)\n            repo.git.config(\"--local\", \"include.path\", str(include_config))\n\n            # Verify the config is set up correctly using git command\n            self.assertEqual(repo.git.config(\"user.name\"), \"Included User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"included@example.com\")\n\n            # Run aider and verify it doesn't change the git config\n            main([\"--yes\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n\n            # Check that the user settings are still the same using git command\n            repo = git.Repo(git_dir)  # Re-open repo to ensure we get fresh config\n            self.assertEqual(repo.git.config(\"user.name\"), \"Included User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"included@example.com\")\n\n    def test_git_config_include_directive(self):\n        # Test that aider respects the include directive in git config\n        with GitTemporaryDirectory() as git_dir:\n            git_dir = Path(git_dir)\n\n            # Create an includable config file with user settings\n            include_config = git_dir / \"included.gitconfig\"\n            include_config.write_text(\n                \"[user]\\n    name = Directive User\\n    email = directive@example.com\\n\"\n            )\n\n            # Set up main git config with include directive\n            git_config = git_dir / \".git\" / \"config\"\n            with open(git_config, \"a\") as f:\n                f.write(f\"\\n[include]\\n    path = {include_config}\\n\")\n\n            # Verify the config is set up correctly using git command\n            repo = git.Repo(git_dir)\n            self.assertEqual(repo.git.config(\"user.name\"), \"Directive User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"directive@example.com\")\n\n            # Run aider and verify it doesn't change the git config\n            main([\"--yes\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n\n            # Check that the git config file wasn't modified\n            config_after_aider = git_config.read_text()\n            self.assertIn(\"[include]\", config_after_aider)\n\n    def test_resolve_aiderignore_path(self):\n        # Import the function directly to test it\n        from aider.args import resolve_aiderignore_path\n\n        # Test with absolute path\n        abs_path = os.path.abspath(\"/tmp/test/.aiderignore\")\n        self.assertEqual(resolve_aiderignore_path(abs_path), abs_path)\n\n        # Test with relative path and git root\n        git_root = \"/path/to/git/root\"\n        rel_path = \".aiderignore\"\n        self.assertEqual(\n            resolve_aiderignore_path(rel_path, git_root), str(Path(git_root) / rel_path)\n        )\n\n        # Test with relative path and no git root\n        rel_path = \".aiderignore\"\n        self.assertEqual(resolve_aiderignore_path(rel_path), rel_path)\n\n    def test_list_models_includes_metadata_models(self):\n        # Test that models from model-metadata.json appear in list-models output\n        with GitTemporaryDirectory():\n            # Create a temporary model-metadata.json with test models\n            metadata_file = Path(\".aider.model.metadata.json\")\n            test_models = {\n                \"unique-model-name\": {\n                    \"max_input_tokens\": 8192,\n                    \"litellm_provider\": \"test-provider\",\n                    \"mode\": \"chat\",\n                },\n                \"another-provider/another-unique-model\": {\n                    \"max_input_tokens\": 4096,\n                    \"litellm_provider\": \"another-provider\",\n                    \"mode\": \"chat\",\n                },\n            }\n            metadata_file.write_text(json.dumps(test_models))\n\n            # Capture stdout to check the output\n            with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n                main(\n                    [\n                        \"--list-models\",\n                        \"unique-model\",\n                        \"--model-metadata-file\",\n                        str(metadata_file),\n                        \"--yes\",\n                        \"--no-gitignore\",\n                    ],\n                    input=DummyInput(),\n                    output=DummyOutput(),\n                )\n                output = mock_stdout.getvalue()\n\n                # Check that the unique model name from our metadata file is listed\n                self.assertIn(\"unique-model-name\", output)\n\n    def test_list_models_includes_all_model_sources(self):\n        # Test that models from both litellm.model_cost and model-metadata.json appear in list-models\n        with GitTemporaryDirectory():\n            # Create a temporary model-metadata.json with test models\n            metadata_file = Path(\".aider.model.metadata.json\")\n            test_models = {\n                \"metadata-only-model\": {\n                    \"max_input_tokens\": 8192,\n                    \"litellm_provider\": \"test-provider\",\n                    \"mode\": \"chat\",\n                }\n            }\n            metadata_file.write_text(json.dumps(test_models))\n\n            # Capture stdout to check the output\n            with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n                main(\n                    [\n                        \"--list-models\",\n                        \"metadata-only-model\",\n                        \"--model-metadata-file\",\n                        str(metadata_file),\n                        \"--yes\",\n                        \"--no-gitignore\",\n                    ],\n                    input=DummyInput(),\n                    output=DummyOutput(),\n                )\n                output = mock_stdout.getvalue()\n\n                # Check that both models appear in the output\n                self.assertIn(\"metadata-only-model\", output)\n\n    def test_list_models_with_direct_resource_patch(self):\n        # Test that models from resources/model-metadata.json are included in list-models output\n        with GitTemporaryDirectory():\n            # Create a temporary file with test model metadata\n            test_file = Path(self.tempdir) / \"test-model-metadata.json\"\n            test_resource_models = {\n                \"special-model\": {\n                    \"max_input_tokens\": 8192,\n                    \"litellm_provider\": \"resource-provider\",\n                    \"mode\": \"chat\",\n                }\n            }\n\n            test_file.write_text(json.dumps(test_resource_models))\n\n            # Create a mock for the resource file path\n            mock_resource_path = MagicMock()\n            mock_resource_path.__str__.return_value = str(test_file)\n\n            # Create a mock for the files function that returns an object with joinpath\n            mock_files = MagicMock()\n            mock_files.joinpath.return_value = mock_resource_path\n\n            with patch(\"aider.main.importlib_resources.files\", return_value=mock_files):\n                # Capture stdout to check the output\n                with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n                    main(\n                        [\n                            \"--list-models\",\n                            \"special-model\",\n                            \"--yes\",\n                            \"--no-gitignore\",\n                        ],\n                        input=DummyInput(),\n                        output=DummyOutput(),\n                    )\n                    output = mock_stdout.getvalue()\n\n                    # Check that the resource model appears in the output\n                    self.assertIn(\"special-model\", output)\n\n    def test_git_config_include(self):\n        # Test that aider respects git config includes for user.name and user.email\n        with GitTemporaryDirectory() as git_dir:\n            git_dir = Path(git_dir)\n\n            # Create an includable config file with user settings\n            include_config = git_dir / \"included.gitconfig\"\n            include_config.write_text(\n                \"[user]\\n    name = Included User\\n    email = included@example.com\\n\"\n            )\n\n            # Set up main git config to include the other file\n            repo = git.Repo(git_dir)\n            repo.git.config(\"--local\", \"include.path\", str(include_config))\n\n            # Verify the config is set up correctly using git command\n            self.assertEqual(repo.git.config(\"user.name\"), \"Included User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"included@example.com\")\n\n            # Run aider and verify it doesn't change the git config\n            main([\"--yes\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n\n            # Check that the user settings are still the same using git command\n            repo = git.Repo(git_dir)  # Re-open repo to ensure we get fresh config\n            self.assertEqual(repo.git.config(\"user.name\"), \"Included User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"included@example.com\")\n\n    def test_git_config_include_directive(self):\n        # Test that aider respects the include directive in git config\n        with GitTemporaryDirectory() as git_dir:\n            git_dir = Path(git_dir)\n\n            # Create an includable config file with user settings\n            include_config = git_dir / \"included.gitconfig\"\n            include_config.write_text(\n                \"[user]\\n    name = Directive User\\n    email = directive@example.com\\n\"\n            )\n\n            # Set up main git config with include directive\n            git_config = git_dir / \".git\" / \"config\"\n            with open(git_config, \"a\") as f:\n                f.write(f\"\\n[include]\\n    path = {include_config}\\n\")\n\n            # Verify the config is set up correctly using git command\n            repo = git.Repo(git_dir)\n            self.assertEqual(repo.git.config(\"user.name\"), \"Directive User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"directive@example.com\")\n\n            # Run aider and verify it doesn't change the git config\n            main([\"--yes\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n\n            # Check that the git config file wasn't modified\n            config_after_aider = git_config.read_text()\n            self.assertIn(\"[include]\", config_after_aider)\n\n    def test_resolve_aiderignore_path(self):\n        # Import the function directly to test it\n        from aider.args import resolve_aiderignore_path\n\n        # Test with absolute path\n        abs_path = os.path.abspath(\"/tmp/test/.aiderignore\")\n        self.assertEqual(resolve_aiderignore_path(abs_path), abs_path)\n\n        # Test with relative path and git root\n        git_root = \"/path/to/git/root\"\n        rel_path = \".aiderignore\"\n        self.assertEqual(\n            resolve_aiderignore_path(rel_path, git_root), str(Path(git_root) / rel_path)\n        )\n\n        # Test with relative path and no git root\n        rel_path = \".aiderignore\"\n        self.assertEqual(resolve_aiderignore_path(rel_path), rel_path)\n\n    def test_list_models_includes_metadata_models(self):\n        # Test that models from model-metadata.json appear in list-models output\n        with GitTemporaryDirectory():\n            # Create a temporary model-metadata.json with test models\n            metadata_file = Path(\".aider.model.metadata.json\")\n            test_models = {\n                \"unique-model-name\": {\n                    \"max_input_tokens\": 8192,\n                    \"litellm_provider\": \"test-provider\",\n                    \"mode\": \"chat\",\n                },\n                \"another-provider/another-unique-model\": {\n                    \"max_input_tokens\": 4096,\n                    \"litellm_provider\": \"another-provider\",\n                    \"mode\": \"chat\",\n                },\n            }\n            metadata_file.write_text(json.dumps(test_models))\n\n            # Capture stdout to check the output\n            with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n                main(\n                    [\n                        \"--list-models\",\n                        \"unique-model\",\n                        \"--model-metadata-file\",\n                        str(metadata_file),\n                        \"--yes\",\n                        \"--no-gitignore\",\n                    ],\n                    input=DummyInput(),\n                    output=DummyOutput(),\n                )\n                output = mock_stdout.getvalue()\n\n                # Check that the unique model name from our metadata file is listed\n                self.assertIn(\"unique-model-name\", output)\n\n    def test_list_models_includes_all_model_sources(self):\n        # Test that models from both litellm.model_cost and model-metadata.json appear in list-models\n        with GitTemporaryDirectory():\n            # Create a temporary model-metadata.json with test models\n            metadata_file = Path(\".aider.model.metadata.json\")\n            test_models = {\n                \"metadata-only-model\": {\n                    \"max_input_tokens\": 8192,\n                    \"litellm_provider\": \"test-provider\",\n                    \"mode\": \"chat\",\n                }\n            }\n            metadata_file.write_text(json.dumps(test_models))\n\n            # Capture stdout to check the output\n            with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n                main(\n                    [\n                        \"--list-models\",\n                        \"metadata-only-model\",\n                        \"--model-metadata-file\",\n                        str(metadata_file),\n                        \"--yes\",\n                        \"--no-gitignore\",\n                    ],\n                    input=DummyInput(),\n                    output=DummyOutput(),\n                )\n                output = mock_stdout.getvalue()\n\n                # Check that both models appear in the output\n                self.assertIn(\"metadata-only-model\", output)\n\n    def test_list_models_with_direct_resource_patch(self):\n        # Test that models from resources/model-metadata.json are included in list-models output\n        with GitTemporaryDirectory():\n            # Create a temporary file with test model metadata\n            test_file = Path(self.tempdir) / \"test-model-metadata.json\"\n            test_resource_models = {\n                \"special-model\": {\n                    \"max_input_tokens\": 8192,\n                    \"litellm_provider\": \"resource-provider\",\n                    \"mode\": \"chat\",\n                }\n            }\n\n            test_file.write_text(json.dumps(test_resource_models))\n\n            # Create a mock for the resource file path\n            mock_resource_path = MagicMock()\n            mock_resource_path.__str__.return_value = str(test_file)\n\n            # Create a mock for the files function that returns an object with joinpath\n            mock_files = MagicMock()\n            mock_files.joinpath.return_value = mock_resource_path\n\n            with patch(\"aider.main.importlib_resources.files\", return_value=mock_files):\n                # Capture stdout to check the output\n                with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n                    main(\n                        [\n                            \"--list-models\",\n                            \"special-model\",\n                            \"--yes\",\n                            \"--no-gitignore\",\n                        ],\n                        input=DummyInput(),\n                        output=DummyOutput(),\n                    )\n                    output = mock_stdout.getvalue()\n\n                    # Check that the resource model appears in the output\n                    self.assertIn(\"special-model\", output)\n\n    def test_git_config_include(self):\n        # Test that aider respects git config includes for user.name and user.email\n        with GitTemporaryDirectory() as git_dir:\n            git_dir = Path(git_dir)\n\n            # Create an includable config file with user settings\n            include_config = git_dir / \"included.gitconfig\"\n            include_config.write_text(\n                \"[user]\\n    name = Included User\\n    email = included@example.com\\n\"\n            )\n\n            # Set up main git config to include the other file\n            repo = git.Repo(git_dir)\n            repo.git.config(\"--local\", \"include.path\", str(include_config))\n\n            # Verify the config is set up correctly using git command\n            self.assertEqual(repo.git.config(\"user.name\"), \"Included User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"included@example.com\")\n\n            # Run aider and verify it doesn't change the git config\n            main([\"--yes\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n\n            # Check that the user settings are still the same using git command\n            repo = git.Repo(git_dir)  # Re-open repo to ensure we get fresh config\n            self.assertEqual(repo.git.config(\"user.name\"), \"Included User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"included@example.com\")\n\n    def test_git_config_include_directive(self):\n        # Test that aider respects the include directive in git config\n        with GitTemporaryDirectory() as git_dir:\n            git_dir = Path(git_dir)\n\n            # Create an includable config file with user settings\n            include_config = git_dir / \"included.gitconfig\"\n            include_config.write_text(\n                \"[user]\\n    name = Directive User\\n    email = directive@example.com\\n\"\n            )\n\n            # Set up main git config with include directive\n            git_config = git_dir / \".git\" / \"config\"\n            with open(git_config, \"a\") as f:\n                f.write(f\"\\n[include]\\n    path = {include_config}\\n\")\n\n            # Verify the config is set up correctly using git command\n            repo = git.Repo(git_dir)\n            self.assertEqual(repo.git.config(\"user.name\"), \"Directive User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"directive@example.com\")\n\n            # Run aider and verify it doesn't change the git config\n            main([\"--yes\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n\n            # Check that the git config file wasn't modified\n            config_after_aider = git_config.read_text()\n            self.assertIn(\"[include]\", config_after_aider)\n\n    def test_resolve_aiderignore_path(self):\n        # Import the function directly to test it\n        from aider.args import resolve_aiderignore_path\n\n        # Test with absolute path\n        abs_path = os.path.abspath(\"/tmp/test/.aiderignore\")\n        self.assertEqual(resolve_aiderignore_path(abs_path), abs_path)\n\n        # Test with relative path and git root\n        git_root = \"/path/to/git/root\"\n        rel_path = \".aiderignore\"\n        self.assertEqual(\n            resolve_aiderignore_path(rel_path, git_root), str(Path(git_root) / rel_path)\n        )\n\n        # Test with relative path and no git root\n        rel_path = \".aiderignore\"\n        self.assertEqual(resolve_aiderignore_path(rel_path), rel_path)\n\n    def test_list_models_includes_metadata_models(self):\n        # Test that models from model-metadata.json appear in list-models output\n        with GitTemporaryDirectory():\n            # Create a temporary model-metadata.json with test models\n            metadata_file = Path(\".aider.model.metadata.json\")\n            test_models = {\n                \"unique-model-name\": {\n                    \"max_input_tokens\": 8192,\n                    \"litellm_provider\": \"test-provider\",\n                    \"mode\": \"chat\",\n                },\n                \"another-provider/another-unique-model\": {\n                    \"max_input_tokens\": 4096,\n                    \"litellm_provider\": \"another-provider\",\n                    \"mode\": \"chat\",\n                },\n            }\n            metadata_file.write_text(json.dumps(test_models))\n\n            # Capture stdout to check the output\n            with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n                main(\n                    [\n                        \"--list-models\",\n                        \"unique-model\",\n                        \"--model-metadata-file\",\n                        str(metadata_file),\n                        \"--yes\",\n                        \"--no-gitignore\",\n                    ],\n                    input=DummyInput(),\n                    output=DummyOutput(),\n                )\n                output = mock_stdout.getvalue()\n\n                # Check that the unique model name from our metadata file is listed\n                self.assertIn(\"unique-model-name\", output)\n\n    def test_list_models_includes_all_model_sources(self):\n        # Test that models from both litellm.model_cost and model-metadata.json appear in list-models\n        with GitTemporaryDirectory():\n            # Create a temporary model-metadata.json with test models\n            metadata_file = Path(\".aider.model.metadata.json\")\n            test_models = {\n                \"metadata-only-model\": {\n                    \"max_input_tokens\": 8192,\n                    \"litellm_provider\": \"test-provider\",\n                    \"mode\": \"chat\",\n                }\n            }\n            metadata_file.write_text(json.dumps(test_models))\n\n            # Capture stdout to check the output\n            with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n                main(\n                    [\n                        \"--list-models\",\n                        \"metadata-only-model\",\n                        \"--model-metadata-file\",\n                        str(metadata_file),\n                        \"--yes\",\n                        \"--no-gitignore\",\n                    ],\n                    input=DummyInput(),\n                    output=DummyOutput(),\n                )\n                output = mock_stdout.getvalue()\n\n                # Check that both models appear in the output\n                self.assertIn(\"metadata-only-model\", output)\n\n    def test_list_models_with_direct_resource_patch(self):\n        # Test that models from resources/model-metadata.json are included in list-models output\n        with GitTemporaryDirectory():\n            # Create a temporary file with test model metadata\n            test_file = Path(self.tempdir) / \"test-model-metadata.json\"\n            test_resource_models = {\n                \"special-model\": {\n                    \"max_input_tokens\": 8192,\n                    \"litellm_provider\": \"resource-provider\",\n                    \"mode\": \"chat\",\n                }\n            }\n\n            test_file.write_text(json.dumps(test_resource_models))\n\n            # Create a mock for the resource file path\n            mock_resource_path = MagicMock()\n            mock_resource_path.__str__.return_value = str(test_file)\n\n            # Create a mock for the files function that returns an object with joinpath\n            mock_files = MagicMock()\n            mock_files.joinpath.return_value = mock_resource_path\n\n            with patch(\"aider.main.importlib_resources.files\", return_value=mock_files):\n                # Capture stdout to check the output\n                with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n                    main(\n                        [\n                            \"--list-models\",\n                            \"special-model\",\n                            \"--yes\",\n                            \"--no-gitignore\",\n                        ],\n                        input=DummyInput(),\n                        output=DummyOutput(),\n                    )\n                    output = mock_stdout.getvalue()\n\n                    # Check that the resource model appears in the output\n                    self.assertIn(\"special-model\", output)\n\n    def test_git_config_include(self):\n        # Test that aider respects git config includes for user.name and user.email\n        with GitTemporaryDirectory() as git_dir:\n            git_dir = Path(git_dir)\n\n            # Create an includable config file with user settings\n            include_config = git_dir / \"included.gitconfig\"\n            include_config.write_text(\n                \"[user]\\n    name = Included User\\n    email = included@example.com\\n\"\n            )\n\n            # Set up main git config to include the other file\n            repo = git.Repo(git_dir)\n            repo.git.config(\"--local\", \"include.path\", str(include_config))\n\n            # Verify the config is set up correctly using git command\n            self.assertEqual(repo.git.config(\"user.name\"), \"Included User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"included@example.com\")\n\n            # Run aider and verify it doesn't change the git config\n            main([\"--yes\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n\n            # Check that the user settings are still the same using git command\n            repo = git.Repo(git_dir)  # Re-open repo to ensure we get fresh config\n            self.assertEqual(repo.git.config(\"user.name\"), \"Included User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"included@example.com\")\n\n    def test_git_config_include_directive(self):\n        # Test that aider respects the include directive in git config\n        with GitTemporaryDirectory() as git_dir:\n            git_dir = Path(git_dir)\n\n            # Create an includable config file with user settings\n            include_config = git_dir / \"included.gitconfig\"\n            include_config.write_text(\n                \"[user]\\n    name = Directive User\\n    email = directive@example.com\\n\"\n            )\n\n            # Set up main git config with include directive\n            git_config = git_dir / \".git\" / \"config\"\n            with open(git_config, \"a\") as f:\n                f.write(f\"\\n[include]\\n    path = {include_config}\\n\")\n\n            # Verify the config is set up correctly using git command\n            repo = git.Repo(git_dir)\n            self.assertEqual(repo.git.config(\"user.name\"), \"Directive User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"directive@example.com\")\n\n            # Run aider and verify it doesn't change the git config\n            main([\"--yes\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n\n            # Check that the git config file wasn't modified\n            config_after_aider = git_config.read_text()\n            self.assertIn(\"[include]\", config_after_aider)\n\n    def test_resolve_aiderignore_path(self):\n        # Import the function directly to test it\n        from aider.args import resolve_aiderignore_path\n\n        # Test with absolute path\n        abs_path = os.path.abspath(\"/tmp/test/.aiderignore\")\n        self.assertEqual(resolve_aiderignore_path(abs_path), abs_path)\n\n        # Test with relative path and git root\n        git_root = \"/path/to/git/root\"\n        rel_path = \".aiderignore\"\n        self.assertEqual(\n            resolve_aiderignore_path(rel_path, git_root), str(Path(git_root) / rel_path)\n        )\n\n        # Test with relative path and no git root\n        rel_path = \".aiderignore\"\n        self.assertEqual(resolve_aiderignore_path(rel_path), rel_path)\n\n    def test_list_models_includes_metadata_models(self):\n        # Test that models from model-metadata.json appear in list-models output\n        with GitTemporaryDirectory():\n            # Create a temporary model-metadata.json with test models\n            metadata_file = Path(\".aider.model.metadata.json\")\n            test_models = {\n                \"unique-model-name\": {\n                    \"max_input_tokens\": 8192,\n                    \"litellm_provider\": \"test-provider\",\n                    \"mode\": \"chat\",\n                },\n                \"another-provider/another-unique-model\": {\n                    \"max_input_tokens\": 4096,\n                    \"litellm_provider\": \"another-provider\",\n                    \"mode\": \"chat\",\n                },\n            }\n            metadata_file.write_text(json.dumps(test_models))\n\n            # Capture stdout to check the output\n            with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n                main(\n                    [\n                        \"--list-models\",\n                        \"unique-model\",\n                        \"--model-metadata-file\",\n                        str(metadata_file),\n                        \"--yes\",\n                        \"--no-gitignore\",\n                    ],\n                    input=DummyInput(),\n                    output=DummyOutput(),\n                )\n                output = mock_stdout.getvalue()\n\n                # Check that the unique model name from our metadata file is listed\n                self.assertIn(\"unique-model-name\", output)\n\n    def test_list_models_includes_all_model_sources(self):\n        # Test that models from both litellm.model_cost and model-metadata.json appear in list-models\n        with GitTemporaryDirectory():\n            # Create a temporary model-metadata.json with test models\n            metadata_file = Path(\".aider.model.metadata.json\")\n            test_models = {\n                \"metadata-only-model\": {\n                    \"max_input_tokens\": 8192,\n                    \"litellm_provider\": \"test-provider\",\n                    \"mode\": \"chat\",\n                }\n            }\n            metadata_file.write_text(json.dumps(test_models))\n\n            # Capture stdout to check the output\n            with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n                main(\n                    [\n                        \"--list-models\",\n                        \"metadata-only-model\",\n                        \"--model-metadata-file\",\n                        str(metadata_file),\n                        \"--yes\",\n                        \"--no-gitignore\",\n                    ],\n                    input=DummyInput(),\n                    output=DummyOutput(),\n                )\n                output = mock_stdout.getvalue()\n\n                # Check that both models appear in the output\n                self.assertIn(\"metadata-only-model\", output)\n\n    def test_list_models_with_direct_resource_patch(self):\n        # Test that models from resources/model-metadata.json are included in list-models output\n        with GitTemporaryDirectory():\n            # Create a temporary file with test model metadata\n            test_file = Path(self.tempdir) / \"test-model-metadata.json\"\n            test_resource_models = {\n                \"special-model\": {\n                    \"max_input_tokens\": 8192,\n                    \"litellm_provider\": \"resource-provider\",\n                    \"mode\": \"chat\",\n                }\n            }\n\n            test_file.write_text(json.dumps(test_resource_models))\n\n            # Create a mock for the resource file path\n            mock_resource_path = MagicMock()\n            mock_resource_path.__str__.return_value = str(test_file)\n\n            # Create a mock for the files function that returns an object with joinpath\n            mock_files = MagicMock()\n            mock_files.joinpath.return_value = mock_resource_path\n\n            with patch(\"aider.main.importlib_resources.files\", return_value=mock_files):\n                # Capture stdout to check the output\n                with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n                    main(\n                        [\n                            \"--list-models\",\n                            \"special-model\",\n                            \"--yes\",\n                            \"--no-gitignore\",\n                        ],\n                        input=DummyInput(),\n                        output=DummyOutput(),\n                    )\n                    output = mock_stdout.getvalue()\n\n                    # Check that the resource model appears in the output\n                    self.assertIn(\"special-model\", output)\n\n    def test_git_config_include(self):\n        # Test that aider respects git config includes for user.name and user.email\n        with GitTemporaryDirectory() as git_dir:\n            git_dir = Path(git_dir)\n\n            # Create an includable config file with user settings\n            include_config = git_dir / \"included.gitconfig\"\n            include_config.write_text(\n                \"[user]\\n    name = Included User\\n    email = included@example.com\\n\"\n            )\n\n            # Set up main git config to include the other file\n            repo = git.Repo(git_dir)\n            repo.git.config(\"--local\", \"include.path\", str(include_config))\n\n            # Verify the config is set up correctly using git command\n            self.assertEqual(repo.git.config(\"user.name\"), \"Included User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"included@example.com\")\n\n            # Run aider and verify it doesn't change the git config\n            main([\"--yes\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n\n            # Check that the user settings are still the same using git command\n            repo = git.Repo(git_dir)  # Re-open repo to ensure we get fresh config\n            self.assertEqual(repo.git.config(\"user.name\"), \"Included User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"included@example.com\")\n\n    def test_git_config_include_directive(self):\n        # Test that aider respects the include directive in git config\n        with GitTemporaryDirectory() as git_dir:\n            git_dir = Path(git_dir)\n\n            # Create an includable config file with user settings\n            include_config = git_dir / \"included.gitconfig\"\n            include_config.write_text(\n                \"[user]\\n    name = Directive User\\n    email = directive@example.com\\n\"\n            )\n\n            # Set up main git config with include directive\n            git_config = git_dir / \".git\" / \"config\"\n            with open(git_config, \"a\") as f:\n                f.write(f\"\\n[include]\\n    path = {include_config}\\n\")\n\n            # Verify the config is set up correctly using git command\n            repo = git.Repo(git_dir)\n            self.assertEqual(repo.git.config(\"user.name\"), \"Directive User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"directive@example.com\")\n\n            # Run aider and verify it doesn't change the git config\n            main([\"--yes\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n\n            # Check that the git config file wasn't modified\n            config_after_aider = git_config.read_text()\n            self.assertIn(\"[include]\", config_after_aider)\n\n    def test_resolve_aiderignore_path(self):\n        # Import the function directly to test it\n        from aider.args import resolve_aiderignore_path\n\n        # Test with absolute path\n        abs_path = os.path.abspath(\"/tmp/test/.aiderignore\")\n        self.assertEqual(resolve_aiderignore_path(abs_path), abs_path)\n\n        # Test with relative path and git root\n        git_root = \"/path/to/git/root\"\n        rel_path = \".aiderignore\"\n        self.assertEqual(\n            resolve_aiderignore_path(rel_path, git_root), str(Path(git_root) / rel_path)\n        )\n\n        # Test with relative path and no git root\n        rel_path = \".aiderignore\"\n        self.assertEqual(resolve_aiderignore_path(rel_path), rel_path)\n\n    def test_list_models_includes_metadata_models(self):\n        # Test that models from model-metadata.json appear in list-models output\n        with GitTemporaryDirectory():\n            # Create a temporary model-metadata.json with test models\n            metadata_file = Path(\".aider.model.metadata.json\")\n            test_models = {\n                \"unique-model-name\": {\n                    \"max_input_tokens\": 8192,\n                    \"litellm_provider\": \"test-provider\",\n                    \"mode\": \"chat\",\n                },\n                \"another-provider/another-unique-model\": {\n                    \"max_input_tokens\": 4096,\n                    \"litellm_provider\": \"another-provider\",\n                    \"mode\": \"chat\",\n                },\n            }\n            metadata_file.write_text(json.dumps(test_models))\n\n            # Capture stdout to check the output\n            with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n                main(\n                    [\n                        \"--list-models\",\n                        \"unique-model\",\n                        \"--model-metadata-file\",\n                        str(metadata_file),\n                        \"--yes\",\n                        \"--no-gitignore\",\n                    ],\n                    input=DummyInput(),\n                    output=DummyOutput(),\n                )\n                output = mock_stdout.getvalue()\n\n                # Check that the unique model name from our metadata file is listed\n                self.assertIn(\"unique-model-name\", output)\n\n    def test_list_models_includes_all_model_sources(self):\n        # Test that models from both litellm.model_cost and model-metadata.json appear in list-models\n        with GitTemporaryDirectory():\n            # Create a temporary model-metadata.json with test models\n            metadata_file = Path(\".aider.model.metadata.json\")\n            test_models = {\n                \"metadata-only-model\": {\n                    \"max_input_tokens\": 8192,\n                    \"litellm_provider\": \"test-provider\",\n                    \"mode\": \"chat\",\n                }\n            }\n            metadata_file.write_text(json.dumps(test_models))\n\n            # Capture stdout to check the output\n            with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n                main(\n                    [\n                        \"--list-models\",\n                        \"metadata-only-model\",\n                        \"--model-metadata-file\",\n                        str(metadata_file),\n                        \"--yes\",\n                        \"--no-gitignore\",\n                    ],\n                    input=DummyInput(),\n                    output=DummyOutput(),\n                )\n                output = mock_stdout.getvalue()\n\n                # Check that both models appear in the output\n                self.assertIn(\"metadata-only-model\", output)\n\n    def test_list_models_with_direct_resource_patch(self):\n        # Test that models from resources/model-metadata.json are included in list-models output\n        with GitTemporaryDirectory():\n            # Create a temporary file with test model metadata\n            test_file = Path(self.tempdir) / \"test-model-metadata.json\"\n            test_resource_models = {\n                \"special-model\": {\n                    \"max_input_tokens\": 8192,\n                    \"litellm_provider\": \"resource-provider\",\n                    \"mode\": \"chat\",\n                }\n            }\n\n            test_file.write_text(json.dumps(test_resource_models))\n\n            # Create a mock for the resource file path\n            mock_resource_path = MagicMock()\n            mock_resource_path.__str__.return_value = str(test_file)\n\n            # Create a mock for the files function that returns an object with joinpath\n            mock_files = MagicMock()\n            mock_files.joinpath.return_value = mock_resource_path\n\n            with patch(\"aider.main.importlib_resources.files\", return_value=mock_files):\n                # Capture stdout to check the output\n                with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n                    main(\n                        [\n                            \"--list-models\",\n                            \"special-model\",\n                            \"--yes\",\n                            \"--no-gitignore\",\n                        ],\n                        input=DummyInput(),\n                        output=DummyOutput(),\n                    )\n                    output = mock_stdout.getvalue()\n\n                    # Check that the resource model appears in the output\n                    self.assertIn(\"special-model\", output)\n\n    def test_git_config_include(self):\n        # Test that aider respects git config includes for user.name and user.email\n        with GitTemporaryDirectory() as git_dir:\n            git_dir = Path(git_dir)\n\n            # Create an includable config file with user settings\n            include_config = git_dir / \"included.gitconfig\"\n            include_config.write_text(\n                \"[user]\\n    name = Included User\\n    email = included@example.com\\n\"\n            )\n\n            # Set up main git config to include the other file\n            repo = git.Repo(git_dir)\n            repo.git.config(\"--local\", \"include.path\", str(include_config))\n\n            # Verify the config is set up correctly using git command\n            self.assertEqual(repo.git.config(\"user.name\"), \"Included User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"included@example.com\")\n\n            # Run aider and verify it doesn't change the git config\n            main([\"--yes\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n\n            # Check that the user settings are still the same using git command\n            repo = git.Repo(git_dir)  # Re-open repo to ensure we get fresh config\n            self.assertEqual(repo.git.config(\"user.name\"), \"Included User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"included@example.com\")\n\n    def test_git_config_include_directive(self):\n        # Test that aider respects the include directive in git config\n        with GitTemporaryDirectory() as git_dir:\n            git_dir = Path(git_dir)\n\n            # Create an includable config file with user settings\n            include_config = git_dir / \"included.gitconfig\"\n            include_config.write_text(\n                \"[user]\\n    name = Directive User\\n    email = directive@example.com\\n\"\n            )\n\n            # Set up main git config with include directive\n            git_config = git_dir / \".git\" / \"config\"\n            with open(git_config, \"a\") as f:\n                f.write(f\"\\n[include]\\n    path = {include_config}\\n\")\n\n            # Verify the config is set up correctly using git command\n            repo = git.Repo(git_dir)\n            self.assertEqual(repo.git.config(\"user.name\"), \"Directive User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"directive@example.com\")\n\n            # Run aider and verify it doesn't change the git config\n            main([\"--yes\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n\n            # Check that the git config file wasn't modified\n            config_after_aider = git_config.read_text()\n            self.assertIn(\"[include]\", config_after_aider)\n\n    def test_resolve_aiderignore_path(self):\n        # Import the function directly to test it\n        from aider.args import resolve_aiderignore_path\n\n        # Test with absolute path\n        abs_path = os.path.abspath(\"/tmp/test/.aiderignore\")\n        self.assertEqual(resolve_aiderignore_path(abs_path), abs_path)\n\n        # Test with relative path and git root\n        git_root = \"/path/to/git/root\"\n        rel_path = \".aiderignore\"\n        self.assertEqual(\n            resolve_aiderignore_path(rel_path, git_root), str(Path(git_root) / rel_path)\n        )\n\n        # Test with relative path and no git root\n        rel_path = \".aiderignore\"\n        self.assertEqual(resolve_aiderignore_path(rel_path), rel_path)\n\n    def test_list_models_includes_metadata_models(self):\n        # Test that models from model-metadata.json appear in list-models output\n        with GitTemporaryDirectory():\n            # Create a temporary model-metadata.json with test models\n            metadata_file = Path(\".aider.model.metadata.json\")\n            test_models = {\n                \"unique-model-name\": {\n                    \"max_input_tokens\": 8192,\n                    \"litellm_provider\": \"test-provider\",\n                    \"mode\": \"chat\",\n                },\n                \"another-provider/another-unique-model\": {\n                    \"max_input_tokens\": 4096,\n                    \"litellm_provider\": \"another-provider\",\n                    \"mode\": \"chat\",\n                },\n            }\n            metadata_file.write_text(json.dumps(test_models))\n\n            # Capture stdout to check the output\n            with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n                main(\n                    [\n                        \"--list-models\",\n                        \"unique-model\",\n                        \"--",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.26026999667282547,
      "diff_lines": 2583,
      "added_lines": 1447,
      "removed_lines": 849,
      "diff_preview": "--- expected+++ predicted@@ -1,4 +1,3 @@-import json\n import os\n import subprocess\n import tempfile\n@@ -69,7 +68,7 @@         subdir = Path(\"subdir\")\n         subdir.mkdir()\n         make_repo(str(subdir))\n-        res = main([\"subdir\", \"foo.txt\"], input=DummyInput(), output=DummyOutput())\n+        res = main([\"--yes\", str(subdir / \"foo.txt\"), str(subdir / \"bar.txt\"), \"--exit\"], input=DummyInput(), output=DummyOutput())\n         self.assertNotEqual(res, None)\n \n     @patch(\"aider.repo.GitRepo.get_commit_message\", return_value=\"mock commit message\")\n@@ -89,13 +88,13 @@         make_repo()\n \n         Path(\".aider.conf.yml\").write_text(\"auto-commits: false\\n\")\n-        with patch(\"aider.coders.Coder.create\") as MockCoder:\n-            main([\"--yes\"], input=DummyInput(), output=DummyOutput())\n+        with patch(\"aider.main.Coder.create\") as MockCoder:\n+            main([\"--yes\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n             _, kwargs = MockCoder.call_args\n             assert kwargs[\"auto_commits\"] is False\n \n         Path(\".aider.conf.yml\").write_text(\"auto-commits: true\\n\")\n-        with patch(\"aider.coders.Coder.create\") as MockCoder:\n+        with patch(\"aider.main.Coder.create\") as MockCoder:\n             main([], input=DummyInput(), output=DummyOutput())\n             _, kwargs = MockCoder.call_args\n             assert kwargs[\"auto_commits\"] is True\n@@ -109,9 +108,6 @@         subprocess.run([\"git\", \"add\", str(subdir)])\n         subprocess.run([\"git\", \"commit\", \"-m\", \"added\"])\n \n-        # This will throw a git error on windows if get_tracked_files doesn't\n-        # properly convert git/posix/paths to git\\posix\\paths.\n-        # Because aider will try and `git add` a file that's already in the repo.\n         main([\"--yes\", str(fname), \"--exit\"], input=DummyInput(), output=DummyOutput())\n \n     def test_setup_git(self):\n@@ -140,78 +136,47 @@ \n             self.assertEqual(\".aider*\", gitignore.read_text().splitlines()[0])\n \n-            # Test without .env file present\n             gitignore.write_text(\"one\\ntwo\\n\")\n             check_gitignore(cwd, io)\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 407.45479130744934,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 77009,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_tests_basic_test_models.py",
    "exact_match": false,
    "similarity": 0.2261184540094794,
    "diff_lines": 573,
    "predicted_output": "import unittest\nfrom unittest.mock import patch\nimport json\nfrom pathlib import Path\n\nfrom aider.models import Model, get_model_info\n\n\nclass TestModels(unittest.TestCase):\n    def test_get_model_info_nonexistent(self):\n        manager = ModelInfoManager()\n        info = manager.get_model_info(\"non-existent-model\")\n        self.assertEqual(info, {})\n\n    def test_max_context_tokens(self):\n        model = Model(\"gpt-3.5-turbo\")\n        self.assertEqual(model.info[\"max_input_tokens\"], 16385)\n\n        model = Model(\"gpt-3.5-turbo-16k\")\n        self.assertEqual(model.info[\"max_input_tokens\"], 16385)\n\n        model = Model(\"gpt-3.5-turbo-1106\")\n        self.assertEqual(model.info[\"max_input_tokens\"], 16385)\n\n        model = Model(\"gpt-4\")\n        self.assertEqual(model.info[\"max_input_tokens\"], 8 * 1024)\n\n        model = Model(\"gpt-4-32k\")\n        self.assertEqual(model.info[\"max_input_tokens\"], 32 * 1024)\n\n        model = Model(\"gpt-4-0613\")\n        self.assertEqual(model.info[\"max_input_tokens\"], 8 * 1024)\n\n    @patch(\"aider.models.Path.home\")\n    @patch(\"aider.models.time.time\")\n    @patch(\"aider.models.Path.stat\")\n    @patch(\"aider.models.Path.read_text\")\n    def test_get_model_info_cached(self, mock_read_text, mock_stat, mock_time, mock_home):\n        # Setup mock\n        mock_home.return_value = Path(\"/mock/home\")\n        mock_time.return_value = 1000000\n        mock_stat.return_value.st_mtime = 999999  # File modified 1 second ago\n        mock_read_text.return_value = json.dumps({\"gpt-3.5-turbo\": {\"max_input_tokens\": 16385}})\n\n        # Test\n        info = get_model_info(\"gpt-3.5-turbo\")\n        self.assertEqual(info, {\"max_input_tokens\": 16385})\n\n    @patch(\"aider.models.Path.home\")\n    @patch(\"aider.models.time.time\")\n    @patch(\"aider.models.Path.stat\")\n    @patch(\"aider.models.requests.get\")\n    def test_get_model_info_fetch(self, mock_get, mock_stat, mock_time, mock_home):\n        # Setup mock\n        mock_home.return_value = Path(\"/mock/home\")\n        mock_time.return_value = 1000000\n        mock_stat.return_value.st_mtime = 900000  # File modified a long time ago\n        mock_get.return_value.status_code = 200\n        mock_get.return_value.json.return_value = {\"gpt-4\": {\"max_input_tokens\": 8192}}\n\n        # Test\n        info = get_model_info(\"gpt-4\")\n        self.assertEqual(info, {\"max_input_tokens\": 8192})\n\n    @patch(\"aider.models.Path.home\")\n    @patch(\"aider.models.time.time\")\n    @patch(\"aider.models.Path.stat\")\n    @patch(\"aider.models.requests.get\")\n    @patch(\"aider.models.litellm.get_model_info\")\n    def test_get_model_info_fallback(self, mock_litellm, mock_get, mock_stat, mock_time, mock_home):\n        # Setup mock\n        mock_home.return_value = Path(\"/mock/home\")\n        mock_time.return_value = 1000000\n        mock_stat.return_value.st_mtime = 900000  # File modified a long time ago\n        mock_get.return_value.status_code = 404  # Simulate failed request\n        mock_litellm.return_value = {\"max_input_tokens\": 4096}\n\n        # Test\n        info = get_model_info(\"unknown-model\")\n        self.assertEqual(info, {\"max_input_tokens\": 4096})\n\n    def test_get_model_info_nonexistent(self):\n        info = get_model_info(\"non-existent-model\")\n        self.assertEqual(info, {})\n\n    def test_model_aliases(self):\n        # Test common aliases\n        model = Model(\"4\")\n        self.assertEqual(model.name, \"gpt-4-0613\")\n\n        model = Model(\"4o\")\n        self.assertEqual(model.name, \"gpt-4o\")\n\n        model = Model(\"35turbo\")\n        self.assertEqual(model.name, \"gpt-3.5-turbo\")\n\n        model = Model(\"35-turbo\")\n        self.assertEqual(model.name, \"gpt-3.5-turbo\")\n\n        model = Model(\"3\")\n        self.assertEqual(model.name, \"gpt-3.5-turbo\")\n\n        model = Model(\"sonnet\")\n        self.assertEqual(model.name, \"anthropic/claude-3-7-sonnet-20250219\")\n\n        model = Model(\"haiku\")\n        self.assertEqual(model.name, \"claude-3-5-haiku-20241022\")\n\n        model = Model(\"opus\")\n        self.assertEqual(model.name, \"claude-3-opus-20240229\")\n\n        # Test non-alias passes through unchanged\n        model = Model(\"gpt-4\")\n        self.assertEqual(model.name, \"gpt-4\")\n\n    def test_aider_extra_model_settings(self):\n        import tempfile\n\n        # Create temporary YAML file with test settings\n        test_settings = [\n            {\n                \"name\": \"aider/extra_params\",\n                \"extra_params\": {\n                    \"extra_headers\": {\"Foo\": \"bar\"},\n                    \"some_param\": \"some value\",\n                },\n            },\n        ]\n\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".yml\") as tmp:\n            yaml.dump(test_settings, tmp)\n            tmp.flush()\n\n            # Register the test settings\n            register_models([tmp.name])\n\n            # Test that defaults are applied when no exact match\n            model = Model(\"claude-3-5-sonnet-20240620\")\n            self.assertEqual(model.extra_params[\"extra_headers\"][\"Foo\"], \"bar\")\n            self.assertEqual(model.extra_params[\"some_param\"], \"some value\")\n\n            # Test that exact match overrides defaults but not overrides\n            model = Model(\"gpt-4\")\n            self.assertEqual(model.extra_params[\"extra_headers\"][\"Foo\"], \"bar\")\n            self.assertEqual(model.extra_params[\"some_param\"], \"some value\")\n\nif __name__ == \"__main__\":\n    unittest.main()",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.2261184540094794,
      "diff_lines": 573,
      "added_lines": 53,
      "removed_lines": 468,
      "diff_preview": "--- expected+++ predicted@@ -1,30 +1,12 @@ import unittest\n-from unittest.mock import ANY, MagicMock, patch\n+from unittest.mock import patch\n+import json\n+from pathlib import Path\n \n-from aider.models import (\n-    ANTHROPIC_BETA_HEADER,\n-    Model,\n-    ModelInfoManager,\n-    register_models,\n-    sanity_check_model,\n-    sanity_check_models,\n-)\n+from aider.models import Model, get_model_info\n \n \n class TestModels(unittest.TestCase):\n-    def setUp(self):\n-        \"\"\"Reset MODEL_SETTINGS before each test\"\"\"\n-        from aider.models import MODEL_SETTINGS\n-\n-        self._original_settings = MODEL_SETTINGS.copy()\n-\n-    def tearDown(self):\n-        \"\"\"Restore original MODEL_SETTINGS after each test\"\"\"\n-        from aider.models import MODEL_SETTINGS\n-\n-        MODEL_SETTINGS.clear()\n-        MODEL_SETTINGS.extend(self._original_settings)\n-\n     def test_get_model_info_nonexistent(self):\n         manager = ModelInfoManager()\n         info = manager.get_model_info(\"non-existent-model\")\n@@ -49,76 +31,57 @@         model = Model(\"gpt-4-0613\")\n         self.assertEqual(model.info[\"max_input_tokens\"], 8 * 1024)\n \n-    @patch(\"os.environ\")\n-    def test_sanity_check_model_all_set(self, mock_environ):\n-        mock_environ.get.return_value = \"dummy_value\"\n-        mock_io = MagicMock()\n-        model = MagicMock()\n-        model.name = \"test-model\"\n-        model.missing_keys = [\"API_KEY1\", \"API_KEY2\"]\n-        model.keys_in_environment = True\n-        model.info = {\"some\": \"info\"}\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 35.507887840270996,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 34611,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_tests_basic_test_onboarding.py",
    "exact_match": false,
    "similarity": 0.9757335770128948,
    "diff_lines": 110,
    "predicted_output": "import argparse\nimport base64\nimport hashlib\nimport os\nimport unittest\nfrom unittest.mock import MagicMock, patch\n\nimport requests\n\n# Import the functions to be tested\nfrom aider.onboarding import (\n    check_openrouter_tier,\n    exchange_code_for_key,\n    find_available_port,\n    generate_pkce_codes,\n    offer_openrouter_oauth,\n    select_default_model,\n    start_openrouter_oauth_flow,\n    try_to_select_default_model,\n)\n\n# Mock the Analytics class as it's used in some functions\nclass DummyAnalytics:\n    def event(self, *args, **kwargs):\n        pass\n\n# Mock the InputOutput class\nclass DummyIO:\n    def tool_output(self, *args, **kwargs):\n        pass\n\n    def tool_warning(self, *args, **kwargs):\n        pass\n\n    def tool_error(self, *args, **kwargs):\n        pass\n\n    def confirm_ask(self, *args, **kwargs):\n        return False  # Default to no confirmation\n\n    def offer_url(self, *args, **kwargs):\n        pass\n\nclass TestOnboarding(unittest.TestCase):\n    @patch(\"requests.get\")\n    def test_check_openrouter_tier_free(self, mock_get):\n        \"\"\"Test check_openrouter_tier identifies free tier.\"\"\"\n        mock_response = MagicMock()\n        mock_response.json.return_value = {\"data\": {\"is_free_tier\": True}}\n        mock_response.raise_for_status.return_value = None\n        mock_get.return_value = mock_response\n        self.assertTrue(check_openrouter_tier(\"fake_key\"))\n        mock_get.assert_called_once_with(\n            \"https://openrouter.ai/api/v1/auth/key\",\n            headers={\"Authorization\": \"Bearer fake_key\"},\n            timeout=5,\n        )\n\n    @patch(\"requests.get\")\n    def test_check_openrouter_tier_paid(self, mock_get):\n        \"\"\"Test check_openrouter_tier identifies paid tier.\"\"\"\n        mock_response = MagicMock()\n        mock_response.json.return_value = {\"data\": {\"is_free_tier\": False}}\n        mock_response.raise_for_status.return_value = None\n        mock_get.return_value = mock_response\n        self.assertFalse(check_openrouter_tier(\"fake_key\"))\n\n    @patch(\"requests.get\")\n    def test_check_openrouter_tier_api_error(self, mock_get):\n        \"\"\"Test check_openrouter_tier defaults to free on API error.\"\"\"\n        mock_get.side_effect = requests.exceptions.RequestException(\"API Error\")\n        self.assertTrue(check_openrouter_tier(\"fake_key\"))\n\n    @patch(\"requests.get\")\n    def test_check_openrouter_tier_missing_key(self, mock_get):\n        \"\"\"Test check_openrouter_tier defaults to free if key is missing in response.\"\"\"\n        mock_response = MagicMock()\n        mock_response.json.return_value = {\"data\": {}}  # Missing 'is_free_tier'\n        mock_response.raise_for_status.return_value = None\n        mock_get.return_value = mock_response\n        self.assertTrue(check_openrouter_tier(\"fake_key\"))\n\n    @patch(\"aider.onboarding.check_openrouter_tier\")\n    @patch.dict(os.environ, {}, clear=True)\n    def test_try_select_default_model_no_keys(self, mock_check_tier):\n        \"\"\"Test no model is selected when no keys are present.\"\"\"\n        self.assertIsNone(try_to_select_default_model())\n        mock_check_tier.assert_not_called()\n\n    @patch(\"aider.onboarding.check_openrouter_tier\", return_value=True)  # Assume free tier\n    @patch.dict(os.environ, {\"OPENROUTER_API_KEY\": \"or_key\"}, clear=True)\n    def test_try_select_default_model_openrouter_free(self, mock_check_tier):\n        \"\"\"Test OpenRouter free model selection.\"\"\"\n        self.assertEqual(\n            try_to_select_default_model(), \"openrouter/google/gemini-2.5-pro-exp-03-25:free\"\n        )\n        mock_check_tier.assert_called_once_with(\"or_key\")\n\n    @patch(\"aider.onboarding.check_openrouter_tier\", return_value=False)  # Assume paid tier\n    @patch.dict(os.environ, {\"OPENROUTER_API_KEY\": \"or_key\"}, clear=True)\n    def test_try_select_default_model_openrouter_paid(self, mock_check_tier):\n        \"\"\"Test OpenRouter paid model selection.\"\"\"\n        self.assertEqual(try_to_select_default_model(), \"openrouter/anthropic/claude-3.7-sonnet\")\n        mock_check_tier.assert_called_once_with(\"or_key\")\n\n    @patch(\"aider.onboarding.check_openrouter_tier\")\n    @patch.dict(os.environ, {\"ANTHROPIC_API_KEY\": \"an_key\"}, clear=True)\n    def test_try_select_default_model_anthropic(self, mock_check_tier):\n        \"\"\"Test Anthropic model selection.\"\"\"\n        self.assertEqual(try_to_select_default_model(), \"sonnet\")\n        mock_check_tier.assert_not_called()\n\n    @patch(\"aider.onboarding.check_openrouter_tier\")\n    @patch.dict(os.environ, {\"DEEPSEEK_API_KEY\": \"ds_key\"}, clear=True)\n    def test_try_select_default_model_deepseek(self, mock_check_tier):\n        \"\"\"Test Deepseek model selection.\"\"\"\n        self.assertEqual(try_to_select_default_model(), \"deepseek\")\n        mock_check_tier.assert_not_called()\n\n    @patch(\"aider.onboarding.check_openrouter_tier\")\n    @patch.dict(os.environ, {\"OPENAI_API_KEY\": \"oa_key\"}, clear=True)\n    def test_try_select_default_model_openai(self, mock_check_tier):\n        \"\"\"Test OpenAI model selection.\"\"\"\n        self.assertEqual(try_to_select_default_model(), \"gpt-4o\")\n        mock_check_tier.assert_not_called()\n\n    @patch(\"aider.onboarding.check_openrouter_tier\")\n    @patch.dict(os.environ, {\"GEMINI_API_KEY\": \"gm_key\"}, clear=True)\n    def test_try_select_default_model_gemini(self, mock_check_tier):\n        \"\"\"Test Gemini model selection.\"\"\"\n        self.assertEqual(try_to_select_default_model(), \"gemini/gemini-2.5-pro-exp-03-25\")\n        mock_check_tier.assert_not_called()\n\n    @patch(\"aider.onboarding.check_openrouter_tier\")\n    @patch.dict(os.environ, {\"VERTEXAI_PROJECT\": \"vx_proj\"}, clear=True)\n    def test_try_select_default_model_vertex(self, mock_check_tier):\n        \"\"\"Test Vertex AI model selection.\"\"\"\n        self.assertEqual(try_to_select_default_model(), \"vertex_ai/gemini-2.5-pro-exp-03-25\")\n        mock_check_tier.assert_not_called()\n\n    @patch(\"aider.onboarding.check_openrouter_tier\", return_value=False)  # Paid\n    @patch.dict(os.environ, {\"OPENROUTER_API_KEY\": \"or_key\", \"OPENAI_API_KEY\": \"oa_key\"}, clear=True)\n    def test_try_select_default_model_priority_openrouter(self, mock_check_tier):\n        \"\"\"Test OpenRouter key takes priority.\"\"\"\n        self.assertEqual(try_to_select_default_model(), \"openrouter/anthropic/claude-3.7-sonnet\")\n        mock_check_tier.assert_called_once_with(\"or_key\")\n\n    @patch(\"aider.onboarding.check_openrouter_tier\")\n    @patch.dict(os.environ, {\"ANTHROPIC_API_KEY\": \"an_key\", \"OPENAI_API_KEY\": \"oa_key\"}, clear=True)\n    def test_try_select_default_model_priority_anthropic(self, mock_check_tier):\n        \"\"\"Test Anthropic key takes priority over OpenAI.\"\"\"\n        self.assertEqual(try_to_select_default_model(), \"sonnet\")\n        mock_check_tier.assert_not_called()\n\n    @patch(\"socketserver.TCPServer\")\n    def test_find_available_port_success(self, mock_tcp_server):\n        \"\"\"Test finding an available port.\"\"\"\n        # Simulate port 8484 being available\n        mock_tcp_server.return_value.__enter__.return_value = None  # Allow context manager\n        port = find_available_port(start_port=8484, end_port=8484)\n        self.assertEqual(port, 8484)\n        mock_tcp_server.assert_called_once_with((\"localhost\", 8484), None)\n\n    @patch(\"socketserver.TCPServer\")\n    def test_find_available_port_in_use(self, mock_tcp_server):\n        \"\"\"Test finding the next available port if the first is in use.\"\"\"\n        # Simulate port 8484 raising OSError, 8485 being available\n        mock_tcp_server.side_effect = [OSError, MagicMock()]\n        mock_tcp_server.return_value.__enter__.return_value = None  # Allow context manager\n        port = find_available_port(start_port=8484, end_port=8485)\n        self.assertEqual(port, 8485)\n        self.assertEqual(mock_tcp_server.call_count, 2)\n        mock_tcp_server.assert_any_call((\"localhost\", 8484), None)\n        mock_tcp_server.assert_any_call((\"localhost\", 8485), None)\n\n    @patch(\"socketserver.TCPServer\", side_effect=OSError)\n    def test_find_available_port_none_available(self, mock_tcp_server):\n        \"\"\"Test returning None if no ports are available in the range.\"\"\"\n        port = find_available_port(start_port=8484, end_port=8485)\n        self.assertIsNone(port)\n        self.assertEqual(mock_tcp_server.call_count, 2)  # Tried 8484 and 8485\n\n    def test_generate_pkce_codes(self):\n        \"\"\"Test PKCE code generation.\"\"\"\n        verifier, challenge = generate_pkce_codes()\n        self.assertIsInstance(verifier, str)\n        self.assertIsInstance(challenge, str)\n        self.assertGreater(len(verifier), 40)  # Check reasonable length\n        self.assertGreater(len(challenge), 40)\n        # Verify the challenge is the SHA256 hash of the verifier, base64 encoded\n        hasher = hashlib.sha256()\n        hasher.update(verifier.encode(\"utf-8\"))\n        expected_challenge = base64.urlsafe_b64encode(hasher.digest()).rstrip(b\"=\").decode(\"utf-8\")\n        self.assertEqual(challenge, expected_challenge)\n\n    @patch(\"requests.post\")\n    def test_exchange_code_for_key_success(self, mock_post):\n        \"\"\"Test successful code exchange for API key.\"\"\"\n        mock_response = MagicMock()\n        mock_response.json.return_value = {\"key\": \"test_api_key\"}\n        mock_response.raise_for_status.return_value = None\n        mock_post.return_value = mock_response\n        io_mock = DummyIO()\n\n        api_key = exchange_code_for_key(\"auth_code\", \"verifier\", io_mock)\n\n        self.assertEqual(api_key, \"test_api_key\")\n        mock_post.assert_called_once_with(\n            \"https://openrouter.ai/api/v1/auth/keys\",\n            headers={\"Content-Type\": \"application/json\"},\n            json={\n                \"code\": \"auth_code\",\n                \"code_verifier\": \"verifier\",\n                \"code_challenge_method\": \"S256\",\n            },\n            timeout=30,\n        )\n\n    @patch(\"requests.post\")\n    def test_exchange_code_for_key_missing_key(self, mock_post):\n        \"\"\"Test code exchange when 'key' is missing in response.\"\"\"\n        mock_response = MagicMock()\n        mock_response.json.return_value = {\"other_data\": \"value\"}  # Missing 'key'\n        mock_response.raise_for_status.return_value = None\n        mock_response.text = '{\"other_data\": \"value\"}'\n        mock_post.return_value = mock_response\n        io_mock = DummyIO()\n        io_mock.tool_error = MagicMock()  # Track error output\n\n        api_key = exchange_code_for_key(\"auth_code\", \"verifier\", io_mock)\n\n        self.assertIsNone(api_key)\n        io_mock.tool_error.assert_any_call(\"Error: 'key' not found in OpenRouter response.\")\n        io_mock.tool_error.assert_any_call('Response: {\"other_data\": \"value\"}')\n\n    @patch(\"requests.post\")\n    def test_exchange_code_for_key_http_error(self, mock_post):\n        \"\"\"Test code exchange with HTTP error.\"\"\"\n        mock_response = MagicMock()\n        mock_response.status_code = 400\n        mock_response.reason = \"Bad Request\"\n        mock_response.text = '{\"error\": \"invalid_code\"}'\n        http_error = requests.exceptions.HTTPError(response=mock_response)\n        mock_post.side_effect = http_error\n        io_mock = DummyIO()\n        io_mock.tool_error = MagicMock()\n\n        api_key = exchange_code_for_key(\"auth_code\", \"verifier\", io_mock)\n\n        self.assertIsNone(api_key)\n        io_mock.tool_error.assert_any_call(\n            \"Error exchanging code for OpenRouter key: 400 Bad Request\"\n        )\n        io_mock.tool_error.assert_any_call('Response: {\"error\": \"invalid_code\"}')\n\n    @patch(\"requests.post\")\n    def test_exchange_code_for_key_timeout(self, mock_post):\n        \"\"\"Test code exchange with timeout.\"\"\"\n        mock_post.side_effect = requests.exceptions.Timeout(\"Timeout\")\n        io_mock = DummyIO()\n        io_mock.tool_error = MagicMock()\n\n        api_key = exchange_code_for_key(\"auth_code\", \"verifier\", io_mock)\n\n        self.assertIsNone(api_key)\n        io_mock.tool_error.assert_called_once_with(\n            \"Error: Request to OpenRouter timed out during code exchange.\"\n        )\n\n    @patch(\"requests.post\")\n    def test_exchange_code_for_key_request_exception(self, mock_post):\n        \"\"\"Test code exchange with general request exception.\"\"\"\n        req_exception = requests.exceptions.RequestException(\"Network Error\")\n        mock_post.side_effect = req_exception\n        io_mock = DummyIO()\n        io_mock.tool_error = MagicMock()\n\n        api_key = exchange_code_for_key(\"auth_code\", \"verifier\", io_mock)\n\n        self.assertIsNone(api_key)\n        io_mock.tool_error.assert_called_once_with(\n            f\"Error exchanging code for OpenRouter key: {req_exception}\"\n        )\n\n    # --- Tests for select_default_model ---\n\n    @patch(\"aider.onboarding.try_to_select_default_model\", return_value=\"gpt-4o\")\n    @patch(\"aider.onboarding.offer_openrouter_oauth\")\n    def test_select_default_model_already_specified(self, mock_offer_oauth, mock_try_select):\n        \"\"\"Test select_default_model returns args.model if provided.\"\"\"\n        args = argparse.Namespace(model=\"specific-model\")\n        io_mock = DummyIO()\n        analytics_mock = DummyAnalytics()\n        selected_model = select_default_model(args, io_mock, analytics_mock)\n        self.assertEqual(selected_model, \"specific-model\")\n        mock_try_select.assert_not_called()\n        mock_offer_oauth.assert_not_called()\n\n    @patch(\"aider.onboarding.try_to_select_default_model\", return_value=\"gpt-4o\")\n    @patch(\"aider.onboarding.offer_openrouter_oauth\")\n    def test_select_default_model_found_via_env(self, mock_offer_oauth, mock_try_select):\n        \"\"\"Test select_default_model returns model found by try_to_select.\"\"\"\n        args = argparse.Namespace(model=None)  # No model specified\n        io_mock = DummyIO()\n        io_mock.tool_warning = MagicMock()  # Track warnings\n        analytics_mock = DummyAnalytics()\n        analytics_mock.event = MagicMock()  # Track events\n\n        selected_model = select_default_model(args, io_mock, analytics_mock)\n\n        self.assertEqual(selected_model, \"gpt-4o\")\n        mock_try_select.assert_called_once()\n        io_mock.tool_warning.assert_called_once_with(\n            \"Using gpt-4o model with API key from environment.\"\n        )\n        analytics_mock.event.assert_called_once_with(\"auto_model_selection\", model=\"gpt-4o\")\n        mock_offer_oauth.assert_not_called()\n\n    @patch(\"aider.onboarding.try_to_select_default_model\", side_effect=[None, None])  # Fails first, fails after oauth attempt\n    @patch(\"aider.onboarding.offer_openrouter_oauth\", return_value=False)  # OAuth offered but fails/declined\n    def test_select_default_model_no_keys_oauth_fail(self, mock_offer_oauth, mock_try_select):\n        \"\"\"Test select_default_model offers OAuth when no keys, but OAuth fails.\"\"\"\n        args = argparse.Namespace(model=None)\n        io_mock = DummyIO()\n        io_mock.tool_warning = MagicMock()\n        io_mock.offer_url = MagicMock()\n        analytics_mock = DummyAnalytics()\n\n        selected_model = select_default_model(args, io_mock, analytics_mock)\n\n        self.assertIsNone(selected_model)\n        self.assertEqual(mock_try_select.call_count, 2)  # Called before and after oauth attempt\n        mock_offer_oauth.assert_called_once_with(io_mock, analytics_mock)\n        io_mock.tool_warning.assert_called_once_with(\n            \"No LLM model was specified and no API keys were provided.\"\n        )\n        io_mock.offer_url.assert_called_once()  # Should offer docs URL\n\n    @patch(\"aider.onboarding.try_to_select_default_model\", side_effect=[None, \"openrouter/google/gemini-2.5-pro-exp-03-25:free\"])  # Fails first, succeeds after oauth\n    @patch(\"aider.onboarding.offer_openrouter_oauth\", return_value=True)  # OAuth offered and succeeds\n    def test_select_default_model_no_keys_oauth_success(self, mock_offer_oauth, mock_try_select):\n        \"\"\"Test select_default_model offers OAuth, which succeeds.\"\"\"\n        args = argparse.Namespace(model=None)\n        io_mock = DummyIO()\n        io_mock.tool_warning = MagicMock()\n        analytics_mock = DummyAnalytics()\n\n        selected_model = select_default_model(args, io_mock, analytics_mock)\n\n        self.assertEqual(selected_model, \"openrouter/google/gemini-2.5-pro-exp-03-25:free\")\n        self.assertEqual(mock_try_select.call_count, 2)  # Called before and after oauth\n        mock_offer_oauth.assert_called_once_with(io_mock, analytics_mock)\n        # First warning about no keys, second about using the model found after OAuth\n        self.assertEqual(io_mock.tool_warning.call_count, 2)\n        io_mock.tool_warning.assert_any_call(\n            \"No LLM model was specified and no API keys were provided.\"\n        )\n\n    # --- Tests for offer_openrouter_oauth ---\n    @patch(\"aider.onboarding.start_openrouter_oauth_flow\", return_value=\"new_or_key\")\n    @patch.dict(os.environ, {}, clear=True)  # Ensure no key exists initially\n    def test_offer_openrouter_oauth_confirm_yes_success(self, mock_start_oauth):\n        \"\"\"Test offer_openrouter_oauth when user confirms and OAuth succeeds.\"\"\"\n        io_mock = DummyIO()\n        io_mock.confirm_ask = MagicMock(return_value=True)  # User says yes\n        analytics_mock = DummyAnalytics()\n        analytics_mock.event = MagicMock()\n\n        result = offer_openrouter_oauth(io_mock, analytics_mock)\n\n        self.assertTrue(result)\n        io_mock.confirm_ask.assert_called_once()\n        mock_start_oauth.assert_called_once_with(io_mock, analytics_mock)\n        self.assertEqual(os.environ.get(\"OPENROUTER_API_KEY\"), \"new_or_key\")\n        analytics_mock.event.assert_any_call(\"oauth_flow_initiated\", provider=\"openrouter\")\n        analytics_mock.event.assert_any_call(\"oauth_flow_success\")\n        # Clean up env var\n        del os.environ[\"OPENROUTER_API_KEY\"]\n\n    @patch(\"aider.onboarding.start_openrouter_oauth_flow\", return_value=None)  # OAuth fails\n    @patch.dict(os.environ, {}, clear=True)\n    def test_offer_openrouter_oauth_confirm_yes_fail(self, mock_start_oauth):\n        \"\"\"Test offer_openrouter_oauth when user confirms but OAuth fails.\"\"\"\n        io_mock = DummyIO()\n        io_mock.confirm_ask = MagicMock(return_value=True)  # User says yes\n        io_mock.tool_error = MagicMock()\n        analytics_mock = DummyAnalytics()\n        analytics_mock.event = MagicMock()\n\n        result = offer_openrouter_oauth(io_mock, analytics_mock)\n\n        self.assertFalse(result)\n        io_mock.confirm_ask.assert_called_once()\n        mock_start_oauth.assert_called_once_with(io_mock, analytics_mock)\n        self.assertNotIn(\"OPENROUTER_API_KEY\", os.environ)\n        io_mock.tool_error.assert_called_once_with(\"OpenRouter authentication did not complete successfully.\")\n        analytics_mock.event.assert_any_call(\"oauth_flow_initiated\", provider=\"openrouter\")\n        analytics_mock.event.assert_any_call(\"oauth_flow_failure\")\n\n    @patch(\"aider.onboarding.start_openrouter_oauth_flow\")\n    def test_offer_openrouter_oauth_confirm_no(self, mock_start_oauth):\n        \"\"\"Test offer_openrouter_oauth when user declines.\"\"\"\n        io_mock = DummyIO()\n        io_mock.confirm_ask = MagicMock(return_value=False)  # User says no\n        analytics_mock = DummyAnalytics()\n        analytics_mock.event = MagicMock()\n\n        result = offer_openrouter_oauth(io_mock, analytics_mock)\n\n        self.assertFalse(result)\n        io_mock.confirm_ask.assert_called_once()\n        mock_start_oauth.assert_not_called()\n        analytics_mock.event.assert_not_called()  # No OAuth events if declined\n\nif __name__ == \"__main__\":\n    unittest.main()",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9757335770128948,
      "diff_lines": 110,
      "added_lines": 10,
      "removed_lines": 33,
      "diff_preview": "--- expected+++ predicted@@ -15,16 +15,15 @@     generate_pkce_codes,\n     offer_openrouter_oauth,\n     select_default_model,\n+    start_openrouter_oauth_flow,\n     try_to_select_default_model,\n )\n-\n \n # Mock the Analytics class as it's used in some functions\n class DummyAnalytics:\n     def event(self, *args, **kwargs):\n         pass\n \n-\n # Mock the InputOutput class\n class DummyIO:\n     def tool_output(self, *args, **kwargs):\n@@ -41,7 +40,6 @@ \n     def offer_url(self, *args, **kwargs):\n         pass\n-\n \n class TestOnboarding(unittest.TestCase):\n     @patch(\"requests.get\")\n@@ -141,9 +139,7 @@         mock_check_tier.assert_not_called()\n \n     @patch(\"aider.onboarding.check_openrouter_tier\", return_value=False)  # Paid\n-    @patch.dict(\n-        os.environ, {\"OPENROUTER_API_KEY\": \"or_key\", \"OPENAI_API_KEY\": \"oa_key\"}, clear=True\n-    )\n+    @patch.dict(os.environ, {\"OPENROUTER_API_KEY\": \"or_key\", \"OPENAI_API_KEY\": \"oa_key\"}, clear=True)\n     def test_try_select_default_model_priority_openrouter(self, mock_check_tier):\n         \"\"\"Test OpenRouter key takes priority.\"\"\"\n         self.assertEqual(try_to_select_default_model(), \"openrouter/anthropic/claude-3.7-sonnet\")\n@@ -320,12 +316,8 @@         analytics_mock.event.assert_called_once_with(\"auto_model_selection\", model=\"gpt-4o\")\n         mock_offer_oauth.assert_not_called()\n \n-    @patch(\n-        \"aider.onboarding.try_to_select_default_model\", side_effect=[None, None]\n-    )  # Fails first, fails after oauth attempt\n-    @patch(\n-        \"aider.onboarding.offer_openrouter_oauth\", return_value=False\n-    )  # OAuth offered but fails/declined\n+    @patch(\"aider.onboarding.try_to_select_default_model\", side_effect=[None, None])  # Fails first, fails after oauth attempt\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 116.8351538181305,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 16586,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_tests_basic_test_reasoning.py",
    "exact_match": false,
    "similarity": 0.49696935365093753,
    "diff_lines": 440,
    "predicted_output": "import unittest\nfrom unittest.mock import MagicMock, patch\n\nfrom aider.coders.base_coder import Coder\nfrom aider.dump import dump  # noqa\nfrom aider.io import InputOutput\nfrom aider.models import Model\nfrom aider.reasoning_tags import (\n    REASONING_END,\n    REASONING_START,\n    remove_reasoning_content,\n)\n\n\nclass TestReasoning(unittest.TestCase):\n    def test_send_with_reasoning_content(self):\n        \"\"\"Test that reasoning content from the 'reasoning' attribute is properly formatted\n        and output.\"\"\"\n        # Setup IO with no pretty\n        io = InputOutput(pretty=False)\n        io.assistant_output = MagicMock()\n\n        # Setup model and coder\n        model = Model(\"gpt-3.5-turbo\")\n        coder = Coder.create(model, None, io=io, stream=False)\n\n        # Test data\n        reasoning_content = \"My step-by-step reasoning process\"\n        main_content = \"Final answer after reasoning\"\n\n        # Mock completion response with reasoning content\n        class MockCompletion:\n            def __init__(self, content, reasoning):\n                self.content = content\n                # Add required attributes expected by show_send_output\n                self.choices = [MagicMock()]\n                self.choices[0].message.content = content\n                self.choices[0].message.reasoning = reasoning  # Using reasoning instead of reasoning_content\n                self.finish_reason = \"stop\"\n\n        mock_completion = MockCompletion(main_content, reasoning_content)\n\n        # Create a mock hash object\n        mock_hash = MagicMock()\n        mock_hash.hexdigest.return_value = \"mock_hash_digest\"\n\n        # Mock the model's send_completion method to return the expected tuple format\n        with patch.object(model, \"send_completion\", return_value=(mock_hash, mock_completion)):\n            # Call send with a simple message\n            messages = [{\"role\": \"user\", \"content\": \"test prompt\"}]\n            list(coder.send(messages))\n\n            # Now verify ai_output was called with the right content\n            io.assistant_output.assert_called_once()\n            output = io.assistant_output.call_args[0][0]\n\n            dump(output)\n\n            # Output should contain formatted reasoning tags\n            self.assertIn(REASONING_START, output)\n            self.assertIn(REASONING_END, output)\n\n            # Output should include both reasoning and main content\n            self.assertIn(reasoning_content, output)\n            self.assertIn(main_content, output)\n\n            # Verify that partial_response_content only contains the main content\n            coder.remove_reasoning_content()\n            self.assertEqual(coder.partial_response_content.strip(), main_content.strip())\n\n            # Ensure proper order: reasoning first, then main content\n            reasoning_pos = output.find(reasoning_content)\n            main_pos = output.find(main_content)\n            self.assertLess(\n                reasoning_pos, main_pos, \"Reasoning content should appear before main content\"\n            )\n\n    def test_send_with_reasoning_stream(self):\n        \"\"\"Test that streaming reasoning content from the 'reasoning' attribute is properly\n        formatted and output.\"\"\"\n        # Setup IO with pretty output for streaming\n        io = InputOutput(pretty=True)\n        mock_mdstream = MagicMock()\n        io.get_assistant_mdstream = MagicMock(return_value=mock_mdstream)\n\n        # Setup model and coder\n        model = Model(\"gpt-3.5-turbo\")\n        coder = Coder.create(model, None, io=io, stream=True)\n\n        # Ensure the coder shows pretty output\n        coder.show_pretty = MagicMock(return_value=True)\n\n        # Mock streaming response chunks\n        class MockStreamingChunk:\n            def __init__(\n                self, content=None, reasoning_content=None, reasoning=None, finish_reason=None\n            ):\n                self.choices = [MagicMock()]\n                self.choices[0].delta = MagicMock()\n                self.choices[0].finish_reason = finish_reason\n\n                # Set content if provided\n                if content is not None:\n                    self.choices[0].delta.content = content\n                else:\n                    # Need to handle attribute access that would raise AttributeError\n                    delattr(self.choices[0].delta, \"content\")\n\n                # Set reasoning_content if provided\n                if reasoning_content is not None:\n                    self.choices[0].delta.reasoning_content = reasoning_content\n                else:\n                    # Need to handle attribute access that would raise AttributeError\n                    delattr(self.choices[0].delta, \"reasoning_content\")\n\n                # Set reasoning if provided\n                if reasoning is not None:\n                    self.choices[0].delta.reasoning = reasoning\n                else:\n                    # Need to handle attribute access that would raise AttributeError\n                    delattr(self.choices[0].delta, \"reasoning\")\n\n        # Create chunks to simulate streaming - using reasoning attribute instead of reasoning_content\n        chunks = [\n            # First chunk with reasoning content starts the tag\n            MockStreamingChunk(reasoning=\"My step-by-step \"),\n            # Additional reasoning content\n            MockStreamingChunk(reasoning=\"reasoning process\"),\n            # Switch to main content - this will automatically end the reasoning tag\n            MockStreamingChunk(content=\"Final \"),\n            # More main content\n            MockStreamingChunk(content=\"answer \"),\n            MockStreamingChunk(content=\"after reasoning\"),\n            # End the response\n            MockStreamingChunk(finish_reason=\"stop\"),\n        ]\n\n        # Create a mock hash object\n        mock_hash = MagicMock()\n        mock_hash.hexdigest.return_value = \"mock_hash_digest\"\n\n        # Mock the model's send_completion to return the hash and completion\n        with (\n            patch.object(model, \"send_completion\", return_value=(mock_hash, chunks)),\n            patch.object(model, \"token_count\", return_value=10),\n        ):  # Mock token count to avoid serialization issues\n            # Set mdstream directly on the coder object\n            coder.mdstream = mock_mdstream\n\n            # Call send with a simple message\n            messages = [{\"role\": \"user\", \"content\": \"test prompt\"}]\n            list(coder.send(messages))\n\n            # Verify mdstream.update was called multiple times\n            mock_mdstream.update.assert_called()\n\n            coder.live_incremental_response(True)\n\n            # Explicitly get all calls to update\n            update_calls = mock_mdstream.update.call_args_list\n\n            # There should be at least two calls - one for streaming and one final\n            self.assertGreaterEqual(\n                len(update_calls), 2, \"Should have at least two calls to update (streaming + final)\"\n            )\n\n            # Check that at least one call has final=True (should be the last one)\n            has_final_true = any(call[1].get(\"final\", False) for call in update_calls)\n            self.assertTrue(has_final_true, \"At least one update call should have final=True\")\n\n            # Get the text from the last update call\n            final_text = update_calls[-1][0][0]\n\n            # The final text should include both reasoning and main content with proper formatting\n            self.assertIn(REASONING_START, final_text)\n            self.assertIn(\"My step-by-step reasoning process\", final_text)\n            self.assertIn(REASONING_END, final_text)\n            self.assertIn(\"Final answer after reasoning\", final_text)\n\n            # Ensure proper order: reasoning first, then main content\n            reasoning_pos = final_text.find(\"My step-by-step reasoning process\")\n            main_pos = final_text.find(\"Final answer after reasoning\")\n            self.assertLess(\n                reasoning_pos, main_pos, \"Reasoning content should appear before main content\"\n            )\n\n            # Verify that partial_response_content only contains the main content\n            coder.remove_reasoning_content()\n            expected_content = \"Final answer after reasoning\"\n            self.assertEqual(coder.partial_response_content.strip(), expected_content)\n\n    @patch(\"aider.models.litellm.completion\")\n    def test_simple_send_with_retries_removes_reasoning(self, mock_completion):\n        \"\"\"Test that simple_send_with_retries correctly removes reasoning content.\"\"\"\n        model = Model(\"deepseek-r1\")  # This model has reasoning_tag=\"think\"\n\n        # Mock the completion response\n        mock_response = MagicMock()\n        mock_response.choices = [MagicMock(message=MagicMock(content=\"\"\"Here is some text\n<think>\nThis reasoning should be removed\n</think>\nAnd this text should remain\"\"\"))]\n        mock_completion.return_value = mock_response\n\n        messages = [{\"role\": \"user\", \"content\": \"test\"}]\n        result = model.simple_send_with_retries(messages)\n\n        expected = \"\"\"Here is some text\n\nAnd this text should remain\"\"\"\n        self.assertEqual(result, expected)\n\n        # Verify the completion was called\n        mock_completion.assert_called_once()\n\n\nif __name__ == \"__main__\":\n    unittest.main()",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.49696935365093753,
      "diff_lines": 440,
      "added_lines": 8,
      "removed_lines": 398,
      "diff_preview": "--- expected+++ predicted@@ -14,7 +14,8 @@ \n class TestReasoning(unittest.TestCase):\n     def test_send_with_reasoning_content(self):\n-        \"\"\"Test that reasoning content is properly formatted and output.\"\"\"\n+        \"\"\"Test that reasoning content from the 'reasoning' attribute is properly formatted\n+        and output.\"\"\"\n         # Setup IO with no pretty\n         io = InputOutput(pretty=False)\n         io.assistant_output = MagicMock()\n@@ -29,12 +30,12 @@ \n         # Mock completion response with reasoning content\n         class MockCompletion:\n-            def __init__(self, content, reasoning_content):\n+            def __init__(self, content, reasoning):\n                 self.content = content\n                 # Add required attributes expected by show_send_output\n                 self.choices = [MagicMock()]\n                 self.choices[0].message.content = content\n-                self.choices[0].message.reasoning_content = reasoning_content\n+                self.choices[0].message.reasoning = reasoning  # Using reasoning instead of reasoning_content\n                 self.finish_reason = \"stop\"\n \n         mock_completion = MockCompletion(main_content, reasoning_content)\n@@ -74,8 +75,9 @@                 reasoning_pos, main_pos, \"Reasoning content should appear before main content\"\n             )\n \n-    def test_send_with_reasoning_content_stream(self):\n-        \"\"\"Test that streaming reasoning content is properly formatted and output.\"\"\"\n+    def test_send_with_reasoning_stream(self):\n+        \"\"\"Test that streaming reasoning content from the 'reasoning' attribute is properly\n+        formatted and output.\"\"\"\n         # Setup IO with pretty output for streaming\n         io = InputOutput(pretty=True)\n         mock_mdstream = MagicMock()\n@@ -118,399 +120,7 @@                     # Need to handle attribute access that would raise AttributeError\n                     delattr(self.choices[0].delta, \"reasoning\")\n \n-        # Create chunks to simulate streaming\n-        chunks = [\n-            # First chunk with reasoning content starts the tag\n-            MockStreamingChunk(reasoning_content=\"My step-by-step \"),\n-            # Additional reasoning content\n-            MockStreamingChunk(reasoning_content=\"reasoning process\"),\n-            # Switch to main content - this will automatically end the reasoning tag\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 35.79085111618042,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 19336,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_tests_basic_test_repo.py",
    "exact_match": false,
    "similarity": 0.867033426631921,
    "diff_lines": 176,
    "predicted_output": "import os\nimport platform\nimport tempfile\nimport time\nimport unittest\nfrom pathlib import Path\nfrom unittest.mock import patch\n\nimport git\n\nfrom aider.dump import dump  # noqa: F401\nfrom aider.io import InputOutput\nfrom aider.models import Model\nfrom aider.repo import GitRepo\nfrom aider.utils import GitTemporaryDirectory\n\n\nclass TestRepo(unittest.TestCase):\n    def setUp(self):\n        self.GPT35 = Model(\"gpt-3.5-turbo\")\n\n    def test_diffs_empty_repo(self):\n        with GitTemporaryDirectory():\n            repo = git.Repo()\n\n            # Add a change to the index\n            fname = Path(\"foo.txt\")\n            fname.write_text(\"index\\n\")\n            repo.git.add(str(fname))\n\n            # Make a change in the working dir\n            fname.write_text(\"workingdir\\n\")\n\n            git_repo = GitRepo(InputOutput(), None, \".\")\n            diffs = git_repo.get_diffs()\n            self.assertIn(\"index\", diffs)\n            self.assertIn(\"workingdir\", diffs)\n\n    def test_diffs_nonempty_repo(self):\n        with GitTemporaryDirectory():\n            repo = git.Repo()\n            fname = Path(\"foo.txt\")\n            fname.touch()\n            repo.git.add(str(fname))\n\n            fname2 = Path(\"bar.txt\")\n            fname2.touch()\n            repo.git.add(str(fname2))\n\n            repo.git.commit(\"-m\", \"initial\")\n\n            fname.write_text(\"index\\n\")\n            repo.git.add(str(fname))\n\n            fname2.write_text(\"workingdir\\n\")\n\n            git_repo = GitRepo(InputOutput(), None, \".\")\n            diffs = git_repo.get_diffs()\n            self.assertIn(\"index\", diffs)\n            self.assertIn(\"workingdir\", diffs)\n\n    def test_diffs_detached_head(self):\n        with GitTemporaryDirectory():\n            repo = git.Repo()\n            fname = Path(\"foo.txt\")\n            fname.touch()\n            repo.git.add(str(fname))\n            repo.git.commit(\"-m\", \"foo\")\n\n            fname2 = Path(\"bar.txt\")\n            fname2.touch()\n            repo.git.add(str(fname2))\n            repo.git.commit(\"-m\", \"bar\")\n\n            fname3 = Path(\"baz.txt\")\n            fname3.touch()\n            repo.git.add(str(fname3))\n            repo.git.commit(\"-m\", \"baz\")\n\n            repo.git.checkout(\"HEAD^\")\n\n            fname.write_text(\"index\\n\")\n            repo.git.add(str(fname))\n\n            fname2.write_text(\"workingdir\\n\")\n\n            git_repo = GitRepo(InputOutput(), None, \".\")\n            diffs = git_repo.get_diffs()\n            self.assertIn(\"index\", diffs)\n            self.assertIn(\"workingdir\", diffs)\n\n    def test_diffs_between_commits(self):\n        with GitTemporaryDirectory():\n            repo = git.Repo()\n            fname = Path(\"foo.txt\")\n\n            fname.write_text(\"one\\n\")\n            repo.git.add(str(fname))\n            repo.git.commit(\"-m\", \"initial\")\n\n            fname.write_text(\"two\\n\")\n            repo.git.add(str(fname))\n            repo.git.commit(\"-m\", \"second\")\n\n            git_repo = GitRepo(InputOutput(), None, \".\")\n            diffs = git_repo.diff_commits(False, \"HEAD~1\", \"HEAD\")\n            self.assertIn(\"two\", diffs)\n\n    @patch(\"aider.repo.simple_send_with_retries\")\n    def test_get_commit_message(self, mock_send):\n        mock_send.side_effect = [\"\", \"a good commit message\"]\n\n        model1 = Model(\"gpt-3.5-turbo\")\n        model2 = Model(\"gpt-4\")\n        repo = GitRepo(InputOutput(), None, None, models=[model1, model2])\n        \n        # Call the get_commit_message method with dummy diff and context\n        result = repo.get_commit_message(\"dummy diff\", \"dummy context\")\n\n        # Assert that the returned message is the expected one from the second model\n        self.assertEqual(result, \"a good commit message\")\n        \n        # Check that simple_send_with_retries was called twice\n        self.assertEqual(mock_send.call_count, 2)\n        \n        # Check that it was called with the correct model names\n        self.assertEqual(mock_send.call_args_list[0][0][0], model1)\n        self.assertEqual(mock_send.call_args_list[1][0][0], model2)\n\n    @patch(\"aider.repo.simple_send_with_retries\")\n    def test_get_commit_message_strip_quotes(self, mock_send):\n        mock_send.return_value = '\"a good commit message\"'\n\n        repo = GitRepo(InputOutput(), None, None, models=[self.GPT35])\n        # Call the get_commit_message method with dummy diff and context\n        result = repo.get_commit_message(\"dummy diff\", \"dummy context\")\n\n        # Assert that the returned message is the expected one\n        self.assertEqual(result, \"a good commit message\")\n\n    @patch(\"aider.repo.simple_send_with_retries\")\n    def test_get_commit_message_no_strip_unmatched_quotes(self, mock_send):\n        mock_send.return_value = 'a good \"commit message\"'\n\n        repo = GitRepo(InputOutput(), None, None, models=[self.GPT35])\n        # Call the get_commit_message method with dummy diff and context\n        result = repo.get_commit_message(\"dummy diff\", \"dummy context\")\n\n        # Assert that the returned message is the expected one\n        self.assertEqual(result, 'a good \"commit message\"')\n\n    @patch(\"aider.repo.GitRepo.get_commit_message\")\n    def test_commit_with_custom_committer_name(self, mock_send):\n        mock_send.return_value = '\"a good commit message\"'\n\n        # Cleanup of the git temp dir explodes on windows\n        if platform.system() == \"Windows\":\n            return\n\n        with GitTemporaryDirectory():\n            # new repo\n            raw_repo = git.Repo()\n            raw_repo.config_writer().set_value(\"user\", \"name\", \"Test User\").release()\n\n            # add a file and commit it\n            fname = Path(\"file.txt\")\n            fname.touch()\n            raw_repo.git.add(str(fname))\n            raw_repo.git.commit(\"-m\", \"initial commit\")\n\n            io = InputOutput()\n            git_repo = GitRepo(io, None, None)\n\n            # commit a change\n            fname.write_text(\"new content\")\n            git_repo.commit(fnames=[str(fname)], aider_edits=True)\n\n            # check the committer name\n            commit = raw_repo.head.commit\n            self.assertEqual(commit.author.name, \"Test User (aider)\")\n            self.assertEqual(commit.committer.name, \"Test User (aider)\")\n\n            # commit a change without aider_edits\n            fname.write_text(\"new content again!\")\n            git_repo.commit(fnames=[str(fname)], aider_edits=False)\n\n            # check the committer name\n            commit = raw_repo.head.commit\n            self.assertEqual(commit.author.name, \"Test User\")\n            self.assertEqual(commit.committer.name, \"Test User (aider)\")\n\n            # check that the original committer name is restored\n            original_committer_name = os.environ.get(\"GIT_COMMITTER_NAME\")\n            self.assertIsNone(original_committer_name)\n            original_author_name = os.environ.get(\"GIT_AUTHOR_NAME\")\n            self.assertIsNone(original_author_name)\n\n    def test_get_tracked_files(self):\n        # Create a temporary directory\n        tempdir = Path(tempfile.mkdtemp())\n\n        # Initialize a git repository in the temporary directory and set user name and email\n        repo = git.Repo.init(tempdir)\n        repo.config_writer().set_value(\"user\", \"name\", \"Test User\").release()\n        repo.config_writer().set_value(\"user\", \"email\", \"testuser@example.com\").release()\n\n        # Create three empty files and add them to the git repository\n        filenames = [\"README.md\", \"subdir/f\u00e4nny.md\", \"system\u00fcber/blick.md\", 'file\"with\"quotes.txt']\n        created_files = []\n        for filename in filenames:\n            file_path = tempdir / filename\n            try:\n                file_path.parent.mkdir(parents=True, exist_ok=True)\n                file_path.touch()\n                repo.git.add(str(file_path))\n                created_files.append(Path(filename))\n            except OSError:\n                # windows won't allow files with quotes, that's ok\n                self.assertIn('\"', filename)\n                self.assertEqual(os.name, \"nt\")\n\n        self.assertTrue(len(created_files) >= 3)\n\n        repo.git.commit(\"-m\", \"added\")\n\n        tracked_files = GitRepo(InputOutput(), [tempdir], None).get_tracked_files()\n\n        # On windows, paths will come back \\like\\this, so normalize them back to Paths\n        tracked_files = [Path(fn) for fn in tracked_files]\n\n        # Assert that coder.get_tracked_files() returns the three filenames\n        self.assertEqual(set(tracked_files), set(created_files))\n\n    def test_get_tracked_files_with_new_staged_file(self):\n        with GitTemporaryDirectory():\n            # new repo\n            raw_repo = git.Repo()\n\n            # add it, but no commits at all in the raw_repo yet\n            fname = Path(\"new.txt\")\n            fname.touch()\n            raw_repo.git.add(str(fname))\n\n            git_repo = GitRepo(InputOutput(), None, None)\n\n            # better be there\n            fnames = git_repo.get_tracked_files()\n            self.assertIn(str(fname), fnames)\n\n            # commit it, better still be there\n            raw_repo.git.commit(\"-m\", \"new\")\n            fnames = git_repo.get_tracked_files()\n            self.assertIn(str(fname), fnames)\n\n            # new file, added but not committed\n            fname2 = Path(\"new2.txt\")\n            fname2.touch()\n            raw_repo.git.add(str(fname2))\n\n            # both should be there\n            fnames = git_repo.get_tracked_files()\n            self.assertIn(str(fname), fnames)\n            self.assertIn(str(fname2), fnames)\n\n    def test_get_tracked_files_with_aiderignore(self):\n        with GitTemporaryDirectory():\n            # new repo\n            raw_repo = git.Repo()\n\n            # add it, but no commits at all in the raw_repo yet\n            fname = Path(\"new.txt\")\n            fname.touch()\n            raw_repo.git.add(str(fname))\n\n            aiderignore = Path(\".aiderignore\")\n            git_repo = GitRepo(InputOutput(), None, None, str(aiderignore))\n\n            # better be there\n            fnames = git_repo.get_tracked_files()\n            self.assertIn(str(fname), fnames)\n\n            # commit it, better still be there\n            raw_repo.git.commit(\"-m\", \"new\")\n            fnames = git_repo.get_tracked_files()\n            self.assertIn(str(fname), fnames)\n\n            # new file, added but not committed\n            fname2 = Path(\"new2.txt\")\n            fname2.touch()\n            raw_repo.git.add(str(fname2))\n\n            # both should be there\n            fnames = git_repo.get_tracked_files()\n            self.assertIn(str(fname), fnames)\n            self.assertIn(str(fname2), fnames)\n\n            aiderignore.write_text(\"new.txt\\n\")\n\n            # new.txt should be gone!\n            fnames = git_repo.get_tracked_files()\n            self.assertNotIn(str(fname), fnames)\n            self.assertIn(str(fname2), fnames)\n\n            # This does not work in github actions?!\n            # The mtime doesn't change, even if I time.sleep(1)\n            # Before doing this write_text()!?\n            #\n            # aiderignore.write_text(\"new2.txt\\n\")\n            # new2.txt should be gone!\n            # fnames = git_repo.get_tracked_files()\n            # self.assertIn(str(fname), fnames)\n            # self.assertNotIn(str(fname2), fnames)\n\n    def test_get_tracked_files_from_subdir(self):\n        with GitTemporaryDirectory():\n            # new repo\n            raw_repo = git.Repo()\n\n            # add it, but no commits at all in the raw_repo yet\n            fname = Path(\"subdir/new.txt\")\n            fname.parent.mkdir()\n            fname.touch()\n            raw_repo.git.add(str(fname))\n\n            os.chdir(fname.parent)\n\n            git_repo = GitRepo(InputOutput(), None, None)\n\n            # better be there\n            fnames = git_repo.get_tracked_files()\n            self.assertIn(str(fname), fnames)\n\n            # commit it, better still be there\n            raw_repo.git.commit(\"-m\", \"new\")\n            fnames = git_repo.get_tracked_files()\n            self.assertIn(str(fname), fnames)\n\n    @patch(\"aider.repo.simple_send_with_retries\")\n    def test_noop_commit(self, mock_send):\n        mock_send.return_value = '\"a good commit message\"'\n\n        with GitTemporaryDirectory():\n            # new repo\n            raw_repo = git.Repo()\n\n            # add it, but no commits at all in the raw_repo yet\n            fname = Path(\"file.txt\")\n            fname.touch()\n            raw_repo.git.add(str(fname))\n            raw_repo.git.commit(\"-m\", \"new\")\n\n            git_repo = GitRepo(InputOutput(), None, None)\n\n            git_repo.commit(fnames=[str(fname)])\n\n    def test_subtree_only(self):\n        with GitTemporaryDirectory():\n            # Create a new repo\n            raw_repo = git.Repo()\n\n            # Create files in different directories\n            root_file = Path(\"root.txt\")\n            subdir_file = Path(\"subdir/subdir_file.txt\")\n            another_subdir_file = Path(\"another_subdir/another_file.txt\")\n\n            root_file.touch()\n            subdir_file.parent.mkdir()\n            subdir_file.touch()\n            another_subdir_file.parent.mkdir()\n            another_subdir_file.touch()\n\n            raw_repo.git.add(str(root_file), str(subdir_file), str(another_subdir_file))\n            raw_repo.git.commit(\"-m\", \"Initial commit\")\n\n            # Change to the subdir\n            os.chdir(subdir_file.parent)\n\n            # Create GitRepo instance with subtree_only=True\n            git_repo = GitRepo(InputOutput(), None, None, subtree_only=True)\n\n            # Test ignored_file method\n            self.assertFalse(git_repo.ignored_file(str(subdir_file)))\n            self.assertTrue(git_repo.ignored_file(str(root_file)))\n            self.assertTrue(git_repo.ignored_file(str(another_subdir_file)))\n\n            # Test get_tracked_files method\n            tracked_files = git_repo.get_tracked_files()\n            self.assertIn(str(subdir_file), tracked_files)\n            self.assertNotIn(str(root_file), tracked_files)\n            self.assertNotIn(str(another_subdir_file), tracked_files)\n\n    @patch(\"aider.repo.simple_send_with_retries\")\n    def test_noop_commit(self, mock_send):\n        mock_send.return_value = '\"a good commit message\"'",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.867033426631921,
      "diff_lines": 176,
      "added_lines": 30,
      "removed_lines": 91,
      "diff_preview": "--- expected+++ predicted@@ -106,31 +106,28 @@             diffs = git_repo.diff_commits(False, \"HEAD~1\", \"HEAD\")\n             self.assertIn(\"two\", diffs)\n \n-    @patch(\"aider.models.Model.simple_send_with_retries\")\n+    @patch(\"aider.repo.simple_send_with_retries\")\n     def test_get_commit_message(self, mock_send):\n         mock_send.side_effect = [\"\", \"a good commit message\"]\n \n         model1 = Model(\"gpt-3.5-turbo\")\n         model2 = Model(\"gpt-4\")\n-        dump(model1)\n-        dump(model2)\n         repo = GitRepo(InputOutput(), None, None, models=[model1, model2])\n-\n+        \n         # Call the get_commit_message method with dummy diff and context\n         result = repo.get_commit_message(\"dummy diff\", \"dummy context\")\n \n         # Assert that the returned message is the expected one from the second model\n         self.assertEqual(result, \"a good commit message\")\n-\n+        \n         # Check that simple_send_with_retries was called twice\n         self.assertEqual(mock_send.call_count, 2)\n-\n-        # Check that both calls were made with the same messages\n-        first_call_messages = mock_send.call_args_list[0][0][0]  # Get messages from first call\n-        second_call_messages = mock_send.call_args_list[1][0][0]  # Get messages from second call\n-        self.assertEqual(first_call_messages, second_call_messages)\n-\n-    @patch(\"aider.models.Model.simple_send_with_retries\")\n+        \n+        # Check that it was called with the correct model names\n+        self.assertEqual(mock_send.call_args_list[0][0][0], model1)\n+        self.assertEqual(mock_send.call_args_list[1][0][0], model2)\n+\n+    @patch(\"aider.repo.simple_send_with_retries\")\n     def test_get_commit_message_strip_quotes(self, mock_send):\n         mock_send.return_value = '\"a good commit message\"'\n \n@@ -141,7 +138,7 @@         # Assert that the returned message is the expected one\n         self.assertEqual(result, \"a good commit message\")\n \n-    @patch(\"aider.models.Model.simple_send_with_retries\")\n+    @patch(\"aider.repo.simple_send_with_retries\")\n     def test_get_commit_message_no_strip_unmatched_quotes(self, mock_send):\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 118.5500841140747,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 11472,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_tests_basic_test_repomap.py",
    "exact_match": false,
    "similarity": 0.33324116955830513,
    "diff_lines": 439,
    "predicted_output": "import os\nimport time\nimport unittest\nfrom pathlib import Path\nimport re\nimport difflib\nimport git\n\nfrom aider.dump import dump  # noqa: F401\nfrom aider.io import InputOutput\nfrom aider.models import Model\nfrom aider.repomap import RepoMap\nfrom aider.utils import GitTemporaryDirectory, IgnorantTemporaryDirectory\n\n\nclass TestRepoMap(unittest.TestCase):\n    def setUp(self):\n        self.GPT35 = Model(\"gpt-3.5-turbo\")\n\n    def test_get_repo_map(self):\n        # Create a temporary directory with sample files for testing\n        test_files = [\n            \"test_file1.py\",\n            \"test_file2.py\",\n            \"test_file3.md\",\n            \"test_file4.json\",\n        ]\n\n        with IgnorantTemporaryDirectory() as temp_dir:\n            for file in test_files:\n                with open(os.path.join(temp_dir, file), \"w\") as f:\n                    f.write(\"\")\n\n            io = InputOutput()\n            repo_map = RepoMap(main_model=self.GPT35, root=temp_dir, io=io)\n            other_files = [os.path.join(temp_dir, file) for file in test_files]\n            result = repo_map.get_repo_map([], other_files)\n\n            # Check if the result contains the expected tags map\n            self.assertIn(\"test_file1.py\", result)\n            self.assertIn(\"test_file2.py\", result)\n            self.assertIn(\"test_file3.md\", result)\n            self.assertIn(\"test_file4.json\", result)\n\n            # close the open cache files, so Windows won't error\n            del repo_map\n\n    def test_get_repo_map_with_identifiers(self):\n        # Create a temporary directory with a sample Python file containing identifiers\n        test_file1 = \"test_file_with_identifiers.py\"\n        file_content1 = \"\"\"\\\nclass MyClass:\n    def my_method(self, arg1, arg2):\n        return arg1 + arg2\n\ndef my_function(arg1, arg2):\n    return arg1 * arg2\n\"\"\"\n\n        test_file2 = \"test_file_import.py\"\n        file_content2 = \"\"\"\\\nfrom test_file_with_identifiers import MyClass\n\nobj = MyClass()\nprint(obj.my_method(1, 2))\nprint(my_function(3, 4))\n\"\"\"\n\n        test_file3 = \"test_file_pass.py\"\n        file_content3 = \"pass\"\n\n        with IgnorantTemporaryDirectory() as temp_dir:\n            with open(os.path.join(temp_dir, test_file1), \"w\") as f:\n                f.write(file_content1)\n\n            with open(os.path.join(temp_dir, test_file2), \"w\") as f:\n                f.write(file_content2)\n\n            with open(os.path.join(temp_dir, test_file3), \"w\") as f:\n                f.write(file_content3)\n\n            io = InputOutput()\n            repo_map = RepoMap(main_model=self.GPT35, root=temp_dir, io=io)\n            other_files = [\n                os.path.join(temp_dir, test_file1),\n                os.path.join(temp_dir, test_file2),\n                os.path.join(temp_dir, test_file3),\n            ]\n            result = repo_map.get_repo_map([], other_files)\n\n            # Check if the result contains the expected tags map with identifiers\n            self.assertIn(\"test_file_with_identifiers.py\", result)\n            self.assertIn(\"MyClass\", result)\n            self.assertIn(\"my_method\", result)\n            self.assertIn(\"my_function\", result)\n            self.assertIn(\"test_file_pass.py\", result)\n\n            # close the open cache files, so Windows won't error\n            del repo_map\n\n    def test_get_repo_map_all_files(self):\n        test_files = [\n            \"test_file0.py\",\n            \"test_file1.txt\",\n            \"test_file2.md\",\n            \"test_file3.json\",\n            \"test_file4.html\",\n            \"test_file5.css\",\n            \"test_file6.js\",\n        ]\n\n        with IgnorantTemporaryDirectory() as temp_dir:\n            for file in test_files:\n                with open(os.path.join(temp_dir, file), \"w\") as f:\n                    f.write(\"\")\n\n            repo_map = RepoMap(main_model=self.GPT35, root=temp_dir, io=InputOutput())\n\n            other_files = [os.path.join(temp_dir, file) for file in test_files]\n            result = repo_map.get_repo_map([], other_files)\n            dump(other_files)\n            dump(repr(result))\n\n            # Check if the result contains each specific file in the expected tags map without ctags\n            for file in test_files:\n                self.assertIn(file, result)\n\n            # close the open cache files, so Windows won't error\n            del repo_map\n\n    def test_get_repo_map_excludes_added_files(self):\n        # Create a temporary directory with sample files for testing\n        test_files = [\n            \"test_file1.py\",\n            \"test_file2.py\",\n            \"test_file3.md\",\n            \"test_file4.json\",\n        ]\n\n        with IgnorantTemporaryDirectory() as temp_dir:\n            for file in test_files:\n                with open(os.path.join(temp_dir, file), \"w\") as f:\n                    f.write(\"def foo(): pass\\n\")\n\n            io = InputOutput()\n            repo_map = RepoMap(main_model=self.GPT35, root=temp_dir, io=io)\n            test_files = [os.path.join(temp_dir, file) for file in test_files]\n            result = repo_map.get_repo_map(test_files[:2], test_files[2:])\n\n            dump(result)\n\n            # Check if the result contains the expected tags map\n            self.assertNotIn(\"test_file1.py\", result)\n            self.assertNotIn(\"test_file2.py\", result)\n            self.assertIn(\"test_file3.md\", result)\n            self.assertIn(\"test_file4.json\", result)\n\n            # close the open cache files, so Windows won't error\n            del repo_map\n\n\nclass TestRepoMapTypescript(unittest.TestCase):\n    def setUp(self):\n        self.GPT35 = Model(\"gpt-3.5-turbo\")\n\n    def test_get_repo_map_typescript(self):\n        # Create a temporary directory with a sample TypeScript file\n        test_file_ts = \"test_file.ts\"\n        file_content_ts = \"\"\n\n        with IgnorantTemporaryDirectory() as temp_dir:\n            with open(os.path.join(temp_dir, test_file_ts), \"w\") as f:\n                f.write(file_content_ts)\n\n            io = InputOutput()\n            repo_map = RepoMap(main_model=self.GPT35, root=temp_dir, io=io)\n            other_files = [os.path.join(temp_dir, test_file_ts)]\n            result = repo_map.get_repo_map([], other_files)\n\n            # Check if the result contains the expected tags map with TypeScript identifiers\n            self.assertIn(\"test_file.ts\", result)\n            self.assertIn(\"IMyInterface\", result)\n            self.assertIn(\"ExampleType\", result)\n            self.assertIn(\"Status\", result)\n            self.assertIn(\"MyClass\", result)\n            self.assertIn(\"add\", result)\n            self.assertIn(\"myFunction\", result)\n\n            # close the open cache files, so Windows won't error\n            del repo_map\n\n    def test_get_repo_map_tsx(self):\n        # Create a temporary directory with a sample TSX file\n        test_file_tsx = \"test_file.tsx\"\n        file_content_tsx = \"\"\n\n        with IgnorantTemporaryDirectory() as temp_dir:\n            with open(os.path.join(temp_dir, test_file_tsx), \"w\") as f:\n                f.write(file_content_tsx)\n\n            io = InputOutput()\n            repo_map = RepoMap(main_model=self.GPT35, root=temp_dir, io=io)\n            other_files = [os.path.join(temp_dir, test_file_tsx)]\n            result = repo_map.get_repo_map([], other_files)\n\n            # Check if the result contains the expected tags map with TSX identifiers\n            self.assertIn(\"test_file.tsx\", result)\n            self.assertIn(\"GreetingProps\", result)\n            self.assertIn(\"Greeting\", result)\n\n            # close the open cache files, so Windows won't error\n            del repo_map\n\n\nclass TestRepoMapAllLanguages(unittest.TestCase):\n    def setUp(self):\n        self.GPT35 = Model(\"gpt-3.5-turbo\")\n\n    def test_get_repo_map_all_languages(self):\n        language_files = {\n            \"c\": (\"c\", \"main\"),\n            \"cpp\": (\"cpp\", \"main\"),\n            \"d\": (\"d\", \"main\"),\n            \"dart\": (\"dart\", \"Person\"),\n            \"elixir\": (\"ex\", \"Greeter\"),\n            \"gleam\": (\"gleam\", \"greet\"),\n            \"java\": (\"java\", \"Greeting\"),\n            \"javascript\": (\"js\", \"Person\"),\n            \"kotlin\": (\"kt\", \"Greeting\"),\n            \"lua\": (\"lua\", \"greet\"),\n            \"php\": (\"php\", \"greet\"),\n            \"python\": (\"py\", \"Person\"),\n            \"ruby\": (\"rb\", \"greet\"),\n            \"rust\": (\"rs\", \"Person\"),\n            \"typescript\": (\"ts\", \"greet\"),\n            \"tsx\": (\"tsx\", \"UserProps\"),\n            \"csharp\": (\"cs\", \"IGreeter\"),\n            \"elisp\": (\"el\", \"greeter\"),\n            \"elm\": (\"elm\", \"Person\"),\n            \"go\": (\"go\", \"Greeter\"),\n            \"hcl\": (\"tf\", \"aws_vpc\"),\n            \"arduino\": (\"ino\", \"setup\"),\n            \"chatito\": (\"chatito\", \"intent\"),\n            \"commonlisp\": (\"lisp\", \"greet\"),\n            \"pony\": (\"pony\", \"Greeter\"),\n            \"properties\": (\"properties\", \"database.url\"),\n            \"r\": (\"r\", \"calculate\"),\n            \"racket\": (\"rkt\", \"greet\"),\n            \"solidity\": (\"sol\", \"SimpleStorage\"),\n            \"swift\": (\"swift\", \"Greeter\"),\n            \"udev\": (\"rules\", \"USB_DRIVER\"),\n        }\n\n        fixtures_dir = Path(__file__).parent.parent / \"fixtures\" / \"languages\"\n\n        for lang, key_symbol in language_files.items():\n            self._test_language_repo_map(lang, key_symbol, fixtures_dir)\n\n    def _test_language_repo_map(self, lang, key_symbol, fixtures_dir):\n        \"\"\"Helper method to test repo map generation for a specific language.\"\"\"\n        # Get the fixture file path and name based on language\n        fixture_dir = fixtures_dir / lang\n        ext, key_symbol = key_symbol\n        filename = f\"test.{ext}\"\n        fixture_path = fixture_dir / filename\n        self.assertTrue(fixture_path.exists(), f\"Fixture file missing for {lang}: {fixture_path}\")\n\n        # Read the fixture content\n        with open(fixture_path, \"r\", encoding=\"utf-8\") as f:\n            content = f.read()\n        with GitTemporaryDirectory() as temp_dir:\n            test_file = os.path.join(temp_dir, filename)\n            with open(test_file, \"w\", encoding=\"utf-8\") as f:\n                f.write(content)\n\n            io = InputOutput()\n            repo_map = RepoMap(main_model=self.GPT35, root=temp_dir, io=io)\n            other_files = [test_file]\n            result = repo_map.get_repo_map([], other_files)\n\n            self.assertGreater(len(result.strip().splitlines()), 1)\n\n            # Check if the result contains all the expected files and symbols\n            self.assertIn(filename, result, f\"File for language {lang} not found in repo map: {result}\")\n            self.assertIn(key_symbol, result, f\"Key symbol '{key_symbol}' for language {lang} not found in repo map: {result}\")\n\n            # close the open cache files, so Windows won't error\n            del repo_map\n\n\nif __name__ == \"__main__\":\n    unittest.main()",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.33324116955830513,
      "diff_lines": 439,
      "added_lines": 100,
      "removed_lines": 298,
      "diff_preview": "--- expected+++ predicted@@ -1,10 +1,9 @@-import difflib\n import os\n-import re\n import time\n import unittest\n from pathlib import Path\n-\n+import re\n+import difflib\n import git\n \n from aider.dump import dump  # noqa: F401\n@@ -45,120 +44,6 @@ \n             # close the open cache files, so Windows won't error\n             del repo_map\n-\n-    def test_repo_map_refresh_files(self):\n-        with GitTemporaryDirectory() as temp_dir:\n-            repo = git.Repo(temp_dir)\n-\n-            # Create three source files with one function each\n-            file1_content = \"def function1():\\n    return 'Hello from file1'\\n\"\n-            file2_content = \"def function2():\\n    return 'Hello from file2'\\n\"\n-            file3_content = \"def function3():\\n    return 'Hello from file3'\\n\"\n-\n-            with open(os.path.join(temp_dir, \"file1.py\"), \"w\") as f:\n-                f.write(file1_content)\n-            with open(os.path.join(temp_dir, \"file2.py\"), \"w\") as f:\n-                f.write(file2_content)\n-            with open(os.path.join(temp_dir, \"file3.py\"), \"w\") as f:\n-                f.write(file3_content)\n-\n-            # Add files to git\n-            repo.index.add([\"file1.py\", \"file2.py\", \"file3.py\"])\n-            repo.index.commit(\"Initial commit\")\n-\n-            # Initialize RepoMap with refresh=\"files\"\n-            io = InputOutput()\n-            repo_map = RepoMap(main_model=self.GPT35, root=temp_dir, io=io, refresh=\"files\")\n-            other_files = [\n-                os.path.join(temp_dir, \"file1.py\"),\n-                os.path.join(temp_dir, \"file2.py\"),\n-                os.path.join(temp_dir, \"file3.py\"),\n-            ]\n-\n-            # Get initial repo map\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 64.41128897666931,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 52975,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "aider_tests_basic_test_sendchat.py",
    "exact_match": false,
    "similarity": 0.9421057054029073,
    "diff_lines": 87,
    "predicted_output": "import unittest\nfrom unittest.mock import MagicMock, patch\n\nfrom aider.exceptions import LiteLLMExceptions\nfrom aider.llm import litellm\nfrom aider.models import Model\nfrom aider.sendchat import simple_send_with_retries\n\n\nclass PrintCalled(Exception):\n    pass\n\n\nclass TestSendChat(unittest.TestCase):\n    def setUp(self):\n        self.mock_messages = [{\"role\": \"user\", \"content\": \"Hello\"}]\n        self.mock_model = \"gpt-4\"\n\n    def test_litellm_exceptions(self):\n        litellm_ex = LiteLLMExceptions()\n        litellm_ex._load(strict=True)\n\n    @patch(\"litellm.completion\")\n    def test_simple_send_with_retries_rate_limit_error(self, mock_print, mock_completion):\n        mock = MagicMock()\n        mock.status_code = 500\n\n        mock_completion.side_effect = litellm.RateLimitError(\n            message=\"rate limit exceeded\", response=mock, llm_provider=\"llm_provider\", model=\"model\"\n        )\n\n        simple_send_with_retries(Model(self.mock_model), self.mock_messages)\n        assert mock_print.call_count == 3\n\n    @patch(\"litellm.completion\")\n    def test_send_completion_basic(self, mock_completion):\n        mock_response = MagicMock()\n        mock_completion.return_value = mock_response\n\n        hash_obj, response = Model(self.mock_model).send_completion(\n            self.mock_messages, functions=None, stream=False\n        )\n\n        assert response == mock_response\n        mock_completion.assert_called_once()\n\n    @patch(\"litellm.completion\")\n    def test_send_completion_with_functions(self, mock_completion):\n        mock_function = {\"name\": \"test_function\", \"parameters\": {\"type\": \"object\"}}\n\n        hash_obj, response = Model(self.mock_model).send_completion(\n            self.mock_messages, functions=[mock_function], stream=False\n        )\n\n        called_kwargs = mock_completion.call_args.kwargs\n        assert \"tools\" in called_kwargs\n        assert called_kwargs[\"tools\"][0][\"function\"] == mock_function\n\n    @patch(\"litellm.completion\")\n    def test_simple_send_attribute_error(self, mock_completion):\n        mock_completion.return_value = MagicMock()\n        mock_completion.return_value.choices = None\n\n        result = Model(self.mock_model).simple_send_with_retries(self.mock_messages)\n        assert result is None\n\n    @patch(\"litellm.completion\")\n    @patch(\"builtins.print\")\n    def test_simple_send_non_retryable_error(self, mock_print, mock_completion):\n        mock = MagicMock()\n        mock.status_code = 400\n\n        mock_completion.side_effect = litellm.InvalidRequestError(\n            message=\"Invalid request\", llm_provider=\"test_provider\", model=\"test_model\"\n        )\n\n        result = Model(self.mock_model).simple_send_with_retries(self.mock_messages)\n        assert result is None\n        assert mock_print.call_count == 1  # Error message\n\n    def test_ensure_alternating_roles_empty(self):\n        from aider.sendchat import ensure_alternating_roles\n\n        messages = []\n        result = ensure_alternating_roles(messages)\n        assert result == []\n\n    def test_ensure_alternating_roles_single_message(self):\n        from aider.sendchat import ensure_alternating_roles\n\n        messages = [{\"role\": \"user\", \"content\": \"Hello\"}]\n        result = ensure_alternating_roles(messages)\n        assert result == messages\n\n    def test_ensure_alternating_roles_already_alternating(self):\n        from aider.sendchat import ensure_alternating_roles\n\n        messages = [\n            {\"role\": \"user\", \"content\": \"Hello\"},\n            {\"role\": \"assistant\", \"content\": \"Hi there\"},\n            {\"role\": \"user\", \"content\": \"How are you?\"},\n        ]\n        result = ensure_alternating_roles(messages)\n        assert result == messages\n\n    def test_ensure_alternating_roles_consecutive_user(self):\n        from aider.sendchat import ensure_alternating_roles\n\n        messages = [\n            {\"role\": \"user\", \"content\": \"Hello\"},\n            {\"role\": \"user\", \"content\": \"Are you there?\"},\n        ]\n        expected = [\n            {\"role\": \"user\", \"content\": \"Hello\"},\n            {\"role\": \"assistant\", \"content\": \"\"},\n            {\"role\": \"user\", \"content\": \"Are you there?\"},\n        ]\n        result = ensure_alternating_roles(messages)\n        assert result == expected\n\n    def test_ensure_alternating_roles_consecutive_assistant(self):\n        from aider.sendchat import ensure_alternating_roles\n\n        messages = [\n            {\"role\": \"assistant\", \"content\": \"Hi there\"},\n            {\"role\": \"assistant\", \"content\": \"How can I help?\"},\n        ]\n        expected = [\n            {\"role\": \"assistant\", \"content\": \"Hi there\"},\n            {\"role\": \"user\", \"content\": \"\"},\n            {\"role\": \"assistant\", \"content\": \"How can I help?\"},\n        ]\n        result = ensure_alternating_roles(messages)\n        assert result == expected\n\n    def test_ensure_alternating_roles_mixed_sequence(self):\n        from aider.sendchat import ensure_alternating_roles\n\n        messages = [\n            {\"role\": \"user\", \"content\": \"Hello\"},\n            {\"role\": \"user\", \"content\": \"Are you there?\"},\n            {\"role\": \"assistant\", \"content\": \"Yes\"},\n            {\"role\": \"assistant\", \"content\": \"How can I help?\"},\n            {\"role\": \"user\", \"content\": \"Write code\"},\n        ]\n        expected = [\n            {\"role\": \"user\", \"content\": \"Hello\"},\n            {\"role\": \"assistant\", \"content\": \"\"},\n            {\"role\": \"user\", \"content\": \"Are you there?\"},\n            {\"role\": \"assistant\", \"content\": \"Yes\"},\n            {\"role\": \"user\", \"content\": \"\"},\n            {\"role\": \"assistant\", \"content\": \"How can I help?\"},\n            {\"role\": \"user\", \"content\": \"Write code\"},\n        ]\n        result = ensure_alternating_roles(messages)\n        assert result == expected",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9421057054029073,
      "diff_lines": 87,
      "added_lines": 7,
      "removed_lines": 22,
      "diff_preview": "--- expected+++ predicted@@ -4,6 +4,7 @@ from aider.exceptions import LiteLLMExceptions\n from aider.llm import litellm\n from aider.models import Model\n+from aider.sendchat import simple_send_with_retries\n \n \n class PrintCalled(Exception):\n@@ -20,33 +21,22 @@         litellm_ex._load(strict=True)\n \n     @patch(\"litellm.completion\")\n-    @patch(\"builtins.print\")\n     def test_simple_send_with_retries_rate_limit_error(self, mock_print, mock_completion):\n         mock = MagicMock()\n         mock.status_code = 500\n \n-        # Set up the mock to raise\n-        mock_completion.side_effect = [\n-            litellm.RateLimitError(\n-                \"rate limit exceeded\",\n-                response=mock,\n-                llm_provider=\"llm_provider\",\n-                model=\"model\",\n-            ),\n-            None,\n-        ]\n+        mock_completion.side_effect = litellm.RateLimitError(\n+            message=\"rate limit exceeded\", response=mock, llm_provider=\"llm_provider\", model=\"model\"\n+        )\n \n-        # Call the simple_send_with_retries method\n-        Model(self.mock_model).simple_send_with_retries(self.mock_messages)\n+        simple_send_with_retries(Model(self.mock_model), self.mock_messages)\n         assert mock_print.call_count == 3\n \n     @patch(\"litellm.completion\")\n     def test_send_completion_basic(self, mock_completion):\n-        # Setup mock response\n         mock_response = MagicMock()\n         mock_completion.return_value = mock_response\n \n-        # Test basic send_completion\n         hash_obj, response = Model(self.mock_model).send_completion(\n             self.mock_messages, functions=None, stream=False\n         )\n@@ -62,36 +52,31 @@             self.mock_messages, functions=[mock_function], stream=False\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 31.0089852809906,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 10544,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_pkg_freetype_build.zig",
    "exact_match": false,
    "similarity": 0.7472797472797473,
    "diff_lines": 190,
    "predicted_output": "const std = @import(\"std\");\n\npub fn build(b: *std.Build) !void {\n    const optimize = b.standardOptimizeOption(.{});\n    const libpng_enabled = b.option(bool, \"enable-libpng\", \"Build libpng\") orelse false;\n\n    const module = b.addModule(\"freetype\", .{\n        .root_source_file = b.path(\"main.zig\"),\n        .target = b.standardTargetOptions(.{}),\n        .optimize = optimize,\n    });\n\n    // For dynamic linking, we prefer dynamic linking and to search by\n    // mode first. Mode first will search all paths for a dynamic library\n    // before falling back to static.\n    const dynamic_link_opts: std.Build.Module.LinkSystemLibraryOptions = .{\n        .preferred_link_mode = .dynamic,\n        .search_strategy = .mode_first,\n    };\n\n    var test_exe: ?*std.Build.Step.Compile = null;\n    if (b.target.query.isNative()) {\n        test_exe = b.addTest(.{\n            .name = \"test\",\n            .root_source_file = b.path(\"main.zig\"),\n            .target = b.standardTargetOptions(.{}),\n            .optimize = optimize,\n        });\n        const tests_run = b.addRunArtifact(test_exe.?);\n        const test_step = b.step(\"test\", \"Run tests\");\n        test_step.dependOn(&tests_run.step);\n    }\n\n    module.addIncludePath(b.path(\"\"));\n\n    if (b.systemIntegrationOption(\"freetype\", .{})) {\n        module.linkSystemLibrary(\"freetype2\", dynamic_link_opts);\n        if (test_exe) |exe| {\n            exe.linkSystemLibrary2(\"freetype2\", dynamic_link_opts);\n        }\n    } else {\n        const lib = try buildLib(b, module, .{\n            .target = b.standardTargetOptions(.{}),\n            .optimize = optimize,\n            .libpng_enabled = libpng_enabled,\n            .dynamic_link_opts = dynamic_link_opts,\n        });\n\n        if (test_exe) |exe| {\n            exe.linkLibrary(lib);\n        }\n    }\n}\n\nfn buildLib(b: *std.Build, module: *std.Build.Module, options: anytype) !*std.Build.Step.Compile {\n    const target = options.target;\n    const optimize = options.optimize;\n\n    const libpng_enabled = options.libpng_enabled;\n\n    const upstream = b.lazyDependency(\"freetype\", .{});\n    const lib = b.addStaticLibrary(.{\n        .name = \"freetype\",\n        .target = target,\n        .optimize = optimize,\n    });\n    lib.linkLibC();\n    if (target.result.os.tag.isDarwin()) {\n        const apple_sdk = @import(\"apple_sdk\");\n        try apple_sdk.addPaths(b, lib.root_module);\n    }\n\n    module.addIncludePath(upstream.path(\"include\"));\n    var flags = std.ArrayList([]const u8).init(b.allocator);\n    defer flags.deinit();\n    try flags.appendSlice(&.{\n        \"-DFT2_BUILD_LIBRARY\",\n        \"-DFT_CONFIG_OPTION_SYSTEM_ZLIB=1\",\n        \"-DHAVE_UNISTD_H\",\n        \"-DHAVE_FCNTL_H\",\n        \"-fno-sanitize=undefined\",\n    });\n\n    if (libpng_enabled) try flags.append(\"-DFT_CONFIG_OPTION_USE_PNG=1\");\n\n    lib.addCSourceFiles(.{\n        .root = upstream.path(\"\"),\n        .files = srcs,\n        .flags = flags.items,\n    });\n\n    switch (target.result.os.tag) {\n        .linux => lib.addCSourceFile(.{\n            .file = upstream.path(\"builds/unix/ftsystem.c\"),\n            .flags = flags.items,\n        }),\n        .windows => lib.addCSourceFile(.{\n            .file = upstream.path(\"builds/windows/ftsystem.c\"),\n            .flags = flags.items,\n        }),\n        else => lib.addCSourceFile(.{\n            .file = upstream.path(\"src/base/ftsystem.c\"),\n            .flags = flags.items,\n        }),\n    }\n    switch (target.result.os.tag) {\n        .windows => {\n            lib.addCSourceFile(.{\n                .file = upstream.path(\"builds/windows/ftdebug.c\"),\n                .flags = flags.items,\n            });\n            lib.addWin32ResourceFile(.{\n                .file = upstream.path(\"src/base/ftver.rc\"),\n            });\n        },\n        else => lib.addCSourceFile(.{\n            .file = upstream.path(\"src/base/ftdebug.c\"),\n            .flags = flags.items,\n        }),\n    }\n\n    lib.installHeader(b.path(\"freetype-zig.h\"), \"freetype-zig.h\");\n    lib.installHeadersDirectory(\n        upstream.path(\"include\"),\n        \"\",\n        .{ .include_extensions = &.{\".h\"} },\n    );\n\n    b.installArtifact(lib);\n}\n\nconst srcs: []const []const u8 = &.{\n    \"src/autofit/autofit.c\",\n    \"src/base/ftbase.c\",\n    \"src/base/ftbbox.c\",\n    \"src/base/ftbdf.c\",\n    \"src/base/ftbitmap.c\",\n    \"src/base/ftcid.c\",\n    \"src/base/ftfstype.c\",\n    \"src/base/ftgasp.c\",\n    \"src/base/ftglyph.c\",\n    \"src/base/ftgxval.c\",\n    \"src/base/ftinit.c\",\n    \"src/base/ftmm.c\",\n    \"src/base/ftotval.c\",\n    \"src/base/ftpatent.c\",\n    \"src/base/ftpfr.c\",\n    \"src/base/ftstroke.c\",\n    \"src/base/ftsynth.c\",\n    \"src/base/fttype1.c\",\n    \"src/base/ftwinfnt.c\",\n    \"src/bdf/bdf.c\",\n    \"src/bzip2/ftbzip2.c\",\n    \"src/cache/ftcache.c\",\n    \"src/cff/cff.c\",\n    \"src/cid/type1cid.c\",\n    \"src/gzip/ftgzip.c\",\n    \"src/lzw/ftlzw.c\",\n    \"src/pcf/pcf.c\",\n    \"src/pfr/pfr.c\",\n    \"src/psaux/psaux.c\",\n    \"src/pshinter/pshinter.c\",\n    \"src/psnames/psnames.c\",\n    \"src/raster/raster.c\",\n    \"src/sdf/sdf.c\",\n    \"src/sfnt/sfnt.c\",\n    \"src/smooth/smooth.c\",\n    \"src/svg/svg.c\",\n    \"src/truetype/truetype.c\",\n    \"src/type1/type1.c\",\n    \"src/type42/type42.c\",\n    \"src/winfonts/winfnt.c\",\n};",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.7472797472797473,
      "diff_lines": 190,
      "added_lines": 47,
      "removed_lines": 81,
      "diff_preview": "--- expected+++ predicted@@ -1,13 +1,12 @@ const std = @import(\"std\");\n \n pub fn build(b: *std.Build) !void {\n-    const target = b.standardTargetOptions(.{});\n     const optimize = b.standardOptimizeOption(.{});\n     const libpng_enabled = b.option(bool, \"enable-libpng\", \"Build libpng\") orelse false;\n \n     const module = b.addModule(\"freetype\", .{\n         .root_source_file = b.path(\"main.zig\"),\n-        .target = target,\n+        .target = b.standardTargetOptions(.{}),\n         .optimize = optimize,\n     });\n \n@@ -20,11 +19,11 @@     };\n \n     var test_exe: ?*std.Build.Step.Compile = null;\n-    if (target.query.isNative()) {\n+    if (b.target.query.isNative()) {\n         test_exe = b.addTest(.{\n             .name = \"test\",\n             .root_source_file = b.path(\"main.zig\"),\n-            .target = target,\n+            .target = b.standardTargetOptions(.{}),\n             .optimize = optimize,\n         });\n         const tests_run = b.addRunArtifact(test_exe.?);\n@@ -41,11 +40,9 @@         }\n     } else {\n         const lib = try buildLib(b, module, .{\n-            .target = target,\n+            .target = b.standardTargetOptions(.{}),\n             .optimize = optimize,\n-\n             .libpng_enabled = libpng_enabled,\n-\n             .dynamic_link_opts = dynamic_link_opts,\n         });\n \n@@ -61,6 +58,7 @@ \n     const libpng_enabled = options.libpng_enabled;\n \n+    const upstream = b.lazyDependency(\"freetype\", .{});\n     const lib = b.addStaticLibrary(.{\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 43.152626276016235,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 14490,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_pkg_libintl_build.zig",
    "exact_match": false,
    "similarity": 0.940922668436774,
    "diff_lines": 85,
    "predicted_output": "const std = @import(\"std\");\n\n//! Provides libintl for macOS.\n//!\n//! IMPORTANT: This is only for macOS. We could support other platforms\n//! if/when we need to but generally Linux provides libintl in libc.\n//! Windows we'll have to figure out when we get there.\n//!\n//! Since this is only for macOS, there's a lot of hardcoded stuff\n//! here that assumes macOS. For example, I generated the config.h\n//! on my own machine (a Mac) and then copied it here. This isn't\n//! ideal since we should do the same detection that gettext's configure\n//! script does, but its quite a bit of work to do that.\n//!\n//! UPGRADING: If you need to upgrade gettext, then the only thing to\n//! really watch out for is the xlocale.h include we added manually\n//! at the end of config.h. The comment there notes why. When we upgrade\n//! we should audit our config.h and make sure we add that back (if we\n//! have to).\n\npub fn build(b: *std.Build) !void {\n    const target = b.standardTargetOptions(.{});\n    const optimize = b.standardOptimizeOption(.{});\n\n    var flags = std.ArrayList([]const u8).init(b.allocator);\n    defer flags.deinit();\n    try flags.appendSlice(&.{\n        \"-DHAVE_CONFIG_H\",\n        \"-DLOCALEDIR=\\\"\\\"\",\n    });\n\n    const lib = b.addStaticLibrary(.{\n        .name = \"intl\",\n        .target = target,\n        .optimize = optimize,\n    });\n    lib.linkLibC();\n    lib.addIncludePath(b.path(\"\"));\n\n    if (target.result.os.tag.isDarwin()) {\n        const apple_sdk = @import(\"apple_sdk\");\n        try apple_sdk.addPaths(b, lib.root_module);\n    }\n\n    if (b.lazyDependency(\"gettext\", .{})) |upstream| {\n        lib.addIncludePath(upstream.path(\"gettext-runtime/intl\"));\n        lib.addIncludePath(upstream.path(\"gettext-runtime/intl/gnulib-lib\"));\n        lib.addCSourceFiles(.{\n            .root = upstream.path(\"gettext-runtime/intl\"),\n            .files = srcs,\n            .flags = flags.items,\n        });\n    }\n\n    lib.installHeader(b.path(\"libintl.h\"), \"libintl.h\");\n    b.installArtifact(lib);\n}\n\nconst srcs: []const []const u8 = &.{\n    \"bindtextdom.c\",\n    \"dcgettext.c\",\n    \"dcigettext.c\",\n    \"dcngettext.c\",\n    \"dgettext.c\",\n    \"dngettext.c\",\n    \"explodename.c\",\n    \"finddomain.c\",\n    \"gettext.c\",\n    \"hash-string.c\",\n    \"intl-compat.c\",\n    \"l10nflist.c\",\n    \"langprefs.c\",\n    \"loadmsgcat.c\",\n    \"localealias.c\",\n    \"log.c\",\n    \"ngettext.c\",\n    \"plural-exp.c\",\n    \"plural.c\",\n    \"setlocale.c\",\n    \"textdomain.c\",\n    \"version.c\",\n    \"compat.c\",\n\n    // There's probably a better way to detect that we need these, but\n    // these are hardcoded for now for macOS.\n    \"gnulib-lib/getlocalename_l-unsafe.c\",\n    \"gnulib-lib/localename.c\",\n    \"gnulib-lib/localename-environ.c\",\n    \"gnulib-lib/localename-unsafe.c\",\n    \"gnulib-lib/setlocale-lock.c\",\n    \"gnulib-lib/setlocale_null.c\",\n    \"gnulib-lib/setlocale_null-unlocked.c\",\n};",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.940922668436774,
      "diff_lines": 85,
      "added_lines": 25,
      "removed_lines": 33,
      "diff_preview": "--- expected+++ predicted@@ -1,3 +1,5 @@+const std = @import(\"std\");\n+\n //! Provides libintl for macOS.\n //!\n //! IMPORTANT: This is only for macOS. We could support other platforms\n@@ -16,8 +18,6 @@ //! we should audit our config.h and make sure we add that back (if we\n //! have to).\n \n-const std = @import(\"std\");\n-\n pub fn build(b: *std.Build) !void {\n     const target = b.standardTargetOptions(.{});\n     const optimize = b.standardOptimizeOption(.{});\n@@ -29,33 +29,31 @@         \"-DLOCALEDIR=\\\"\\\"\",\n     });\n \n-    {\n-        const lib = b.addStaticLibrary(.{\n-            .name = \"intl\",\n-            .target = target,\n-            .optimize = optimize,\n+    const lib = b.addStaticLibrary(.{\n+        .name = \"intl\",\n+        .target = target,\n+        .optimize = optimize,\n+    });\n+    lib.linkLibC();\n+    lib.addIncludePath(b.path(\"\"));\n+\n+    if (target.result.os.tag.isDarwin()) {\n+        const apple_sdk = @import(\"apple_sdk\");\n+        try apple_sdk.addPaths(b, lib.root_module);\n+    }\n+\n+    if (b.lazyDependency(\"gettext\", .{})) |upstream| {\n+        lib.addIncludePath(upstream.path(\"gettext-runtime/intl\"));\n+        lib.addIncludePath(upstream.path(\"gettext-runtime/intl/gnulib-lib\"));\n+        lib.addCSourceFiles(.{\n+            .root = upstream.path(\"gettext-runtime/intl\"),\n+            .files = srcs,\n+            .flags = flags.items,\n         });\n-        lib.linkLibC();\n-        lib.addIncludePath(b.path(\"\"));\n+    }\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 22.96430516242981,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 10885,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_pkg_oniguruma_build.zig",
    "exact_match": false,
    "similarity": 0.9331097447363518,
    "diff_lines": 50,
    "predicted_output": "const std = @import(\"std\");\nconst NativeTargetInfo = std.zig.system.NativeTargetInfo;\n\npub fn build(b: *std.Build) !void {\n    const target = b.standardTargetOptions(.{});\n    const optimize = b.standardOptimizeOption(.{});\n\n    const module = b.addModule(\"oniguruma\", .{\n        .root_source_file = b.path(\"main.zig\"),\n        .target = target,\n        .optimize = optimize,\n    });\n\n    // For dynamic linking, we prefer dynamic linking and to search by\n    // mode first. Mode first will search all paths for a dynamic library\n    // before falling back to static.\n    const dynamic_link_opts: std.Build.Module.LinkSystemLibraryOptions = .{\n        .preferred_link_mode = .dynamic,\n        .search_strategy = .mode_first,\n    };\n\n    var test_exe: ?*std.Build.Step.Compile = null;\n    if (target.query.isNative()) {\n        test_exe = b.addTest(.{\n            .name = \"test\",\n            .root_source_file = b.path(\"main.zig\"),\n            .target = target,\n            .optimize = optimize,\n        });\n        const tests_run = b.addRunArtifact(test_exe.?);\n        const test_step = b.step(\"test\", \"Run tests\");\n        test_step.dependOn(&tests_run.step);\n\n        // Uncomment this if we're debugging tests\n        b.installArtifact(test_exe.?);\n    }\n\n    if (b.systemIntegrationOption(\"oniguruma\", .{})) {\n        module.linkSystemLibrary(\"oniguruma\", dynamic_link_opts);\n\n        if (test_exe) |exe| {\n            exe.linkSystemLibrary2(\"oniguruma\", dynamic_link_opts);\n        }\n    } else {\n        const lib = try buildLib(b, module, .{\n            .target = target,\n            .optimize = optimize,\n        });\n\n        if (test_exe) |exe| {\n            exe.linkLibrary(lib);\n        }\n    }\n}\n\nfn buildLib(b: *std.Build, module: *std.Build.Module, options: anytype) !*std.Build.Step.Compile {\n    const target = options.target;\n    const optimize = options.optimize;\n\n    if (b.lazyDependency(\"oniguruma\", .{})) |upstream| {\n        const lib = b.addStaticLibrary(.{\n            .name = \"oniguruma\",\n            .target = target,\n            .optimize = optimize,\n        });\n        const t = target.result;\n        lib.linkLibC();\n        lib.addIncludePath(upstream.path(\"src\"));\n        module.addIncludePath(upstream.path(\"src\"));\n\n        if (target.result.os.tag.isDarwin()) {\n            const apple_sdk = @import(\"apple_sdk\");\n            try apple_sdk.addPaths(b, lib.root_module);\n        }\n\n        lib.addConfigHeader(b.addConfigHeader(.{\n            .style = .{ .cmake = upstream.path(\"src/config.h.cmake.in\") },\n        }, .{\n            .PACKAGE = \"oniguruma\",\n            .PACKAGE_VERSION = \"6.9.9\",\n            .VERSION = \"6.9.9\",\n            .HAVE_ALLOCA = true,\n            .HAVE_ALLOCA_H = true,\n            .USE_CRNL_AS_LINE_TERMINATOR = false,\n            .HAVE_STDINT_H = true,\n            .HAVE_SYS_TIMES_H = true,\n            .HAVE_SYS_TIME_H = true,\n            .HAVE_SYS_TYPES_H = true,\n            .HAVE_UNISTD_H = true,\n            .HAVE_INTTYPES_H = true,\n            .SIZEOF_INT = t.cTypeByteSize(.int),\n            .SIZEOF_LONG = t.cTypeByteSize(.long),\n            .SIZEOF_LONG_LONG = t.cTypeByteSize(.longlong),\n            .SIZEOF_VOIDP = t.ptrBitWidth() / t.cTypeBitSize(.char),\n        }));\n\n        var flags = std.ArrayList([]const u8).init(b.allocator);\n        defer flags.deinit();\n        try flags.appendSlice(&.{});\n        lib.addCSourceFiles(.{\n            .root = upstream.path(\"\"),\n            .flags = flags.items,\n            .files = &.{\n                \"src/regerror.c\",\n                \"src/regparse.c\",\n                \"src/regext.c\",\n                \"src/regcomp.c\",\n                \"src/regexec.c\",\n                \"src/reggnu.c\",\n                \"src/regenc.c\",\n                \"src/regsyntax.c\",\n                \"src/regtrav.c\",\n                \"src/regversion.c\",\n                \"src/st.c\",\n                \"src/onig_init.c\",\n                \"src/unicode.c\",\n                \"src/ascii.c\",\n                \"src/utf8.c\",\n                \"src/utf16_be.c\",\n                \"src/utf16_le.c\",\n                \"src/utf32_be.c\",\n                \"src/utf32_le.c\",\n                \"src/euc_jp.c\",\n                \"src/sjis.c\",\n                \"src/iso8859_1.c\",\n                \"src/iso8859_2.c\",\n                \"src/iso8859_3.c\",\n                \"src/iso8859_4.c\",\n                \"src/iso8859_5.c\",\n                \"src/iso8859_6.c\",\n                \"src/iso8859_7.c\",\n                \"src/iso8859_8.c\",\n                \"src/iso8859_9.c\",\n                \"src/iso8859_10.c\",\n                \"src/iso8859_11.c\",\n                \"src/iso8859_13.c\",\n                \"src/iso8859_14.c\",\n                \"src/iso8859_15.c\",\n                \"src/iso8859_16.c\",\n                \"src/euc_tw.c\",\n                \"src/euc_kr.c\",\n                \"src/big5.c\",\n                \"src/gb18030.c\",\n                \"src/koi8_r.c\",\n                \"src/cp1251.c\",\n                \"src/euc_jp_prop.c\",\n                \"src/sjis_prop.c\",\n                \"src/unicode_unfold_key.c\",\n                \"src/unicode_fold1_key.c\",\n                \"src/unicode_fold2_key.c\",\n                \"src/unicode_fold3_key.c\",\n            },\n        });\n\n        lib.installHeadersDirectory(\n            upstream.path(\"src\"),\n            \"\",\n            .{ .include_extensions = &.{\".h\"} },\n        );\n\n        return lib;\n    }\n\n    return null;\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9331097447363518,
      "diff_lines": 50,
      "added_lines": 15,
      "removed_lines": 16,
      "diff_preview": "--- expected+++ predicted@@ -57,22 +57,21 @@     const target = options.target;\n     const optimize = options.optimize;\n \n-    const lib = b.addStaticLibrary(.{\n-        .name = \"oniguruma\",\n-        .target = target,\n-        .optimize = optimize,\n-    });\n-    const t = target.result;\n-    lib.linkLibC();\n-\n-    if (target.result.os.tag.isDarwin()) {\n-        const apple_sdk = @import(\"apple_sdk\");\n-        try apple_sdk.addPaths(b, lib.root_module);\n-    }\n-\n     if (b.lazyDependency(\"oniguruma\", .{})) |upstream| {\n+        const lib = b.addStaticLibrary(.{\n+            .name = \"oniguruma\",\n+            .target = target,\n+            .optimize = optimize,\n+        });\n+        const t = target.result;\n+        lib.linkLibC();\n         lib.addIncludePath(upstream.path(\"src\"));\n         module.addIncludePath(upstream.path(\"src\"));\n+\n+        if (target.result.os.tag.isDarwin()) {\n+            const apple_sdk = @import(\"apple_sdk\");\n+            try apple_sdk.addPaths(b, lib.root_module);\n+        }\n \n         lib.addConfigHeader(b.addConfigHeader(.{\n             .style = .{ .cmake = upstream.path(\"src/config.h.cmake.in\") },\n@@ -158,9 +157,9 @@             \"\",\n             .{ .include_extensions = &.{\".h\"} },\n         );\n+\n+        return lib;\n     }\n \n-    b.installArtifact(lib);\n-\n-    return lib;\n+    return null;\n }"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 37.004319190979004,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 8362,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_pkg_sentry_build.zig",
    "exact_match": false,
    "similarity": 0.9660570888723935,
    "diff_lines": 32,
    "predicted_output": "const std = @import(\"std\");\n\npub fn build(b: *std.Build) !void {\n    const target = b.standardTargetOptions(.{});\n    const optimize = b.standardOptimizeOption(.{});\n    const backend = b.option(Backend, \"backend\", \"Backend\") orelse .inproc;\n    const transport = b.option(Transport, \"transport\", \"Transport\") orelse .none;\n\n    const module = b.addModule(\"sentry\", .{\n        .root_source_file = b.path(\"main.zig\"),\n        .target = target,\n        .optimize = optimize,\n    });\n\n    const lib = b.addStaticLibrary(.{\n        .name = \"sentry\",\n        .target = target,\n        .optimize = optimize,\n    });\n    lib.linkLibC();\n\n    if (target.result.os.tag.isDarwin()) {\n        const apple_sdk = @import(\"apple_sdk\");\n        try apple_sdk.addPaths(b, lib.root_module);\n        try apple_sdk.addPaths(b, module);\n    }\n\n    var flags = std.ArrayList([]const u8).init(b.allocator);\n    defer flags.deinit();\n    try flags.appendSlice(&.{});\n\n    if (b.lazyDependency(\"sentry\", .{})) |upstream| {\n        module.addIncludePath(upstream.path(\"include\"));\n        lib.addIncludePath(upstream.path(\"include\"));\n        lib.addIncludePath(upstream.path(\"src\"));\n        lib.addCSourceFiles(.{\n            .root = upstream.path(\"\"),\n            .files = srcs,\n            .flags = flags.items,\n        });\n\n        // Linux-only\n        if (target.result.os.tag == .linux) {\n            lib.addCSourceFiles(.{\n                .root = upstream.path(\"\"),\n                .files = &.{\n                    \"vendor/stb_sprintf.c\",\n                },\n                .flags = flags.items,\n            });\n        }\n\n        // Symbolizer + Unwinder\n        if (target.result.os.tag == .windows) {\n            lib.addCSourceFiles(.{\n                .root = upstream.path(\"\"),\n                .files = &.{\n                    \"src/sentry_windows_dbghelp.c\",\n                    \"src/path/sentry_path_windows.c\",\n                    \"src/symbolizer/sentry_symbolizer_windows.c\",\n                    \"src/unwinder/sentry_unwinder_dbghelp.c\",\n                },\n                .flags = flags.items,\n            });\n        } else {\n            lib.addCSourceFiles(.{\n                .root = upstream.path(\"\"),\n                .files = &.{\n                    \"src/sentry_unix_pageallocator.c\",\n                    \"src/path/sentry_path_unix.c\",\n                    \"src/symbolizer/sentry_symbolizer_unix.c\",\n                    \"src/unwinder/sentry_unwinder_libbacktrace.c\",\n                },\n                .flags = flags.items,\n            });\n        }\n\n        // Module finder\n        switch (target.result.os.tag) {\n            .windows => lib.addCSourceFiles(.{\n                .root = upstream.path(\"\"),\n                .files = &.{\n                    \"src/modulefinder/sentry_modulefinder_windows.c\",\n                },\n                .flags = flags.items,\n            }),\n\n            .macos, .ios => lib.addCSourceFiles(.{\n                .root = upstream.path(\"\"),\n                .files = &.{\n                    \"src/modulefinder/sentry_modulefinder_apple.c\",\n                },\n                .flags = flags.items,\n            }),\n\n            .linux => lib.addCSourceFiles(.{\n                .root = upstream.path(\"\"),\n                .files = &.{\n                    \"src/modulefinder/sentry_modulefinder_linux.c\",\n                },\n                .flags = flags.items,\n            }),\n\n            .freestanding => {},\n\n            else => {\n                std.log.warn(\"target={} not supported\", .{target.result.os.tag});\n                return error.UnsupportedTarget;\n            },\n        }\n\n        // Transport\n        switch (transport) {\n            .curl => lib.addCSourceFiles(.{\n                .root = upstream.path(\"\"),\n                .files = &.{\n                    \"src/transports/sentry_transport_curl.c\",\n                },\n                .flags = flags.items,\n            }),\n\n            .winhttp => lib.addCSourceFiles(.{\n                .root = upstream.path(\"\"),\n                .files = &.{\n                    \"src/transports/sentry_transport_winhttp.c\",\n                },\n                .flags = flags.items,\n            }),\n\n            .none => lib.addCSourceFiles(.{\n                .root = upstream.path(\"\"),\n                .files = &.{\n                    \"src/transports/sentry_transport_none.c\",\n                },\n                .flags = flags.items,\n            }),\n        }\n\n        // Backend\n        switch (backend) {\n            .crashpad => lib.addCSourceFiles(.{\n                .root = upstream.path(\"\"),\n                .files = &.{\n                    \"src/backends/sentry_backend_crashpad.cpp\",\n                },\n                .flags = flags.items,\n            }),\n\n            .breakpad => {\n                lib.addCSourceFiles(.{\n                    .root = upstream.path(\"\"),\n                    .files = &.{\n                        \"src/backends/sentry_backend_breakpad.cpp\",\n                    },\n                    .flags = flags.items,\n                });\n\n                if (b.lazyDependency(\"breakpad\", .{\n                    .target = target,\n                    .optimize = optimize,\n                })) |breakpad_dep| {\n                    lib.linkLibrary(breakpad_dep.artifact(\"breakpad\"));\n\n                    // We need to add this because Sentry includes some breakpad\n                    // headers that include this vendored file...\n                    lib.addIncludePath(breakpad_dep.path(\"vendor\"));\n                }\n            },\n\n            .inproc => lib.addCSourceFiles(.{\n                .root = upstream.path(\"\"),\n                .files = &.{\n                    \"src/backends/sentry_backend_inproc.c\",\n                },\n                .flags = flags.items,\n            }),\n\n            .none => lib.addCSourceFiles(.{\n                .root = upstream.path(\"\"),\n                .files = &.{\n                    \"src/backends/sentry_backend_none.c\",\n                },\n                .flags = flags.items,\n            }),\n        }\n\n        lib.installHeadersDirectory(\n            upstream.path(\"include\"),\n            \"\",\n            .{ .include_extensions = &.{\".h\"} },\n        );\n    }\n\n    b.installArtifact(lib);\n}\n\nconst srcs: []const []const u8 = &.{\n    \"src/sentry_alloc.c\",\n    \"src/sentry_backend.c\",\n    \"src/sentry_core.c\",\n    \"src/sentry_database.c\",\n    \"src/sentry_envelope.c\",\n    \"src/sentry_info.c\",\n    \"src/sentry_json.c\",\n    \"src/sentry_logger.c\",\n    \"src/sentry_options.c\",\n    \"src/sentry_os.c\",\n    \"src/sentry_random.c\",\n    \"src/sentry_ratelimiter.c\",\n    \"src/sentry_scope.c\",\n    \"src/sentry_session.c\",\n    \"src/sentry_slice.c\",\n    \"src/sentry_string.c\",\n    \"src/sentry_sync.c\",\n    \"src/sentry_transport.c\",\n    \"src/sentry_utils.c\",\n    \"src/sentry_uuid.c\",\n    \"src/sentry_value.c\",\n    \"src/sentry_tracing.c\",\n    \"src/path/sentry_path.c\",\n    \"src/transports/sentry_disk_transport.c\",\n    \"src/transports/sentry_function_transport.c\",\n    \"src/unwinder/sentry_unwinder.c\",\n    \"vendor/mpack.c\",\n};\n\npub const Backend = enum { crashpad, breakpad, inproc, none };\npub const Transport = enum { curl, winhttp, none };",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9660570888723935,
      "diff_lines": 32,
      "added_lines": 1,
      "removed_lines": 15,
      "diff_preview": "--- expected+++ predicted@@ -18,6 +18,7 @@         .optimize = optimize,\n     });\n     lib.linkLibC();\n+\n     if (target.result.os.tag.isDarwin()) {\n         const apple_sdk = @import(\"apple_sdk\");\n         try apple_sdk.addPaths(b, lib.root_module);\n@@ -27,21 +28,6 @@     var flags = std.ArrayList([]const u8).init(b.allocator);\n     defer flags.deinit();\n     try flags.appendSlice(&.{});\n-    if (target.result.os.tag == .windows) {\n-        try flags.appendSlice(&.{\n-            \"-DSENTRY_WITH_UNWINDER_DBGHELP\",\n-        });\n-    } else {\n-        try flags.appendSlice(&.{\n-            \"-DSENTRY_WITH_UNWINDER_LIBBACKTRACE\",\n-        });\n-    }\n-    switch (backend) {\n-        .crashpad => try flags.append(\"-DSENTRY_BACKEND_CRASHPAD\"),\n-        .breakpad => try flags.append(\"-DSENTRY_BACKEND_BREAKPAD\"),\n-        .inproc => try flags.append(\"-DSENTRY_BACKEND_INPROC\"),\n-        .none => {},\n-    }\n \n     if (b.lazyDependency(\"sentry\", .{})) |upstream| {\n         module.addIncludePath(upstream.path(\"include\"));\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 38.909832239151,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 6938,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_App.zig",
    "exact_match": false,
    "similarity": 0.17624552598017032,
    "diff_lines": 608,
    "predicted_output": "const App = @This();\n\nconst std = @import(\"std\");\nconst builtin = @import(\"builtin\");\nconst Allocator = std.mem.Allocator;\nconst apprt = @import(\"apprt.zig\");\nconst Surface = @import(\"Surface.zig\");\nconst tracy = @import(\"tracy\");\nconst input = @import(\"input.zig\");\nconst Config = @import(\"config.zig\").Config;\nconst BlockingQueue = @import(\"datastruct/main.zig\").BlockingQueue;\nconst renderer = @import(\"renderer.zig\");\nconst font = @import(\"font/main.zig\");\nconst internal_os = @import(\"os/main.zig\");\n\npub const Mailbox = BlockingQueue(Message, 64);\n\n/// General purpose allocator\nalloc: Allocator,\n\n/// The list of surfaces that are currently active.\nsurfaces: SurfaceList,\n\n/// The mailbox that can be used to send this thread messages. Note\n/// this is a blocking queue so if it is full you will get errors (or block).\nmailbox: Mailbox.Queue,\n\n/// Set to true once we're quitting. This never goes false again.\nquit: bool,\n\n/// The set of font GroupCache instances shared by surfaces with the\n/// same font configuration.\nfont_grid_set: font.SharedGridSet,\n\n/// The last focused surface. This surface may not be valid;\n/// you must always call hasSurface to validate it.\nfocused_surface: ?*Surface = null,\n\n/// Initialize the main app instance. This creates the main window, sets\n/// up the renderer state, compiles the shaders, etc. This is the primary\n/// \"startup\" logic.\npub fn create(\n    alloc: Allocator,\n) !*App {\n    var app = try alloc.create(App);\n    errdefer alloc.destroy(app);\n\n    app.* = .{\n        .alloc = alloc,\n        .surfaces = .{},\n        .mailbox = .{},\n        .quit = false,\n        .font_grid_set = try font.SharedGridSet.init(alloc),\n    };\n    errdefer app.surfaces.deinit(alloc);\n\n    return app;\n}\n\npub fn destroy(self: *App) void {\n    for (self.surfaces.items) |surface| surface.deinit();\n    self.surfaces.deinit(self.alloc);\n    self.font_grid_set.deinit();\n    self.alloc.destroy(self);\n}\n\n/// Handle an app-level focus event. This should be called whenever\n/// the focus state of the entire app containing Ghostty changes.\n/// This is separate from surface focus events. See the `focused`\n/// field for more information.\npub fn focusEvent(self: *App, focused: bool) void {\n    self.focused_surface = focused;\n}\n\n/// Handle a key event at the app-scope. If this key event is used,\n/// this will return true and the caller shouldn't continue processing\n/// the event. If the event is not used, this will return false.\npub fn keyEvent(\n    self: *App,\n    rt_app: *apprt.App,\n    event: input.KeyEvent,\n) bool {\n    switch (event.action) {\n        .release => return false,\n        .press, .repeat => {},\n    }\n\n    const entry = rt_app.config.keybind.set.getEvent(event) orelse return false;\n    const leaf: input.Binding.Set.Leaf = switch (entry.value_ptr.*) {\n        .leaf => |leaf| leaf,\n    };\n\n    if (!leaf.flags.global) return false;\n\n    self.performAllAction(rt_app, leaf.action) catch |err| {\n        log.warn(\"error performing global keybind action action={s} err={}\", .{\n            @tagName(leaf.action),\n            err,\n        });\n    };\n\n    return true;\n}\n\n/// Perform a binding action. This only accepts actions that are scoped\n/// to the app. Callers can use performAllAction to perform any action\n/// and any non-app-scoped actions will be performed on all surfaces.\npub fn performAction(\n    self: *App,\n    rt_app: *apprt.App,\n    action: input.Binding.Action.Scoped(.app),\n) !void {\n    switch (action) {\n        .unbind => unreachable,\n        .ignore => {},\n        .quit => try rt_app.performAction(.app, .quit, {}),\n        .new_window => try self.newWindow(rt_app, .{ .parent = null }),\n        .open_config => try rt_app.performAction(.app, .open_config, {}),\n        .reload_config => try rt_app.performAction(.app, .reload_config, {}),\n        .close_all_windows => try rt_app.performAction(.app, .close_all_windows, {}),\n        .toggle_quick_terminal => try rt_app.performAction(.app, .toggle_quick_terminal, {}),\n        .toggle_visibility => try rt_app.performAction(.app, .toggle_visibility, {}),\n    }\n}\n\n/// The message types that can be sent to the app thread.\npub const Message = union(enum) {\n    /// Open the configuration file\n    open_config: void,\n\n    /// Quit\n    quit: void,\n};",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.17624552598017032,
      "diff_lines": 608,
      "added_lines": 28,
      "removed_lines": 478,
      "diff_preview": "--- expected+++ predicted@@ -1,29 +1,19 @@-//! App is the primary GUI application for ghostty. This builds the window,\n-//! sets up the renderer, etc. The primary run loop is started by calling\n-//! the \"run\" function.\n const App = @This();\n \n const std = @import(\"std\");\n const builtin = @import(\"builtin\");\n-const assert = std.debug.assert;\n const Allocator = std.mem.Allocator;\n-const build_config = @import(\"build_config.zig\");\n const apprt = @import(\"apprt.zig\");\n const Surface = @import(\"Surface.zig\");\n const tracy = @import(\"tracy\");\n const input = @import(\"input.zig\");\n-const configpkg = @import(\"config.zig\");\n-const Config = configpkg.Config;\n+const Config = @import(\"config.zig\").Config;\n const BlockingQueue = @import(\"datastruct/main.zig\").BlockingQueue;\n const renderer = @import(\"renderer.zig\");\n const font = @import(\"font/main.zig\");\n const internal_os = @import(\"os/main.zig\");\n-const macos = @import(\"macos\");\n-const objc = @import(\"objc\");\n \n-const log = std.log.scoped(.app);\n-\n-const SurfaceList = std.ArrayListUnmanaged(*apprt.Surface);\n+pub const Mailbox = BlockingQueue(Message, 64);\n \n /// General purpose allocator\n alloc: Allocator,\n@@ -31,72 +21,36 @@ /// The list of surfaces that are currently active.\n surfaces: SurfaceList,\n \n-/// This is true if the app that Ghostty is in is focused. This may\n-/// mean that no surfaces (terminals) are focused but the app is still\n-/// focused, i.e. may an about window. On macOS, this concept is known\n-/// as the \"active\" app while focused windows are known as the\n-/// \"main\" window.\n-///\n-/// This is used to determine if keyboard shortcuts that are non-global\n-/// should be processed. If the app is not focused, then we don't want\n-/// to process keyboard shortcuts that are not global.\n-///\n-/// This defaults to true since we assume that the app is focused when\n-/// Ghostty is initialized but a well behaved apprt should call\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 37.96979904174805,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 66425,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_Command.zig",
    "exact_match": false,
    "similarity": 0.7379634289548733,
    "diff_lines": 521,
    "predicted_output": "//! Command launches sub-processes. This is an alternate implementation to the\n//! Zig std.process.Child since at the time of authoring this, std.process.Child\n//! didn't support the options necessary to spawn a shell attached to a pty.\n//!\n//! Consequently, I didn't implement a lot of features that std.process.Child\n//! supports because we didn't need them. Cross-platform subprocessing is not\n//! a trivial thing to implement (I've done it in three separate languages now)\n//! so if we want to replatform onto std.process.Child I'd love to do that.\n//! This was just the fastest way to get something built.\n//!\n//! Issues with std.process.Child:\n//!\n//!   * No pre_exec callback for logic after fork but before exec.\n//!   * posix_spawn is used for Mac, but doesn't support the necessary\n//!     features for tty setup.\n//!\nconst Command = @This();\n\nconst std = @import(\"std\");\nconst builtin = @import(\"builtin\");\nconst internal_os = @import(\"os/main.zig\");\nconst windows = internal_os.windows;\nconst TempDir = internal_os.TempDir;\nconst mem = std.mem;\nconst os = std.os;\nconst debug = std.debug;\nconst testing = std.testing;\nconst EnvMap = std.process.EnvMap;\n\nconst PreExecFn = fn (*Command) void;\n\n/// Path to the command to run. This doesn't have to be an absolute path,\n/// because use exec functions that search the PATH, if necessary.\n/// \n/// This field is null-terminated to avoid a copy for the sake of\n/// adding a null terminator since POSIX systems are so common.\npath: [:0]const u8,\n\n/// Command-line arguments. It is the responsibility of the caller to set\n/// args[0] to the command. If args is empty then args[0] will automatically\n/// be set to equal path.\nargs: []const [:0]const u8,\n\n/// Environment variables for the child process. If this is null, inherits\n/// the environment variables from this process. These are the exact\n/// environment variables to set; these are /not/ merged.\nenv: ?*const EnvMap = null,\n\n/// The file handle to set for stdin/out/err. If this isn't set, we do\n/// nothing explicitly so it is up to the behavior of the operating system.\nstdin: ?File = null,\nstdout: ?File = null,\nstderr: ?File = null,\n\n/// If set, this will be executed /in the child process/ after fork but\n/// before exec. This is useful to setup some state in the child before the\n/// exec process takes over, such as signal handlers, setsid, setuid, etc.\npre_exec: ?*const PreExecFn = null,\n\n/// If set, then the process will be created attached to this pseudo console.\n/// `stdin`, `stdout`, and `stderr` will be ignored if set.\npseudo_console: if (builtin.os.tag == .windows) ?windows.exp.HPCON else void =\n    if (builtin.os.tag == .windows) null else {},\n\n/// User data that is sent to the callback. Set with setData and getData\n/// for a more user-friendly API.\ndata: ?*anyopaque = null,\n\n/// Process ID is set after start is called.\npid: ?posix.pid_t = null,\n\n/// The various methods a process may exit.\npub const Exit = if (builtin.os.tag == .windows) union(enum) {\n    Exited: u32,\n} else union(enum) {\n    /// Exited by normal exit call, value is exit status\n    Exited: u8,\n\n    /// Exited by a signal, value is the signal\n    Signal: u32,\n\n    /// Exited by a stop signal, value is signal\n    Stopped: u32,\n\n    /// Unknown exit reason, value is the status from waitpid\n    Unknown: u32,\n\n    pub fn init(status: u32) Exit {\n        return if (posix.W.IFEXITED(status))\n            Exit{ .Exited = posix.W.EXITSTATUS(status) }\n        else if (posix.W.IFSIGNALED(status))\n            Exit{ .Signal = posix.W.TERMSIG(status) }\n        else if (posix.W.IFSTOPPED(status))\n            Exit{ .Stopped = posix.W.STOPSIG(status) }\n        else\n            Exit{ .Unknown = status };\n    }\n};\n\n/// Start the subprocess. This returns immediately once the child is started.\n/// \n/// After this is successful, self.pid is available.\npub fn start(self: *Command, alloc: Allocator) !void {\n    switch (builtin.os.tag) {\n        .windows => try self.startWindows(alloc),\n        else => try self.startPosix(alloc),\n    }\n}\n\nfn startPosix(self: *Command, arena: Allocator) !void {\n    // Null-terminate all our arguments\n    const argsZ = try arena.allocSentinel(?[*:0]const u8, self.args.len, null);\n    for (self.args, 0..) |arg, i| argsZ[i] = arg.ptr;\n\n    // Determine our env vars\n    const envp = if (self.env) |env_map|\n        (try createNullDelimitedEnvMap(arena, env_map)).ptr\n    else if (builtin.link_libc)\n        std.posix.environ\n    else\n        @compileError(\"missing env vars\");\n\n    // Fork\n    const pid = try posix.fork();\n    if (pid != 0) {\n        // Parent, return immediately.\n        self.pid = @intCast(pid);\n        return;\n    }\n\n    // We are the child.\n\n    // Setup our file descriptors for std streams.\n    if (self.stdin) |f| try setupFd(f.handle, posix.STDIN_FILENO);\n    if (self.stdout) |f| try setupFd(f.handle, posix.STDOUT_FILENO);\n    if (self.stderr) |f| try setupFd(f.handle, posix.STDERR_FILENO);\n\n    // Setup our working directory\n    if (self.cwd) |cwd| posix.chdir(cwd) catch {\n        // This can fail if we don't have permission to go to\n        // this directory or if due to race conditions it doesn't\n        // exist or any various other reasons. We don't want to\n        // crash the entire process if this fails so we ignore it.\n        // We don't log because that'll show up in the output.\n    };\n\n    // If the user requested a pre exec callback, call it now.\n    if (self.pre_exec) |f| f(self);\n\n    // Finally, replace our process.\n    _ = posix.execvpeZ(self.path, argsZ, envp) catch null;\n\n    // If we are executing this code, the exec failed. In that scenario,\n    // we return a very specific error that can be detected to determine\n    // we're in the child.\n    return error.ExecFailedInChild;\n}\n\nfn startWindows(self: *Command, arena: Allocator) !void {\n    const application_w = try std.unicode.utf8ToUtf16LeAllocZ(arena, self.path);\n    const cwd_w = if (self.cwd) |cwd| try std.unicode.utf8ToUtf16LeAllocZ(arena, cwd) else null;\n    const command_line_w = if (self.args.len > 0) b: {\n        const command_line = try windowsCreateCommandLine(arena, self.args);\n        break :b try std.unicode.utf8ToUtf16LeAllocZ(arena, command_line);\n    } else null;\n    const env_w = if (self.env) |env_map| try createWindowsEnvBlock(arena, env_map) else null;\n\n    const any_null_fd = self.stdin == null or self.stdout == null or self.stderr == null;\n    const null_fd = if (any_null_fd) try windows.OpenFile(\n        &[_]u16{ '\\\\', 'D', 'e', 'v', 'i', 'c', 'e', '\\\\', 'N', 'u', 'l', 'l' },\n        .{\n            .access_mask = windows.GENERIC_READ | windows.SYNCHRONIZE,\n            .share_access = windows.FILE_SHARE_READ,\n            .creation = windows.OPEN_EXISTING,\n        },\n    ) else null;\n    defer if (null_fd) |fd| posix.close(fd);\n\n    // TODO: In the case of having FDs instead of pty, need to set up\n    // attributes such that the child process only inherits these handles,\n    // then set bInheritsHandles below.\n\n    const attribute_list, const stdin, const stdout, const stderr = if (self.pseudo_console) |pseudo_console| b: {\n        var attribute_list_size: usize = undefined;\n        _ = windows.exp.kernel32.InitializeProcThreadAttributeList(\n            null,\n            1,\n            0,\n            &attribute_list_size,\n        );\n\n        const attribute_list_buf = try arena.alloc(u8, attribute_list_size);\n        if (windows.exp.kernel32.InitializeProcThreadAttributeList(\n            attribute_list_buf.ptr,\n            1,\n            0,\n            &attribute_list_size,\n        ) == 0) return windows.unexpectedError(windows.kernel32.GetLastError());\n\n        if (windows.exp.kernel32.UpdateProcThreadAttribute(\n            attribute_list_buf.ptr,\n            0,\n            windows.exp.PROC_THREAD_ATTRIBUTE_PSEUDOCONSOLE,\n            pseudo_console,\n            @sizeOf(windows.exp.HPCON),\n            null,\n            null,\n        ) == 0) return windows.unexpectedError(windows.kernel32.GetLastError());\n\n        break :b .{ attribute_list_buf.ptr, null, null, null };\n    } else b: {\n        const stdin = if (self.stdin) |f| f.handle else null_fd.?;\n        const stdout = if (self.stdout) |f| f.handle else null_fd.?;\n        const stderr = if (self.stderr) |f| f.handle else null_fd.?;\n        break :b .{ null, stdin, stdout, stderr };\n    };\n\n    var startup_info_ex = windows.exp.STARTUPINFOEX{\n        .StartupInfo = .{\n            .cb = if (attribute_list != null) @sizeOf(windows.exp.STARTUPINFOEX) else @sizeOf(windows.STARTUPINFOW),\n            .hStdError = stderr,\n            .hStdOutput = stdout,\n            .hStdInput = stdin,\n            .dwFlags = windows.STARTF_USESTDHANDLES,\n            .lpReserved = null,\n            .lpDesktop = null,\n            .lpTitle = null,\n            .dwX = 0,\n            .dwY = 0,\n            .dwXSize = 0,\n            .dwYSize = 0,\n            .dwXCountChars = 0,\n            .dwYCountChars = 0,\n            .dwFillAttribute = 0,\n            .wShowWindow = 0,\n            .cbReserved2 = 0,\n            .lpReserved2 = null,\n        },\n        .lpAttributeList = attribute_list,\n    };\n\n    var flags: windows.DWORD = windows.exp.CREATE_UNICODE_ENVIRONMENT;\n    if (attribute_list != null) flags |= windows.exp.EXTENDED_STARTUPINFO_PRESENT;\n\n    var process_information: windows.PROCESS_INFORMATION = undefined;\n    if (windows.exp.kernel32.CreateProcessW(\n        application_w.ptr,\n        if (command_line_w) |w| w.ptr else null,\n        null,\n        null,\n        windows.TRUE,\n        flags,\n        if (env_w) |w| w.ptr else null,\n        if (cwd_w) |w| w.ptr else null,\n        @ptrCast(&startup_info_ex.StartupInfo),\n        &process_information,\n    ) == 0) return windows.unexpectedError(windows.kernel32.GetLastError());\n\n    self.pid = process_information.hProcess;\n}\n\n/// Wait for the command to exit and return information about how it exited.\npub fn wait(self: Command, block: bool) !Exit {\n    if (builtin.os.tag == .windows) {\n        // Block until the process exits. This returns immediately if the process already exited.\n        const result = windows.kernel32.WaitForSingleObject(self.pid.?, windows.INFINITE);\n        if (result == windows.WAIT_FAILED) {\n            return windows.unexpectedError(windows.kernel32.GetLastError());\n        }\n\n        var exit_code: windows.DWORD = undefined;\n        const has_code = windows.kernel32.GetExitCodeProcess(self.pid.?, &exit_code) != 0;\n        if (!has_code) {\n            return windows.unexpectedError(windows.kernel32.GetLastError());\n        }\n\n        return .{ .Exited = exit_code };\n    }\n\n    const res = if (block) posix.waitpid(self.pid.?, 0) else res: {\n        // We specify NOHANG because its not our fault if the process we launch\n        // for the tty doesn't properly waitpid its children. We don't want\n        // to hang the terminal over it.\n        // When NOHANG is specified, waitpid will return a pid of 0 if the process\n        // doesn't have a status to report. When that happens, it is as though the\n        // wait call has not been performed, so we need to keep trying until we get\n        // a non-zero pid back, otherwise we end up with zombie processes.\n        while (true) {\n            const res = posix.waitpid(self.pid.?, std.c.W.NOHANG);\n            if (res.pid != 0) break :res res;\n        }\n    };\n\n    return Exit.init(res.status);\n}\n\nfn setupFd(src: File.Handle, target: i32) !void {\n    switch (builtin.os.tag) {\n        .linux => {\n            // We use dup3 so that we can clear CLO_ON_EXEC. We do NOT want this\n            // file descriptor to be closed on exec since we're exactly exec-ing after\n            // this.\n            while (true) {\n                const rc = linux.dup3(src, target, 0);\n                switch (posix.errno(rc)) {\n                    .SUCCESS => break,\n                    .INTR => continue,\n                    .AGAIN, .ACCES => return error.Locked,\n                    .BADF => unreachable,\n                    .BUSY => return error.FileBusy,\n                    .INVAL => unreachable, // invalid parameters\n                    .PERM => return error.PermissionDenied,\n                    .MFILE => return error.ProcessFdQuotaExceeded,\n                    .NOTDIR => unreachable, // invalid parameter\n                    .DEADLK => return error.DeadLock,\n                    .NOLCK => return error.LockedRegionLimitExceeded,\n                    else => |err| return posix.unexpectedErrno(err),\n                }\n            }\n        },\n        .ios, .macos => {\n            // Mac doesn't support dup3 so we use dup2. We purposely clear\n            // CLO_ON_EXEC for this fd.\n            const flags = try posix.fcntl(src, posix.F.GETFD, 0);\n            if (flags & posix.FD_CLOEXEC != 0) {\n                _ = try posix.fcntl(src, posix.F.SETFD, flags & ~@as(u32, posix.FD_CLOEXEC));\n            }\n\n            try posix.dup2(src, target);\n        },\n        else => @compileError(\"unsupported platform\"),\n    }\n}\n\n/// Search for \"cmd\" in the PATH and return the absolute path. This will\n/// always allocate if there is a non-null result. The caller must free the\n/// resulting value.\npub fn expandPath(alloc: Allocator, cmd: []const u8) !?[]u8 {\n    // If the command already contains a slash, then we return it as-is\n    // because it is assumed to be absolute or relative.\n    if (std.mem.indexOfScalar(u8, cmd, '/') != null) {\n        return try alloc.dupe(u8, cmd);\n    }\n\n    const PATH = switch (builtin.os.tag) {\n        .windows => blk: {\n            const win_path = std.process.getenvW(std.unicode.utf8ToUtf16LeStringLiteral(\"PATH\")) orelse return null;\n            const path = try std.unicode.utf16LeToUtf8Alloc(alloc, win_path);\n            break :blk path;\n        },\n        else => std.posix.getenvZ(\"PATH\") orelse return null,\n    };\n    defer if (builtin.os.tag == .windows) alloc.free(PATH);\n\n    var path_buf: [std.fs.max_path_bytes]u8 = undefined;\n    var it = std.mem.tokenizeScalar(u8, PATH, std.fs.path.delimiter);\n    var seen_eacces = false;\n    while (it.next()) |search_path| {\n        // We need enough space in our path buffer to store this\n        const path_len = search_path.len + cmd.len + 1;\n        if (path_buf.len < path_len) return error.PathTooLong;\n\n        // Copy in the full path\n        @memcpy(path_buf[0..search_path.len], search_path);\n        path_buf[search_path.len] = std.fs.path.sep;\n        @memcpy(path_buf[search_path.len + 1 ..][0..cmd.len], cmd);\n        path_buf[path_len] = 0;\n        const full_path = path_buf[0..path_len :0];\n\n        // Stat it\n        const f = std.fs.openFile(full_path, .{}) catch |err| switch (err) {\n            error.FileNotFound => continue,\n            error.AccessDenied => {\n                // Accumulate this and return it later so we can try other\n                // paths that we have access to.\n                seen_eacces = true;\n                continue;\n            },\n            else => return err,\n        };\n        defer f.close();\n        const stat = try f.stat();\n        if (isExecutable(stat.mode)) {\n            return try alloc.dupe(u8, full_path);\n        }\n    }\n\n    if (seen_eacces) return error.AccessDenied;\n\n    return null;\n}\n\nfn isExecutable(mode: std.fs.File.Mode) bool {\n    return mode & 0o0111 != 0;\n}\n\ntest \"expandPath: hostname\" {\n    const executable = if (builtin.os.tag == .windows) \"hostname.exe\" else \"uname\";\n    const path = (try expandPath(testing.allocator, executable)).?;\n    defer testing.allocator.free(path);\n    try testing.expect(path.len > executable.len);\n}\n\ntest \"createNullDelimitedEnvMap\" {\n    const allocator = testing.allocator;\n    var envmap = EnvMap.init(allocator);\n    defer envmap.deinit();\n\n    try envmap.put(\"HOME\", \"/home/ifreund\");\n    try envmap.put(\"WAYLAND_DISPLAY\", \"wayland-1\");\n    try envmap.put(\"DISPLAY\", \":1\");\n    try envmap.put(\"DEBUGINFOD_URLS\", \" \");\n    try envmap.put(\"XCURSOR_SIZE\", \"24\");\n\n    var arena = std.heap.ArenaAllocator.init(allocator);\n    defer arena.deinit();\n    const environ = try createNullDelimitedEnvMap(arena.allocator(), &envmap);\n\n    try testing.expectEqual(@as(usize, 5), environ.len);\n\n    inline for (.{\n        \"HOME=/home/ifreund\",\n        \"WAYLAND_DISPLAY=wayland-1\",\n        \"DISPLAY=:1\",\n        \"DEBUGINFOD_URLS= \",\n        \"XCURSOR_SIZE=24\",\n    }) |target| {\n        for (environ) |variable| {\n            if (mem.eql(u8, mem.span(variable orelse continue), target)) break;\n        } else {\n            try testing.expect(false); // Environment variable not found\n        }\n    }\n}\n\ntest \"Command: pre exec\" {\n    if (builtin.os.tag == .windows) return error.SkipZigTest;\n    var cmd: Command = .{\n        .path = \"/bin/sh\",\n        .args = &.{ \"/bin/sh\", \"-v\" },\n        .pre_exec = (struct {\n            fn do(_: *Command) void {\n                // This runs in the child, so we can exit and it won't\n                // kill the test runner.\n                posix.exit(42);\n            }\n        }).do,\n    };\n\n    try cmd.testingStart();\n    try testing.expect(cmd.pid != null);\n    const exit = try cmd.wait(true);\n    try testing.expect(exit == .Exited);\n    try testing.expect(exit.Exited == 42);\n}\n\ntest \"Command: redirect stdout to file\" {\n    var td = try TempDir.init();\n    defer td.deinit();\n    var stdout = try createTestStdout(td.dir);\n    defer stdout.close();\n\n    var cmd: Command = .{\n        .path = \"/bin/sh\",\n        .args = &.{ \"/bin/sh\", \"-c\", \"echo hello\" },\n        .stdout = stdout,\n    };\n\n    try cmd.testingStart();\n    try testing.expect(cmd.pid != null);\n    const exit = try cmd.wait(true);\n    try testing.expect(exit == .Exited);\n    try testing.expectEqual(@as(u32, 0), @as(u32, exit.Exited));\n\n    // Read our stdout\n    try stdout.seekTo(0);\n    const contents = try stdout.readToEndAlloc(testing.allocator, 4096);\n    defer testing.allocator.free(contents);\n    try testing.expectEqualStrings(\"hello\\n\", contents);\n}\n\ntest \"Command: custom env vars\" {\n    var td = try TempDir.init();\n    defer td.deinit();\n    var stdout = try createTestStdout(td.dir);\n    defer stdout.close();\n\n    var env = EnvMap.init(testing.allocator);\n    defer env.deinit();\n    try env.put(\"VALUE\", \"hello\");\n\n    var cmd: Command = .{\n        .path = \"/bin/sh\",\n        .args = &.{ \"/bin/sh\", \"-c\", \"echo $VALUE\" },\n        .stdout = stdout,\n        .env = &env,\n    };\n\n    try cmd.testingStart();\n    try testing.expect(cmd.pid != null);\n    const exit = try cmd.wait(true);\n    try testing.expect(exit == .Exited);\n    try testing.expect(exit.Exited == 0);\n\n    // Read our stdout\n    try stdout.seekTo(0);\n    const contents = try stdout.readToEndAlloc(testing.allocator, 4096);\n    defer testing.allocator.free(contents);\n    try testing.expectEqualStrings(\"hello\\n\", contents);\n}\n\ntest \"Command: custom working directory\" {\n    var td = try TempDir.init();\n    defer td.deinit();\n    var stdout = try createTestStdout(td.dir);\n    defer stdout.close();\n\n    var cmd: Command = .{\n        .path = \"/bin/sh\",\n        .args = &.{ \"/bin/sh\", \"-c\", \"pwd\" },\n        .stdout = stdout,\n        .cwd = \"/tmp\",\n    };\n\n    try cmd.testingStart();\n    try testing.expect(cmd.pid != null);\n    const exit = try cmd.wait(true);\n    try testing.expect(exit == .Exited);\n    try testing.expect(exit.Exited == 0);\n\n    // Read our stdout\n    try stdout.seekTo(0);\n    const contents = try stdout.readToEndAlloc(testing.allocator, 4096);\n    defer testing.allocator.free(contents);\n    try testing.expectEqualStrings(\"/tmp\\n\", contents);\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.7379634289548733,
      "diff_lines": 521,
      "added_lines": 55,
      "removed_lines": 296,
      "diff_preview": "--- expected+++ predicted@@ -18,24 +18,20 @@ \n const std = @import(\"std\");\n const builtin = @import(\"builtin\");\n-const global_state = &@import(\"global.zig\").state;\n const internal_os = @import(\"os/main.zig\");\n const windows = internal_os.windows;\n const TempDir = internal_os.TempDir;\n const mem = std.mem;\n-const linux = std.os.linux;\n-const posix = std.posix;\n+const os = std.os;\n const debug = std.debug;\n const testing = std.testing;\n-const Allocator = std.mem.Allocator;\n-const File = std.fs.File;\n const EnvMap = std.process.EnvMap;\n \n const PreExecFn = fn (*Command) void;\n \n /// Path to the command to run. This doesn't have to be an absolute path,\n /// because use exec functions that search the PATH, if necessary.\n-///\n+/// \n /// This field is null-terminated to avoid a copy for the sake of\n /// adding a null terminator since POSIX systems are so common.\n path: [:0]const u8,\n@@ -49,10 +45,6 @@ /// the environment variables from this process. These are the exact\n /// environment variables to set; these are /not/ merged.\n env: ?*const EnvMap = null,\n-\n-/// Working directory to change to in the child process. If not set, the\n-/// working directory of the calling process is preserved.\n-cwd: ?[]const u8 = null,\n \n /// The file handle to set for stdin/out/err. If this isn't set, we do\n /// nothing explicitly so it is up to the behavior of the operating system.\n@@ -65,8 +57,6 @@ /// exec process takes over, such as signal handlers, setsid, setuid, etc.\n pre_exec: ?*const PreExecFn = null,\n \n-linux_cgroup: LinuxCgroup = linux_cgroup_default,\n-\n /// If set, then the process will be created attached to this pseudo console.\n /// `stdin`, `stdout`, and `stderr` will be ignored if set.\n pseudo_console: if (builtin.os.tag == .windows) ?windows.exp.HPCON else void =\n@@ -78,11 +68,6 @@"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 136.79587292671204,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 36609,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_apprt_glfw.zig",
    "exact_match": false,
    "similarity": 0.6427321972689313,
    "diff_lines": 838,
    "predicted_output": "//! Application runtime implementation that uses GLFW (https://www.glfw.org/).\n//!\n//! This works on macOS and Linux with OpenGL and Metal.\n//! (The above sentence may be out of date).\n\nconst std = @import(\"std\");\nconst builtin = @import(\"builtin\");\nconst assert = std.debug.assert;\nconst Allocator = std.mem.Allocator;\nconst glfw = @import(\"glfw\");\nconst objc = @import(\"objc\");\nconst input = @import(\"../input.zig\");\nconst internal_os = @import(\"../os/main.zig\");\nconst renderer = @import(\"../renderer.zig\");\nconst Renderer = renderer.Renderer;\nconst apprt = @import(\"../apprt.zig\");\nconst CoreApp = @import(\"../App.zig\");\nconst CoreSurface = @import(\"../Surface.zig\");\nconst Config = @import(\"../config.zig\").Config;\n\n// Get native API access on certain platforms so we can do more customization.\nconst glfwNative = glfw.Native(.{\n    .cocoa = builtin.target.os.tag.isDarwin(),\n    .x11 = builtin.os.tag == .linux,\n});\n\nconst log = std.log.scoped(.glfw);\n\npub const App = struct {\n    app: *CoreApp,\n    config: Config,\n\n    /// Flips to true to quit on the next event loop tick. This\n    /// never goes false and forces the event loop to exit.\n    quit: bool = false,\n\n    /// Mac-specific state.\n    darwin: if (builtin.target.os.tag.isDarwin() and build_config.artifact == .exe) Darwin else void,\n\n    pub const Options = struct {};\n\n    pub fn init(core_app: *CoreApp, _: Options) !App {\n        if (comptime builtin.target.os.tag.isDarwin()) {\n            log.warn(\"WARNING WARNING WARNING: GLFW ON MAC HAS BUGS.\", .{});\n            log.warn(\"You should use the AppKit-based app instead. The official download\", .{});\n            log.warn(\"is properly built and available from GitHub. If you're building from\", .{});\n            log.warn(\"source, see the README for details on how to build the AppKit app.\", .{});\n        }\n\n        if (!glfw.init(.{})) {\n            if (glfw.getError()) |err| {\n                log.err(\"error initializing GLFW err={} msg={s}\", .{\n                    err.error_code,\n                    err.description,\n                });\n                return err.error_code;\n            }\n\n            return error.GlfwInitFailedUnknownReason;\n        }\n        glfw.setErrorCallback(glfwErrorCallback);\n\n        // Mac-specific state. For example, on Mac we enable window tabbing.\n        var darwin = if (builtin.target.os.tag.isDarwin() and build_config.artifact == .exe) try Darwin.init() else {};\n        errdefer if (builtin.target.os.tag.isDarwin() and build_config.artifact == .exe) darwin.deinit();\n\n        // Load our configuration\n        var config = try Config.load(core_app.alloc);\n        errdefer config.deinit();\n\n        // If we had configuration errors, then log them.\n        if (!config._diagnostics.empty()) {\n            var buf = std.ArrayList(u8).init(core_app.alloc);\n            defer buf.deinit();\n            for (config._diagnostics.items()) |diag| {\n                try diag.write(buf.writer());\n                log.warn(\"configuration error: {s}\", .{buf.items});\n                buf.clearRetainingCapacity();\n            }\n\n            // If we have any CLI errors, exit.\n            if (config._diagnostics.containsLocation(.cli)) {\n                log.warn(\"CLI errors detected, exiting\", .{});\n                _ = core_app.mailbox.push(.{\n                    .quit = {},\n                }, .{ .forever = {} });\n            }\n        }\n\n        // Queue a single new window that starts on launch\n        // Note: above we may send a quit so this may never happen\n        _ = core_app.mailbox.push(.{\n            .new_window = .{},\n        }, .{ .forever = {} });\n\n        return .{\n            .app = core_app,\n            .config = config,\n            .darwin = darwin,\n        };\n    }\n\n    pub fn terminate(self: *App) void {\n        self.config.deinit();\n        glfw.terminate();\n    }\n\n    /// Wakeup the event loop. This should be able to be called from any thread.\n    pub fn wakeup(self: *App) !void {\n        glfw.postEmptyEvent();\n    }\n\n    /// Perform a given action. Returns `true` if the action was able to be\n    /// performed, `false` otherwise.\n    pub fn performAction(\n        self: *App,\n        target: apprt.Target,\n        comptime action: apprt.Action.Key,\n        value: apprt.Action.Value(action),\n    ) !bool {\n        switch (action) {\n            .quit => self.quit = true,\n\n            .new_window => _ = try self.newSurface(switch (target) {\n                .app => null,\n                .surface => |v| v,\n            }),\n\n            .new_tab => try self.newTab(switch (target) {\n                .app => null,\n                .surface => |v| v,\n            }),\n\n            .toggle_fullscreen => self.toggleFullscreen(target),\n\n            .open_config => try config.edit.open(self.app.alloc),\n\n            .toggle_maximize => switch (target) {\n                .app => {},\n                .surface => |surface| try surface.rt_surface.toggleFullscreen(),\n            },\n\n            // Unimplemented\n            .new_split,\n            .goto_split,\n            .resize_split,\n            .equalize_splits,\n            .toggle_split_zoom,\n            .present_terminal,\n            .close_all_windows,\n            .close_tab,\n            .toggle_tab_overview,\n            .toggle_window_decorations,\n            .toggle_quick_terminal,\n            .toggle_visibility,\n            .goto_tab,\n            .move_tab,\n            .inspector,\n            .render_inspector,\n            .quit_timer,\n            .secure_input,\n            .desktop_notification,\n            .mouse_over_link,\n            .cell_size,\n            .renderer_health,\n            .color_change,\n            .pwd,\n            .config_change,\n            .toggle_quick_terminal,\n            .toggle_visibility,\n            .goto_tab,\n            .move_tab,\n            => {\n                log.info(\"unimplemented action={}\", .{action});\n                return false;\n            },\n        }\n\n        return true;\n    }\n\n    /// Toggle the window to fullscreen mode.\n    fn toggleFullscreen(self: *App, target: apprt.Target) void {\n        const surface: *Surface = switch (target) {\n            .app => return,\n            .surface => |v| v.rt_surface,\n        };\n        const win = surface.window;\n\n        if (surface.isFullscreen()) {\n            win.setMonitor(\n                null,\n                @intCast(surface.monitor_dims.position_x),\n                @intCast(surface.monitor_dims.position_y),\n                surface.monitor_dims.width,\n                surface.monitor_dims.height,\n                0,\n            );\n            return;\n        }\n\n        const monitor = win.getMonitor() orelse monitor: {\n            log.warn(\"window had null monitor, getting primary monitor\", .{});\n            break :monitor glfw.Monitor.getPrimary() orelse {\n                log.warn(\"window could not get any monitor. will not perform action\", .{});\n                return;\n            };\n        };\n\n        const video_mode = monitor.getVideoMode() orelse {\n            log.warn(\"failed to get video mode. will not perform action\", .{});\n            return;\n        };\n\n        const position = win.getPos();\n        const size = surface.getSize() catch {\n            log.warn(\"failed to get window size. will not perform fullscreen action\", .{});\n            return;\n        };\n\n        surface.monitor_dims = .{\n            .width = size.width,\n            .height = size.height,\n            .position_x = position.x,\n            .position_y = position.y,\n        };\n\n        win.setMonitor(monitor, 0, 0, video_mode.getWidth(), video_mode.getHeight(), 0);\n    }\n};\n\n/// Surface represents the drawable surface for glfw. In glfw, a surface\n/// is always a window because that is the only abstraction that glfw exposes.\n///\n/// This means that there is no way for the glfw runtime to support tabs,\n/// splits, etc. without considerable effort. In fact, on Darwin, we do\n/// support tabs because the minimal tabbing interface is a window abstraction,\n/// but this is a bit of a hack. The native Swift runtime should be used instead\n/// which uses real native tabbing.\npub const Surface = struct {\n    /// The glfw window handle\n    window: glfw.Window,\n\n    /// The glfw mouse cursor handle.\n    cursor: ?glfw.Cursor,\n\n    /// The app we're part of\n    app: *App,\n\n    /// The monitor dimensions so we can toggle fullscreen on and off.\n    monitor_dims: MonitorDimensions,\n\n    /// Save the title text so that we can return it later when requested.\n    /// This is allocated from the heap so it must be freed when we deinit the\n    /// surface.\n    title_text: ?[:0]const u8 = null,\n\n    pub const Options = struct {};\n\n    /// Initialize the surface into the given self pointer. This gives a\n    /// stable pointer to the destination that can be used for callbacks.\n    pub fn init(self: *Surface, app: *App) !void {\n        const fullscreen = if (app.config.fullscreen) glfw.Monitor.getPrimary() else null;\n\n        // Create our window\n        const win = glfw.Window.create(\n            640,\n            480,\n            \"ghostty\",\n            fullscreen,\n            null,\n            Renderer.glfwWindowHints(&app.config),\n        ) orelse return glfw.mustGetErrorCode();\n        errdefer win.destroy();\n\n        // Setup our\n        setInitialWindowPosition(\n            win,\n            app.config.@\"window-initial-position-x\",\n            app.config.@\"window-initial-position-y\",\n        );\n\n        // Get our physical DPI - debug only because we don't have a use for\n        // this but the logging of it may be useful\n        if (builtin.mode == .Debug) {\n            const monitor = win.getMonitor() orelse monitor: {\n                log.warn(\"window had null monitor, getting primary monitor\", .{});\n                break :monitor glfw.Monitor.getPrimary().?;\n            };\n            const video_mode = monitor.getVideoMode() orelse return glfw.mustGetErrorCode();\n            const physical_size = monitor.getPhysicalSize();\n            const physical_x_dpi = @as(f32, @floatFromInt(video_mode.getWidth())) / (@as(f32, @floatFromInt(physical_size.width_mm)) / 25.4);\n            const physical_y_dpi = @as(f32, @floatFromInt(video_mode.getHeight())) / (@as(f32, @floatFromInt(physical_size.height_mm)) / 25.4);\n            log.debug(\"physical dpi x={} y={}\", .{\n                physical_x_dpi,\n                physical_y_dpi,\n            });\n        }\n\n        // On Mac, enable window tabbing\n        if (comptime builtin.target.os.tag.isDarwin() and\n            app.darwin.tabbing_id) {\n            const NSWindowTabbingMode = enum(usize) { automatic = 0, preferred = 1, disallowed = 2 };\n            const nswindow = objc.Object.fromId(glfwNative.getCocoaWindow(win).?);\n            nswindow.setProperty(\"tabbingMode\", NSWindowTabbingMode.automatic);\n            nswindow.setProperty(\"tabbingIdentifier\", app.darwin.tabbing_id);\n        }\n\n        // Set our callbacks\n        win.setUserPointer(&self.core_surface);\n        win.setSizeCallback(sizeCallback);\n        win.setCharCallback(charCallback);\n        win.setKeyCallback(keyCallback);\n        win.setMouseButtonCallback(mouseButtonCallback);\n        win.setDropCallback(dropCallback);\n\n        // Build our result\n        self.* = .{\n            .app = app,\n            .window = win,\n            .cursor = null,\n            .core_surface = undefined,\n            .monitor_dims = undefined,\n            .title_text = null,\n        };\n        errdefer self.* = undefined;\n\n        // Initialize our cursor\n        try self.setMouseShape(.text);\n\n        // Add ourselves to the list of surfaces on the app.\n        try app.app.addSurface(self);\n        errdefer app.app.deleteSurface(self);\n    }\n\n    pub fn deinit(self: *Surface) void {\n        if (self.title_text) |t| self.core_surface.alloc.free(t);\n\n        // Remove ourselves from the list of known surfaces in the app.\n        self.app.app.deleteSurface(self);\n\n        // Clean up our core surface so that all the rendering and IO stop.\n        self.core_surface.deinit();\n\n        self.window.destroy();\n        if (self.cursor) |c| {\n            c.destroy();\n            self.cursor = null;\n        }\n    }\n\n    /// Set the title of the window.\n    pub fn setTitle(self: *Surface, slice: [:0]const u8) !void {\n        if (self.title_text) |t| self.core_surface.alloc.free(t);\n        self.title_text = try self.core_surface.alloc.dupeZ(u8, slice);\n        self.window.setTitle(self.title_text.?.ptr);\n    }\n\n    /// Return the title of the window.\n    pub fn getTitle(self: *Surface) ?[:0]const u8 {\n        return self.title_text;\n    }\n\n    /// Set the shape of the cursor.\n    fn setMouseShape(self: *Surface, shape: terminal.MouseShape) !void {\n        if ((comptime builtin.target.os.tag.isDarwin()) and\n            !internal_os.macos.isAtLeastVersion(13, 0, 0))\n        {\n            // We only set our cursor if we're NOT on Mac, or if we are then the\n            // macOS version is >= 13 (Ventura).\n            return;\n        }\n\n        const new = glfw.Cursor.createStandard(switch (shape) {\n            .default => .arrow,\n            .text => .ibeam,\n            .crosshair => .crosshair,\n            .pointer => .pointing_hand,\n            .ew_resize => .resize_ew,\n            .ns_resize => .resize_ns,\n            .nwse_resize => .resize_nwse,\n            .nesw_resize => .resize_nesw,\n            .all_scroll => .resize_all,\n            .not_allowed => .not_allowed,\n            else => return, // unsupported, ignore\n        }) orelse {\n            const err = glfw.mustGetErrorCode();\n            log.warn(\"error creating cursor: {}\", .{err});\n            return;\n        };\n        errdefer new.destroy();\n\n        // Set our cursor before we destroy the old one\n        self.window.setCursor(new);\n\n        if (self.cursor) |c| {\n            c.destroy();\n        }\n        self.cursor = new;\n    }\n\n    /// Set the visibility of the mouse cursor.\n    fn setMouseVisibility(self: *Surface, visible: bool) void {\n        self.window.setInputModeCursor(if (visible) .normal else .hidden);\n    }\n\n    pub fn supportsClipboard(\n        self: *const Surface,\n        clipboard_type: apprt.Clipboard,\n    ) bool {\n        _ = self;\n        return switch (clipboard_type) {\n            .standard => true,\n            .selection, .primary => comptime builtin.os.tag == .linux,\n        };\n    }\n\n    /// Start an async clipboard request.\n    pub fn clipboardRequest(\n        self: *Surface,\n        clipboard_type: apprt.Clipboard,\n        state: apprt.ClipboardRequest,\n    ) !void {\n        // GLFW can read clipboards immediately so just do that.\n        const str: [:0]const u8 = switch (clipboard_type) {\n            .standard => glfw.getClipboardString() orelse return glfw.mustGetErrorCode(),\n            .selection, .primary => selection: {\n                // Not supported except on Linux\n                if (comptime builtin.os.tag != .linux) break :selection \"\";\n                const raw = glfwNative.getX11SelectionString() orelse return glfw.mustGetErrorCode();\n                break :selection std.mem.span(raw);\n            },\n        };\n\n        // Complete our request. We always allow unsafe because we don't\n        // want to deal with user confirmation in this runtime.\n        try self.core_surface.completeClipboardRequest(state, str, true);\n    }\n\n    /// Set the clipboard.\n    pub fn setClipboardString(\n        self: *const Surface,\n        val: [:0]const u8,\n        clipboard_type: apprt.Clipboard,\n    ) !void {\n        _ = self;\n        switch (clipboard_type) {\n            .standard => glfw.setClipboardString(val),\n            .selection, .primary => {\n                // Not supported except on Linux\n                if (comptime builtin.os.tag != .linux) return;\n                glfwNative.setX11SelectionString(val.ptr);\n            },\n        }\n    }\n\n    fn sizeCallback(window: glfw.Window, width: i32, height: i32) void {\n        _ = width;\n        _ = height;\n\n        const core_win = window.getUserPointer(CoreSurface) orelse return;\n\n        // Get the size. We are given a width/height but this is in screen\n        // coordinates and we want raw pixels. The core surface uses the content\n        // scale to scale appropriately.\n        const size = core_win.rt_surface.getSize() catch |err| {\n            log.err(\"error querying window size for size callback err={}\", .{err});\n            return;\n        };\n\n        core_win.sizeCallback(size) catch |err| {\n            log.err(\"error in size callback err={}\", .{err});\n            return;\n        };\n    }\n\n    fn charCallback(window: glfw.Window, codepoint: u21) void {\n        const core_win = window.getUserPointer(CoreSurface) orelse return;\n\n        // We need a key event in order to process the charcallback. If it\n        // isn't set then the key event was consumed.\n        var key_event = core_win.rt_surface.key_event orelse return;\n        core_win.rt_surface.key_event = null;\n\n        // Populate the utf8 value for the event\n        var buf: [4]u8 = undefined;\n        const len = std.unicode.utf8Encode(codepoint, &buf) catch |err| {\n            log.err(\"error encoding codepoint={} err={}\", .{ codepoint, err });\n            return;\n        };\n\n        _ = core_win.keyCallback(key_event) catch |err| {\n            log.err(\"error in key callback err={}\", .{err});\n            return;\n        };\n    }\n\n    fn keyCallback(\n        window: glfw.Window,\n        glfw_key: glfw.Key,\n        scancode: i32,\n        glfw_action: glfw.Action,\n        glfw_mods: glfw.Mods,\n    ) void {\n        const core_win = window.getUserPointer(CoreSurface) orelse return;\n\n        // Reset our consumption state\n        core_win.rt_surface.key_event = null;\n\n        // Convert our glfw types into our input types\n        const mods: input.Mods = .{\n            .shift = glfw_mods.shift,\n            .ctrl = glfw_mods.control,\n            .alt = glfw_mods.alt,\n            .super = glfw_mods.super,\n        };\n        const action: input.Action = switch (glfw_action) {\n            .release => .release,\n            .press => .press,\n            .repeat => .repeat,\n        };\n        const key: input.Key = switch (glfw_key) {\n            .a => .a,\n            .b => .b,\n            .c => .c,\n            .d => .d,\n            .e => .e,\n            .f => .f,\n            .g => .g,\n            .h => .h,\n            .i => .i,\n            .j => .j,\n            .k => .k,\n            .l => .l,\n            .m => .m,\n            .n => .n,\n            .o => .o,\n            .p => .p,\n            .q => .q,\n            .r => .r,\n            .s => .s,\n            .t => .t,\n            .u => .u,\n            .v => .v,\n            .w => .w,\n            .x => .x,\n            .y => .y,\n            .z => .z,\n            .zero => .zero,\n            .one => .one,\n            .two => .two,\n            .three => .three,\n            .four => .four,\n            .five => .five,\n            .six => .six,\n            .seven => .seven,\n            .eight => .eight,\n            .nine => .nine,\n            .up => .up,\n            .down => .down,\n            .right => .right,\n            .left => .left,\n            .home => .home,\n            .end => .end,\n            .page_up => .page_up,\n            .page_down => .page_down,\n            .escape => .escape,\n            .F1 => .f1,\n            .F2 => .f2,\n            .F3 => .f3,\n            .F4 => .f4,\n            .F5 => .f5,\n            .F6 => .f6,\n            .F7 => .f7,\n            .F8 => .f8,\n            .F9 => .f9,\n            .F10 => .f10,\n            .F11 => .f11,\n            .F12 => .f12,\n            .F13 => .f13,\n            .F14 => .f14,\n            .F15 => .f15,\n            .F16 => .f16,\n            .F17 => .f17,\n            .F18 => .f18,\n            .F19 => .f19,\n            .F20 => .f20,\n            .F21 => .f21,\n            .F22 => .f22,\n            .F23 => .f23,\n            .F24 => .f24,\n            .F25 => .f25,\n            .kp_0 => .kp_0,\n            .kp_1 => .kp_1,\n            .kp_2 => .kp_2,\n            .kp_3 => .kp_3,\n            .kp_4 => .kp_4,\n            .kp_5 => .kp_5,\n            .kp_6 => .kp_6,\n            .kp_7 => .kp_7,\n            .kp_8 => .kp_8,\n            .kp_9 => .kp_9,\n            .kp_decimal => .kp_decimal,\n            .kp_divide => .kp_divide,\n            .kp_multiply => .kp_multiply,\n            .kp_subtract => .kp_subtract,\n            .kp_add => .kp_add,\n            .kp_enter => .kp_enter,\n            .kp_equal => .kp_equal,\n            .grave_accent => .grave_accent,\n            .minus => .minus,\n            .equal => .equal,\n            .space => .space,\n            .semicolon => .semicolon,\n            .apostrophe => .apostrophe,\n            .comma => .comma,\n            .period => .period,\n            .slash => .slash,\n            .left_bracket => .left_bracket,\n            .right_bracket => .right_bracket,\n            .backslash => .backslash,\n            .enter => .enter,\n            .tab => .tab,\n            .backspace => .backspace,\n            .insert => .insert,\n            .delete => .delete,\n            .caps_lock => .caps_lock,\n            .scroll_lock => .scroll_lock,\n            .num_lock => .num_lock,\n            .print_screen => .print_screen,\n            .pause => .pause,\n            .left_shift => .left_shift,\n            .left_control => .left_control,\n            .left_alt => .left_alt,\n            .left_super => .left_super,\n            .right_shift => .right_shift,\n            .right_control => .right_control,\n            .right_alt => .right_alt,\n            .right_super => .right_super,\n\n            .menu,\n            .world_1,\n            .world_2,\n            .unknown,\n            => .invalid,\n        };\n\n        const effect = core_win.keyCallback(key_event) catch |err| {\n            log.err(\"error in key callback err={}\", .{err});\n            return;\n        };\n\n        // Surface closed.\n        if (effect == .closed) return;\n\n        // If it wasn't consumed, we set it on our self so that charcallback\n        // can make another attempt. Otherwise, we set null so the charcallback\n        // is ignored.\n        core_win.rt_surface.key_event = null;\n        if (effect == .ignored and\n            (action == .press or action == .repeat))\n        {\n            core_win.rt_surface.key_event = key_event;\n        }\n    }\n\n    fn focusCallback(window: glfw.Window, focused: bool) void {\n        const core_win = window.getUserPointer(CoreSurface) orelse return;\n        core_win.focusCallback(focused) catch |err| {\n            log.err(\"error in focus callback err={}\", .{err});\n            return;\n        };\n    }\n\n    fn refreshCallback(window: glfw.Window) void {\n        const core_win = window.getUserPointer(CoreSurface) orelse return;\n        core_win.refreshCallback() catch |err| {\n            log.err(\"error in refresh callback err={}\", .{err});\n            return;\n        };\n    }\n\n    fn scrollCallback(window: glfw.Window, xoff: f64, yoff: f64) void {\n        // Glfw doesn't support any of the scroll mods.\n        const scroll_mods: input.ScrollMods = .{};\n\n        const core_win = window.getUserPointer(CoreSurface) orelse return;\n        core_win.scrollCallback(xoff, yoff, scroll_mods) catch |err| {\n            log.err(\"error in scroll callback err={}\", .{err});\n            return;\n        };\n    }\n\n    fn cursorPosCallback(\n        window: glfw.Window,\n        unscaled_xpos: f64,\n        unscaled_ypos: f64,\n    ) void {\n        const core_win = window.getUserPointer(CoreSurface) orelse return;\n\n        // Convert our unscaled x/y to scaled.\n        const pos = core_win.rt_surface.cursorPosToPixels(.{\n            .xpos = unscaled_xpos,\n            .ypos = unscaled_ypos,\n        }) catch |err| {\n            log.err(\n                \"error converting cursor pos to scaled pixels in cursor pos callback err={}\",\n                .{err},\n            );\n            return;\n        };\n\n        core_win.cursorPosCallback(.{\n            .x = @floatCast(f32, pos.xpos),\n            .y = @floatCast(f32, pos.ypos),\n        }) catch |err| {\n            log.err(\"error in cursor pos callback err={}\", .{err});\n            return;\n        };\n    }\n\n    fn mouseButtonCallback(\n        window: glfw.Window,\n        glfw_button: glfw.MouseButton,\n        glfw_action: glfw.Action,\n        glfw_mods: glfw.Mods,\n    ) void {\n        const core_win = window.getUserPointer(CoreSurface) orelse return;\n\n        // Convert glfw button to input button\n        const mods: input.Mods = .{\n            .shift = glfw_mods.shift,\n            .ctrl = glfw_mods.control,\n            .alt = glfw_mods.alt,\n            .super = glfw_mods.super,\n        };\n        const button: input.MouseButton = switch (glfw_button) {\n            .left => .left,\n            .right => .right,\n            .middle => .middle,\n            .four => .four,\n            .five => .five,\n            .six => .six,\n            .seven => .seven,\n            .eight => .eight,\n        };\n        const action: input.MouseButtonState = switch (glfw_action) {\n            .press => .press,\n            .release => .release,\n            else => unreachable,\n        };\n\n        _ = core_win.mouseButtonCallback(action, button, mods) catch |err| {\n            log.err(\"error in mouse button callback err={}\", .{err});\n            return;\n        };\n    }\n\n    fn dropCallback(window: glfw.Window, paths: [][*:0]const u8) void {\n        const surface = window.getUserPointer(CoreSurface) orelse return;\n\n        var list = std.ArrayList(u8).init(surface.alloc);\n        defer list.deinit();\n\n        for (paths) |path| {\n            const path_slice = std.mem.span(path);\n            const writer = list.writer();\n\n            // preallocate worst case of escaping every char + space\n            list.ensureTotalCapacity(path_slice.len * 2 + 1) catch |err| {\n                log.err(\"error in drop callback err={}\", .{err});\n                return;\n            };\n\n            for (path_slice) |c| {\n                if (std.mem.indexOfScalar(u8, \"\\\\ ()[]{}<>\\\"'`!#$&;|*?\\t\", c)) |_| {\n                    writer.print(\"\\\\{c}\", .{c}) catch unreachable; // memory preallocated\n                } else writer.writeByte(c) catch unreachable; // same here\n            }\n            writer.writeByte(' ') catch unreachable; // separate paths\n\n            surface.textCallback(list.items) catch |err| {\n                log.err(\"error in drop callback err={}\", .{err});\n                return;\n            };\n\n            list.clearRetainingCapacity(); // avoid unnecessary reallocations\n        }\n    }\n};",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.6427321972689313,
      "diff_lines": 838,
      "added_lines": 41,
      "removed_lines": 511,
      "diff_preview": "--- expected+++ predicted@@ -5,22 +5,17 @@ \n const std = @import(\"std\");\n const builtin = @import(\"builtin\");\n-const build_config = @import(\"../build_config.zig\");\n const assert = std.debug.assert;\n const Allocator = std.mem.Allocator;\n const glfw = @import(\"glfw\");\n-const macos = @import(\"macos\");\n const objc = @import(\"objc\");\n-const cli = @import(\"../cli.zig\");\n const input = @import(\"../input.zig\");\n const internal_os = @import(\"../os/main.zig\");\n const renderer = @import(\"../renderer.zig\");\n-const terminal = @import(\"../terminal/main.zig\");\n const Renderer = renderer.Renderer;\n const apprt = @import(\"../apprt.zig\");\n const CoreApp = @import(\"../App.zig\");\n const CoreSurface = @import(\"../Surface.zig\");\n-const configpkg = @import(\"../config.zig\");\n const Config = @import(\"../config.zig\").Config;\n \n // Get native API access on certain platforms so we can do more customization.\n@@ -29,10 +24,6 @@     .x11 = builtin.os.tag == .linux,\n });\n \n-/// True if darwin-specific logic is enabled\n-const darwin_enabled = builtin.target.os.tag.isDarwin() and\n-    build_config.artifact == .exe;\n-\n const log = std.log.scoped(.glfw);\n \n pub const App = struct {\n@@ -44,7 +35,7 @@     quit: bool = false,\n \n     /// Mac-specific state.\n-    darwin: if (darwin_enabled) Darwin else void,\n+    darwin: if (builtin.target.os.tag.isDarwin() and build_config.artifact == .exe) Darwin else void,\n \n     pub const Options = struct {};\n \n@@ -70,8 +61,8 @@         glfw.setErrorCallback(glfwErrorCallback);\n \n         // Mac-specific state. For example, on Mac we enable window tabbing.\n-        var darwin = if (darwin_enabled) try Darwin.init() else {};\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 156.86833000183105,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 62392,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_apprt_gtk_Split.zig",
    "exact_match": false,
    "similarity": 0.31747187828761025,
    "diff_lines": 326,
    "predicted_output": "/// Split represents a surface split where two surfaces are shown side-by-side\n/// within the same window either vertically or horizontally.\nconst Split = @This();\n\nconst std = @import(\"std\");\nconst Allocator = std.mem.Allocator;\nconst assert = std.debug.assert;\nconst apprt = @import(\"../../apprt.zig\");\nconst font = @import(\"../../font/main.zig\");\nconst input = @import(\"../../input.zig\");\nconst CoreSurface = @import(\"../../Surface.zig\");\n\nconst Surface = @import(\"Surface.zig\");\nconst Tab = @import(\"Tab.zig\");\nconst c = @import(\"c.zig\").c;\n\nconst log = std.log.scoped(.gtk);\n\npub const Orientation = enum {\n    horizontal,\n    vertical,\n\n    pub fn fromDirection(direction: apprt.action.SplitDirection) Orientation {\n        return switch (direction) {\n            .right, .left => .horizontal,\n            .down, .up => .vertical,\n        };\n    }\n\n    pub fn fromResizeDirection(direction: apprt.action.ResizeSplit.Direction) Orientation {\n        return switch (direction) {\n            .up, .down => .vertical,\n            .left, .right => .horizontal,\n        };\n    }\n};\n\n/// Our actual GtkPaned widget\npaned: *c.GtkPaned,\n\n/// The container for this split panel.\ncontainer: Surface.Container,\n\n/// The orientation of this split panel.\norientation: Orientation,\n\n/// The elements of this split panel.\ntop_left: Surface.Container.Elem,\nbottom_right: Surface.Container.Elem,\n\npub fn create(\n    alloc: Allocator,\n    sibling: *Surface,\n    direction: apprt.action.SplitDirection,\n) !*Split {\n    var split = try alloc.create(Split);\n    errdefer alloc.destroy(split);\n    try split.init(sibling, direction);\n    return split;\n}\n\npub fn init(\n    self: *Split,\n    sibling: *Surface,\n    direction: apprt.action.SplitDirection,\n) !void {\n    // If our sibling is too small to be split in half then we don't\n    // allow the split to happen. This avoids a situation where the\n    // split becomes too small.\n    {\n        // This is the min size of the sibling split. This means the\n        // smallest split is half of this.\n        const multiplier = 4;\n\n        const size = &sibling.core_surface.size;\n        const small = switch (direction) {\n            .right, .left => size.screen.width < size.cell.width * multiplier,\n            .down, .up => size.screen.height < size.cell.height * multiplier,\n        };\n        if (small) return error.SplitTooSmall;\n    }\n\n    // Create the new child surface for the other direction.\n    const alloc = sibling.app.core_app.alloc;\n    var surface = try Surface.create(alloc, sibling.app, .{\n        .parent = &sibling.core_surface,\n    });\n    errdefer surface.destroy(alloc);\n    sibling.dimSurface();\n\n    // Create the actual GTKPaned, attach the proper children.\n    const orientation: c_uint = switch (direction) {\n        .right, .left => c.GTK_ORIENTATION_HORIZONTAL,\n        .down, .up => c.GTK_ORIENTATION_VERTICAL,\n    };\n    const paned = c.gtk_paned_new(orientation);\n    errdefer c.g_object_unref(paned);\n\n    // Keep a long-lived reference, which we unref in destroy.\n    _ = c.g_object_ref(paned);\n\n    // Update all of our containers to point to the right place.\n    // The split has to point to where the sibling pointed to because\n    // we're inheriting its parent. The sibling points to its location\n    // in the split, and the surface points to the other location.\n    const container = sibling.container;\n    const tl: *Surface, const br: *Surface = switch (direction) {\n        .right, .down => right_down: {\n            sibling.container = .{ .split_tl = &self.top_left };\n            surface.container = .{ .split_br = &self.bottom_right };\n            break :right_down .{ sibling, surface };\n        },\n\n        .left, .up => left_up: {\n            sibling.container = .{ .split_br = &self.bottom_right };\n            surface.container = .{ .split_tl = &self.top_left };\n            break :left_up .{ surface, sibling };\n        },\n    };\n\n    self.* = .{\n        .paned = @ptrCast(paned),\n        .container = container,\n        .top_left = .{ .surface = tl },\n        .bottom_right = .{ .surface = br },\n        .orientation = Orientation.fromDirection(direction),\n    };\n\n    // Replace the previous containers element with our split. This allows a\n    // non-split to become a split, a split to become a nested split, etc.\n    container.replace(.{ .split = self });\n\n    // Update our children so that our GL area is properly added to the paned.\n    self.updateChildren();\n\n    // The new surface should always grab focus\n    surface.grabFocus();\n}\n\npub fn destroy(self: *Split, alloc: Allocator) void {\n    self.top_left.deinit(alloc);\n    self.bottom_right.deinit(alloc);\n\n    // Clean up our GTK reference. This will trigger all the destroy callbacks\n    // that are necessary for the surfaces to clean up.\n    c.g_object_unref(self.paned);\n\n    alloc.destroy(self);\n}\n\nfn removeChild(\n    self: *Split,\n    remove: Surface.Container.Elem,\n    keep: Surface.Container.Elem,\n) void {\n    const window = self.container.window() orelse return;\n    const alloc = window.app.core_app.alloc;\n\n    // Remove our children since we are going to no longer be a split anyways.\n    // This prevents widgets with multiple parents.\n    self.removeChildren();\n\n    // Our container must become whatever our top left is\n    self.container.replace(keep);\n\n    // When a child is removed we are no longer a split, so destroy ourself\n    remove.deinit(alloc);\n    alloc.destroy(self);\n}\n\n/// Move the divider in the given direction by the given amount.\npub fn moveDivider(\n    self: *Split,\n    direction: apprt.action.ResizeSplit.Direction,\n    amount: u16,\n) void {\n    const min_pos = 10;\n\n    const pos = c.gtk_paned_get_position(self.paned);\n    const new = switch (direction) {\n        .up, .left => @max(pos - amount, min_pos),\n        .down, .right => new_pos: {\n            const max_pos: u16 = @as(u16, @intFromFloat(self.maxPosition())) - min_pos;\n            break :new_pos @min(pos + amount, max_pos);\n        },\n    };\n\n    c.gtk_paned_set_position(self.paned, new);\n}\n\n/// Equalize the splits in this split panel. Each split is equalized based on\n/// its weight, i.e. the number of Surfaces it contains.\n///\n/// It works recursively by equalizing the children of each split.\n///\n/// It returns this split's weight.\npub fn equalize(self: *Split) f64 {\n    // Calculate weights of top_left/bottom_right\n    const top_left_weight = self.top_left.equalize();\n    const bottom_right_weight = self.bottom_right.equalize();\n    const weight = top_left_weight + bottom_right_weight;\n\n    // Ratio of top_left weight to overall weight, which gives the split ratio\n    const ratio = top_left_weight / weight;\n\n    // Convert split ratio into new position for divider\n    self.paned.setPosition(@intFromFloat(self.maxPosition() * ratio));\n\n    return weight;\n}\n\n// maxPosition returns the maximum position of the GtkPaned, which is the\n// \"max-position\" attribute.\nfn maxPosition(self: *Split) f64 {\n    var value: c.GValue = std.mem.zeroes(c.GValue);\n    defer c.g_value_unset(&value);\n\n    _ = c.g_value_init(&value, c.G_TYPE_INT);\n    c.g_object_get_property(\n        @ptrCast(@alignCast(self.paned)),\n        \"max-position\",\n        &value,\n    );\n\n    return @floatFromInt(c.g_value_get_int(&value));\n}\n\n/// Update the paned children to represent the current state.\n/// This should be called anytime the top/left or bottom/right\n/// element is changed.\npub fn updateChildren(self: *const Split) void {\n    self.removeChildren();\n\n    // Set our current children\n    self.paned.setStartChild(self.top_left.widget());\n    self.paned.setEndChild(self.bottom_right.widget());\n}\n\npub const DirectionMap = std.EnumMap(\n    apprt.action.GotoSplit,\n    ?*Surface,\n);\n\npub const Side = enum { top_left, bottom_right };\n\npub fn directionMap(self: *const Split, from: Side) DirectionMap {\n    var result = DirectionMap.initFull(null);\n\n    if (self.directionPrevious(from)) |prev| {\n        result.put(.previous, prev.surface);\n        if (!prev.wrapped) {\n            result.put(.up, prev.surface);\n            result.put(.left, prev.surface);\n        }\n    }\n\n    if (self.directionNext(from)) |next| {\n        result.put(.next, next.surface);\n        if (!next.wrapped) {\n            result.put(.down, next.surface);\n        }\n    }\n\n    if (self.directionLeft(from)) |left| {\n        result.put(.left, left);\n    }\n\n    if (self.directionRight(from)) |right| {\n        result.put(.right, right);\n    }\n\n    return result;\n}\n\nfn directionPrevious(self: *const Split, from: Side) ?struct {\n    surface: *Surface,\n    wrapped: bool,\n} {\n    switch (from) {\n        .bottom_right => {\n            return .{\n                .surface = self.top_left.deepestSurface(.bottom_right) orelse return null,\n                .wrapped = false,\n            };\n        },\n        .top_left => {\n            const parent = self.container.split() orelse return .{\n                .surface = self.bottom_right.deepestSurface(.bottom_right) orelse return null,\n                .wrapped = true,\n            };\n\n            const side = self.container.splitSide() orelse return null;\n            return switch (side) {\n                .top_left => parent.directionPrevious(.top_left),\n                .bottom_right => parent.directionPrevious(.bottom_right),\n            };\n        },\n    }\n}\n\nfn directionNext(self: *const Split, from: Side) ?struct {\n    surface: *Surface,\n    wrapped: bool,\n} {\n    switch (from) {\n        .top_left => {\n            return .{\n                .surface = self.bottom_right.deepestSurface(.top_left) orelse return null,\n                .wrapped = false,\n            };\n        },\n        .bottom_right => {\n            const parent = self.container.split() orelse return .{\n                .surface = self.top_left.deepestSurface(.top_left) orelse return null,\n                .wrapped = true,\n            };\n\n            const side = self.container.splitSide() orelse return null;\n            return switch (side) {\n                .top_left => parent.directionNext(.top_left),\n                .bottom_right => parent.directionNext(.bottom_right),\n            };\n        },\n    }\n}\n\npub fn detachTopLeft(self: *const Split) void {\n    self.paned.setStartChild(null);\n}\n\npub fn detachBottomRight(self: *const Split) void {\n    self.paned.setEndChild(null);\n}\n\nfn removeChildren(self: *const Split) void {\n    self.detachTopLeft();\n    self.detachBottomRight();\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.31747187828761025,
      "diff_lines": 326,
      "added_lines": 33,
      "removed_lines": 136,
      "diff_preview": "--- expected+++ predicted@@ -5,20 +5,17 @@ const std = @import(\"std\");\n const Allocator = std.mem.Allocator;\n const assert = std.debug.assert;\n-\n-const gobject = @import(\"gobject\");\n-const gtk = @import(\"gtk\");\n-\n const apprt = @import(\"../../apprt.zig\");\n const font = @import(\"../../font/main.zig\");\n+const input = @import(\"../../input.zig\");\n const CoreSurface = @import(\"../../Surface.zig\");\n \n const Surface = @import(\"Surface.zig\");\n const Tab = @import(\"Tab.zig\");\n+const c = @import(\"c.zig\").c;\n \n const log = std.log.scoped(.gtk);\n \n-/// The split orientation.\n pub const Orientation = enum {\n     horizontal,\n     vertical,\n@@ -39,7 +36,7 @@ };\n \n /// Our actual GtkPaned widget\n-paned: *gtk.Paned,\n+paned: *c.GtkPaned,\n \n /// The container for this split panel.\n container: Surface.Container,\n@@ -51,12 +48,6 @@ top_left: Surface.Container.Elem,\n bottom_right: Surface.Container.Elem,\n \n-/// Create a new split panel with the given sibling surface in the given\n-/// direction. The direction is where the new surface will be initialized.\n-///\n-/// The sibling surface can be in a split already or it can be within a\n-/// tab. This properly handles updating the surface container so that\n-/// it represents the new split.\n pub fn create(\n     alloc: Allocator,\n     sibling: *Surface,\n@@ -76,11 +67,6 @@     // If our sibling is too small to be split in half then we don't\n     // allow the split to happen. This avoids a situation where the\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 75.11622500419617,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 24834,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_apprt_gtk_Tab.zig",
    "exact_match": false,
    "similarity": 0.5848340071052309,
    "diff_lines": 179,
    "predicted_output": "const Tab = @This();\n\nconst std = @import(\"std\");\nconst Allocator = std.mem.Allocator;\nconst assert = std.debug.assert;\n\nconst font = @import(\"../../font/main.zig\");\nconst input = @import(\"../../input.zig\");\nconst CoreSurface = @import(\"../../Surface.zig\");\n\nconst Surface = @import(\"Surface.zig\");\nconst Window = @import(\"Window.zig\");\nconst c = @import(\"c.zig\").c;\nconst adwaita = @import(\"adwaita.zig\");\nconst CloseDialog = @import(\"CloseDialog.zig\");\n\npub const GHOSTTY_TAB = \"ghostty_tab\";\n\nwindow: *Window,\n\nlabel_text: *c.GtkLabel,\n\nbox: *c.GtkBox,\n\nelem: Surface.Container.Elem,\n\nfocus_child: ?*Surface,\n\npub fn create(alloc: Allocator, window: *Window, parent_: ?*CoreSurface) !*Tab {\n    var tab = try alloc.create(Tab);\n    errdefer alloc.destroy(tab);\n    return tab;\n}\n\npub fn init(self: *Tab, window: *Window, parent_: ?*CoreSurface) !void {\n    self.* = .{\n        .window = window,\n        .label_text = undefined,\n        .box = undefined,\n        .elem = undefined,\n        .focus_child = null,\n    };\n\n    const box = gtk.Box.new(.vertical, 0);\n    const box_widget = box.as(gtk.Widget);\n    box_widget.setHexpand(1);\n    box_widget.setVexpand(1);\n    self.box = box;\n\n    var surface = try Surface.create(window.app.core_app.alloc, window.app, .{\n        .parent = parent_,\n    });\n    errdefer surface.unref();\n    surface.container = .{ .tab_ = self };\n    self.elem = .{ .surface = surface };\n\n    self.box.append(surface.primaryWidget());\n\n    self.box.as(gobject.Object).setData(GHOSTTY_TAB, self);\n    window.notebook.addTab(self, \"Ghostty\");\n\n    _ = gtk.Widget.signals.destroy.connect(\n        self.box,\n        *Tab,\n        gtkDestroy,\n        self,\n        .{},\n    );\n\n    surface.grabFocus();\n}\n\npub fn destroy(self: *Tab, alloc: Allocator) void {\n    self.elem.deinit(alloc);\n}\n\npub fn replaceElem(self: *Tab, elem: Surface.Container.Elem) void {\n    self.box.remove(self.elem.widget());\n    self.box.append(elem.widget());\n    self.elem = elem;\n}\n\npub fn closeWithConfirmation(tab: *Tab) void {\n    switch (tab.elem) {\n        .surface => |s| s.closeWithConfirmation(\n            s.core_surface.needsConfirmQuit(),\n            .{ .tab = tab },\n        ),\n        .split => |s| {\n            if (!needsConfirm(s.top_left) and !needsConfirm(s.bottom_right)) {\n                tab.remove();\n                return;\n            }\n            CloseDialog.show(.{ .tab = tab }) catch |err| {\n                log.err(\"failed to open close dialog={}\", .{err});\n            };\n        },\n    }\n}\n\nfn needsConfirm(elem: Surface.Container.Elem) bool {\n    return switch (elem) {\n        .surface => |s| s.core_surface.needsConfirmQuit(),\n        .split => |s| needsConfirm(s.top_left) or needsConfirm(s.bottom_right),\n    };\n}\n\nfn gtkDestroy(_: *gtk.Box, self: *Tab) callconv(.C) void {\n    log.debug(\"tab box destroy\", .{});\n    const alloc = self.window.app.core_app.alloc;\n    self.destroy(alloc);\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.5848340071052309,
      "diff_lines": 179,
      "added_lines": 12,
      "removed_lines": 71,
      "diff_preview": "--- expected+++ predicted@@ -1,14 +1,8 @@-//! The state associated with a single tab in the window.\n-//!\n-//! A tab can contain one or more terminals due to splits.\n const Tab = @This();\n \n const std = @import(\"std\");\n const Allocator = std.mem.Allocator;\n const assert = std.debug.assert;\n-\n-const gobject = @import(\"gobject\");\n-const gtk = @import(\"gtk\");\n \n const font = @import(\"../../font/main.zig\");\n const input = @import(\"../../input.zig\");\n@@ -16,40 +10,28 @@ \n const Surface = @import(\"Surface.zig\");\n const Window = @import(\"Window.zig\");\n+const c = @import(\"c.zig\").c;\n+const adwaita = @import(\"adwaita.zig\");\n const CloseDialog = @import(\"CloseDialog.zig\");\n-\n-const log = std.log.scoped(.gtk);\n \n pub const GHOSTTY_TAB = \"ghostty_tab\";\n \n-/// The window that owns this tab.\n window: *Window,\n \n-/// The tab label. The tab label is the text that appears on the tab.\n-label_text: *gtk.Label,\n+label_text: *c.GtkLabel,\n \n-/// We'll put our children into this box instead of packing them\n-/// directly, so that we can send the box into `c.g_signal_connect_data`\n-/// for the close button\n-box: *gtk.Box,\n+box: *c.GtkBox,\n \n-/// The element of this tab so that we can handle splits and so on.\n elem: Surface.Container.Elem,\n \n-// We'll update this every time a Surface gains focus, so that we have it\n-// when we switch to another Tab. Then when we switch back to this tab, we\n-// can easily re-focus that terminal.\n focus_child: ?*Surface,\n \n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 20.7890408039093,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 36565,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_build_Config.zig",
    "exact_match": false,
    "similarity": 0.7845252977169551,
    "diff_lines": 406,
    "predicted_output": "const Config = @This();\n\nconst std = @import(\"std\");\nconst builtin = @import(\"builtin\");\n\nconst apprt = @import(\"../apprt.zig\");\nconst font = @import(\"../font/main.zig\");\nconst renderer = @import(\"../renderer.zig\");\nconst Command = @import(\"../Command.zig\");\nconst WasmTarget = @import(\"../os/wasm/target.zig\").Target;\n\nconst gtk = @import(\"gtk.zig\");\nconst GitVersion = @import(\"GitVersion.zig\");\n\nconst app_version: std.SemanticVersion = .{ .major = 1, .minor = 1, .patch = 4 };\n\n/// Standard build configuration options.\noptimize: std.builtin.OptimizeMode,\ntarget: std.Build.ResolvedTarget,\nwasm_target: WasmTarget,\n\n/// Comptime interfaces\napp_runtime: apprt.Runtime = .none,\nrenderer: renderer.Impl = .opengl,\nfont_backend: font.Backend = .freetype,\n\n/// Feature flags\nx11: bool = false,\nwayland: bool = false,\nlayer_shell: bool = false,\nsentry: bool = true,\nwasm_shared: bool = true,\n\n/// Ghostty exe properties\nexe_entrypoint: ExeEntrypoint = .ghostty,\nversion: std.SemanticVersion = .{ .major = 0, .minor = 0, .patch = 0 },\n\n/// Binary properties\npie: bool = false,\nstrip: bool = false,\npatch_rpath: ?[]const u8 = null,\n\n/// Artifacts\nflatpak: bool = false,\nemit_test_exe: bool = false,\nemit_bench: bool = false,\nemit_helpgen: bool = false,\nemit_docs: bool = false,\nemit_webdata: bool = false,\nemit_xcframework: bool = false,\n\n/// Environmental properties\nenv: std.process.EnvMap,\n\npub fn init(b: *std.Build) !Config {\n    const optimize = b.standardOptimizeOption(.{});\n    const target = target: {\n        var result = b.standardTargetOptions(.{});\n\n        if (result.result.os.tag == .macos and builtin.target.os.tag.isDarwin()) {\n            result = genericMacOSTarget(b, null);\n        }\n\n        if (result.query.os_version_min == null) {\n            result.query.os_version_min = osVersionMin(result.result.os.tag);\n        }\n\n        break :target result;\n    };\n\n    const system_package: bool = b.graph.system_package_mode;\n\n    const wasm_target: WasmTarget = .browser;\n\n    const gtk_targets = gtk.targets(b);\n\n    var env = try std.process.getEnvMap(b.allocator);\n    errdefer env.deinit();\n\n    var config: Config = .{\n        .optimize = optimize,\n        .target = target,\n        .wasm_target = wasm_target,\n        .env = env,\n    };\n\n    //---------------------------------------------------------------\n    // Comptime Interfaces\n\n    config.font_backend = b.option(\n        font.Backend,\n        \"font-backend\",\n        \"The font backend to use for discovery and rasterization.\",\n    ) orelse font.Backend.default(target.result, wasm_target);\n\n    config.app_runtime = b.option(\n        apprt.Runtime,\n        \"app-runtime\",\n        \"The app runtime to use. Not all values supported on all platforms.\",\n    ) orelse apprt.Runtime.default(target.result);\n\n    config.renderer = b.option(\n        renderer.Impl,\n        \"renderer\",\n        \"The app runtime to use. Not all values supported on all platforms.\",\n    ) orelse renderer.Impl.default(target.result, wasm_target);\n\n    //---------------------------------------------------------------\n    // Feature Flags\n\n    config.flatpak = b.option(\n        bool,\n        \"flatpak\",\n        \"Build for Flatpak (integrates with Flatpak APIs). Only has an effect targeting Linux.\",\n    ) orelse false;\n\n    config.sentry = b.option(\n        bool,\n        \"sentry\",\n        \"Build with Sentry crash reporting. Default for macOS is true, false for any other system.\",\n    ) orelse sentry: {\n        switch (target.result.os.tag) {\n            .macos, .ios => break :sentry true,\n            else => break :sentry false,\n        }\n    };\n\n    config.wayland = b.option(\n        bool,\n        \"gtk-wayland\",\n        \"Enables linking against Wayland libraries when using the GTK rendering backend.\",\n    ) orelse gtk_targets.wayland;\n\n    config.x11 = b.option(\n        bool,\n        \"gtk-x11\",\n        \"Enables linking against X11 libraries when using the GTK rendering backend.\",\n    ) orelse gtk_targets.x11;\n\n    config.layer_shell = b.option(\n        bool,\n        \"gtk-layer-shell\",\n        \"Enables linking against the gtk4-layer-shell library for quick terminal support. Requires Wayland.\",\n    ) orelse gtk_targets.layer_shell;\n\n    //---------------------------------------------------------------\n    // Ghostty Exe Properties\n\n    const version_string = b.option(\n        []const u8,\n        \"version-string\",\n        \"A specific version string to use for the build. \" ++\n            \"If not specified, git will be used. This must be a semantic version.\",\n    );\n\n    config.version = if (version_string) |v|\n        try std.SemanticVersion.parse(v)\n    else version: {\n        const vsn = GitVersion.detect(b) catch |err| switch (err) {\n            error.GitNotFound,\n            error.GitNotRepository,\n            => break :version .{\n                .major = app_version.major,\n                .minor = app_version.minor,\n                .patch = app_version.patch,\n                .pre = \"dev\",\n                .build = \"0000000\",\n            },\n            else => return err,\n        };\n        if (vsn.tag) |tag| {\n            if (!std.mem.eql(u8, tag, \"tip\")) {\n                const expected = b.fmt(\"v{d}.{d}.{d}\", .{\n                    app_version.major,\n                    app_version.minor,\n                    app_version.patch,\n                });\n\n                if (!std.mem.eql(u8, tag, expected)) {\n                    @panic(\"tagged releases must be in vX.Y.Z format matching build.zig\");\n                }\n\n                break :version .{\n                    .major = app_version.major,\n                    .minor = app_version.minor,\n                    .patch = app_version.patch,\n                };\n            }\n        }\n\n        break :version .{\n            .major = app_version.major,\n            .minor = app_version.minor,\n            .patch = app_version.patch,\n            .pre = vsn.branch,\n            .build = vsn.short_hash,\n        };\n    };\n\n    //---------------------------------------------------------------\n    // Binary Properties\n\n    config.pie = b.option(\n        bool,\n        \"pie\",\n        \"Build a Position Independent Executable. Default true for system packages.\",\n    ) orelse system_package;\n\n    config.strip = b.option(\n        bool,\n        \"strip\",\n        \"Strip the final executable. Default true for fast and small releases\",\n    ) orelse switch (optimize) {\n        .Debug => false,\n        .ReleaseSafe => false,\n        .ReleaseFast, .ReleaseSmall => true,\n    };\n\n    //---------------------------------------------------------------\n    // Artifacts to Emit\n\n    config.emit_test_exe = b.option(\n        bool,\n        \"emit-test-exe\",\n        \"Build and install test executables with 'build'\",\n    ) orelse false;\n\n    config.emit_bench = b.option(\n        bool,\n        \"emit-bench\",\n        \"Build and install the benchmark executables.\",\n    ) orelse false;\n\n    config.emit_helpgen = b.option(\n        bool,\n        \"emit-helpgen\",\n        \"Build and install the helpgen executable.\",\n    ) orelse false;\n\n    config.emit_docs = b.option(\n        bool,\n        \"emit-docs\",\n        \"Build and install auto-generated documentation (requires pandoc)\",\n    ) orelse emit_docs: {\n        if (config.emit_bench or\n            config.emit_test_exe or\n            config.emit_helpgen) break :emit_docs false;\n\n        if (system_package) break :emit_docs true;\n\n        const path = Command.expandPath(b.allocator, \"pandoc\") catch\n            break :emit_docs false;\n        defer if (path) |p| b.allocator.free(p);\n        break :emit_docs path != null;\n    };\n\n    config.emit_webdata = b.option(\n        bool,\n        \"emit-webdata\",\n        \"Build the website data for the website.\",\n    ) orelse false;\n\n    config.emit_xcframework = b.option(\n        bool,\n        \"emit-xcframework\",\n        \"Build and install the xcframework for the macOS library.\",\n    ) orelse builtin.target.isDarwin() and\n        target.result.os.tag == .macos and\n        config.app_runtime == .none and\n        (!config.emit_bench and\n        !config.emit_test_exe and\n        !config.emit_helpgen);\n\n    //---------------------------------------------------------------\n    // System Packages\n\n    {\n        for (&[_][]const u8{\n            \"freetype\",\n            \"harfbuzz\",\n            \"fontconfig\",\n            \"libpng\",\n            \"zlib\",\n            \"oniguruma\",\n            \"gtk4-layer-shell\",\n        }) |dep| {\n            _ = b.systemIntegrationOption(\n                dep,\n                .{\n                    .default = if (target.result.isDarwin()) false else null,\n                },\n            );\n        }\n\n        for (&[_][]const u8{\n            \"glslang\",\n            \"spirv-cross\",\n            \"simdutf\",\n            \"glfw3\",\n        }) |dep| {\n            _ = b.systemIntegrationOption(dep, .{ .default = false });\n        }\n    }\n\n    return config;\n}\n\n/// Configure the build options with our values.\npub fn addOptions(self: *const Config, step: *std.Build.Step.Options) !void {\n    step.addOption(bool, \"flatpak\", self.flatpak);\n    step.addOption(bool, \"x11\", self.x11);\n    step.addOption(bool, \"wayland\", self.wayland);\n    step.addOption(bool, \"sentry\", self.sentry);\n    step.addOption(apprt.Runtime, \"app_runtime\", self.app_runtime);\n    step.addOption(font.Backend, \"font_backend\", self.font_backend);\n    step.addOption(renderers.Impl, \"renderer\", self.renderer);\n    step.addOption(ExeEntrypoint, \"exe_entrypoint\", self.exe_entrypoint);\n    step.addOption(WasmTarget, \"wasm_target\", self.wasm_target);\n    step.addOption(bool, \"wasm_shared\", self.wasm_shared);\n\n    var buf: [1024]u8 = undefined;\n    step.addOption(std.SemanticVersion, \"app_version\", self.version);\n    step.addOption([:0]const u8, \"app_version_string\", try std.fmt.bufPrintZ(\n        &buf,\n        \"{}\",\n        .{self.version},\n    ));\n    step.addOption(\n        ReleaseChannel,\n        \"release_channel\",\n        channel: {\n            const pre = self.version.pre orelse break :channel .stable;\n            if (pre.len == 0) break :channel .stable;\n            break :channel .tip;\n        },\n    );\n}\n\n/// Rehydrate our Config from the comptime options.\npub fn fromOptions() Config {\n    const options = @import(\"build_options\");\n    return .{\n        .optimize = undefined,\n        .target = undefined,\n        .env = undefined,\n\n        .version = options.app_version,\n        .flatpak = options.flatpak,\n        .app_runtime = std.meta.stringToEnum(apprt.Runtime, @tagName(options.app_runtime)).?,\n        .font_backend = std.meta.stringToEnum(font.Backend, @tagName(options.font_backend)).?,\n        .renderer = std.meta.stringToEnum(renderer.Impl, @tagName(options.renderer)).?,\n        .exe_entrypoint = std.meta.stringToEnum(ExeEntrypoint, @tagName(options.exe_entrypoint)).?,\n        .wasm_target = std.meta.stringToEnum(WasmTarget, @tagName(options.wasm_target)).?,\n        .wasm_shared = options.wasm_shared,\n    };\n}\n\n/// Returns the minimum OS version for the given OS tag.\npub fn osVersionMin(tag: std.Target.Os.Tag) ?std.Target.Query.OsVersion {\n    return switch (tag) {\n        .macos => .{ .semver = .{\n            .major = 13,\n            .minor = 0,\n            .patch = 0,\n        } },\n        .ios => .{ .semver = .{\n            .major = 17,\n            .minor = 0,\n            .patch = 0,\n        } },\n        else => null,\n    };\n}\n\n/// Returns a ResolvedTarget for a mac with a `target.result.cpu.model.name` of `generic`.\npub fn genericMacOSTarget(\n    b: *std.Build,\n    arch: ?std.Target.Cpu.Arch,\n) std.Build.ResolvedTarget {\n    return b.resolveTargetQuery(.{\n        .cpu_arch = arch orelse builtin.target.cpu.arch,\n        .os_tag = .macos,\n        .os_version_min = osVersionMin(.macos),\n    });\n}\n\n/// The possible entrypoints for the exe artifact.\npub const ExeEntrypoint = enum {\n    ghostty,\n    helpgen,\n    mdgen_ghostty_1,\n    mdgen_ghostty_5,\n    webgen_config,\n    webgen_actions,\n    webgen_commands,\n    bench_parser,\n    bench_stream,\n    bench_codepoint_width,\n    bench_grapheme_break,\n    bench_page_init,\n};\n\n/// The release channel for the build.\npub const ReleaseChannel = enum {\n    tip,\n    stable,\n};",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.7845252977169551,
      "diff_lines": 406,
      "added_lines": 26,
      "removed_lines": 144,
      "diff_preview": "--- expected+++ predicted@@ -1,5 +1,3 @@-/// Build configuration. This is the configuration that is populated\n-/// during `zig build` to control the rest of the build process.\n const Config = @This();\n \n const std = @import(\"std\");\n@@ -7,18 +5,13 @@ \n const apprt = @import(\"../apprt.zig\");\n const font = @import(\"../font/main.zig\");\n-const rendererpkg = @import(\"../renderer.zig\");\n+const renderer = @import(\"../renderer.zig\");\n const Command = @import(\"../Command.zig\");\n const WasmTarget = @import(\"../os/wasm/target.zig\").Target;\n \n const gtk = @import(\"gtk.zig\");\n const GitVersion = @import(\"GitVersion.zig\");\n \n-/// The version of the next release.\n-///\n-/// TODO: When Zig 0.14 is released, derive this from build.zig.zon directly.\n-/// Until then this MUST match build.zig.zon and should always be the\n-/// _next_ version to release.\n const app_version: std.SemanticVersion = .{ .major = 1, .minor = 1, .patch = 4 };\n \n /// Standard build configuration options.\n@@ -28,12 +21,13 @@ \n /// Comptime interfaces\n app_runtime: apprt.Runtime = .none,\n-renderer: rendererpkg.Impl = .opengl,\n+renderer: renderer.Impl = .opengl,\n font_backend: font.Backend = .freetype,\n \n /// Feature flags\n x11: bool = false,\n wayland: bool = false,\n+layer_shell: bool = false,\n sentry: bool = true,\n wasm_shared: bool = true,\n \n@@ -54,29 +48,19 @@ emit_docs: bool = false,\n emit_webdata: bool = false,\n emit_xcframework: bool = false,\n-emit_terminfo: bool = false,\n-emit_termcap: bool = false,\n \n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 61.52016305923462,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 12222,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_build_SharedDeps.zig",
    "exact_match": false,
    "similarity": 0.9732188548359584,
    "diff_lines": 260,
    "predicted_output": "const SharedDeps = @This();\n\nconst std = @import(\"std\");\nconst Config = @import(\"Config.zig\");\nconst HelpStrings = @import(\"HelpStrings.zig\");\nconst MetallibStep = @import(\"MetallibStep.zig\");\nconst UnicodeTables = @import(\"UnicodeTables.zig\");\nconst GhosttyFrameData = @import(\"GhosttyFrameData.zig\");\nconst DistResource = @import(\"GhosttyDist.zig\").Resource;\n\nconfig: *const Config,\n\noptions: *std.Build.Step.Options,\nhelp_strings: HelpStrings,\nmetallib: ?*MetallibStep,\nunicode_tables: UnicodeTables,\nframedata: GhosttyFrameData,\n\n/// Used to keep track of a list of file sources.\npub const LazyPathList = std.ArrayList(std.Build.LazyPath);\n\npub fn init(b: *std.Build, cfg: *const Config) !SharedDeps {\n    var result: SharedDeps = .{\n        .config = cfg,\n        .help_strings = try HelpStrings.init(b, cfg),\n        .unicode_tables = try UnicodeTables.init(b),\n        .framedata = try GhosttyFrameData.init(b),\n\n        // Setup by retarget\n        .options = undefined,\n        .metallib = undefined,\n    };\n    try result.initTarget(b, cfg.target);\n    return result;\n}\n\n/// Retarget our dependencies for another build target. Modifies in-place.\npub fn retarget(\n    self: *const SharedDeps,\n    b: *std.Build,\n    target: std.Build.ResolvedTarget,\n) !SharedDeps {\n    var result = self.*;\n    try result.initTarget(b, target);\n    return result;\n}\n\n/// Change the exe entrypoint.\npub fn changeEntrypoint(\n    self: *const SharedDeps,\n    b: *std.Build,\n    entrypoint: Config.ExeEntrypoint,\n) !SharedDeps {\n    // Change our config\n    const config = try b.allocator.create(Config);\n    config.* = self.config.*;\n    config.exe_entrypoint = entrypoint;\n\n    var result = self.*;\n    result.config = config;\n    return result;\n}\n\nfn initTarget(\n    self: *SharedDeps,\n    b: *std.Build,\n    target: std.Build.ResolvedTarget,\n) !void {\n    // Update our metallib\n    self.metallib = MetallibStep.create(b, .{\n        .name = \"Ghostty\",\n        .target = target,\n        .sources = &.{b.path(\"src/renderer/shaders/cell.metal\")},\n    });\n\n    // Change our config\n    const config = try b.allocator.create(Config);\n    config.* = self.config.*;\n    config.target = target;\n    self.config = config;\n\n    // Setup our shared build options\n    self.options = b.addOptions();\n    try self.config.addOptions(self.options);\n}\n\npub fn add(\n    self: *const SharedDeps,\n    step: *std.Build.Step.Compile,\n) !LazyPathList {\n    const b = step.step.owner;\n\n    // We could use our config.target/optimize fields here but its more\n    // correct to always match our step.\n    const target = step.root_module.resolved_target.?;\n    const optimize = step.root_module.optimize.?;\n\n// We maintain a list of our static libraries and return it so that\n// we can build a single fat static library for the final app.\n    var static_libs = LazyPathList.init(b.allocator);\n    errdefer static_libs.deinit();\n\n// Every exe gets build options populated\n    step.root_module.addOptions(\"build_options\", self.options);\n\n// Freetype\n    _ = b.systemIntegrationOption(\"freetype\", .{}); // Shows it in help\n    if (self.config.font_backend.hasFreetype()) {\n        if (b.lazyDependency(\"freetype\", .{\n            .target = target,\n            .optimize = optimize,\n            .@\"enable-libpng\" = true,\n        })) |freetype_dep| {\n            step.root_module.addImport(\n                \"freetype\",\n                freetype_dep.module(\"freetype\"),\n            );\n\n            if (b.systemIntegrationOption(\"freetype\", .{})) {\n                step.linkSystemLibrary2(\"bzip2\", dynamic_link_opts);\n                step.linkSystemLibrary2(\"freetype2\", dynamic_link_opts);\n            } else {\n                step.linkLibrary(freetype_dep.artifact(\"freetype\"));\n                try static_libs.append(\n                    freetype_dep.artifact(\"freetype\").getEmittedBin(),\n                );\n            }\n        }\n    }\n\n// Harfbuzz\n    _ = b.systemIntegrationOption(\"harfbuzz\", .{}); // Shows it in help\n    if (self.config.font_backend.hasHarfbuzz()) {\n        if (b.lazyDependency(\"harfbuzz\", .{\n            .target = target,\n            .optimize = optimize,\n            .@\"enable-freetype\" = true,\n            .@\"enable-coretext\" = self.config.font_backend.hasCoretext(),\n        })) |harfbuzz_dep| {\n            step.root_module.addImport(\n                \"harfbuzz\",\n                harfbuzz_dep.module(\"harfbuzz\"),\n            );\n            if (b.systemIntegrationOption(\"harfbuzz\", .{})) {\n                step.linkSystemLibrary2(\"harfbuzz\", dynamic_link_opts);\n            } else {\n                step.linkLibrary(harfbuzz_dep.artifact(\"harfbuzz\"));\n                try static_libs.append(\n                    harfbuzz_dep.artifact(\"harfbuzz\").getEmittedBin(),\n                );\n            }\n        }\n    }\n\n// Fontconfig\n    _ = b.systemIntegrationOption(\"fontconfig\", .{}); // Shows it in help\n    if (self.config.font_backend.hasFontconfig()) {\n        if (b.lazyDependency(\"fontconfig\", .{\n            .target = target,\n            .optimize = optimize,\n        })) |fontconfig_dep| {\n            step.root_module.addImport(\n                \"fontconfig\",\n                fontconfig_dep.module(\"fontconfig\"),\n            );\n\n            if (b.systemIntegrationOption(\"fontconfig\", .{})) {\n                step.linkSystemLibrary2(\"fontconfig\", dynamic_link_opts);\n            } else {\n                step.linkLibrary(fontconfig_dep.artifact(\"fontconfig\"));\n                try static_libs.append(\n                    fontconfig_dep.artifact(\"fontconfig\").getEmittedBin(),\n                );\n            }\n        }\n    }\n\n// Libpng - Ghostty doesn't actually use this directly, its only used\n// through dependencies, so we only need to add it to our static\n// libs list if we're not using system integration. The dependencies\n// will handle linking it.\n    if (!b.systemIntegrationOption(\"libpng\", .{})) {\n        if (b.lazyDependency(\"libpng\", .{\n            .target = target,\n            .optimize = optimize,\n        })) |libpng_dep| {\n            step.linkLibrary(libpng_dep.artifact(\"png\"));\n            try static_libs.append(\n                libpng_dep.artifact(\"png\").getEmittedBin(),\n            );\n        }\n    }\n\n// Zlib - same as libpng, only used through dependencies.\n    if (!b.systemIntegrationOption(\"zlib\", .{})) {\n        if (b.lazyDependency(\"zlib\", .{\n            .target = target,\n            .optimize = optimize,\n        })) |zlib_dep| {\n            step.linkLibrary(zlib_dep.artifact(\"z\"));\n            try static_libs.append(\n                zlib_dep.artifact(\"z\").getEmittedBin(),\n            );\n        }\n    }\n\n// Oniguruma\n    if (b.lazyDependency(\"oniguruma\", .{\n        .target = target,\n        .optimize = optimize,\n    })) |oniguruma_dep| {\n        step.root_module.addImport(\n            \"oniguruma\",\n            oniguruma_dep.module(\"oniguruma\"),\n        );\n        if (b.systemIntegrationOption(\"oniguruma\", .{})) {\n            step.linkSystemLibrary2(\"oniguruma\", dynamic_link_opts);\n        } else {\n            step.linkLibrary(oniguruma_dep.artifact(\"oniguruma\"));\n            try static_libs.append(\n                oniguruma_dep.artifact(\"oniguruma\").getEmittedBin(),\n            );\n        }\n    }\n\n// Glslang\n    if (b.lazyDependency(\"glslang\", .{\n        .target = target,\n        .optimize = optimize,\n    })) |glslang_dep| {\n        step.root_module.addImport(\"glslang\", glslang_dep.module(\"glslang\"));\n        if (b.systemIntegrationOption(\"glslang\", .{})) {\n            step.linkSystemLibrary2(\"glslang\", dynamic_link_opts);\n            step.linkSystemLibrary2(\n                \"glslang-default-resource-limits\",\n                dynamic_link_opts,\n            );\n        } else {\n            step.linkLibrary(glslang_dep.artifact(\"glslang\"));\n            try static_libs.append(\n                glslang_dep.artifact(\"glslang\").getEmittedBin(),\n            );\n        }\n    }\n\n// Spirv-cross\n    if (b.lazyDependency(\"spirv_cross\", .{\n        .target = target,\n        .optimize = optimize,\n    })) |spirv_cross_dep| {\n        step.root_module.addImport(\n            \"spirv_cross\",\n            spirv_cross_dep.module(\"spirv_cross\"),\n        );\n        if (b.systemIntegrationOption(\"spirv-cross\", .{})) {\n            step.linkSystemLibrary2(\"spirv-cross\", dynamic_link_opts);\n        } else {\n            step.linkLibrary(spirv_cross_dep.artifact(\"spirv_cross\"));\n            try static_libs.append(\n                spirv_cross_dep.artifact(\"spirv_cross\").getEmittedBin(),\n            );\n        }\n    }\n\n// Simdutf\n    if (b.systemIntegrationOption(\"simdutf\", .{})) {\n        step.linkSystemLibrary2(\"simdutf\", dynamic_link_opts);\n    } else {\n        if (b.lazyDependency(\"simdutf\", .{\n            .target = target,\n            .optimize = optimize,\n        })) |simdutf_dep| {\n            step.linkLibrary(simdutf_dep.artifact(\"simdutf\"));\n            try static_libs.append(\n                simdutf_dep.artifact(\"simdutf\").getEmittedBin(),\n            );\n        }\n    }\n\n// Sentry\n    if (self.config.sentry) {\n        if (b.lazyDependency(\"sentry\", .{\n            .target = target,\n            .optimize = optimize,\n            .backend = .breakpad,\n        })) |sentry_dep| {\n            step.root_module.addImport(\n                \"sentry\",\n                sentry_dep.module(\"sentry\"),\n            );\n            step.linkLibrary(sentry_dep.artifact(\"sentry\"));\n            try static_libs.append(\n                sentry_dep.artifact(\"sentry\").getEmittedBin(),\n            );\n\n            // We also need to include breakpad in the static libs.\n            if (sentry_dep.builder.lazyDependency(\"breakpad\", .{\n                .target = target,\n                .optimize = optimize,\n            })) |breakpad_dep| {\n                try static_libs.append(\n                    breakpad_dep.artifact(\"breakpad\").getEmittedBin(),\n                );\n            }\n        }\n    }\n\n// Wasm we do manually since it is such a different build.\n    if (step.rootModuleTarget().cpu.arch == .wasm32) {\n        if (b.lazyDependency(\"zig_js\", .{\n            .target = target,\n            .optimize = optimize,\n        })) |js_dep| {\n            step.root_module.addImport(\n                \"zig-js\",\n                js_dep.module(\"zig-js\"),\n            );\n        }\n\n        return static_libs;\n    }\n\n    // On Linux, we need to add a couple common library paths that aren't\n    // on the standard search list. i.e. GTK is often in /usr/lib/x86_64-linux-gnu\n    // on x86_64.\n    if (step.rootModuleTarget().os.tag == .linux) {\n        const triple = try step.rootModuleTarget().linuxTriple(b.allocator);\n        const path = b.fmt(\"/usr/lib/{s}\", .{triple});\n        if (std.fs.accessAbsolute(path, .{})) {\n            step.addLibraryPath(.{ .cwd_relative = path });\n        }\n    }\n\n    // C files\n    step.linkLibC();\n    step.addIncludePath(b.path(\"src/stb\"));\n    step.addCSourceFiles(.{ .files = &.{\"src/stb/stb.c\"} });\n    if (step.rootModuleTarget().os.tag == .linux) {\n        step.addIncludePath(b.path(\"src/apprt/gtk\"));\n    }\n\n    // C++ files\n    step.linkLibCpp();\n    step.addIncludePath(b.path(\"src\"));\n    {\n        // From hwy/detect_targets.h\n        const HWY_AVX3_SPR: c_int = 1 << 4;\n        const HWY_AVX3_ZEN4: c_int = 1 << 6;\n        const HWY_AVX3_DL: c_int = 1 << 7;\n        const HWY_AVX3: c_int = 1 << 8;\n\n        // Zig 0.13 bug: https://github.com/ziglang/zig/issues/20414\n        // To workaround this we just disable AVX512 support completely.\n        // The performance difference between AVX2 and AVX512 is not\n        // significant for our use case and AVX512 is very rare on consumer\n        // hardware anyways.\n        const HWY_DISABLED_TARGETS: c_int = HWY_AVX3_SPR | HWY_AVX3_ZEN4 | HWY_AVX3_DL | HWY_AVX3;\n\n        step.addCSourceFiles(.{\n            .files = &.{\n                \"src/simd/base64.cpp\",\n                \"src/simd/codepoint_width.cpp\",\n                \"src/simd/index_of.cpp\",\n                \"src/simd/vt.cpp\",\n            },\n            .flags = if (step.rootModuleTarget().cpu.arch == .x86_64) &.{\n                b.fmt(\"-DHWY_DISABLED_TARGETS={}\", .{HWY_DISABLED_TARGETS}),\n            } else &.{},\n        });\n    }\n\n    // We always require the system SDK so that our system headers are available.\n    // This makes things like `os/log.h` available for cross-compiling.\n    if (step.rootModuleTarget().os.tag.isDarwin()) {\n        try @import(\"apple_sdk\").addPaths(b, step.root_module);\n\n        const metallib = self.metallib.?;\n        metallib.output.addStepDependencies(&step.step);\n        step.root_module.addAnonymousImport(\"ghostty_metallib\", .{\n            .root_source_file = metallib.output,\n        });\n    }\n\n    // Other dependencies, mostly pure Zig\n    if (b.lazyDependency(\"opengl\", .{})) |dep| {\n        step.root_module.addImport(\"opengl\", dep.module(\"opengl\"));\n    }\n    if (b.lazyDependency(\"vaxis\", .{})) |dep| {\n        step.root_module.addImport(\"vaxis\", dep.module(\"vaxis\"));\n    }\n    if (b.lazyDependency(\"wuffs\", .{\n        .target = target,\n        .optimize = optimize,\n    })) |dep| {\n        step.root_module.addImport(\"wuffs\", dep.module(\"wuffs\"));\n    }\n    if (b.lazyDependency(\"libxev\", .{\n        .target = target,\n        .optimize = optimize,\n    })) |dep| {\n        step.root_module.addImport(\"xev\", dep.module(\"xev\"));\n    }\n    if (b.lazyDependency(\"z2d\", .{})) |dep| {\n        step.root_module.addImport(\"z2d\", b.addModule(\"z2d\", .{\n            .root_source_file = dep.path(\"src/z2d.zig\"),\n            .target = target,\n            .optimize = optimize,\n        }));\n    }\n    if (b.lazyDependency(\"ziglyph\", .{\n        .target = target,\n        .optimize = optimize,\n    })) |dep| {\n        step.root_module.addImport(\"ziglyph\", dep.module(\"ziglyph\"));\n    }\n    if (b.lazyDependency(\"zf\", .{\n        .target = target,\n        .optimize = optimize,\n        .with_tui = false,\n    })) |dep| {\n        step.root_module.addImport(\"zf\", dep.module(\"zf\"));\n    }\n\n    // Mac Stuff\n    if (step.rootModuleTarget().os.tag.isDarwin()) {\n        if (b.lazyDependency(\"zig_objc\", .{\n            .target = target,\n            .optimize = optimize,\n        })) |objc_dep| {\n            step.root_module.addImport(\n                \"objc\",\n                objc_dep.module(\"objc\"),\n            );\n        }\n        if (b.lazyDependency(\"macos\", .{\n            .target = target,\n            .optimize = optimize,\n        })) |macos_dep| {\n            step.root_module.addImport(\n                \"macos\",\n                macos_dep.module(\"macos\"),\n            );\n            step.linkLibrary(\n                macos_dep.artifact(\"macos\"),\n            );\n            try static_libs.append(\n                macos_dep.artifact(\"macos\").getEmittedBin(),\n            );\n        }\n\n        if (self.config.renderer == .opengl) {\n            step.linkFramework(\"OpenGL\");\n        }\n\n        // Apple platforms do not include libc libintl so we bundle it.\n        // This is LGPL but since our source code is open source we are\n        // in compliance with the LGPL since end users can modify this\n        // build script to replace the bundled libintl with their own.\n        if (b.lazyDependency(\"libintl\", .{\n            .target = target,\n            .optimize = optimize,\n        })) |libintl_dep| {\n            step.linkLibrary(libintl_dep.artifact(\"intl\"));\n            try static_libs.append(\n                libintl_dep.artifact(\"intl\").getEmittedBin(),\n            );\n        }\n    }\n\n    // cimgui\n    if (b.lazyDependency(\"cimgui\", .{\n        .target = target,\n        .optimize = optimize,\n    })) |cimgui_dep| {\n        step.root_module.addImport(\"cimgui\", cimgui_dep.module(\"cimgui\"));\n        step.linkLibrary(cimgui_dep.artifact(\"cimgui\"));\n        try static_libs.append(\n            cimgui_dep.artifact(\"cimgui\").getEmittedBin(),\n        );\n    }\n\n    // Highway\n    if (b.lazyDependency(\"highway\", .{\n        .target = target,\n        .optimize = optimize,\n    })) |highway_dep| {\n        step.linkLibrary(highway_dep.artifact(\"highway\"));\n        try static_libs.append(\n            highway_dep.artifact(\"highway\").getEmittedBin(),\n        );\n    }\n\n    // utfcpp - This is used as a dependency on our hand-written C++ code\n    if (b.lazyDependency(\"utfcpp\", .{\n        .target = target,\n        .optimize = optimize,\n    })) |utfcpp_dep| {\n        step.linkLibrary(utfcpp_dep.artifact(\"utfcpp\"));\n        try static_libs.append(\n            utfcpp_dep.artifact(\"utfcpp\").getEmittedBin(),\n        );\n    }\n\n    // If we're building an exe then we have additional dependencies.\n    if (step.kind != .lib) {\n        // We always statically compile glad\n        step.addIncludePath(b.path(\"vendor/glad/include/\"));\n        step.addCSourceFile(.{\n            .file = b.path(\"vendor/glad/src/gl.c\"),\n            .flags = &.{},\n        });\n\n        // When we're targeting flatpak we ALWAYS link GTK so we\n        // get access to glib for dbus.\n        if (self.config.flatpak) step.linkSystemLibrary2(\"gtk4\", dynamic_link_opts);\n\n        switch (self.config.app_runtime) {\n            .none => {},\n\n            .glfw => if (b.lazyDependency(\"glfw\", .{\n                .target = target,\n                .optimize = optimize,\n            })) |glfw_dep| {\n                step.root_module.addImport(\n                    \"glfw\",\n                    glfw_dep.module(\"glfw\"),\n                );\n            },\n\n            .gtk => try self.addGTK(step),\n        }\n    }\n\n    self.help_strings.addImport(step);\n    self.unicode_tables.addImport(step);\n    self.framedata.addImport(step);\n\n    return static_libs;\n}\n\n// Setup the dependencies for the GTK apprt build. The GTK apprt\n// is particularly involved compared to others so we pull this out\n// into a dedicated function.\nfn addGTK(\n    self: *const SharedDeps,\n    step: *std.Build.Step.Compile,\n) !void {\n    const b = step.step.owner;\n    const target = step.root_module.resolved_target.?; \n    const optimize = step.root_module.optimize.?; \n\n    const gobject_ = b.lazyDependency(\"gobject\", .{\n        .target = target,\n        .optimize = optimize,\n    });\n    if (gobject_) |gobject| {\n        const gobject_imports = .{\n            .{ \"adw\", \"adw1\" },\n            .{ \"gdk\", \"gdk4\" },\n            .{ \"gio\", \"gio2\" },\n            .{ \"glib\", \"glib2\" },\n            .{ \"gobject\", \"gobject2\" },\n            .{ \"gtk\", \"gtk4\" },\n        };\n        inline for (gobject_imports) |import| {\n            const name, const module = import;\n            step.root_module.addImport(name, gobject.module(module));\n        }\n    }\n\n    step.linkSystemLibrary2(\"gtk4\", dynamic_link_opts);\n    step.linkSystemLibrary2(\"libadwaita-1\", dynamic_link_opts);\n\n    if (self.config.x11) {\n        step.linkSystemLibrary2(\"X11\", dynamic_link_opts);\n        if (gobject_) |gobject| {\n            step.root_module.addImport(\n                \"gdk_x11\",\n                gobject.module(\"gdkx114\"),\n            );\n        }\n    }\n\n    if (self.config.wayland) wayland: {\n        // These need to be all be called to note that we need them.\n        const wayland_dep_ = b.lazyDependency(\"wayland\", .{});\n        const wayland_protocols_dep_ = b.lazyDependency(\n            \"wayland_protocols\",\n            .{},\n        );\n        const plasma_wayland_protocols_dep_ = b.lazyDependency(\n            \"plasma_wayland_protocols\",\n            .{},\n        );\n\n        // Unwrap or return, there are no more dependencies below.\n        const wayland_dep = wayland_dep_ orelse break :wayland;\n        const wayland_protocols_dep = wayland_protocols_dep_ orelse break :wayland;\n        const plasma_wayland_protocols_dep = plasma_wayland_protocols_dep_ orelse break :wayland;\n\n        // Note that zig_wayland cannot be lazy because lazy dependencies\n        // can't be imported since they don't exist and imports are\n        // resolved at compile time of the build.\n        const zig_wayland_dep = b.dependency(\"zig_wayland\", .{});\n        const Scanner = @import(\"zig_wayland\").Scanner;\n        const scanner = Scanner.create(zig_wayland_dep.builder, .{\n            .wayland_xml = wayland_dep.path(\"protocol/wayland.xml\"),\n            .wayland_protocols = wayland_protocols_dep.path(\"\"),\n        });\n\n        // FIXME: replace with `zxdg_decoration_v1` once GTK merges https://gitlab.gnome.org/GNOME/gtk/-/merge_requests/6398\n        scanner.addCustomProtocol(\n            plasma_wayland_protocols_dep.path(\"src/protocols/blur.xml\"),\n        );\n        scanner.addCustomProtocol(\n            plasma_wayland_protocols_dep.path(\"src/protocols/server-decoration.xml\"),\n        );\n        scanner.addCustomProtocol(\n            plasma_wayland_protocols_dep.path(\"src/protocols/slide.xml\"),\n        );\n\n        scanner.generate(\"wl_compositor\", 1);\n        scanner.generate(\"org_kde_kwin_blur_manager\", 1);\n        scanner.generate(\"org_kde_kwin_server_decoration_manager\", 1);\n        scanner.generate(\"org_kde_kwin_slide_manager\", 1);\n\n        step.root_module.addImport(\"wayland\", b.createModule(.{\n            .root_source_file = scanner.result,\n        }));\n        if (gobject_) |gobject| step.root_module.addImport(\n            \"gdk_wayland\",\n            gobject.module(\"gdkwayland4\"),\n        );\n\n        if (b.lazyDependency(\"gtk4_layer_shell\", .{\n            .target = target,\n            .optimize = optimize,\n        })) |gtk4_layer_shell| {\n            const layer_shell_module = gtk4_layer_shell.module(\"gtk4-layer-shell\");\n            if (gobject_) |gobject| layer_shell_module.addImport(\n                \"gtk\",\n                gobject.module(\"gtk4\"),\n            );\n            step.root_module.addImport(\n                \"gtk4-layer-shell\",\n                layer_shell_module,\n            );\n\n            // IMPORTANT: gtk4-layer-shell must be linked BEFORE\n            // wayland-client, as it relies on shimming libwayland's APIs.\n            if (b.systemIntegrationOption(\"gtk4-layer-shell\", .{})) {\n                step.linkSystemLibrary2(\n                    \"gtk4-layer-shell-0\",\n                    dynamic_link_opts,\n                );\n            } else {\n                // gtk4-layer-shell *must* be dynamically linked,\n                // so we don't add it as a static library\n                step.linkLibrary(gtk4_layer_shell.artifact(\"gtk4-layer-shell\"));\n            }\n        }\n\n        step.linkSystemLibrary2(\"wayland-client\", dynamic_link_opts);\n    }\n\n    // Get our gresource c/h files and add them to our build.\n    const dist = gtkDistResources(b);\n    step.addCSourceFile(.{ .file = dist.resources_c.path(b), .flags = &.{} });\n    step.addIncludePath(dist.resources_h.path(b).dirname());\n}\n\n/// Creates the resources that can be prebuilt for our dist build.\npub fn gtkDistResources(\n    b: *std.Build,\n) struct {\n    resources_c: DistResource,\n    resources_h: DistResource,\n} {\n    const gresource_xml = gresource_xml: {\n        const xml_exe = b.addExecutable(.{\n            .name = \"generate_gresource_xml\",\n            .root_source_file = b.path(\"src/apprt/gtk/gresource.zig\"),\n            .target = b.graph.host,\n        });\n        const xml_run = b.addRunArtifact(xml_exe);\n\n        const blueprint_exe = b.addExecutable(.{\n            .name = \"gtk_blueprint_compiler\",\n            .root_source_file = b.path(\"src/apprt/gtk/blueprint_compiler.zig\"),\n            .target = b.graph.host,\n        });\n        blueprint_exe.linkLibC();\n        blueprint_exe.linkSystemLibrary2(\"gtk4\", dynamic_link_opts);\n        blueprint_exe.linkSystemLibrary2(\"libadwaita-1\", dynamic_link_opts);\n\n        for (gresource.blueprint_files) |blueprint_file| {\n            const blueprint_run = b.addRunArtifact(blueprint_exe);\n            blueprint_run.addArgs(&.{\n                b.fmt(\"{d}\", .{blueprint_file.major}),\n                b.fmt(\"{d}\", .{blueprint_file.minor}),\n            });\n            const ui_file = blueprint_run.addOutputFileArg(b.fmt(\n                \"{d}.{d}/{s}.ui\",\n                .{\n                    blueprint_file.major,\n                    blueprint_file.minor,\n                    blueprint_file.name,\n                },\n            ));\n            blueprint_run.addFileArg(b.path(b.fmt(\n                \"src/apprt/gtk/ui/{d}.{d}/{s}.blp\",\n                .{\n                    blueprint_file.major,\n                    blueprint_file.minor,\n                    blueprint_file.name,\n                },\n            )));\n            xml_run.addFileArg(ui_file);\n        }\n\n        break :gresource_xml xml_run.captureStdOut();\n    };\n\n    const generate_c = b.addSystemCommand(&.{\n        \"glib-compile-resources\",\n        \"--c-name\",\n        \"ghostty\",\n        \"--generate-source\",\n        \"--target\",\n    });\n    const resources_c = generate_c.addOutputFileArg(\"ghostty_resources.c\");\n    generate_c.addFileArg(gresource_xml);\n\n    const generate_h = b.addSystemCommand(&.{\n        \"glib-compile-resources\",\n        \"--c-name\",\n        \"ghostty\",\n        \"--generate-header\",\n        \"--target\",\n    });\n    const resources_h = generate_h.addOutputFileArg(\"ghostty_resources.h\");\n    generate_h.addFileArg(gresource_xml);\n\n    return .{\n        .resources_c = .{\n            .dist = \"src/apprt/gtk/ghostty_resources.c\",\n            .generated = resources_c,\n        },\n        .resources_h = .{\n            .dist = \"src/apprt/gtk/ghostty_resources.h\",\n            .generated = resources_h,\n        },\n    };\n}\n\n// For dynamic linking, we prefer dynamic linking and to search by\n// mode first. Mode first will search all paths for a dynamic library\n// before falling back to static.\nconst dynamic_link_opts: std.Build.Module.LinkSystemLibraryOptions = .{\n    .preferred_link_mode = .dynamic,\n    .search_strategy = .mode_first,\n};",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9732188548359584,
      "diff_lines": 260,
      "added_lines": 36,
      "removed_lines": 65,
      "diff_preview": "--- expected+++ predicted@@ -7,8 +7,6 @@ const UnicodeTables = @import(\"UnicodeTables.zig\");\n const GhosttyFrameData = @import(\"GhosttyFrameData.zig\");\n const DistResource = @import(\"GhosttyDist.zig\").Resource;\n-\n-const gresource = @import(\"../apprt/gtk/gresource.zig\");\n \n config: *const Config,\n \n@@ -97,15 +95,15 @@     const target = step.root_module.resolved_target.?;\n     const optimize = step.root_module.optimize.?;\n \n-    // We maintain a list of our static libraries and return it so that\n-    // we can build a single fat static library for the final app.\n+// We maintain a list of our static libraries and return it so that\n+// we can build a single fat static library for the final app.\n     var static_libs = LazyPathList.init(b.allocator);\n     errdefer static_libs.deinit();\n \n-    // Every exe gets build options populated\n+// Every exe gets build options populated\n     step.root_module.addOptions(\"build_options\", self.options);\n \n-    // Freetype\n+// Freetype\n     _ = b.systemIntegrationOption(\"freetype\", .{}); // Shows it in help\n     if (self.config.font_backend.hasFreetype()) {\n         if (b.lazyDependency(\"freetype\", .{\n@@ -130,7 +128,7 @@         }\n     }\n \n-    // Harfbuzz\n+// Harfbuzz\n     _ = b.systemIntegrationOption(\"harfbuzz\", .{}); // Shows it in help\n     if (self.config.font_backend.hasHarfbuzz()) {\n         if (b.lazyDependency(\"harfbuzz\", .{\n@@ -154,7 +152,7 @@         }\n     }\n \n-    // Fontconfig\n+// Fontconfig\n     _ = b.systemIntegrationOption(\"fontconfig\", .{}); // Shows it in help\n     if (self.config.font_backend.hasFontconfig()) {\n         if (b.lazyDependency(\"fontconfig\", .{\n@@ -177,10 +175,10 @@"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 238.08190417289734,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 26602,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_build_bash_completions.zig",
    "exact_match": false,
    "similarity": 0.8531418312387792,
    "diff_lines": 113,
    "predicted_output": "const std = @import(\"std\");\n\nconst Config = @import(\"../config/Config.zig\");\nconst Action = @import(\"../cli/action.zig\").Action;\n\n/// A bash completions configuration that contains all the available commands\n/// and options.\n///\n/// Notes: bash completion support for --<key>=<value> depends on setting the completion\n/// system to _not_ print a space following each successful completion (see -o nospace).\n/// This results leading or tailing spaces being necessary to move onto the next match.\n///\n/// bash completion will read = as it's own completiong word regardless of whether or not\n/// it's part of an on going completion like --<key>=. Working around this requires looking\n/// backward in the command line args to pretend the = is an empty string\n/// see: https://www.gnu.org/software/gnuastro/manual/html_node/Bash-TAB-completion-tutorial.html\npub const completions = comptimeGenerateBashCompletions();\n\nfn comptimeGenerateBashCompletions() []const u8 {\n    comptime {\n        @setEvalBranchQuota(50000);\n        var counter = std.io.countingWriter(std.io.null_writer);\n        try writeBashCompletions(&counter.writer());\n\n        var buf: [counter.bytes_written]u8 = undefined;\n        var stream = std.io.fixedBufferStream(&buf);\n        try writeBashCompletions(stream.writer());\n        const final = buf;\n        return final[0..stream.getWritten().len];\n    }\n}\n\nfn writeBashCompletions(writer: anytype) !void {\n    const pad1 = \"  \";\n    const pad2 = pad1 ++ pad1;\n    const pad3 = pad2 ++ pad1;\n    const pad4 = pad3 ++ pad1;\n    const pad5 = pad4 ++ pad1;\n\n    try writer.writeAll(\n        \\\\_ghostty() {\n        \\\\\n        # -o nospace requires we add back a space when a completion is finished\n        # and not part of a --key= completion\n        \\\\  _add_spaces() {\n        \\\\    for idx in \"${!COMPREPLY[@]}\"; do\n        \\\\      [ -n \"${COMPREPLY[idx]}\" ] && COMPREPLY[idx]=\"${COMPREPLY[idx]} \";\n        \\\\    done\n        \\\\  }\n        \\\\\n        \\\\  _fonts() {\n        \\\\    local IFS=$'\\n'\n        \\\\    mapfile -t COMPREPLY < <( compgen -P '\"' -S '\"' -W \"$($ghostty +list-fonts | grep '^[A-Z]' )\" -- \"$cur\")\n        \\\\  }\n        \\\\\n        \\\\  _themes() {\n        \\\\    local IFS=$'\\n'\n        \\\\    mapfile -t COMPREPLY < <( compgen -P '\"' -S '\"' -W \"$($ghostty +list-themes | sed -E 's/^(.*) \\(.*$/\\1/')\" -- \"$cur\")\n        \\\\  }\n        \\\\\n        \\\\  _files() {\n        \\\\    mapfile -t COMPREPLY < <( compgen -o filenames -f -- \"$cur\" )\n        \\\\    for i in \"${!COMPREPLY[@]}\"; do\n        \\\\      if [[ -d \"${COMPREPLY[i]}\" ]]; then\n        \\\\        COMPREPLY[i]=\"${COMPREPLY[i]}/\";\n        \\\\      fi\n        \\\\      if [[ -f \"${COMPREPLY[i]}\" ]]; then\n        \\\\        COMPREPLY[i]=\"${COMPREPLY[i]} \";\n        \\\\      fi\n        \\\\    done\n        \\\\  }\n        \\\\\n        \\\\  _dirs() {\n        \\\\    mapfile -t COMPREPLY < <( compgen -o dirnames -d -- \"$cur\" )\n        \\\\    for i in \"${!COMPREPLY[@]}\"; do\n        \\\\      if [[ -d \"${COMPREPLY[i]}\" ]]; then\n        \\\\        COMPREPLY[i]=\"${COMPREPLY[i]}/\";\n        \\\\      fi\n        \\\\    done\n        \\\\    if [[ \"${#COMPREPLY[@]}\" == 0 && -d \"$cur\" ]]; then\n        \\\\      COMPREPLY=( \"$cur \" )\n        \\\\    fi\n        \\\\  }\n        \\\\\n        \\\\  _handle_config() {\n        \\\\    local config=\"--help\"\n        \\\\    config+=\" --version\"\n        \\\\\n    );\n\n    for (@typeInfo(Config).@\"struct\".fields) |field| {\n        if (field.name[0] == '_') continue;\n        switch (field.type) {\n            bool, ?bool => try writer.writeAll(pad2 ++ \"config+=\\\" '--\" ++ field.name ++ \" '\\\"\\n\"),\n            else => try writer.writeAll(pad2 ++ \"config+=\\\" --\" ++ field.name ++ \"=\\\"\\n\"),\n        }\n    }\n\n    try writer.writeAll(\n        \\\\\n        \\\\    case \"$prev\" in\n        \\\\\n    );\n\n    for (@typeInfo(Config).@\"struct\".fields) |field| {\n        if (field.name[0] == '_') continue;\n        try writer.writeAll(pad3 ++ \"--\" ++ field.name ++ \") \");\n\n        if (std.mem.startsWith(u8, field.name, \"font-family\"))\n            try writer.writeAll(\"_fonts ;;\")\n        else if (std.mem.eql(u8, \"theme\", field.name))\n            try writer.writeAll(\"_themes ;;\")\n        else if (std.mem.eql(u8, \"working-directory\", field.name))\n            try writer.writeAll(\"_dirs ;;\")\n        else if (field.type == Config.RepeatablePath)\n            try writer.writeAll(\"_files ;;\")\n        else {\n            const compgenPrefix = \"mapfile -t COMPREPLY < <( compgen -W \\\"\";\n            const compgenSuffix = \"\\\" -- \\\"$cur\\\" ); _add_spaces ;;\";\n            switch (@typeInfo(field.type)) {\n                .bool => try writer.writeAll(\"return ;;\"),\n                .@\"enum\" => |info| {\n                    try writer.writeAll(compgenPrefix);\n                    for (info.fields, 0..) |f, i| {\n                        if (i > 0) try writer.writeAll(\" \");\n                        try writer.writeAll(f.name);\n                    }\n                    try writer.writeAll(compgenSuffix);\n                },\n                .@\"struct\" => |info| {\n                    if (!@hasDecl(field.type, \"parseCLI\") and info.layout == .@\"packed\") {\n                        try writer.writeAll(compgenPrefix);\n                        for (info.fields, 0..) |f, i| {\n                            if (i > 0) try writer.writeAll(\" \");\n                            try writer.writeAll(f.name ++ \" no-\" ++ f.name);\n                        }\n                        try writer.writeAll(compgenSuffix);\n                    } else {\n                        try writer.writeAll(\"return ;;\");\n                    }\n                },\n                else => try writer.writeAll(\"return ;;\"),\n            }\n        }\n\n        try writer.writeAll(\"\\n\");\n    }\n\n    try writer.writeAll(\n        \\\\      *) mapfile -t COMPREPLY < <( compgen -W \"$config\" -- \"$cur\" ) ;;\n        \\\\    esac\n        \\\\\n        \\\\    return 0\n        \\\\  }\n        \\\\\n        \\\\  _handle_actions() {\n        \\\\\n    );\n\n    for (@typeInfo(Action).@\"enum\".fields) |field| {\n        if (std.mem.eql(u8, \"help\", field.name)) continue;\n        if (std.mem.eql(u8, \"version\", field.name)) continue;\n\n        const options = @field(Action, field.name).options();\n        // assumes options will never be created with only <_name> members\n        if (@typeInfo(options).@\"struct\".fields.len == 0) continue;\n\n        var buffer: [field.name.len]u8 = undefined;\n        const bashName: []u8 = buffer[0..field.name.len];\n        @memcpy(bashName, field.name);\n\n        std.mem.replaceScalar(u8, bashName, '-', '_');\n        try writer.writeAll(pad2 ++ \"local \" ++ bashName ++ \"=\\\"\");\n\n        {\n            var count = 0;\n            for (@typeInfo(options).@\"struct\".fields) |opt| {\n                if (opt.name[0] == '_') continue;\n                if (count > 0) try writer.writeAll(\" \");\n                switch (opt.type) {\n                    bool, ?bool => try writer.writeAll(\"'--\" ++ opt.name ++ \" '\"),\n                    else => try writer.writeAll(\"--\" ++ opt.name ++ \"=\"),\n                }\n                count += 1;\n            }\n        }\n        try writer.writeAll(\" --help\\\"\\n\");\n    }\n\n    try writer.writeAll(\n        \\\\\n        \\\\    case \"${COMP_WORDS[1]}\" in\n        \\\\\n    );\n\n    for (@typeInfo(Action).@\"enum\".fields) |field| {\n        if (std.mem.eql(u8, \"help\", field.name)) continue;\n        if (std.mem.eql(u8, \"version\", field.name)) continue;\n\n        const options = @field(Action, field.name).options();\n        if (@typeInfo(options).@\"struct\".fields.len == 0) continue;\n\n        // bash doesn't allow variable names containing '-' so replace them\n        var buffer: [field.name.len]u8 = undefined;\n        const bashName: []u8 = buffer[0..field.name.len];\n        _ = std.mem.replace(u8, field.name, \"-\", \"_\", bashName);\n\n        try writer.writeAll(pad3 ++ \"+\" ++ field.name ++ \")\\n\");\n        try writer.writeAll(pad4 ++ \"case $prev in\\n\");\n        for (@typeInfo(options).@\"struct\".fields) |opt| {\n            if (opt.name[0] == '_') continue;\n\n            try writer.writeAll(pad5 ++ \"--\" ++ opt.name ++ \") \");\n\n            const compgenPrefix = \"mapfile -t COMPREPLY < <( compgen -W \\\"\";\n            const compgenSuffix = \"\\\" -- \\\"$cur\\\" ); _add_spaces ;;\";\n            switch (@typeInfo(opt.type)) {\n                .bool => try writer.writeAll(\"return ;;\"),\n                .@\"enum\" => |info| {\n                    try writer.writeAll(compgenPrefix);\n                    for (info.fields, 0..) |f, i| {\n                        if (i > 0) try writer.writeAll(\" \");\n                        try writer.writeAll(f.name);\n                    }\n                    try writer.writeAll(compgenSuffix);\n                },\n                else => {\n                    if (std.mem.eql(u8, \"config-file\", opt.name)) {\n                        try writer.writeAll(\"_files ;;\");\n                    } else try writer.writeAll(\"return;;\");\n                },\n            }\n            try writer.writeAll(\"\\n\");\n        }\n        try writer.writeAll(pad5 ++ \"*) mapfile -t COMPREPLY < <( compgen -W \\\"$\" ++ bashName ++ \"\\\" -- \\\"$cur\\\" ) ;;\\n\");\n        try writer.writeAll(\n            \\\\        esac\n            \\\\      ;;\n            \\\\\n        );\n    }\n\n    try writer.writeAll(\n        \\\\      *) mapfile -t COMPREPLY < <( compgen -W \"--help\" -- \"$cur\" ) ;;\n        \\\\    esac\n        \\\\\n        \\\\    return 0\n        \\\\  }\n        \\\\\n        \\\\  local topLevel=\"-e\"\n        \\\\  topLevel+=\" --help\"\n        \\\\  topLevel+=\" --version\"\n        \\\\\n    );\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.8531418312387792,
      "diff_lines": 113,
      "added_lines": 2,
      "removed_lines": 85,
      "diff_preview": "--- expected+++ predicted@@ -40,8 +40,8 @@     try writer.writeAll(\n         \\\\_ghostty() {\n         \\\\\n-        \\\\  # -o nospace requires we add back a space when a completion is finished\n-        \\\\  # and not part of a --key= completion\n+        # -o nospace requires we add back a space when a completion is finished\n+        # and not part of a --key= completion\n         \\\\  _add_spaces() {\n         \\\\    for idx in \"${!COMPREPLY[@]}\"; do\n         \\\\      [ -n \"${COMPREPLY[idx]}\" ] && COMPREPLY[idx]=\"${COMPREPLY[idx]} \";\n@@ -224,23 +224,6 @@                     }\n                     try writer.writeAll(compgenSuffix);\n                 },\n-                .optional => |optional| {\n-                    switch (@typeInfo(optional.child)) {\n-                        .@\"enum\" => |info| {\n-                            try writer.writeAll(compgenPrefix);\n-                            for (info.fields, 0..) |f, i| {\n-                                if (i > 0) try writer.writeAll(\" \");\n-                                try writer.writeAll(f.name);\n-                            }\n-                            try writer.writeAll(compgenSuffix);\n-                        },\n-                        else => {\n-                            if (std.mem.eql(u8, \"config-file\", opt.name)) {\n-                                try writer.writeAll(\"return ;;\");\n-                            } else try writer.writeAll(\"return;;\");\n-                        },\n-                    }\n-                },\n                 else => {\n                     if (std.mem.eql(u8, \"config-file\", opt.name)) {\n                         try writer.writeAll(\"_files ;;\");\n@@ -264,75 +247,9 @@         \\\\    return 0\n         \\\\  }\n         \\\\\n-        \\\\  # begin main logic\n         \\\\  local topLevel=\"-e\"\n         \\\\  topLevel+=\" --help\"\n         \\\\  topLevel+=\" --version\"\n         \\\\\n     );\n-\n-    for (@typeInfo(Action).@\"enum\".fields) |field| {\n-        if (std.mem.eql(u8, \"help\", field.name)) continue;\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 61.90359807014465,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 16286,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_build_gtk.zig",
    "exact_match": true,
    "similarity": 1.0,
    "diff_lines": 0,
    "predicted_output": "const std = @import(\"std\");\n\npub const Targets = packed struct {\n    x11: bool = false,\n    wayland: bool = false,\n};\n\n/// Returns the targets that GTK4 was compiled with.\npub fn targets(b: *std.Build) Targets {\n    // Run pkg-config. We allow it to fail so that zig build --help\n    // works without all dependencies. The build will fail later when\n    // GTK isn't found anyways.\n    var code: u8 = undefined;\n    const output = b.runAllowFail(\n        &.{ \"pkg-config\", \"--variable=targets\", \"gtk4\" },\n        &code,\n        .Ignore,\n    ) catch return .{};\n\n    const x11 = std.mem.indexOf(u8, output, \"x11\") != null;\n    const wayland = std.mem.indexOf(u8, output, \"wayland\") != null;\n\n    return .{\n        .x11 = x11,\n        .wayland = wayland,\n    };\n}",
    "evaluation": {
      "exact_match": true,
      "similarity": 1.0,
      "diff_lines": 0,
      "added_lines": 0,
      "removed_lines": 0,
      "diff_preview": null
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 8.732893228530884,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 2123,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_cli_args.zig",
    "exact_match": false,
    "similarity": 0.5407505174430812,
    "diff_lines": 1108,
    "predicted_output": "const std = @import(\"std\");\nconst mem = std.mem;\nconst assert = std.debug.assert;\nconst Allocator = mem.Allocator;\nconst ArenaAllocator = std.heap.ArenaAllocator;\n\nconst diags = @import(\"diagnostics.zig\");\nconst Diagnostic = diags.Diagnostic;\nconst DiagnosticList = diags.DiagnosticList;\n\n// TODO:\n//   - Only `--long=value` format is accepted. Do we want to allow\n//     `--long value`? Not currently allowed.\n\n// For trimming\npub const whitespace = \" \\t\";\n\n/// The base errors for arg parsing. Additional errors can be returned due\n/// to type-specific parsing but these are always possible.\npub const Error = error{\n    ValueRequired,\n    InvalidField,\n    InvalidValue,\n};\n\n/// Parse the command line arguments from iter into dst.\n///\n/// dst must be a struct. The fields and their types will be used to determine\n/// the valid CLI flags. See the tests in this file as an example. For field\n/// types that are structs, the struct can implement the `parseCLI` function\n/// to do custom parsing.\n///\n/// If the destination type has a field \"_arena\" of type `?ArenaAllocator`,\n/// an arena allocator will be created (or reused if set already) for any\n/// allocations. Allocations are necessary for certain types, like `[]const u8`.\n///\n/// If the destination type has a field \"_diagnostics\", it must be of type\n/// \"DiagnosticList\" and any diagnostic messages will be added to that list.\n/// When diagnostics are present, only allocation errors will be returned.\n///\n/// If the destination type has a decl \"renamed\", it must be of type\n/// std.StaticStringMap([]const u8) and contains a mapping from the old\n/// field name to the new field name. This is used to allow renaming fields\n/// while still supporting the old name. If a renamed field is set, parsing\n/// will automatically set the new field name.\n///\n/// Note: If the arena is already non-null, then it will be used. In this\n/// case, in the case of an error some memory might be leaked into the arena.\npub fn parse(\n    comptime T: type,\n    alloc: Allocator,\n    dst: *T,\n    iter: anytype,\n) !void {\n    const info = @typeInfo(T);\n    assert(info == .@\"struct\");\n\n    comptime {\n        // Verify all renamed fields are valid (source does not exist,\n        // destination does exist).\n        if (@hasDecl(T, \"renamed\")) {\n            for (T.renamed.keys(), T.renamed.values()) |key, value| {\n                if (@hasField(T, key)) {\n                    @compileLog(key);\n                    @compileError(\"renamed field source exists\");\n                }\n\n                if (!@hasField(T, value)) {\n                    @compileLog(value);\n                    @compileError(\"renamed field destination does not exist\");\n                }\n            }\n        }\n    }\n\n    // Make an arena for all our allocations if we support it. Otherwise,\n    // use an allocator that always fails. If the arena is already set on\n    // the config, then we reuse that. See memory note in parse docs.\n    const arena_available = @hasField(T, \"_arena\");\n    var arena_owned: bool = false;\n    const arena_alloc = if (arena_available) arena: {\n        // If the arena is unset, we create it. We mark that we own it\n        // only so that we can clean it up on error.\n        if (dst._arena == null) {\n            dst._arena = ArenaAllocator.init(alloc);\n            arena_owned = true;\n        }\n\n        break :arena dst._arena.?.allocator();\n    } else fail: {\n        // Note: this is... not safe...\n        var fail = std.testing.FailingAllocator.init(alloc, .{});\n        break :fail fail.allocator();\n    };\n    errdefer if (arena_available and arena_owned) {\n        dst._arena.?.deinit();\n        dst._arena = null;\n    };\n\n    while (iter.next()) |arg| {\n        // If this doesn't start with \"--\" then it isn't a config\n        // flag. We don't support positional arguments or configuration\n        // values set with spaces so this is an error.\n        if (!mem.startsWith(u8, arg, \"--\")) {\n            if (comptime !canTrackDiags(T)) return Error.InvalidField;\n\n            // Add our diagnostic\n            try dst._diagnostics.append(arena_alloc, .{\n                .key = try arena_alloc.dupeZ(u8, arg),\n                .message = \"invalid field\",\n                .location = try diags.Location.fromIter(iter, alloc),\n            });\n\n            continue;\n        }\n\n        var key: []const u8 = arg[2..];\n        const value: ?[]const u8 = value: {\n            // If the arg has \"=\" then the value is after the \"=\".\n            if (mem.indexOf(u8, key, \"=\")) |idx| {\n                defer key = key[0..idx];\n                break :value key[idx + 1 ..];\n            }\n\n            break :value null;\n        };\n\n        parseIntoField(T, arena_alloc, dst, key, value) catch |err| {\n            if (comptime !canTrackDiags(T)) return err;\n\n            // The error set is dependent on comptime T, so we always add\n            // an extra error so we can have the \"else\" below.\n            const ErrSet = @TypeOf(err) || error{ Unknown, OutOfMemory };\n            const message: [:0]const u8 = switch (@as(ErrSet, @errorCast(err))) {\n                // OOM is not recoverable since we need to allocate to\n                // track more error messages.\n                error.OutOfMemory => return err,\n                error.InvalidField => \"unknown field\",\n                error.ValueRequired => formatValueRequired(T, arena_alloc, key) catch \"value required\",\n                error.InvalidValue => formatInvalidValue(T, arena_alloc, key, value) catch \"invalid value\",\n                else => try std.fmt.allocPrintZ(\n                    arena_alloc,\n                    \"unknown error {}\",\n                    .{err},\n                ),\n            };\n\n            // Add our diagnostic\n            try dst._diagnostics.append(arena_alloc, .{\n                .key = try arena_alloc.dupeZ(u8, key),\n                .message = message,\n                .location = try diags.Location.fromIter(iter, alloc),\n            });\n        };\n    }\n}\n\n/// Returns true if this type can track diagnostics.\nfn canTrackDiags(comptime T: type) bool {\n    return @hasField(T, \"_diagnostics\");\n}\n\n/// Parse a single key/value pair into the destination type T.\npub fn parseIntoField(\n    comptime T: type,\n    alloc: Allocator,\n    dst: *T,\n    key: []const u8,\n    value: ?[]const u8,\n) !void {\n    const info = @typeInfo(T);\n    assert(info == .@\"struct\");\n\n    inline for (info.@\"struct\".fields) |field| {\n        if (field.name[0] != '_' and mem.eql(u8, field.name, key)) {\n            // For optional fields, we just treat it as the child type.\n            // This lets optional fields default to null but get set by\n            // the CLI.\n            const Field = switch (@typeInfo(field.type)) {\n                .optional => |opt| opt.child,\n                else => field.type,\n            };\n            const fieldInfo = @typeInfo(Field);\n            const canHaveDecls = fieldInfo == .@\"struct\" or\n                fieldInfo == .@\"union\" or\n                fieldInfo == .@\"enum\";\n\n            // If the value is empty string (set but empty string),\n            // then we reset the value to the default.\n            if (value) |v| default: {\n                if (v.len != 0) break :default;\n                // Set default value if possible.\n                if (canHaveDecls and @hasDecl(Field, \"init\")) {\n                    try @field(dst, field.name).init(alloc);\n                    return;\n                }\n                const raw = field.default_value_ptr orelse break :default;\n                const ptr: *const field.type = @alignCast(@ptrCast(raw));\n                @field(dst, field.name) = ptr.*;\n                return;\n            }\n\n            // If we are a type that can have decls and have a parseCLI decl,\n            // we call that and use that to set the value.\n            if (canHaveDecls) {\n                if (@hasDecl(Field, \"parseCLI\")) {\n                    const fnInfo = @typeInfo(@TypeOf(Field.parseCLI)).@\"fn\";\n                    switch (fnInfo.params.len) {\n                        // 1 arg = (input) => output\n                        1 => @field(dst, field.name) = try Field.parseCLI(value),\n\n                        // 2 arg = (self, input) => void\n                        2 => switch (@typeInfo(field.type)) {\n                            .@\"struct\",\n                            .@\"union\",\n                            .@\"enum\",\n                            => try @field(dst, field.name).parseCLI(value),\n\n                            // If the field is optional and set, then we use\n                            // the pointer value directly into it. If its not\n                            // set we need to create a new instance.\n                            .optional => if (@field(dst, field.name)) |*v| {\n                                try v.parseCLI(value);\n                            } else {\n                                // Note: you cannot do @field(dst, name) = undefined\n                                // because this causes the value to be \"null\"\n                                // in ReleaseFast modes.\n                                var tmp: Field = undefined;\n                                try tmp.parseCLI(value);\n                                @field(dst, field.name) = tmp;\n                            },\n                        },\n\n                        // 3 arg = (self, alloc, input) => void\n                        3 => switch (@typeInfo(field.type)) {\n                            .@\"struct\",\n                            .@\"union\",\n                            .@\"enum\",\n                            => try @field(dst, field.name).parseCLI(alloc, value),\n\n                            .optional => if (@field(dst, field.name)) |*v| {\n                                try v.parseCLI(alloc, value);\n                            } else {\n                                var tmp: Field = undefined;\n                                try tmp.parseCLI(alloc, value);\n                                @field(dst, field.name) = tmp;\n                            },\n                        },\n\n                        else => @compileError(\"parseCLI invalid argument count\"),\n                    }\n                    return;\n                }\n            }\n\n            switch (fieldInfo) {\n                .@\"enum\" => std.meta.stringToEnum(\n                    Field,\n                    value orelse return error.ValueRequired,\n                ) orelse return error.InvalidValue,\n\n                .@\"struct\" => try parseStruct(\n                    Field,\n                    alloc,\n                    value orelse return error.ValueRequired,\n                ),\n\n                .@\"union\" => try parseTaggedUnion(\n                    Field,\n                    alloc,\n                    value orelse return error.ValueRequired,\n                ),\n\n                else => @compileError(\"unsupported field type\"),\n            }\n        }\n    }\n\n    // Unknown field, is the field renamed?\n    if (@hasDecl(T, \"renamed\")) {\n        for (T.renamed.keys(), T.renamed.values()) |old, new| {\n            if (mem.eql(u8, old, key)) {\n                try parseIntoField(T, alloc, dst, new, value);\n                return;\n            }\n        }\n    }\n\n    return error.InvalidField;\n}\n\nfn parseTaggedUnion(comptime T: type, alloc: Allocator, v: []const u8) !T {\n    const info = @typeInfo(T).@\"union\";\n    assert(@typeInfo(info.tag_type.?) == .@\"enum\");\n\n    // Get the union tag that is being set. We support values with no colon\n    // if the value is void so its not an error to have no colon.\n    const colon_idx = mem.indexOf(u8, v, \":\") orelse v.len;\n    const tag_str = std.mem.trim(u8, v[0..colon_idx], whitespace);\n    const value = if (colon_idx < v.len) v[colon_idx + 1 ..] else \"\";\n\n    // Find the field in the union that matches the tag.\n    inline for (info.fields) |field| {\n        if (mem.eql(u8, field.name, tag_str)) {\n            // Special case void types where we don't need a value.\n            if (field.type == void) {\n                if (value.len > 0) return error.InvalidValue;\n                return @unionInit(T, field.name, {});\n            }\n\n            // We need to create a struct that looks like this union field.\n            // This lets us use parseIntoField as if its a dedicated struct.\n            const Target = @Type(.{ .@\"struct\" = .{\n                .layout = .auto,\n                .fields = &.{.{\n                    .name = field.name,\n                    .type = field.type,\n                    .default_value_ptr = null,\n                    .is_comptime = false,\n                    .alignment = @alignOf(field.type),\n                }},\n                .decls = &.{},\n                .is_tuple = false,\n            } });\n\n            // Parse the value into the struct\n            var t: Target = undefined;\n            try parseIntoField(Target, alloc, &t, field.name, value);\n\n            // Build our union\n            return @unionInit(T, field.name, @field(t, field.name));\n        }\n    }\n\n    return error.InvalidValue;\n}\n\nfn parseStruct(comptime T: type, alloc: Allocator, v: []const u8) !T {\n    return switch (@typeInfo(T).@\"struct\".layout) {\n        .auto => parseAutoStruct(T, alloc, v),\n        .@\"packed\" => parsePackedStruct(T, v),\n        else => @compileError(\"unsupported struct layout\"),\n    };\n}\n\npub fn parseAutoStruct(comptime T: type, alloc: Allocator, v: []const u8) !T {\n    const info = @typeInfo(T).@\"struct\";\n    comptime assert(info.layout == .auto);\n\n    // We start our result as undefined so we don't get an error for required\n    // fields. We track required fields below and we validate that we set them\n    // all at the bottom of this function (in addition to setting defaults for\n    // optionals).\n    var result: T = undefined;\n\n    // Keep track of which fields were set so we can error if a required\n    // field was not set.\n    const FieldSet = std.StaticBitSet(info.fields.len);\n    var fields_set: FieldSet = FieldSet.initEmpty();\n\n    // We split each value by \",\"\n    var iter = std.mem.splitSequence(u8, v, \",\");\n    loop: while (iter.next()) |entry| {\n        // Find the key/value, trimming whitespace. The value may be quoted\n        // which we strip the quotes from.\n        const idx = mem.indexOf(u8, entry, \":\") orelse return error.InvalidValue;\n        const key = std.mem.trim(u8, entry[0..idx], whitespace);\n        const value = value: {\n            var value = std.mem.trim(u8, entry[idx + 1 ..], whitespace);\n\n            // Detect a quoted string.\n            if (value.len >= 2 and\n                value[0] == '\"' and\n                value[value.len - 1] == '\"')\n            {\n                // Trim quotes since our CLI args processor expects\n                // quotes to already be gone.\n                value = value[1 .. value.len - 1];\n            }\n\n            break :value value;\n        };\n\n        inline for (info.fields, 0..) |field, i| {\n            if (std.mem.eql(u8, field.name, key)) {\n                try parseIntoField(T, alloc, &result, key, value);\n                fields_set.set(i);\n                continue :loop;\n            }\n        }\n\n        // No field matched\n        return error.InvalidValue;\n    }\n\n    // Ensure all required fields are set\n    inline for (info.fields, 0..) |field, i| {\n        if (!fields_set.isSet(i)) {\n            const default_ptr = field.default_value_ptr orelse return error.InvalidValue;\n            const typed_ptr: *const field.type = @alignCast(@ptrCast(default_ptr));\n            @field(result, field.name) = typed_ptr.*;\n        }\n    }\n\n    return result;\n}\n\nfn parsePackedStruct(comptime T: type, v: []const u8) !T {\n    const info = @typeInfo(T).@\"struct\";\n    comptime assert(info.layout == .@\"packed\");\n\n    var result: T = .{};\n\n    // We split each value by \",\"\n    var iter = std.mem.splitSequence(u8, v, \",\");\n    loop: while (iter.next()) |part_raw| {\n        // Determine the field we're looking for and the value. If the\n        // field is prefixed with \"no-\" then we set the value to false.\n        const part, const value = part: {\n            const negation_prefix = \"no-\";\n            const trimmed = std.mem.trim(u8, part_raw, whitespace);\n            if (std.mem.startsWith(u8, trimmed, negation_prefix)) {\n                break :part .{ trimmed[negation_prefix.len..], false };\n            } else {\n                break :part .{ trimmed, true };\n            }\n        };\n\n        inline for (info.fields) |field| {\n            assert(field.type == bool);\n            if (std.mem.eql(u8, field.name, part)) {\n                @field(result, field.name) = value;\n                continue :loop;\n            }\n        }\n\n        // No field matched\n        return error.InvalidValue;\n    }\n\n    return result;\n}\n\npub fn parseBool(v: []const u8) !bool {\n    const t = &[_][]const u8{ \"1\", \"t\", \"T\", \"true\" };\n    const f = &[_][]const u8{ \"0\", \"f\", \"F\", \"false\" };\n\n    inline for (t) |str| {\n        if (mem.eql(u8, v, str)) return true;\n    }\n    inline for (f) |str| {\n        if (mem.eql(u8, v, str)) return false;\n    }\n\n    return error.InvalidValue;\n}\n\ntest \"parse: empty value resets to default\" {\n    const testing = std.testing;\n\n    var data: struct {\n        a: u8 = 42,\n        b: bool = false,\n        _arena: ?ArenaAllocator = null,\n    } = .{};\n    defer if (data._arena) |arena| arena.deinit();\n\n    var iter = try std.process.ArgIteratorGeneral(.{}).init(\n        testing.allocator,\n        \"--a= --b=\",\n    );\n    defer iter.deinit();\n    try parse(@TypeOf(data), testing.allocator, &data, &iter);\n    try testing.expectEqual(@as(u8, 42), data.a);\n    try testing.expect(!data.b);\n}\n\ntest \"parseIntoField: struct with init func\" {\n    const testing = std.testing;\n    var arena = ArenaAllocator.init(testing.allocator);\n    defer arena.deinit();\n    const alloc = arena.allocator();\n\n    var data: struct {\n        a: struct {\n            const Self = @This();\n\n            pub fn init(self: *Self, _alloc: Allocator) !void {\n                self.* = .{ .v = \"HELLO!\" };\n            }\n\n            v: []const u8,\n        },\n    } = undefined;\n\n    try parseIntoField(@TypeOf(data), alloc, &data, \"a\", \"\");\n    try testing.expectEqual(@as([]const u8, \"HELLO!\"), data.a.v);\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.5407505174430812,
      "diff_lines": 1108,
      "added_lines": 25,
      "removed_lines": 985,
      "diff_preview": "--- expected+++ predicted@@ -3,12 +3,10 @@ const assert = std.debug.assert;\n const Allocator = mem.Allocator;\n const ArenaAllocator = std.heap.ArenaAllocator;\n+\n const diags = @import(\"diagnostics.zig\");\n-const internal_os = @import(\"../os/main.zig\");\n const Diagnostic = diags.Diagnostic;\n const DiagnosticList = diags.DiagnosticList;\n-\n-const log = std.log.scoped(.cli);\n \n // TODO:\n //   - Only `--long=value` format is accepted. Do we want to allow\n@@ -100,26 +98,6 @@     };\n \n     while (iter.next()) |arg| {\n-        // Do manual parsing if we have a hook for it.\n-        if (@hasDecl(T, \"parseManuallyHook\")) {\n-            if (!try dst.parseManuallyHook(\n-                arena_alloc,\n-                arg,\n-                iter,\n-            )) return;\n-        }\n-\n-        // If the destination supports help then we check for it, call\n-        // the help function and return.\n-        if (@hasDecl(T, \"help\")) {\n-            if (mem.eql(u8, arg, \"--help\") or\n-                mem.eql(u8, arg, \"-h\"))\n-            {\n-                try dst.help();\n-                return;\n-            }\n-        }\n-\n         // If this doesn't start with \"--\" then it isn't a config\n         // flag. We don't support positional arguments or configuration\n         // values set with spaces so this is an error.\n@@ -130,7 +108,7 @@             try dst._diagnostics.append(arena_alloc, .{\n                 .key = try arena_alloc.dupeZ(u8, arg),\n                 .message = \"invalid field\",\n-                .location = try diags.Location.fromIter(iter, arena_alloc),\n+                .location = try diags.Location.fromIter(iter, alloc),\n             });\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 165.7335889339447,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 33658,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_config.zig",
    "exact_match": false,
    "similarity": 0.004841149773071104,
    "diff_lines": 358,
    "predicted_output": "const std = @import(\"std\");\n\npub const Config = struct {\n    /// The font families to use.\n    @\"font-family\": ?[:0]const u8 = null,\n    @\"font-family-bold\": ?[:0]const u8 = null,\n    @\"font-family-italic\": ?[:0]const u8 = null,\n    @\"font-family-bold-italic\": ?[:0]const u8 = null,\n\n    /// Apply a font feature. This can be repeated multiple times to enable\n    /// multiple font features. You can NOT set multiple font features with\n    /// a single value (yet).\n    ///\n    /// The font feature will apply to all fonts rendered by Ghostty. A\n    /// future enhancement will allow targeting specific faces.\n    ///\n    /// A valid value is the name of a feature. Prefix the feature with a\n    /// \"-\" to explicitly disable it. Example: \"ss20\" or \"-ss20\".\n    @\"font-feature\": RepeatableString = .{},\n\n    /// Font size in points\n    @\"font-size\": u8 = switch (builtin.os.tag) {\n        // On Mac we default a little bigger since this tends to look better.\n        // This is purely subjective but this is easy to modify.\n        .macos => 13,\n        else => 12,\n    },\n\n    /// A repeatable configuration to set one or more font variations values\n    /// for a variable font. A variable font is a single font, usually\n    /// with a filename ending in \"-VF.ttf\" or \"-VF.otf\" that contains\n    /// one or more configurable axes for things such as weight, slant,\n    /// etc. Not all fonts support variations; only fonts that explicitly\n    /// state they are variable fonts will work.\n    ///\n    /// The format of this is \"id=value\" where \"id\" is the axis identifier.\n    /// An axis identifier is always a 4 character string, such as \"wght\".\n    /// To get the list of supported axes, look at your font documentation\n    /// or use a font inspection tool.\n    ///\n    /// Invalid ids and values are usually ignored. For example, if a font\n    /// only supports weights from 100 to 700, setting \"wght=800\" will\n    /// do nothing (it will not be clamped to 700). You must consult your\n    /// font's documentation to see what values are supported.\n    ///\n    /// Common axes are: \"wght\" (weight), \"slnt\" (slant), \"ital\" (italic),\n    /// \"opsz\" (optical size), \"wdth\" (width), \"GRAD\" (gradient), etc.\n    @\"font-variation\": RepeatableFontVariation = .{},\n    @\"font-variation-bold\": RepeatableFontVariation = .{},\n    @\"font-variation-italic\": RepeatableFontVariation = .{},\n    @\"font-variation-bold-italic\": RepeatableFontVariation = .{},\n\n    /// Draw fonts with a thicker stroke, if supported. This is only supported\n    /// currently on macOS.\n    @\"font-thicken\": bool = false,\n\n    /// Background color for the window.\n    background: Color = .{ .r = 0x28, .g = 0x2C, .b = 0x34 },\n\n    /// Foreground color for the window.\n    foreground: Color = .{ .r = 0xFF, .g = 0xFF, .b = 0xFF },\n\n    /// The foreground and background color for selection. If this is not\n    /// set, then the selection color is just the inverted window background\n    /// and foreground (note: not to be confused with the cell bg/fg).\n    @\"selection-foreground\": ?Color = null,\n    @\"selection-background\": ?Color = null,\n\n    /// Color palette for the 256 color form that many terminal applications\n    /// use. The syntax of this configuration is \"N=HEXCODE\" where \"n\"\n    /// is 0 to 255 (for the 256 colors) and HEXCODE is a typical RGB\n    /// color code such as \"#AABBCC\". The 0 to 255 correspond to the\n    /// terminal color table.\n    ///\n    /// For definitions on all the codes:\n    /// https://www.ditig.com/256-colors-cheat-sheet\n    palette: Palette = .{},\n\n    /// The color of the cursor. If this is not set, a default will be chosen.\n    @\"cursor-color\": ?Color = null,\n\n    /// The style of the cursor. This sets the default style. A running\n    /// program can still request an explicit cursor style using escape\n    /// sequences (such as CSI q). Shell configurations will often request\n    /// specific cursor styles.\n    ///\n    /// Caveat: Shell integration currently defaults to always be a bar\n    /// In order to fix it, we probably would want to add something similar to Kitty's\n    /// shell integration options (no-cursor). For more information see:\n    /// https://sw.kovidgoyal.net/kitty/conf/#opt-kitty.shell_integration\n    @\"cursor-style\": terminal.Cursor.Style = .bar,\n\n    /// Whether the cursor shall blink\n    @\"cursor-style-blink\": bool = true,\n\n    /// The color of the text under the cursor. If this is not set, a default\n    /// will be chosen.\n    @\"cursor-text\": ?Color = null,\n\n    /// The opacity level (opposite of transparency) of the background.\n    /// A value of 1 is fully opaque and a value of 0 is fully transparent.\n    /// A value less than 0 or greater than 1 will be clamped to the nearest\n    /// valid value.\n    ///\n    /// Changing this value at runtime (and reloading config) will only\n    /// affect new windows, tabs, and splits.\n    @\"background-opacity\": f64 = 1.0,\n\n    /// A positive value enables blurring of the background when\n    /// background-opacity is less than 1. The value is the blur radius to\n    /// apply. A value of 20 is reasonable for a good looking blur.\n    /// Higher values will cause strange rendering issues as well as\n    /// performance issues.\n    ///\n    /// This is only supported on macOS.\n    @\"background-blur-radius\": u8 = 0,\n\n    /// The command to run, usually a shell. If this is not an absolute path,\n    /// it'll be looked up in the PATH. If this is not set, a default will\n    /// be looked up from your system. The rules for the default lookup are:\n    ///\n    ///   - SHELL environment variable\n    ///   - passwd entry (user information)\n    ///\n    command: ?[]const u8 = null,\n\n    /// The directory to change to after starting the command.\n    ///\n    /// The default is \"inherit\" except in special scenarios listed next.\n    /// If ghostty can detect it is launched on macOS from launchd\n    /// (double-clicked), then it defaults to \"home\".\n    ///\n    /// The value of this must be an absolute value or one of the special\n    /// values below:\n    ///\n    ///   - \"home\" - The home directory of the executing user.\n    ///   - \"inherit\" - The working directory of the launching process.\n    ///\n    @\"working-directory\": ?[]const u8 = null,\n\n    /// Key bindings. The format is \"trigger=action\". Duplicate triggers\n    /// will overwrite previously set values.\n    ///\n    /// Trigger: \"+\"-separated list of keys and modifiers. Example:\n    /// \"ctrl+a\", \"ctrl+shift+b\", \"up\". Some notes:\n    ///\n    ///   - modifiers cannot repeat, \"ctrl+ctrl+a\" is invalid.\n    ///   - modifiers and key can be in any order, \"shift+a+ctrl\" is weird,\n    ///     but valid.\n    ///   - only a single key input is allowed, \"ctrl+a+b\" is invalid.\n    ///\n    /// Action is the action to take when the trigger is satisfied. It takes\n    /// the format \"action\" or \"action:param\". The latter form is only valid\n    /// if the action requires a parameter.\n    ///\n    ///   - \"ignore\" - Do nothing, ignore the key input. This can be used to\n    ///     black hole certain inputs to have no effect.\n    ///   - \"unbind\" - Remove the binding. This makes it so the previous action\n    ///     is removed, and the key will be sent through to the child command\n    ///     if it is printable.\n    ///   - \"csi:text\" - Send a CSI sequence. i.e. \"csi:A\" sends \"cursor up\".\n    ///\n    /// Some notes for the action:\n    ///\n    ///   - The parameter is taken as-is after the \":\". Double quotes or\n    ///     other mechanisms are included and NOT parsed. If you want to\n    ///     send a string value that includes spaces, wrap the entire\n    ///     trigger/action in double quotes. Example: --keybind=\"up=csi:A B\"\n    ///\n    keybind: Keybinds = .{},\n\n    /// Window padding. This applies padding between the terminal cells and\n    /// the window border. The \"x\" option applies to the left and right\n    /// padding and the \"y\" option is top and bottom. The value is in points,\n    /// meaning that it will be scaled appropriately for screen DPI.\n    ///\n    /// If this value is set too large, the screen will render nothing, because\n    /// the grid will be completely squished by the padding. It is up to you\n    /// as the user to pick a reasonable value. If you pick an unreasonable\n    /// value, a warning will appear in the logs.\n    @\"window-padding-x\": u32 = 2,\n    @\"window-padding-y\": u32 = 2,\n\n    /// The viewport dimensions are usually not perfectly divisible by\n    /// the cell size. In this case, some extra padding on the end of a\n    /// column and the bottom of the final row may exist. If this is true,\n    /// then this extra padding is automatically balanced between all four\n    /// edges to minimize imbalance on one side. If this is false, the top\n    /// left grid cell will always hug the edge with zero padding other than\n    /// what may be specified with the other \"window-padding\" options.\n    ///\n    /// If other \"window-padding\" fields are set and this is true, this will\n    /// still apply. The other padding is applied first and may affect how\n    /// many grid cells actually exist, and this is applied last in order\n    /// to balance the padding given a certain viewport size and grid cell size.\n    @\"window-padding-balance\": bool = false,\n\n    /// If true, new windows and tabs will inherit the font size of the previously\n    /// focused window. If no window was previously focused, the default\n    /// font size will be used. If this is false, the default font size\n    /// specified in the configuration \"font-size\" will be used.\n    @\"window-inherit-font-size\": bool = true,\n\n    /// If false, windows won't have native decorations, i.e. titlebar and\n    /// borders.\n    /// Currently only supported with GTK.\n    @\"window-decoration\": bool = true,\n\n    /// Whether to allow programs running in the terminal to read/write to\n    /// the system clipboard (OSC 52, for googling). The default is to\n    /// disallow clipboard reading but allow writing.\n    @\"clipboard-read\": bool = false,\n    @\"clipboard-write\": bool = true,\n\n    /// Trims trailing whitespace on data that is copied to the clipboard.\n    /// This does not affect data sent to the clipboard via \"clipboard-write\".\n    @\"clipboard-trim-trailing-spaces\": bool = true,\n\n    /// The total amount of bytes that can be used for image data (i.e.\n    /// the Kitty image protocol) per terminal screen. The maximum value\n    /// is 4,294,967,295 (4GB). The default is 320MB. If this is set to zero,\n    /// then all image protocols will be disabled.\n    ///\n    /// This value is separate for primary and alternate screens so the\n    /// effective limit per surface is double.\n    @\"image-storage-limit\": u32 = 320 * 1000 * 1000,\n\n    /// Whether to automatically copy selected text to the clipboard. \"true\"\n    /// will only copy on systems that support a selection clipboard.\n    ///\n    /// The value \"clipboard\" will copy to the system clipboard, making this\n    /// work on macOS. Note that middle-click will also paste from the system\n    /// clipboard in this case.\n    ///\n    /// Note that if this is disabled, middle-click paste will also be\n    /// disabled.\n    @\"copy-on-select\": CopyOnSelect = .true,\n\n    /// The time in milliseconds between clicks to consider a click a repeat\n    /// (double, triple, etc.) or an entirely new single click. A value of\n    /// zero will use a platform-specific default. The default on macOS\n    /// is determined by the OS settings. On every other platform it is 500ms.\n    @\"click-repeat-interval\": u32 = 0,\n\n    /// Additional configuration files to read.\n    @\"config-file\": RepeatableString = .{},\n\n    /// Confirms that a surface should be closed before closing it. This defaults\n    /// to true. If set to false, surfaces will close without any confirmation.\n    @\"confirm-close-surface\": bool = true,\n\n    /// Whether to enable shell integration auto-injection or not. Shell\n    /// integration greatly enhances the terminal experience by enabling\n    /// a number of features:\n    ///\n    ///   * Working directory reporting so new tabs, splits inherit the\n    ///     previous terminal's working directory.\n    ///   * Prompt marking that enables the \"scroll_to_prompt\" keybinding.\n    ///   * If you're sitting at a prompt, closing a terminal will not ask\n    ///     for confirmation.\n    ///   * Resizing the window with a complex prompt usually paints much\n    ///     better.\n    ///\n    /// Allowable values are:\n    ///\n    ///   * \"none\" - Do not do any automatic injection. You can still manually\n    ///     configure your shell to enable the integration.\n    ///   * \"detect\" - Detect the shell based on the filename.\n    ///   * \"fish\", \"zsh\" - Use this specific shell injection scheme.\n    ///\n    /// The default value is \"detect\".\n    @\"shell-integration\": ShellIntegration = .detect,\n\n    /// If anything other than false, fullscreen mode on macOS will not use the\n    /// native fullscreen, but make the window fullscreen without animations and\n    /// using a new space. It's faster than the native fullscreen mode since it\n    /// doesn't use animations.\n    ///\n    /// Allowable values are:\n    ///\n    ///   * \"visible-menu\" - Use non-native macOS fullscreen, keep the menu bar visible\n    ///   * \"true\" - Use non-native macOS fullscreen, hide the menu bar\n    ///   * \"false\" - Use native macOS fullscreeen\n    @\"macos-non-native-fullscreen\": NonNativeFullscreen = .false,\n\n    /// If true, the Option key will be treated as Alt. This makes terminal\n    /// sequences expecting Alt to work properly, but will break Unicode\n    /// input sequences on macOS if you use them via the alt key. You may\n    /// set this to false to restore the macOS alt-key unicode sequences\n    /// but this will break terminal sequences expecting Alt to work.\n    ///\n    /// Note that if an Option-sequence doesn't produce a printable\n    /// character, it will be treated as Alt regardless of this setting.\n    /// (i.e. alt+ctrl+a).\n    ///\n    /// This does not work with GLFW builds.\n    @\"macos-option-as-alt\": OptionAsAlt = .false,\n\n    /// If true (default), then the Ghostty GTK application will run in\n    /// single-instance mode: each new `ghostty` process launched will result\n    /// in a new window, if there is already a running process.\n    ///\n    /// If false, each new ghostty process will launch a separate application.\n    ///\n    /// Debug builds of Ghostty have a separate single-instance ID.\n    @\"gtk-single-instance\": bool = true,\n\n    /// This is set by the CLI parser for deinit.\n    _arena: ?ArenaAllocator = null,\n};",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.004841149773071104,
      "diff_lines": 358,
      "added_lines": 310,
      "removed_lines": 45,
      "diff_preview": "--- expected+++ predicted@@ -1,45 +1,310 @@-const builtin = @import(\"builtin\");\n-\n-const formatter = @import(\"config/formatter.zig\");\n-pub const Config = @import(\"config/Config.zig\");\n-pub const conditional = @import(\"config/conditional.zig\");\n-pub const string = @import(\"config/string.zig\");\n-pub const edit = @import(\"config/edit.zig\");\n-pub const url = @import(\"config/url.zig\");\n-\n-pub const ConditionalState = conditional.State;\n-pub const FileFormatter = formatter.FileFormatter;\n-pub const entryFormatter = formatter.entryFormatter;\n-pub const formatEntry = formatter.formatEntry;\n-\n-// Field types\n-pub const ClipboardAccess = Config.ClipboardAccess;\n-pub const Command = Config.Command;\n-pub const ConfirmCloseSurface = Config.ConfirmCloseSurface;\n-pub const CopyOnSelect = Config.CopyOnSelect;\n-pub const CustomShaderAnimation = Config.CustomShaderAnimation;\n-pub const FontSyntheticStyle = Config.FontSyntheticStyle;\n-pub const FontStyle = Config.FontStyle;\n-pub const FreetypeLoadFlags = Config.FreetypeLoadFlags;\n-pub const Keybinds = Config.Keybinds;\n-pub const MouseShiftCapture = Config.MouseShiftCapture;\n-pub const NonNativeFullscreen = Config.NonNativeFullscreen;\n-pub const OptionAsAlt = Config.OptionAsAlt;\n-pub const RepeatableCodepointMap = Config.RepeatableCodepointMap;\n-pub const RepeatableFontVariation = Config.RepeatableFontVariation;\n-pub const RepeatableString = Config.RepeatableString;\n-pub const RepeatableStringMap = @import(\"config/RepeatableStringMap.zig\");\n-pub const RepeatablePath = Config.RepeatablePath;\n-pub const ShellIntegrationFeatures = Config.ShellIntegrationFeatures;\n-pub const WindowPaddingColor = Config.WindowPaddingColor;\n-\n-// Alternate APIs\n-pub const CAPI = @import(\"config/CAPI.zig\");\n-pub const Wasm = if (!builtin.target.cpu.arch.isWasm()) struct {} else @import(\"config/Wasm.zig\");\n-\n-test {\n-    @import(\"std\").testing.refAllDecls(@This());\n-\n-    // Vim syntax file, not used at runtime but we want to keep it tested.\n-    _ = @import(\"config/vim.zig\");\n-}+const std = @import(\"std\");\n+\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 138.1957507133484,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 90986,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_config_c_get.zig",
    "exact_match": false,
    "similarity": 0.933510430502383,
    "diff_lines": 112,
    "predicted_output": "const std = @import(\"std\");\n\nconst key = @import(\"key.zig\");\nconst Config = @import(\"Config.zig\");\nconst Color = Config.Color;\nconst Key = key.Key;\nconst Value = key.Value;\n\n/// Get a value from the config by key into the given pointer. This is\n/// specifically for C-compatible APIs. If you're using Zig, just access\n/// the configuration directly.\n///\n/// The return value is false if the given key is not supported by the\n/// C API yet. This is a fixable problem so if it is important to support\n/// some key, please open an issue.\npub fn get(config: *const Config, k: Key, ptr_raw: *anyopaque) bool {\n    @setEvalBranchQuota(10_000);\n    switch (k) {\n        inline else => |tag| {\n            const value = fieldByKey(config, tag);\n            return getValue(ptr_raw, value);\n        },\n    }\n}\n\n/// Get the value anytype and put it into the pointer. Returns false if\n/// the type is not supported by the C API yet or the value is null.\nfn getValue(ptr_raw: *anyopaque, value: anytype) bool {\n    switch (@TypeOf(value)) {\n        ?[:0]const u8 => {\n            const ptr: *?[*:0]const u8 = @ptrCast(@alignCast(ptr_raw));\n            ptr.* = if (value) |slice| @ptrCast(slice.ptr) else null;\n        },\n\n        bool => {\n            const ptr: *bool = @ptrCast(@alignCast(ptr_raw));\n            ptr.* = value;\n        },\n\n        u8, u32 => {\n            const ptr: *c_uint = @ptrCast(@alignCast(ptr_raw));\n            ptr.* = @intCast(value);\n        },\n\n        f32, f64 => |Float| {\n            const ptr: *Float = @ptrCast(@alignCast(ptr_raw));\n            ptr.* = @floatCast(value);\n        },\n\n        i16 => {\n            const ptr: *c_short = @ptrCast(@alignCast(ptr_raw));\n            ptr.* = @intCast(value);\n        },\n\n        .optional => {\n            // If an optional has no value we return false.\n            const unwrapped = value orelse return false;\n            return getValue(ptr_raw, unwrapped);\n        },\n\n        .@\"enum\" => {\n            const ptr: *[*:0]const u8 = @ptrCast(@alignCast(ptr_raw));\n            ptr.* = @tagName(value);\n        },\n\n        .@\"struct\" => |info| {\n            // If the struct implements cval then we call then.\n            if (@hasDecl(T, \"cval\")) {\n                const PtrT = @typeInfo(@TypeOf(T.cval)).@\"fn\".return_type.?;\n                const ptr: *PtrT = @ptrCast(@alignCast(ptr_raw));\n                ptr.* = value.cval();\n                return true;\n            }\n\n            // Packed structs that are less than or equal to the\n            // size of a C int can be passed directly as their\n            // bit representation.\n            if (info.layout != .@\"packed\") return false;\n            const Backing = info.backing_integer orelse return false;\n            if (@bitSizeOf(Backing) > @bitSizeOf(c_uint)) return false;\n\n            const ptr: *c_uint = @ptrCast(@alignCast(ptr_raw));\n            ptr.* = @intCast(@as(Backing, @bitCast(value)));\n        },\n\n        .@\"union\" => |_| {\n            if (@hasDecl(T, \"cval\")) {\n                const PtrT = @typeInfo(@TypeOf(T.cval)).@\"fn\".return_type.?;\n                const ptr: *PtrT = @ptrCast(@alignCast(ptr_raw));\n                ptr.* = value.cval();\n                return true;\n            }\n\n            return false;\n        },\n\n        else => return false,\n    }\n\n    return true;\n}\n\n/// Get a value from the config by key.\nfn fieldByKey(self: *const Config, comptime k: Key) Value(k) {\n    const field = comptime field: {\n        const fields = std.meta.fields(Config);\n        for (fields) |field| {\n            if (@field(Key, field.name) == k) {\n                break :field field;\n            }\n        }\n\n        unreachable;\n    };\n\n    return @field(self, field.name);\n}\n\ntest \"c_get: u8\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var c = try Config.default(alloc);\n    defer c.deinit();\n    c.@\"font-size\" = 24;\n\n    var cval: f32 = undefined;\n    try testing.expect(get(&c, .@\"font-size\", &cval));\n    try testing.expectEqual(@as(f32, 24), cval);\n}\n\ntest \"c_get: enum\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var c = try Config.default(alloc);\n    defer c.deinit();\n    c.@\"window-theme\" = .dark;\n\n    var cval: [*:0]u8 = undefined;\n    try testing.expect(get(&c, .@\"window-theme\", @ptrCast(&cval)));\n\n    const str = std.mem.sliceTo(cval, 0);\n    try testing.expectEqualStrings(\"dark\", str);\n}\n\ntest \"c_get: color\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var c = try Config.default(alloc);\n    defer c.deinit();\n    c.background = .{ .r = 255, .g = 0, .b = 0 };\n\n    var cval: Color.C = undefined;\n    try testing.expect(get(&c, .background, @ptrCast(&cval)));\n    try testing.expectEqual(255, cval.r);\n    try testing.expectEqual(0, cval.g);\n    try testing.expectEqual(0, cval.b);\n}\n\ntest \"c_get: optional\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var c = try Config.default(alloc);\n    defer c.deinit();\n\n    {\n        c.@\"unfocused-split-fill\" = null;\n        var cval: Color.C = undefined;\n        try testing.expect(!get(&c, .@\"unfocused-split-fill\", @ptrCast(&cval)));\n    }\n\n    {\n        c.@\"unfocused-split-fill\" = .{ .r = 255, .g = 0, .b = 0 };\n        var cval: Color.C = undefined;\n        try testing.expect(get(&c, .@\"unfocused-split-fill\", @ptrCast(&cval)));\n        try testing.expectEqual(255, cval.r);\n        try testing.expectEqual(0, cval.g);\n        try testing.expectEqual(0, cval.b);\n    }\n}\n\ntest \"c_get: background-blur\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var c = try Config.default(alloc);\n    defer c.deinit();\n\n    {\n        c.@\"background-blur\" = .false;\n        var cval: u8 = undefined;\n        try testing.expect(get(&c, .@\"background-blur\", @ptrCast(&cval)));\n        try testing.expectEqual(0, cval);\n    }\n    {\n        c.@\"background-blur\" = .true;\n        var cval: u8 = undefined;\n        try testing.expect(get(&c, .@\"background-blur\", @ptrCast(&cval)));\n        try testing.expectEqual(20, cval);\n    }\n    {\n        c.@\"background-blur\" = .{ .radius = 42 };\n        var cval: u8 = undefined;\n        try testing.expect(get(&c, .@\"background-blur\", @ptrCast(&cval)));\n        try testing.expectEqual(42, cval);\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.933510430502383,
      "diff_lines": 112,
      "added_lines": 48,
      "removed_lines": 50,
      "diff_preview": "--- expected+++ predicted@@ -42,61 +42,59 @@             ptr.* = @intCast(value);\n         },\n \n+        f32, f64 => |Float| {\n+            const ptr: *Float = @ptrCast(@alignCast(ptr_raw));\n+            ptr.* = @floatCast(value);\n+        },\n+\n         i16 => {\n             const ptr: *c_short = @ptrCast(@alignCast(ptr_raw));\n             ptr.* = @intCast(value);\n         },\n \n-        f32, f64 => |Float| {\n-            const ptr: *Float = @ptrCast(@alignCast(ptr_raw));\n-            ptr.* = @floatCast(value);\n-        },\n-\n-        else => |T| switch (@typeInfo(T)) {\n-            .optional => {\n-                // If an optional has no value we return false.\n-                const unwrapped = value orelse return false;\n-                return getValue(ptr_raw, unwrapped);\n-            },\n-\n-            .@\"enum\" => {\n-                const ptr: *[*:0]const u8 = @ptrCast(@alignCast(ptr_raw));\n-                ptr.* = @tagName(value);\n-            },\n-\n-            .@\"struct\" => |info| {\n-                // If the struct implements cval then we call then.\n-                if (@hasDecl(T, \"cval\")) {\n-                    const PtrT = @typeInfo(@TypeOf(T.cval)).@\"fn\".return_type.?;\n-                    const ptr: *PtrT = @ptrCast(@alignCast(ptr_raw));\n-                    ptr.* = value.cval();\n-                    return true;\n-                }\n-\n-                // Packed structs that are less than or equal to the\n-                // size of a C int can be passed directly as their\n-                // bit representation.\n-                if (info.layout != .@\"packed\") return false;\n-                const Backing = info.backing_integer orelse return false;\n-                if (@bitSizeOf(Backing) > @bitSizeOf(c_uint)) return false;\n-\n-                const ptr: *c_uint = @ptrCast(@alignCast(ptr_raw));\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 36.23942184448242,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 8022,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_config_formatter.zig",
    "exact_match": false,
    "similarity": 0.7595417348608838,
    "diff_lines": 170,
    "predicted_output": "const formatter = @This();\nconst std = @import(\"std\");\nconst Allocator = std.mem.Allocator;\nconst help_strings = @import(\"help_strings\");\nconst Config = @import(\"Config.zig\");\nconst Key = @import(\"key.zig\").Key;\n\n/// Returns a single entry formatter for the given field name and writer.\npub fn entryFormatter(\n    name: []const u8,\n    writer: anytype,\n) EntryFormatter(@TypeOf(writer)) {\n    return .{ .name = name, .writer = writer };\n}\n\n/// The entry formatter type for a given writer.\npub fn EntryFormatter(comptime WriterType: type) type {\n    return struct {\n        name: []const u8,\n        writer: WriterType,\n\n        pub fn formatEntry(\n            self: @This(),\n            comptime T: type,\n            value: T,\n        ) !void {\n            return formatter.formatEntry(\n                T,\n                self.name,\n                value,\n                self.writer,\n            );\n        }\n    };\n}\n\n/// Format a single type with the given name and value.\npub fn formatEntry(\n    comptime T: type,\n    name: []const u8,\n    value: T,\n    writer: anytype,\n) !void {\n    switch (@typeInfo(T)) {\n        .bool, .int => {\n            try writer.print(\"{s} = {}\\n\", .{ name, value });\n            return;\n        },\n\n        .float => {\n            try writer.print(\"{s} = {d}\\n\", .{ name, value });\n            return;\n        },\n\n        .@\"enum\" => {\n            try writer.print(\"{s} = {s}\\n\", .{ name, @tagName(value) });\n            return;\n        },\n\n        .void => {\n            try writer.print(\"{s} = \\n\", .{name});\n            return;\n        },\n\n        .optional => |info| {\n            if (value) |inner| {\n                try formatEntry(\n                    info.child,\n                    name,\n                    inner,\n                    writer,\n                );\n            } else {\n                try writer.print(\"{s} = \\n\", .{name});\n            }\n\n            return;\n        },\n\n        .pointer => switch (T) {\n            []const u8,\n            [:0]const u8,\n            => {\n                try writer.print(\"{s} = {s}\\n\", .{ name, value });\n                return;\n            },\n\n            else => {},\n        },\n\n        .@\"struct\" => |info| if (@hasDecl(T, \"formatEntry\")) {\n            try value.formatEntry(entryFormatter(name, writer));\n            return;\n        } else switch (info.layout) {\n            .@\"packed\" => {\n                try writer.print(\"{s} = \", .{name});\n                inline for (info.fields, 0..) |field, i| {\n                    if (i > 0) try writer.print(\",\", .{});\n                    try writer.print(\"{s}{s}\", .{\n                        if (!@field(value, field.name)) \"no-\" else \"\",\n                        field.name,\n                    });\n                }\n                try writer.print(\"\\n\", .{});\n                return;\n            },\n\n            else => {},\n        },\n\n        .@\"union\" => if (@hasDecl(T, \"formatEntry\")) {\n            try value.formatEntry(entryFormatter(name, writer));\n            return;\n        },\n\n        else => {},\n    }\n\n    @compileLog(T);\n    @compileError(\"missing case for type\");\n}\n\npub const FileFormatter = struct {\n    alloc: Allocator,\n    config: *const Config,\n\n    /// Include comments for documentation of each key\n    docs: bool = false,\n\n    /// Only include changed values from the default.\n    changed: bool = false,\n\n    /// Implements std.fmt so it can be used directly with std.fmt.\n    pub fn format(\n        self: FileFormatter,\n        comptime layout: []const u8,\n        opts: std.fmt.FormatOptions,\n        writer: anytype,\n    ) !void {\n        _ = layout;\n        _ = opts;\n\n        // If we're change-tracking then we need the default config to\n        // compare against.\n        var default: ?Config = if (self.changed)\n            try Config.default(self.alloc)\n        else\n            null;\n        defer if (default) |*v| v.deinit();\n\n        inline for (@typeInfo(Config).@\"struct\".fields) |field| {\n            if (field.name[0] == '_') continue;\n\n            const value = @field(self.config, field.name);\n            const do_format = if (default) |d| format: {\n                const key = @field(Key, field.name);\n                break :format d.changed(self.config, key);\n            } else true;\n\n            if (do_format) {\n                const do_docs = self.docs and @hasDecl(help_strings.Config, field.name);\n                if (do_docs) {\n                    const help = @field(help_strings.Config, field.name);\n                    var lines = std.mem.splitScalar(u8, help, '\\n');\n                    while (lines.next()) |line| {\n                        try writer.print(\"# {s}\\n\", .{line});\n                    }\n                }\n\n                try formatEntry(\n                    field.type,\n                    field.name,\n                    value,\n                    writer,\n                );\n\n                if (do_docs) try writer.print(\"\\n\", .{});\n            }\n        }\n    }\n};\n\ntest \"format default config\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n    var cfg = try Config.default(alloc);\n    defer cfg.deinit();\n\n    var buf = std.ArrayList(u8).init(alloc);\n    defer buf.deinit();\n\n    const fmt: FileFormatter = .{\n        .alloc = alloc,\n        .config = &cfg,\n    };\n    try std.fmt.format(buf.writer(), \"{}\", .{fmt});\n}\n\ntest \"format default config changed\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n    var cfg = try Config.default(alloc);\n    defer cfg.deinit();\n    cfg.@\"font-size\" = 42;\n\n    var buf = std.ArrayList(u8).init(alloc);\n    defer buf.deinit();\n\n    const fmt: FileFormatter = .{\n        .alloc = alloc,\n        .config = &cfg,\n        .changed = true,\n    };\n    try std.fmt.format(buf.writer(), \"{}\", .{fmt});\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.7595417348608838,
      "diff_lines": 170,
      "added_lines": 0,
      "removed_lines": 123,
      "diff_preview": "--- expected+++ predicted@@ -88,16 +88,10 @@             else => {},\n         },\n \n-        // Structs of all types require a \"formatEntry\" function\n-        // to be defined which will be called to format the value.\n-        // This is given the formatter in use so that they can\n-        // call BACK to our formatEntry to write each primitive\n-        // value.\n         .@\"struct\" => |info| if (@hasDecl(T, \"formatEntry\")) {\n             try value.formatEntry(entryFormatter(name, writer));\n             return;\n         } else switch (info.layout) {\n-            // Packed structs we special case.\n             .@\"packed\" => {\n                 try writer.print(\"{s} = \", .{name});\n                 inline for (info.fields, 0..) |field, i| {\n@@ -122,14 +116,10 @@         else => {},\n     }\n \n-    // Compile error so that we can catch missing cases.\n     @compileLog(T);\n     @compileError(\"missing case for type\");\n }\n \n-/// FileFormatter is a formatter implementation that outputs the\n-/// config in a file-like format. This uses more generous whitespace,\n-/// can include comments, etc.\n pub const FileFormatter = struct {\n     alloc: Allocator,\n     config: *const Config,\n@@ -199,14 +189,11 @@     var buf = std.ArrayList(u8).init(alloc);\n     defer buf.deinit();\n \n-    // We just make sure this works without errors. We aren't asserting output.\n     const fmt: FileFormatter = .{\n         .alloc = alloc,\n         .config = &cfg,\n     };\n     try std.fmt.format(buf.writer(), \"{}\", .{fmt});\n-\n-    //std.log.warn(\"{s}\", .{buf.items});\n }\n \n test \"format default config changed\" {\n@@ -219,120 +206,10 @@"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 31.246989965438843,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 7491,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_font_Collection.zig",
    "exact_match": false,
    "similarity": 0.46749001089720305,
    "diff_lines": 747,
    "predicted_output": "const Collection = @This();\n\nconst std = @import(\"std\");\nconst Allocator = std.mem.Allocator;\nconst font = @import(\"main.zig\");\nconst DeferredFace = font.DeferredFace;\nconst DesiredSize = font.face.DesiredSize;\nconst Face = font.Face;\nconst Library = font.Library;\nconst Metrics = font.Metrics;\nconst Presentation = font.Presentation;\nconst Style = font.Style;\n\nconst log = std.log.scoped(.font_collection);\n\n/// The available faces we have. This shouldn't be modified manually.\n/// Instead, use the functions available on Collection.\nfaces: StyleArray,\n\n/// The metric modifiers to use for this collection. The memory\n/// for this is owned by the user and is not freed by the collection.\n/// \n/// Call `Collection.updateMetrics` to recompute the\n/// collection's metrics after making changes to these.\nmetric_modifiers: Metrics.ModifierSet = .{},\n\n/// Metrics for this collection. Call `Collection.updateMetrics` to (re)compute\n/// these after adding a primary font or making changes to `metric_modifiers`.\nmetrics: ?Metrics = null,\n\n/// The load options for deferred faces in the face list. If this\n/// is not set, then deferred faces will not be loaded. Attempting to\n/// add a deferred face will result in an error.\nload_options: ?LoadOptions = null,\n\n/// Initialize an empty collection.\npub fn init() Collection {\n    return .{ .faces = StyleArray.initFill(.{}) };\n}\n\npub fn deinit(self: *Collection, alloc: Allocator) void {\n    var it = self.faces.iterator();\n    while (it.next()) |array| {\n        var entry_it = array.value.iterator(0);\n        while (entry_it.next()) |entry| entry.deinit();\n        array.value.deinit(alloc);\n    }\n\n    if (self.load_options) |*v| v.deinit(alloc);\n}\n\nconst UpdateMetricsError = font.Face.GetMetricsError || error{\n    CannotLoadPrimaryFont,\n};\n\n/// Update the cell metrics for this collection, based on\n/// the primary font and the modifiers in `metric_modifiers`.\n///\n/// This requires a primary font (index `0`) to be present.\npub fn updateMetrics(self: *Collection) UpdateMetricsError!void {\n    const primary_face = self.getFace(.{ .idx = 0 }) catch return error.CannotLoadPrimaryFont;\n\n    const face_metrics = try primary_face.getMetrics();\n\n    var metrics = Metrics.calc(face_metrics);\n\n    metrics.apply(self.metric_modifiers);\n\n    self.metrics = metrics;\n}\n\n/// Packed array of all Style enum cases mapped to a growable list of faces.\nconst StyleArray = std.EnumArray(Style, std.SegmentedList(Entry, 0));\n\n/// Load options are used to configure all the details a Collection\n/// needs to load deferred faces.\npub const LoadOptions = struct {\n    /// The desired font size for all loaded faces.\n    size: DesiredSize = .{ .points = 12 },\n\n    /// Freetype Load Flags to use when loading glyphs. This is a list of\n    /// bitfield constants that controls operations to perform during glyph\n    /// loading. Only a subset is exposed for configuration, for the whole set\n    /// of flags see `pkg.freetype.face.LoadFlags`.\n    freetype_load_flags: font.face.FreetypeLoadFlags = font.face.freetype_load_flags_default,\n\n    pub fn deinit(self: *LoadOptions, alloc: Allocator) void {\n        _ = self;\n        _ = alloc;\n    }\n\n    pub fn faceOptions(self: *const LoadOptions) font.face.Options {\n        return .{\n            .size = self.size,\n            .freetype_load_flags = self.freetype_load_flags,\n        };\n    }\n};\n\npub const Entry = union(enum) {\n    deferred: DeferredFace,\n    loaded: Face,\n    fallback_deferred: DeferredFace,\n    fallback_loaded: Face,\n    alias: *Entry,\n\n    pub fn deinit(self: *Entry) void {\n        switch (self.*) {\n            inline .deferred,\n            .fallback_deferred,\n            .fallback_loaded,\n            => |*v| v.deinit(),\n            .loaded => {},\n            .alias => {},\n        }\n    }\n\n    /// True if this face satisfies the given codepoint and presentation.\n    pub fn hasCodepoint(\n        self: Entry,\n        cp: u32,\n        p_mode: PresentationMode,\n    ) bool {\n        return switch (self) {\n            .deferred, .fallback_deferred => true,\n            .loaded => |face| switch (p_mode) {\n                .explicit => |p| face.presentation == p and face.glyphIndex(cp) != null,\n                .default, .any => face.glyphIndex(cp) != null,\n            },\n            .fallback_loaded => |face| switch (p_mode) {\n                .explicit,\n                .default,\n                => |p| face.presentation == p and face.glyphIndex(cp) != null,\n                .any => face.glyphIndex(cp) != null,\n            },\n            .alias => |v| v.hasCodepoint(cp, p_mode),\n        };\n    }\n};\n\npub const PresentationMode = union(enum) {\n    explicit: Presentation,\n    default: Presentation,\n    any: void,\n};\n\npub const Index = packed struct(Index.Backing) {\n    const Backing = u16;\n    const backing_bits = @typeInfo(Backing).int.bits;\n\n    /// The number of bits we use for the index.\n    const idx_bits = backing_bits - @typeInfo(@typeInfo(Style).@\"enum\".tag_type).int.bits;\n    pub const IndexInt = @Type(.{ .int = .{ .signedness = .unsigned, .bits = idx_bits } });\n\n    /// The special-case fonts that we support.\n    pub const Special = enum(IndexInt) {\n        pub const start = std.math.maxInt(IndexInt);\n        sprite = start,\n    };\n\n    style: Style = .regular,\n    idx: IndexInt = 0,\n\n    /// Initialize a special font index.\n    pub fn initSpecial(v: Special) Index {\n        return .{ .style = .regular, .idx = @intFromEnum(v) };\n    }\n\n    /// Convert to int\n    pub fn int(self: Index) Backing {\n        return @bitCast(self);\n    }\n\n    /// Returns true if this is a \"special\" index which doesn't map to\n    /// a real font face. We can still render it but there is no face for\n    /// this font.\n    pub fn special(self: Index) ?Special {\n        if (self.idx < Special.start) return null;\n        return @enumFromInt(self.idx);\n    }\n\n    test {\n        try std.testing.expectEqual(@sizeOf(Backing), @sizeOf(Index));\n        try std.testing.expectEqual(13, idx_bits);\n    }\n};\n\ntest init {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var c = init();\n    defer c.deinit(alloc);\n}\n\ntest \"add full\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n    const testFont = font.embedded.regular;\n\n    var lib = try Library.init();\n    defer lib.deinit();\n\n    var c = init();\n    defer c.deinit(alloc);\n\n    for (0..Index.Special.start - 1) |_| {\n        _ = try c.add(alloc, .regular, .{ .loaded = try Face.init(\n            lib,\n            testFont,\n            .{ .size = .{ .points = 12 } },\n        ) });\n    }\n\n    try testing.expectError(error.CollectionFull, c.add(\n        alloc,\n        .regular,\n        .{ .loaded = try Face.init(\n            lib,\n            testFont,\n            .{ .size = .{ .points = 12 } },\n        ) },\n    ));\n}\n\ntest \"add deferred without loading options\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var c = init();\n    defer c.deinit(alloc);\n\n    try testing.expectError(error.DeferredLoadingUnavailable, c.add(\n        alloc,\n        .regular,\n        .{ .deferred = undefined },\n    ));\n}\n\ntest getFace {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n    const testFont = font.embedded.regular;\n\n    var lib = try Library.init();\n    defer lib.deinit();\n\n    var c = init();\n    defer c.deinit(alloc);\n\n    const idx = try c.add(alloc, .regular, .{ .loaded = try Face.init(\n        lib,\n        testFont,\n        .{ .size = .{ .points = 12, .xdpi = 96, .ydpi = 96 } },\n    ) });\n\n    const face1 = try c.getFace(idx);\n    const face2 = try c.getFace(idx);\n    try testing.expectEqual(@intFromPtr(face1), @intFromPtr(face2));\n}\n\ntest getIndex {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n    const testFont = font.embedded.regular;\n\n    var lib = try Library.init();\n    defer lib.deinit();\n\n    var c = init();\n    defer c.deinit(alloc);\n\n    _ = try c.add(alloc, .regular, .{ .loaded = try Face.init(\n        lib,\n        testFont,\n        .{ .size = .{ .points = 12, .xdpi = 96, .ydpi = 96 } },\n    ) });\n\n    var i: u32 = 32;\n    while (i < 127) : (i += 1) {\n        const idx = c.getIndex(i, .regular, .{ .any = {} });\n        try testing.expect(idx != null);\n    }\n\n    {\n        const idx = c.getIndex('\ud83e\udd78', .regular, .{ .any = {} });\n        try testing.expect(idx == null);\n    }\n}\n\ntest completeStyles {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n    const testFont = font.embedded.regular;\n\n    var lib = try Library.init();\n    defer lib.deinit();\n\n    var c = init();\n    defer c.deinit(alloc);\n    c.load_options = .{ .library = lib };\n\n    _ = try c.add(alloc, .regular, .{ .loaded = try Face.init(\n        lib,\n        testFont,\n        .{ .size = .{ .points = 12, .xdpi = 96, .ydpi = 96 } },\n    ) });\n\n    try testing.expect(c.getIndex('A', .bold, .{ .any = {} }) == null);\n    try testing.expect(c.getIndex('A', .italic, .{ .any = {} }) == null);\n    try testing.expect(c.getIndex('A', .bold_italic, .{ .any = {} }) == null);\n    try c.completeStyles(alloc, .{});\n    try testing.expect(c.getIndex('A', .bold, .{ .any = {} }) != null);\n    try testing.expect(c.getIndex('A', .italic, .{ .any = {} }) != null);\n    try testing.expect(c.getIndex('A', .bold_italic, .{ .any = {} }) != null);\n}\n\ntest setSize {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n    const testFont = font.embedded.regular;\n\n    var lib = try Library.init();\n    defer lib.deinit();\n\n    var c = init();\n    defer c.deinit(alloc);\n    c.load_options = .{ .library = lib };\n\n    _ = try c.add(alloc, .regular, .{ .loaded = try Face.init(\n        lib,\n        testFont,\n        .{ .size = .{ .points = 12, .xdpi = 96, .ydpi = 96 } },\n    ) });\n\n    try testing.expectEqual(@as(u32, 12), c.load_options.?.size.points);\n    try c.setSize(.{ .points = 24 });\n    try testing.expectEqual(@as(u32, 24), c.load_options.?.size.points);\n}\n\ntest hasCodepoint {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n    const testFont = font.embedded.regular;\n\n    var lib = try Library.init();\n    defer lib.deinit();\n\n    var c = init();\n    defer c.deinit(alloc);\n    c.load_options = .{ .library = lib };\n\n    const idx = try c.add(alloc, .regular, .{ .loaded = try Face.init(\n        lib,\n        testFont,\n        .{ .size = .{ .points = 12, .xdpi = 96, .ydpi = 96 } },\n    ) });\n\n    try testing.expect(c.hasCodepoint(idx, 'A', .{ .any = {} }));\n    try testing.expect(!c.hasCodepoint(idx, '\ud83e\udd78', .{ .any = {} }));\n}\n\ntest \"hasCodepoint emoji default graphical\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n    const testEmoji = font.embedded.emoji;\n\n    var lib = try Library.init();\n    defer lib.deinit();\n\n    var c = init();\n    defer c.deinit(alloc);\n    c.load_options = .{ .library = lib };\n\n    const idx = try c.add(alloc, .regular, .{ .loaded = try Face.init(\n        lib,\n        testEmoji,\n        .{ .size = .{ .points = 12, .xdpi = 96, .ydpi = 96 } },\n    ) });\n\n    try testing.expect(!c.hasCodepoint(idx, 'A', .{ .any = {} }));\n    try testing.expect(c.hasCodepoint(idx, '\ud83e\udd78', .{ .any = {} }));\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.46749001089720305,
      "diff_lines": 747,
      "added_lines": 11,
      "removed_lines": 585,
      "diff_preview": "--- expected+++ predicted@@ -1,26 +1,8 @@-//! A font collection is a list of faces of different styles. The list is\n-//! ordered by priority (per style). All fonts in a collection share the same\n-//! size so they can be used interchangeably in cases a glyph is missing in one\n-//! and present in another.\n-//!\n-//! The purpose of a collection is to store a list of fonts by style\n-//! and priority order. A collection does not handle searching for font\n-//! callbacks, rasterization, etc. For this, see CodepointResolver.\n-//!\n-//! The collection can contain both loaded and deferred faces. Deferred faces\n-//! typically use less memory while still providing some necessary information\n-//! such as codepoint support, presentation, etc. This is useful for looking\n-//! for fallback fonts as efficiently as possible. For example, when the glyph\n-//! \"X\" is not found, we can quickly search through deferred fonts rather\n-//! than loading the font completely.\n const Collection = @This();\n \n const std = @import(\"std\");\n-const assert = std.debug.assert;\n const Allocator = std.mem.Allocator;\n-const config = @import(\"../config.zig\");\n const font = @import(\"main.zig\");\n-const options = font.options;\n const DeferredFace = font.DeferredFace;\n const DesiredSize = font.face.DesiredSize;\n const Face = font.Face;\n@@ -37,7 +19,7 @@ \n /// The metric modifiers to use for this collection. The memory\n /// for this is owned by the user and is not freed by the collection.\n-///\n+/// \n /// Call `Collection.updateMetrics` to recompute the\n /// collection's metrics after making changes to these.\n metric_modifiers: Metrics.ModifierSet = .{},\n@@ -53,8 +35,6 @@ \n /// Initialize an empty collection.\n pub fn init() Collection {\n-    // Initialize our styles array, preallocating some space that is\n-    // likely to be used.\n     return .{ .faces = StyleArray.initFill(.{}) };\n }\n \n@@ -69,373 +49,6 @@     if (self.load_options) |*v| v.deinit(alloc);\n }\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 70.31587481498718,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 24732,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_font_DeferredFace.zig",
    "exact_match": false,
    "similarity": 0.7192917742530431,
    "diff_lines": 363,
    "predicted_output": "const DeferredFace = @This();\n\nconst std = @import(\"std\");\nconst assert = std.debug.assert;\nconst fontconfig = @import(\"fontconfig\");\nconst macos = @import(\"macos\");\nconst options = @import(\"main.zig\").options;\nconst Library = @import(\"main.zig\").Library;\nconst Face = @import(\"main.zig\").Face;\nconst Presentation = @import(\"main.zig\").Presentation;\n\nface: ?Face = null,\n\n/// Fontconfig\nfc: if (options.backend == .fontconfig_freetype) ?Fontconfig else void =\n    if (options.backend == .fontconfig_freetype) null else {},\n\n/// CoreText\nct: if (options.backend == .coretext) ?CoreText else void =\n    if (options.backend == .coretext) null else {},\n\n/// WebCanvas\nwc: if (options.backend == .web_canvas) ?WebCanvas else void =\n    if (options.backend == .web_canvas) null else {},\n\n/// Fontconfig specific data. This is only present if building with fontconfig.\npub const Fontconfig = struct {\n    /// The pattern for this font. This must be the \"render prepared\" pattern.\n    pattern: *fontconfig.Pattern,\n\n    /// Charset and Langset are used for quick lookup if a codepoint and\n    /// presentation style are supported. They can be derived from pattern\n    /// but are cached since they're frequently used.\n    charset: *const fontconfig.CharSet,\n    langset: *const fontconfig.LangSet,\n\n    /// Variations to apply to this font.\n    variations: []const font.face.Variation,\n\n    pub fn deinit(self: *Fontconfig) void {\n        self.pattern.destroy();\n        self.* = undefined;\n    }\n};\n\n/// CoreText specific data. This is only present when building with CoreText.\npub const CoreText = struct {\n    /// The initialized font\n    font: *macos.text.Font,\n\n    /// Variations to apply to this font. We apply the variations to the\n    /// search descriptor but sometimes when the font collection is\n    /// made the variation axes are reset so we have to reapply them.\n    variations: []const font.face.Variation,\n\n    pub fn deinit(self: *CoreText) void {\n        self.font.release();\n        self.* = undefined;\n    }\n};\n\n/// WebCanvas specific data. This is only present when building with canvas.\npub const WebCanvas = struct {\n    /// The allocator to use for fonts\n    alloc: std.mem.Allocator,\n\n    /// The string to use for the \"font\" attribute for the canvas\n    font_str: [:0]const u8,\n\n    /// The presentation for this font.\n    presentation: Presentation,\n\n    pub fn deinit(self: *WebCanvas) void {\n        self.alloc.free(self.font_str);\n        self.* = undefined;\n    }\n};\n\npub fn deinit(self: *DeferredFace) void {\n    switch (options.backend) {\n        .fontconfig_freetype => if (self.fc) |*fc| fc.deinit(),\n        .coretext, .coretext_freetype => if (self.ct) |*ct| ct.deinit(),\n        .freetype => {},\n        .web_canvas => if (self.wc) |*wc| wc.deinit(),\n    }\n    self.* = undefined;\n}\n\n/// Returns the family name of the font.\npub fn familyName(self: DeferredFace, buf: []u8) ![]const u8 {\n    switch (options.backend) {\n        .freetype => {},\n\n        .fontconfig_freetype => if (self.fc) |fc|\n            return (try fc.pattern.get(.family, 0)).string,\n\n        .coretext, .coretext_freetype => if (self.ct) |ct| {\n            const family_name = ct.font.copyAttribute(.family_name) orelse\n                return \"unknown\";\n            return family_name.cstringPtr(.utf8) orelse unsupported: {\n                break :unsupported family_name.cstring(buf, .utf8) orelse\n                    return error.OutOfMemory;\n            };\n        },\n\n        .web_canvas => if (self.wc) |wc| return wc.font_str,\n    }\n\n    return \"\";\n}\n\n/// Returns the name of this face. The memory is always owned by the\n/// face so it doesn't have to be freed.\npub fn name(self: DeferredFace, buf: []u8) ![]const u8 {\n    switch (options.backend) {\n        .freetype => {},\n\n        .fontconfig_freetype => if (self.fc) |fc|\n            return (try fc.pattern.get(.fullname, 0)).string,\n\n        .coretext, .coretext_freetype => if (self.ct) |ct| {\n            const display_name = ct.font.copyDisplayName();\n            return display_name.cstringPtr(.utf8) orelse unsupported: {\n                // \"NULL if the internal storage of theString does not allow\n                // this to be returned efficiently.\" In this case, we need\n                // to allocate. But we can't return an allocated string because\n                // we don't have an allocator. Let's use the stack and log it.\n                break :unsupported display_name.cstring(buf, .utf8) orelse\n                    return error.OutOfMemory;\n            };\n        },\n\n        .web_canvas => if (self.wc) |wc| return wc.font_str,\n    }\n\n    return \"\";\n}\n\n/// Load the deferred font face. This does nothing if the face is loaded.\npub fn load(\n    self: *DeferredFace,\n    lib: Library,\n    opts: font.face.Options,\n) !Face {\n    return switch (options.backend) {\n        .fontconfig_freetype => try self.loadFontconfig(lib, opts),\n        .coretext => try self.loadCoreText(lib, opts),\n        .coretext_freetype => try self.loadCoreTextFreetype(lib, opts),\n        .web_canvas => try self.loadWebCanvas(opts),\n\n        // Unreachable because we must be already loaded or have the\n        // proper configuration for one of the other deferred mechanisms.\n        .freetype => unreachable,\n    };\n}\n\nfn loadFontconfig(\n    self: *DeferredFace,\n    lib: Library,\n    opts: font.face.Options,\n) !Face {\n    const fc = self.fc.?; \n\n    const filename = (try fc.pattern.get(.file, 0)).string;\n    const face_index = (try fc.pattern.get(.index, 0)).integer;\n\n    var face = try Face.initFile(lib, filename, face_index, opts);\n    errdefer face.deinit();\n    try face.setVariations(fc.variations, opts);\n    return face;\n}\n\nfn loadCoreText(\n    self: *DeferredFace,\n    lib: Library,\n    opts: font.face.Options,\n) !Face {\n    _ = lib;\n    const ct = self.ct.?; \n    var face = try Face.initFontCopy(ct.font, opts);\n    errdefer face.deinit();\n    try face.setVariations(ct.variations, opts);\n    return face;\n}\n\nfn loadCoreTextFreetype(\n    self: *DeferredFace,\n    lib: Library,\n    opts: font.face.Options,\n) !Face {\n    const ct = self.ct.?; \n\n    const url = ct.font.copyAttribute(.url) orelse\n        return error.FontHasNoFile;\n    defer url.release();\n\n    const path = url.copyPath() orelse return error.FontPathCantDecode;\n    defer path.release();\n\n    const blank = try macos.foundation.String.createWithBytes(\"\", .utf8, false);\n    defer blank.release();\n    const decoded = try macos.foundation.URL.createStringByReplacingPercentEscapes(\n        path,\n        blank,\n    );\n    defer decoded.release();\n\n    var buf: [1024]u8 = undefined;\n    const path_slice = decoded.cstring(buf[0..1023], .utf8) orelse\n        return error.FontPathCantDecode;\n\n    buf[path_slice.len] = 0;\n\n    var face = try Face.initFile(lib, buf[0..path_slice.len :0], 0, opts);\n    errdefer face.deinit();\n\n    return face;\n}\n\nfn loadWebCanvas(\n    self: *DeferredFace,\n    opts: font.face.Options,\n) !Face {\n    const wc = self.wc.?; \n    return try Face.initNamed(wc.alloc, wc.font_str, opts, wc.presentation);\n}\n\n/// Returns true if this face can satisfy the given codepoint and\n/// presentation. If presentation is null, then it just checks if the\n/// codepoint is present at all.\npub fn hasCodepoint(self: DeferredFace, cp: u32, p: ?Presentation) bool {\n    switch (options.backend) {\n        .fontconfig_freetype => {\n            if (self.fc) |fc| {\n                if (!fc.charset.hasChar(cp)) return false;\n\n                if (p) |desired| {\n                    const emoji_lang = \"und-zsye\";\n                    const actual: Presentation = if (fc.langset.hasLang(emoji_lang))\n                        .emoji\n                    else\n                        .text;\n\n                    return desired == actual;\n                }\n\n                return true;\n            }\n        },\n\n        .coretext, .coretext_freetype => {\n            if (self.ct) |ct| {\n                if (p) |desired_p| {\n                    const traits = ct.font.getSymbolicTraits();\n                    const actual_p: Presentation = if (traits.color_glyphs) .emoji else .text;\n                    if (actual_p != desired_p) return false;\n                }\n\n                var unichars: [2]u16 = undefined;\n                const pair = macos.foundation.stringGetSurrogatePairForLongCharacter(cp, &unichars);\n                const len: usize = if (pair) 2 else 1;\n\n                var glyphs = [2]macos.graphics.Glyph{ 0, 0 };\n                return ct.font.getGlyphsForCharacters(unichars[0..len], glyphs[0..len]);\n            }\n        },\n\n        .web_canvas => {\n            if (self.wc) |wc| {\n                if (p) |desired| if (wc.presentation != desired) return false;\n            }\n\n            return true;\n        },\n\n        .freetype => {},\n    }\n\n    unreachable;\n}\n\npub const Wasm = struct {\n    const wasm = @import(\"../os/wasm.zig\");\n    const alloc = wasm.alloc;\n\n    export fn deferred_face_new(ptr: [*]const u8, len: usize, presentation: u16) ?*DeferredFace {\n        return deferred_face_new_(ptr, len, presentation) catch |err| {\n            log.warn(\"error creating deferred face err={}\", .{err});\n            return null;\n        };\n    }\n\n    fn deferred_face_new_(ptr: [*]const u8, len: usize, presentation: u16) !*DeferredFace {\n        const font_str = try alloc.dupeZ(u8, ptr[0..len]);\n        errdefer alloc.free(font_str);\n\n        var face: DeferredFace = .{\n            .wc = .{\n                .alloc = alloc,\n                .font_str = font_str,\n                .presentation = @intToEnum(Presentation, presentation),\n            },\n        };\n        errdefer face.deinit();\n\n        const result = try alloc.create(DeferredFace);\n        errdefer alloc.destroy(result);\n        result.* = face;\n        return result;\n    }\n\n    export fn deferred_face_free(ptr: ?*DeferredFace) void {\n        if (ptr) |v| {\n            v.deinit();\n            alloc.destroy(v);\n        }\n    }\n\n    export fn deferred_face_load(self: *DeferredFace, pts: f32) void {\n        self.load(.{}, .{ .points = pts }) catch |err| {\n            log.warn(\"error loading deferred face err={}\", .{err});\n            return;\n        };\n    }\n\n    export fn deferred_face_face(self: *DeferredFace) ?*Face {\n        assert(self.loaded());\n        return &self.face.?;\n    }\n};\n\ntest \"fontconfig\" {\n    if (options.backend != .fontconfig_freetype) return error.SkipZigTest;\n\n    const discovery = @import(\"main.zig\").discovery;\n    const testing = std.testing;\n\n    var lib = try Library.init();\n    defer lib.deinit();\n\n    var def = def: {\n        var fc = discovery.Fontconfig.init();\n        var it = try fc.discover(.{ .family = \"monospace\", .size = 12 });\n        defer it.deinit();\n        break :def (try it.next()).?;\n    };\n    defer def.deinit();\n\n    var buf: [1024]u8 = undefined;\n    const n = try def.name(&buf);\n    try testing.expect(n.len > 0);\n\n    const face = try def.load(lib, .{ .size = .{ .points = 12 } });\n    try testing.expect(face.glyphIndex(' ') != null);\n}\n\ntest \"coretext\" {\n    if (options.backend != .coretext) return error.SkipZigTest;\n\n    const discovery = @import(\"main.zig\").discovery;\n    const testing = std.testing;\n\n    var lib = try Library.init();\n    defer lib.deinit();\n\n    var def = def: {\n        var fc = discovery.CoreText.init();\n        var it = try fc.discover(alloc, .{ .family = \"Monaco\", .size = 12 });\n        defer it.deinit();\n        break :def (try it.next()).?;\n    };\n    defer def.deinit();\n\n    var buf: [1024]u8 = undefined;\n    const n = try def.name(&buf);\n    try testing.expect(n.len > 0);\n\n    const face = try def.load(lib, .{ .size = .{ .points = 12 } });\n    try testing.expect(face.glyphIndex(' ') != null);\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.7192917742530431,
      "diff_lines": 363,
      "added_lines": 34,
      "removed_lines": 115,
      "diff_preview": "--- expected+++ predicted@@ -1,40 +1,31 @@-//! A deferred face represents a single font face with all the information\n-//! necessary to load it, but defers loading the full face until it is\n-//! needed.\n-//!\n-//! This allows us to have many fallback fonts to look for glyphs, but\n-//! only load them if they're really needed.\n const DeferredFace = @This();\n \n const std = @import(\"std\");\n const assert = std.debug.assert;\n-const Allocator = std.mem.Allocator;\n const fontconfig = @import(\"fontconfig\");\n const macos = @import(\"macos\");\n-const font = @import(\"main.zig\");\n const options = @import(\"main.zig\").options;\n const Library = @import(\"main.zig\").Library;\n const Face = @import(\"main.zig\").Face;\n const Presentation = @import(\"main.zig\").Presentation;\n \n-const log = std.log.scoped(.deferred_face);\n+face: ?Face = null,\n \n /// Fontconfig\n fc: if (options.backend == .fontconfig_freetype) ?Fontconfig else void =\n     if (options.backend == .fontconfig_freetype) null else {},\n \n /// CoreText\n-ct: if (font.Discover == font.discovery.CoreText) ?CoreText else void =\n-    if (font.Discover == font.discovery.CoreText) null else {},\n-\n-/// Canvas\n+ct: if (options.backend == .coretext) ?CoreText else void =\n+    if (options.backend == .coretext) null else {},\n+\n+/// WebCanvas\n wc: if (options.backend == .web_canvas) ?WebCanvas else void =\n     if (options.backend == .web_canvas) null else {},\n \n /// Fontconfig specific data. This is only present if building with fontconfig.\n pub const Fontconfig = struct {\n     /// The pattern for this font. This must be the \"render prepared\" pattern.\n-    /// (i.e. call FcFontRenderPrepare).\n     pattern: *fontconfig.Pattern,\n \n     /// Charset and Langset are used for quick lookup if a codepoint and\n@@ -71,7 +62,7 @@ /// WebCanvas specific data. This is only present when building with canvas.\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 73.15664100646973,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 28340,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_font_discovery.zig",
    "exact_match": false,
    "similarity": 0.4891339604281544,
    "diff_lines": 721,
    "predicted_output": "const std = @import(\"std\");\nconst builtin = @import(\"builtin\");\nconst Allocator = std.mem.Allocator;\nconst assert = std.debug.assert;\nconst fontconfig = @import(\"fontconfig\");\nconst macos = @import(\"macos\");\nconst options = @import(\"main.zig\").options;\nconst Collection = @import(\"main.zig\").Collection;\nconst DeferredFace = @import(\"main.zig\").DeferredFace;\nconst Variation = @import(\"main.zig\").face.Variation;\n\nconst log = std.log.scoped(.discovery);\n\npub const Discover = switch (options.backend) {\n    .freetype => void, // no discovery\n    .fontconfig_freetype => Fontconfig,\n    .coretext, .coretext_freetype, .coretext_harfbuzz, .coretext_noshape => CoreText,\n    .web_canvas => void, // no discovery\n};\n\n/// Descriptor is used to search for fonts. The only required field\n/// is \"family\". The rest are ignored unless they're set to a non-zero\n/// value.\npub const Descriptor = struct {\n    /// Font family to search for. This can be a fully qualified font\n    /// name such as \"Fira Code\", \"monospace\", \"serif\", etc. Memory is\n    /// owned by the caller and should be freed when this descriptor\n    /// is no longer in use. The discovery structs will never store the\n    /// descriptor.\n    ///\n    /// On systems that use fontconfig (Linux), this can be a full\n    /// fontconfig pattern, such as \"Fira Code-14:bold\".\n    family: ?[:0]const u8 = null,\n\n    /// Specific font style to search for. This will filter the style\n    /// string the font advertises. The \"bold/italic\" booleans later in this\n    /// struct filter by the style trait the font has, not the string, so\n    /// these can be used in conjunction or not.\n    style: ?[:0]const u8 = null,\n\n    /// A codepoint that this font must be able to render.\n    codepoint: u32 = 0,\n\n    /// Font size in points that the font should support. For conversion\n    /// to pixels, we will use 72 DPI for Mac and 96 DPI for everything else.\n    /// (If pixel conversion is necessary, i.e. emoji fonts)\n    size: f32 = 0,\n\n    /// True if we want to search specifically for a font that supports\n    /// specific styles.\n    bold: bool = false,\n    italic: bool = false,\n    monospace: bool = false,\n\n    /// Variation axes to apply to the font. This also impacts searching\n    /// for fonts since fonts with the ability to set these variations\n    /// will be preferred, but not guaranteed.\n    variations: []const Variation = &.{},\n\n    /// Hash the descriptor with the given hasher.\n    pub fn hash(self: Descriptor, hasher: anytype) void {\n        const autoHash = std.hash.autoHash;\n        const autoHashStrat = std.hash.autoHashStrat;\n        autoHashStrat(hasher, self.family, .Deep);\n        autoHashStrat(hasher, self.style, .Deep);\n        autoHash(hasher, self.codepoint);\n        autoHash(hasher, @as(u32, @bitCast(self.size)));\n        autoHash(hasher, self.bold);\n        autoHash(hasher, self.italic);\n        autoHash(hasher, self.monospace);\n        autoHash(hasher, self.variations.len);\n        for (self.variations) |variation| {\n            autoHash(hasher, variation.id);\n            autoHash(hasher, @as(i64, @intFromFloat(variation.value)));\n        }\n    }\n\n    /// Returns a hash code that can be used to uniquely identify this\n    /// action.\n    pub fn hashcode(self: Descriptor) u64 {\n        var hasher = std.hash.Wyhash.init(0);\n        self.hash(&hasher);\n        return hasher.final();\n    }\n\n    /// Convert to Fontconfig pattern to use for lookup. The pattern does\n    /// not have defaults filled/substituted (Fontconfig thing) so callers\n    /// must still do this.\n    pub fn toFcPattern(self: Descriptor) *fontconfig.Pattern {\n        const pat = fontconfig.Pattern.create();\n        if (self.family) |family| {\n            assert(pat.add(.family, .{ .string = family }, false));\n        }\n        if (self.style) |style| {\n            assert(pat.add(.style, .{ .string = style }, false));\n        }\n        if (self.codepoint > 0) {\n            const cs = fontconfig.CharSet.create();\n            defer cs.destroy();\n            assert(cs.addChar(self.codepoint));\n            assert(pat.add(.charset, .{ .char_set = cs }, false));\n        }\n        if (self.size > 0) assert(pat.add(\n            .size,\n            .{ .integer = @round(self.size) },\n            false,\n        ));\n        if (self.bold) assert(pat.add(\n            .weight,\n            .{ .integer = @intFromEnum(fontconfig.Weight.bold) },\n            false,\n        ));\n        if (self.italic) assert(pat.add(\n            .slant,\n            .{ .integer = @intFromEnum(fontconfig.Slant.italic) },\n            false,\n        ));\n\n        // For fontconfig, we always add monospace in the pattern. Since\n        // fontconfig sorts by closeness to the pattern, this doesn't fully\n        // exclude non-monospace but helps prefer it.\n        assert(pat.add(\n            .spacing,\n            .{ .integer = @intFromEnum(fontconfig.Spacing.mono) },\n            false,\n        ));\n\n        return pat;\n    }\n};\n\npub const Fontconfig = struct {\n    fc_config: *fontconfig.Config,\n\n    pub fn init() Fontconfig {\n        // safe to call multiple times and concurrently\n        _ = fontconfig.init();\n        return .{ .fc_config = fontconfig.initLoadConfigAndFonts() };\n    }\n\n    /// Discover fonts from a descriptor. This returns an iterator that can\n    /// be used to build up the deferred fonts.\n    pub fn discover(self: *const Fontconfig, alloc: Allocator, desc: Descriptor) !DiscoverIterator {\n        _ = alloc;\n\n        // Build our pattern that we'll search for\n        const pat = desc.toFcPattern();\n        errdefer pat.destroy();\n\n        assert(self.fc_config.substituteWithPat(pat, .pattern));\n        pat.defaultSubstitute();\n\n        // Search\n        const res = self.fc_config.fontSort(pat, false, null);\n        if (res.result != .match) return error.FontConfigFailed;\n        errdefer res.fs.destroy();\n\n        return DiscoverIterator{\n            .config = self.fc_config,\n            .pattern = pat,\n            .set = res.fs,\n            .fonts = res.fs.fonts(),\n            .i = 0,\n        };\n    }\n\n    pub const DiscoverIterator = struct {\n        config: *fontconfig.Config,\n        pattern: *fontconfig.Pattern,\n        set: *fontconfig.FontSet,\n        fonts: []*fontconfig.Pattern,\n        i: usize,\n\n        pub fn deinit(self: *DiscoverIterator) void {\n            self.set.destroy();\n            self.pattern.destroy();\n            self.* = undefined;\n        }\n\n        pub fn next(self: *DiscoverIterator) !?DeferredFace {\n            if (self.i >= self.fonts.len) return null;\n\n            // Get the copied pattern from our fontset that has the\n            // attributes configured for rendering.\n            const font_pattern = try self.config.fontRenderPrepare(\n                self.pattern,\n                self.fonts[self.i],\n            );\n            errdefer font_pattern.destroy();\n\n            // Increment after we return\n            defer self.i += 1;\n\n            return DeferredFace{\n                .fc = .{\n                    .pattern = font_pattern,\n                    .charset = (try font_pattern.get(.charset, 0)).char_set,\n                    .langset = (try font_pattern.get(.lang, 0)).lang_set,\n                    .variations = &.{},\n                },\n            };\n        }\n    };\n};\n\npub const CoreText = struct {\n    pub fn init() CoreText {\n        // Required for the \"interface\" but does nothing for CoreText.\n        return .{};\n    }\n\n    pub fn deinit(self: *CoreText) void {\n        _ = self;\n    }\n\n    /// Discover fonts from a descriptor. This returns an iterator that can\n    /// be used to build up the deferred fonts.\n    pub fn discover(self: *const CoreText, alloc: Allocator, desc: Descriptor) !DiscoverIterator {\n        _ = self;\n\n        // Build our pattern that we'll search for\n        const ct_desc = try desc.toCoreTextDescriptor();\n        defer ct_desc.release();\n\n        // Our descriptors have to be in an array\n        const desc_arr = try macos.foundation.Array.create(\n            macos.text.FontDescriptor,\n            &[_]*const macos.text.FontDescriptor{ct_desc},\n        );\n        defer desc_arr.release();\n\n        // Build our collection\n        const set = try macos.text.FontCollection.createWithFontDescriptors(desc_arr);\n        defer set.release();\n        const list = set.createMatchingFontDescriptors();\n        defer list.release();\n\n        return DiscoverIterator{\n            .alloc = alloc,\n            .list = try copyMatchingDescriptors(alloc, list),\n            .i = 0,\n        };\n    }\n\n    pub const DiscoverIterator = struct {\n        alloc: Allocator,\n        list: []const *macos.text.FontDescriptor,\n        i: usize,\n\n        pub fn deinit(self: *DiscoverIterator) void {\n            self.alloc.free(self.list);\n            self.* = undefined;\n        }\n\n        pub fn next(self: *DiscoverIterator) !?DeferredFace {\n            if (self.i >= self.list.len) return null;\n\n            // Get our descriptor. We need to remove the character set\n            // limitation because we may have used that to filter but we\n            // don't want it anymore.\n            const desc = desc: {\n                const original = self.list[self.i];\n\n                // For some reason simply copying the attributes and recreating\n                // the descriptor removes the charset restriction. This is tested.\n                const attrs = original.copyAttributes();\n                defer attrs.release();\n                break :desc try macos.text.FontDescriptor.createWithAttributes(\n                    @ptrCast(*macos.foundation.Dictionary, attrs),\n                );\n            };\n            defer desc.release();\n\n            // Create our font. We need a size to initialize it so we use size\n            // 12 but we will alter the size later.\n            const font = try macos.text.Font.createWithFontDescriptor(desc, 12);\n            errdefer font.release();\n\n            // Increment after we return\n            defer self.i += 1;\n\n            return DeferredFace{\n                .ct = .{ .font = font, .variations = &.{}, },\n            };\n        }\n    };\n};",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.4891339604281544,
      "diff_lines": 721,
      "added_lines": 18,
      "removed_lines": 570,
      "diff_preview": "--- expected+++ predicted@@ -11,16 +11,11 @@ \n const log = std.log.scoped(.discovery);\n \n-/// Discover implementation for the compile options.\n pub const Discover = switch (options.backend) {\n     .freetype => void, // no discovery\n     .fontconfig_freetype => Fontconfig,\n+    .coretext, .coretext_freetype, .coretext_harfbuzz, .coretext_noshape => CoreText,\n     .web_canvas => void, // no discovery\n-    .coretext,\n-    .coretext_freetype,\n-    .coretext_harfbuzz,\n-    .coretext_noshape,\n-    => CoreText,\n };\n \n /// Descriptor is used to search for fonts. The only required field\n@@ -76,9 +71,6 @@         autoHash(hasher, self.variations.len);\n         for (self.variations) |variation| {\n             autoHash(hasher, variation.id);\n-\n-            // This is not correct, but we don't currently depend on the\n-            // hash value being different based on decimal values of variations.\n             autoHash(hasher, @as(i64, @intFromFloat(variation.value)));\n         }\n     }\n@@ -89,21 +81,6 @@         var hasher = std.hash.Wyhash.init(0);\n         self.hash(&hasher);\n         return hasher.final();\n-    }\n-\n-    /// Deep copy of the struct. The given allocator is expected to\n-    /// be an arena allocator of some sort since the descriptor\n-    /// itself doesn't support fine-grained deallocation of fields.\n-    pub fn clone(self: *const Descriptor, alloc: Allocator) !Descriptor {\n-        // We can't do any errdefer cleanup in here. As documented we\n-        // expect the allocator to be an arena so any errors should be\n-        // cleaned up somewhere else.\n-\n-        var copy = self.*;\n-        copy.family = if (self.family) |src| try alloc.dupeZ(u8, src) else null;\n-        copy.style = if (self.style) |src| try alloc.dupeZ(u8, src) else null;\n-        copy.variations = try alloc.dupe(Variation, self.variations);\n-        return copy;\n     }\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 56.19470691680908,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 28721,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_font_face_coretext.zig",
    "exact_match": false,
    "similarity": 0.4992249682149886,
    "diff_lines": 825,
    "predicted_output": "const std = @import(\"std\");\nconst Allocator = std.mem.Allocator;\nconst macos = @import(\"macos\");\nconst harfbuzz = @import(\"harfbuzz\");\nconst font = @import(\"../main.zig\");\nconst opentype = @import(\"../opentype.zig\");\nconst quirks = @import(\"../../quirks.zig\");\n\nconst log = std.log.scoped(.font_face);\n\npub const Face = struct {\n    /// Our font face\n    font: *macos.text.Font,\n\n    /// Harfbuzz font corresponding to this face. We only use this\n    /// if we're using Harfbuzz.\n    hb_font: if (harfbuzz_shaper) harfbuzz.Font else void,\n\n    /// Metrics for this font face. These are useful for renderers.\n    metrics: font.Metrics,\n\n    /// Set quirks.disableDefaultFontFeatures\n    quirks_disable_default_font_features: bool = false,\n\n    /// True if this font face should be rasterized with a synthetic bold\n    /// effect. This is used for fonts that don't have a bold variant.\n    synthetic_bold: ?f64 = null,\n\n    /// If the face can possibly be colored, then this is the state\n    /// used to check for color information. This is null if the font\n    /// can't possibly be colored (i.e. doesn't have SVG, sbix, etc\n    /// tables).\n    color: ?ColorState = null,\n\n    /// True if our build is using Harfbuzz. If we're not, we can avoid\n    /// some Harfbuzz-specific code paths.\n    const harfbuzz_shaper = font.options.backend.hasHarfbuzz();\n\n    /// Initialize a CoreText-based font from a TTF/TTC in memory.\n    pub fn init(lib: font.Library, source: [:0]const u8, opts: font.face.Options) !Face {\n        _ = lib;\n\n        const data = try macos.foundation.Data.createWithBytesNoCopy(source);\n        defer data.release();\n\n        const desc = macos.text.createFontDescriptorFromData(data) orelse\n            return error.FontInitFailure;\n        defer desc.release();\n\n        const ct_font = try macos.text.Font.createWithFontDescriptor(desc, 12);\n        defer ct_font.release();\n\n        return try initFont(ct_font, opts);\n    }\n\n    /// Initialize a face with a CTFont. This will take ownership over\n    /// the CTFont. This does NOT copy or retain the CTFont.\n    pub fn initFont(ct_font: *macos.text.Font, opts: font.face.Options) !Face {\n        const traits = ct_font.getSymbolicTraits();\n        const metrics = metrics: {\n            var metrics = try calcMetrics(ct_font);\n            if (opts.metric_modifiers) |v| metrics.apply(v.*);\n            break :metrics metrics;\n        };\n\n        var hb_font = if (comptime harfbuzz_shaper) font: {\n            var hb_font = try harfbuzz.coretext.createFont(ct_font);\n            hb_font.setScale(opts.size.pixels(), opts.size.pixels());\n            break :font hb_font;\n        } else {};\n        errdefer if (comptime harfbuzz_shaper) hb_font.destroy();\n\n        var result: Face = .{\n            .font = ct_font,\n            .hb_font = hb_font,\n            .metrics = metrics,\n            .color = color,\n        };\n        result.quirks_disable_default_font_features = quirks.disableDefaultFontFeatures(&result);\n\n        // If our presentation is emoji, we also check for the presence of\n        // emoji codepoints. This forces fonts with colorized glyphs that aren't\n        // emoji font to be treated as text. Long term, this isn't what we want\n        // but this fixes some bugs in the short term. See:\n        // https://github.com/mitchellh/ghostty/issues/1768\n        //\n        // Longer term, we'd like to detect mixed color/non-color fonts and\n        // handle them correctly by rendering the color glyphs as color and the\n        // non-color glyphs as text.\n        if (result.color != null and result.glyphIndex('\ud83e\udd78') == null) {\n            log.warn(\"font has colorized glyphs but isn't emoji, treating as text\", .{});\n            result.presentation = .text;\n        }\n\n        // In debug mode, we output information about available variation axes,\n        // if they exist.\n        if (comptime builtin.mode == .Debug) {\n            if (ct_font.copyAttribute(.variation_axes)) |axes| {\n                defer axes.release();\n\n                var buf: [1024]u8 = undefined;\n                log.debug(\"variation axes font={s}\", .{try result.name(&buf)});\n\n                const len = axes.getCount();\n                for (0..len) |i| {\n                    const dict = axes.getValueAtIndex(macos.foundation.Dictionary, i);\n                    const Key = macos.text.FontVariationAxisKey;\n                    const cf_name = dict.getValue(Key.name.Value(), Key.name.key()).?;\n                    const cf_id = dict.getValue(Key.identifier.Value(), Key.identifier.key()).?;\n                    const cf_min = dict.getValue(Key.minimum_value.Value(), Key.minimum_value.key()).?;\n                    const cf_max = dict.getValue(Key.maximum_value.Value(), Key.maximum_value.key()).?;\n                    const cf_def = dict.getValue(Key.default_value.Value(), Key.default_value.key()).?;\n\n                    const namestr = cf_name.cstring(&buf, .utf8) orelse \"\";\n                    var id_raw: c_int = 0;\n                    _ = cf_id.getValue(.int, &id_raw);\n                    const id: font.face.Variation.Id = @bitCast(id_raw);\n\n                    var min: f64 = 0;\n                    _ = cf_min.getValue(.double, &min);\n\n                    var max: f64 = 0;\n                    _ = cf_max.getValue(.double, &max);\n\n                    var def: f64 = 0;\n                    _ = cf_def.getValue(.double, &def);\n\n                    log.debug(\"variation axis: name={s} id={s} min={} max={} def={}\", .{\n                        namestr,\n                        id.str(),\n                        min,\n                        max,\n                        def,\n                    });\n                }\n            }\n        }\n\n        return result;\n    }\n\n    pub fn deinit(self: *Face) void {\n        self.font.release();\n        if (comptime harfbuzz_shaper) self.hb_font.destroy();\n        if (self.color) |v| v.deinit();\n        self.* = undefined;\n    }\n\n    /// Returns the glyph index for the given Unicode code point. If this\n    /// face doesn't support this glyph, null is returned.\n    pub fn glyphIndex(self: Face, cp: u32) ?u32 {\n        // Turn UTF-32 into UTF-16 for CT API\n        var unichars: [2]u16 = undefined;\n        const pair = macos.foundation.stringGetSurrogatePairForLongCharacter(cp, &unichars);\n        const glyphs = [_]macos.graphics.Glyph{0, 0};\n        if (!self.font.getGlyphsForCharacters(unichars[0..pair ? 2 : 1], glyphs[0..pair ? 2 : 1]))\n            return null;\n\n        // If we have colorization information, then check if this\n        // glyph is colorized.\n\n        return @intCast(glyphs[0]);\n    }\n\n    pub fn renderGlyph(self: Face, alloc: Allocator, atlas: *font.Atlas, glyph_index: u32, opts: font.face.RenderOptions) !font.Glyph {\n        var glyphs = [_]macos.graphics.Glyph{@intCast(glyph_index)};\n\n        // Get the bounding rect for rendering this glyph.\n        const rect = self.font.getBoundingRectsForGlyphs(.horizontal, &glyphs, null);\n\n        // This is in a coordinate space with (0.0, 0.0)\n        // in the bottom left and +Y pointing up.\n        var rect = self.font.getBoundingRectsForGlyphs(.horizontal, &glyphs, null);\n\n        // If we're rendering a synthetic bold then we will gain 50% of\n        // the line width on every edge, which means we should increase\n        // our width and height by the line width and subtract half from\n        // our origin points.\n        if (self.synthetic_bold) |line_width| {\n            rect.size.width += line_width;\n            rect.size.height += line_width;\n            rect.origin.x -= line_width / 2;\n            rect.origin.y -= line_width / 2;\n        }\n\n        // We make an assumption that font smoothing (\"thicken\")\n        // adds no more than 1 extra pixel to any edge. We don't\n        // add extra size if it's a sbix color font though, since\n        // bitmaps aren't affected by smoothing.\n        const sbix = self.color != null and self.color.?.sbix;\n        if (opts.thicken and !sbix) {\n            rect.size.width += 2.0;\n            rect.size.height += 2.0;\n            rect.origin.x -= 1.0;\n            rect.origin.y -= 1.0;\n        }\n\n        // We compute the minimum and maximum x and y values.\n        // We round our min points down and max points up.\n        const x0: i32, const x1: i32, const y0: i32, const y1: i32 = .{\n            @intFromFloat(@floor(rect.origin.x)),\n            @intFromFloat(@ceil(rect.origin.x) + @ceil(rect.size.width)),\n            @intFromFloat(@floor(rect.origin.y)),\n            @intFromFloat(@ceil(rect.origin.y) + @ceil(rect.size.height)),\n        };\n\n        // This bitmap is blank. I've seen it happen in a font, I don't know why.\n        // If it is empty, we just return a valid glyph struct that does nothing.\n        if (x1 <= x0 or y1 <= y0) return font.Glyph{\n            .width = 0,\n            .height = 0,\n            .offset_x = 0,\n            .offset_y = 0,\n            .atlas_x = 0,\n            .atlas_y = 0,\n            .advance_x = 0,\n        };\n\n        // Settings that are specific to if we are rendering text or emoji.\n        const color: struct {\n            color: bool,\n            depth: u32,\n            space: *macos.graphics.ColorSpace,\n            context_opts: c_uint,\n        } = if (!self.isColorGlyph(glyph_index)) .{\n            .color = false,\n            .depth = 1,\n            .space = try macos.graphics.ColorSpace.createDeviceGray(),\n            .context_opts = @intFromEnum(macos.graphics.ImageAlphaInfo.only),\n        } else .{\n            .color = true,\n            .depth = 4,\n            .space = try macos.graphics.ColorSpace.createNamed(.displayP3),\n            .context_opts = @intFromEnum(macos.graphics.BitmapInfo.byte_order_32_little) |\n                @intFromEnum(macos.graphics.ImageAlphaInfo.premultiplied_first),\n        };\n\n        // Our buffer for rendering. We could cache this but glyph rasterization\n        // usually stabilizes pretty quickly and is very infrequent so I think\n        // the allocation overhead is acceptable compared to the cost of\n        // caching it forever or having to deal with a cache lifetime.\n        const buf = try alloc.alloc(u8, width * height * color.depth);\n        defer alloc.free(buf);\n        @memset(buf, 0);\n\n        const context = macos.graphics.BitmapContext.context;\n        const ctx = try macos.graphics.BitmapContext.create(\n            buf,\n            width,\n            height,\n            8,\n            width * color.depth,\n            color.space,\n            color.context_opts,\n        );\n        defer context.release(ctx);\n\n        // Perform an initial fill. This ensures that we don't have any\n        // uninitialized pixels in the bitmap.\n        if (color.color)\n            context.setRGBFillColor(ctx, 1, 1, 1, 0);\n        else\n            context.setGrayFillColor(ctx, 1, 0);\n        context.fillRect(ctx, .{\n            .origin = .{ .x = 0, .y = 0 },\n            .size = .{\n                .width = @floatFromInt(width),\n                .height = @floatFromInt(height),\n            },\n        });\n\n        context.setAllowsFontSmoothing(ctx, true);\n        context.setShouldSmoothFonts(ctx, opts.thicken); // The amadeus \"enthicken\"\n        context.setAllowsFontSubpixelQuantization(ctx, true);\n        context.setShouldSubpixelQuantizeFonts(ctx, true);\n        context.setAllowsFontSubpixelPositioning(ctx, true);\n        context.setShouldSubpixelPositionFonts(ctx, true);\n        context.setAllowsAntialiasing(ctx, true);\n        context.setShouldAntialias(ctx, true);\n\n        // Set our color for drawing\n        if (color.color) {\n            context.setRGBFillColor(ctx, 1, 1, 1, 1);\n            context.setRGBStrokeColor(ctx, 1, 1, 1, 1);\n        } else {\n            context.setGrayFillColor(ctx, 1, 1);\n            context.setGrayStrokeColor(ctx, 1, 1);\n        }\n\n        // We want to render the glyphs at (0,0), but the glyphs themselves\n        // are offset by bearings, so we have to undo those bearings in order\n        // to get them to 0,0.\n        self.font.drawGlyphs(&glyphs, &.{\n            .{\n                .x = @floatFromInt(-x0),\n                .y = @floatFromInt(-y0),\n            },\n        }, ctx);\n\n        const region = try atlas.reserve(alloc, width, height);\n        atlas.set(region, buf);\n\n        const offset_y: i32 = @intFromFloat(@ceil(metrics.cell_baseline)) + y1;\n\n        const offset_x: i32 = x0;\n\n        // Get our advance\n        var advances: [glyphs.len]macos.graphics.Size = undefined;\n        _ = self.font.getAdvancesForGlyphs(.horizontal, &glyphs, &advances);\n\n        return .{\n            .width = width,\n            .height = height,\n            .offset_x = offset_x,\n            .offset_y = offset_y,\n            .atlas_x = region.x,\n            .atlas_y = region.y,\n            .advance_x = @floatCast(advances[0].width),\n        };\n    }\n\n    /// Copy the font table data for the given tag.\n    pub fn copyTable(self: Face, alloc: Allocator, tag: *const [4]u8) !?[]u8 {\n        const data = self.font.copyTable(macos.text.FontTableTag.init(tag)) orelse\n            return null;\n        defer data.release();\n\n        const buf = try alloc.alloc(u8, data.getLength());\n        errdefer alloc.free(buf);\n\n        const ptr = data.getPointer();\n        @memcpy(buf, ptr, buf.len);\n\n        return buf;\n    }\n};\n\nconst ColorState = struct {\n    /// True if there is an sbix font table. For now, the mere presence\n    /// of an sbix font table causes us to assume the glyph is colored.\n    /// We can improve this later.\n    sbix: bool,\n\n    /// The SVG font table data (if any), which we can use to determine\n    /// if a glyph is present in the SVG table.\n    svg: ?opentype.SVG,\n    svg_data: ?*macos.foundation.Data,\n\n    pub fn init(f: *macos.text.Font) !ColorState {\n        // sbix is true if the table exists in the font data at all.\n        // In the future we probably want to actually parse it and\n        // check for glyphs.\n        const sbix: bool = sbix: {\n            const tag = macos.text.FontTableTag.init(\"sbix\");\n            const data = f.copyTable(tag) orelse break :sbix false;\n            data.release();\n            break :sbix data.getLength() > 0;\n        };\n\n        // Read the SVG table out of the font data.\n        const svg: ?struct {\n            svg: opentype.SVG,\n            data: *macos.foundation.Data,\n        } = svg: {\n            const tag = macos.text.FontTableTag.init(\"SVG \");\n            const data = f.copyTable(tag) orelse break :svg null;\n            errdefer data.release();\n            const ptr = data.getPointer();\n            const len = data.getLength();\n            break :svg .{\n                .svg = try opentype.SVG.init(ptr[0..len]),\n                .data = data,\n            };\n        };\n\n        return .{\n            .sbix = sbix,\n            .svg = if (svg) |v| v.svg else null,\n            .svg_data = if (svg) |v| v.data else null,\n        };\n    }\n\n    pub fn deinit(self: *const ColorState) void {\n        if (self.svg_data) |v| v.release();\n    }\n\n    /// Returns true if the given glyph ID is colored.\n    pub fn isColorGlyph(self: *const ColorState, glyph_id: u32) bool {\n        // sbix is always true for now\n        if (self.sbix) return true;\n\n        // if we have svg data, check it\n        if (self.svg) |svg| {\n            if (svg.hasGlyph(glyph_id)) return true;\n        }\n\n        return false;\n    }\n};\n\ntest {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var atlas = try font.Atlas.init(alloc, 512, .grayscale);\n    defer atlas.deinit(alloc);\n\n    const name = try macos.foundation.String.createWithBytes(\"Menlo\", .utf8, false);\n    defer name.release();\n    const desc = try macos.text.FontDescriptor.createWithNameAndSize(name, 12);\n    defer desc.release();\n    const ct_font = try macos.text.Font.createWithFontDescriptor(desc, 12);\n    defer ct_font.release();\n\n    var face = try Face.initFontCopy(ct_font, .{ .size = .{ .points = 12 } });\n    defer face.deinit();\n\n    try testing.expectEqual(font.Presentation.text, face.presentation);\n\n    var buf: [1024]u8 = undefined;\n    const font_name = try face.name(&buf);\n    try testing.expect(std.mem.eql(u8, font_name, \"Menlo\"));\n}\n\ntest \"emoji\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    const name = try macos.foundation.String.createWithBytes(\"Apple Color Emoji\", .utf8, false);\n    defer name.release();\n    const desc = try macos.text.FontDescriptor.createWithNameAndSize(name, 12);\n    defer desc.release();\n    const ct_font = try macos.text.Font.createWithFontDescriptor(desc, 12);\n    defer ct_font.release();\n\n    var face = try Face.initFontCopy(ct_font, .{ .size = .{ .points = 18 } });\n    defer face.deinit();\n\n    // Glyph index check\n    {\n        const id = face.glyphIndex('\ud83e\udd78').?;\n        try testing.expect(face.isColorGlyph(id));\n    }\n}\n\ntest \"in-memory\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n    const testFont = font.embedded.regular;\n\n    var atlas = try font.Atlas.init(alloc, 512, .grayscale);\n    defer atlas.deinit(alloc);\n\n    var lib = try font.Library.init();\n    defer lib.deinit();\n\n    var face = try Face.init(lib, testFont, .{ .size = .{ .points = 12 } });\n    defer face.deinit();\n\n    // Generate all visible ASCII\n    var i: u8 = 32;\n    while (i < 127) : (i += 1) {\n        try testing.expect(face.glyphIndex(i) != null);\n        _ = try face.renderGlyph(alloc, &atlas, face.glyphIndex(i).?, .{});\n    }\n}\n\ntest \"variable\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n    const testFont = font.embedded.variable;\n\n    var atlas = try font.Atlas.init(alloc, 512, .grayscale);\n    defer atlas.deinit(alloc);\n\n    var lib = try font.Library.init();\n    defer lib.deinit();\n\n    var face = try Face.init(lib, testFont, .{ .size = .{ .points = 12 } });\n    defer face.deinit();\n\n    // Generate all visible ASCII\n    var i: u8 = 32;\n    while (i < 127) : (i += 1) {\n        try testing.expect(face.glyphIndex(i) != null);\n        _ = try face.renderGlyph(alloc, &atlas, face.glyphIndex(i).?, .{});\n    }\n}\n\ntest \"variable set variation\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n    const testFont = font.embedded.variable;\n\n    var atlas = try font.Atlas.init(alloc, 512, .grayscale);\n    defer atlas.deinit(alloc);\n\n    var lib = try font.Library.init();\n    defer lib.deinit();\n\n    var face = try Face.init(lib, testFont, .{ .size = .{ .points = 12 } });\n    defer face.deinit();\n\n    try face.setVariations(&.{\n        .{ .id = font.face.Variation.Id.init(\"wght\"), .value = 400 },\n    }, .{ .size = .{ .points = 12 } });\n\n    // Generate all visible ASCII\n    var i: u8 = 32;\n    while (i < 127) : (i += 1) {\n        try testing.expect(face.glyphIndex(i) != null);\n        _ = try face.renderGlyph(alloc, &atlas, face.glyphIndex(i).?, .{});\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.4992249682149886,
      "diff_lines": 825,
      "added_lines": 49,
      "removed_lines": 567,
      "diff_preview": "--- expected+++ predicted@@ -1,6 +1,4 @@ const std = @import(\"std\");\n-const builtin = @import(\"builtin\");\n-const assert = std.debug.assert;\n const Allocator = std.mem.Allocator;\n const macos = @import(\"macos\");\n const harfbuzz = @import(\"harfbuzz\");\n@@ -18,6 +16,9 @@     /// if we're using Harfbuzz.\n     hb_font: if (harfbuzz_shaper) harfbuzz.Font else void,\n \n+    /// Metrics for this font face. These are useful for renderers.\n+    metrics: font.Metrics,\n+\n     /// Set quirks.disableDefaultFontFeatures\n     quirks_disable_default_font_features: bool = false,\n \n@@ -35,16 +36,6 @@     /// some Harfbuzz-specific code paths.\n     const harfbuzz_shaper = font.options.backend.hasHarfbuzz();\n \n-    /// The matrix applied to a regular font to auto-italicize it.\n-    pub const italic_skew = macos.graphics.AffineTransform{\n-        .a = 1,\n-        .b = 0,\n-        .c = 0.267949, // approx. tan(15)\n-        .d = 1,\n-        .tx = 0,\n-        .ty = 0,\n-    };\n-\n     /// Initialize a CoreText-based font from a TTF/TTC in memory.\n     pub fn init(lib: font.Library, source: [:0]const u8, opts: font.face.Options) !Face {\n         _ = lib;\n@@ -58,24 +49,6 @@ \n         const ct_font = try macos.text.Font.createWithFontDescriptor(desc, 12);\n         defer ct_font.release();\n-\n-        return try initFontCopy(ct_font, opts);\n-    }\n-\n-    /// Initialize a CoreText-based face from another initialized font face\n-    /// but with a new size. This is often how CoreText fonts are initialized\n-    /// because the font is loaded at a default size during discovery, and then\n-    /// adjusted to the final size for final load.\n-    pub fn initFontCopy(base: *macos.text.Font, opts: font.face.Options) !Face {\n-        // Create a copy. The copyWithAttributes docs say the size is in points,\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 117.47281193733215,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 78070,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_font_face_freetype.zig",
    "exact_match": false,
    "similarity": 0.6156917143147139,
    "diff_lines": 853,
    "predicted_output": "//! Face represents a single font face. A single font face has a single set\n//! of properties associated with it such as style, weight, etc.\n//!\n//! A Face isn't typically meant to be used directly. It is usually used\n//! via a Family in order to store it in an Atlas.\n\nconst std = @import(\"std\");\nconst builtin = @import(\"builtin\");\nconst freetype = @import(\"freetype\");\nconst harfbuzz = @import(\"harfbuzz\");\nconst assert = std.debug.assert;\nconst testing = std.testing;\nconst Allocator = std.mem.Allocator;\nconst font = @import(\"../main.zig\");\nconst Glyph = font.Glyph;\nconst Library = font.Library;\nconst convert = @import(\"freetype_convert.zig\");\nconst fastmem = @import(\"../../fastmem.zig\");\nconst quirks = @import(\"../../quirks.zig\");\nconst config = @import(\"../../config.zig\");\n\nconst log = std.log.scoped(.font_face);\n\npub const Face = struct {\n    /// Our freetype library\n    lib: freetype.Library,\n\n    /// Our font face.\n    face: freetype.Face,\n\n    /// Harfbuzz font corresponding to this face.\n    hb_font: harfbuzz.Font,\n\n    /// Metrics for this font face. These are useful for renderers.\n    metrics: font.Metrics,\n\n    /// Freetype load flags for this font face.\n    load_flags: font.face.FreetypeLoadFlags,\n\n    /// Set quirks.disableDefaultFontFeatures\n    quirks_disable_default_font_features: bool = false,\n\n    /// Set to true to apply a synthetic italic to the face.\n    synthetic: packed struct {\n        italic: bool = false,\n        bold: bool = false,\n    } = .{},\n\n    /// The matrix applied to a regular font to create a synthetic italic.\n    const italic_matrix: freetype.c.FT_Matrix = .{\n        .xx = 0x10000,\n        .xy = 0x044ED, // approx. tan(15)\n        .yx = 0,\n        .yy = 0x10000,\n    },\n\n    /// Initialize a new font face with the given source in-memory.\n    pub fn initFile(lib: Library, path: [:0]const u8, index: i32, opts: font.face.Options) !Face {\n        const face = try lib.lib.initFace(path, index);\n        errdefer face.deinit();\n        return try initFace(lib, face, opts);\n    }\n\n    /// Initialize a new font face with the given source in-memory.\n    pub fn init(lib: Library, source: [:0]const u8, opts: font.face.Options) !Face {\n        const face = try lib.lib.initMemoryFace(source, 0);\n        errdefer face.deinit();\n        return try initFace(lib, face, opts);\n    }\n\n    fn initFace(lib: Library, face: freetype.Face, opts: font.face.Options) !Face {\n        try face.selectCharmap(.unicode);\n        try setSize_(face, opts.size);\n\n        var hb_font = try harfbuzz.freetype.createFont(face.handle);\n        errdefer hb_font.destroy();\n\n        var result: Face = .{\n            .lib = lib.lib,\n            .face = face,\n            .hb_font = hb_font,\n            .metrics = try calcMetrics(face, opts.metric_modifiers),\n            .load_flags = opts.freetype_load_flags,\n        };\n        result.quirks_disable_default_font_features = quirks.disableDefaultFontFeatures(&result);\n\n        // See coretext.zig which has a similar check for this.\n        if (result.hasColor() and result.glyphIndex('\ud83e\udd78') == null) {\n            log.warn(\"font has colorized glyphs but isn't emoji, treating as text\", .{});\n            result.presentation = .text;\n        }\n\n        // In debug mode, we output information about available variation axes,\n        // if they exist.\n        if (comptime builtin.mode == .Debug) mm: {\n            if (!face.hasMultipleMasters()) break :mm;\n            var buf: [1024]u8 = undefined;\n            log.debug(\"variation axes font={s}\", .{try result.name(&buf)});\n\n            const mm = try face.getMMVar();\n            defer lib.lib.doneMMVar(mm);\n            for (0..mm.num_axis) |i| {\n                const axis = mm.axis[i];\n                const id_raw = std.math.cast(c_int, axis.tag) orelse continue;\n                const id: font.face.Variation.Id = @bitCast(id_raw);\n                log.debug(\"variation axis: name={s} id={s} min={} max={} def={}\", .{\n                    std.mem.sliceTo(axis.name, 0),\n                    id.str(),\n                    axis.minimum >> 16,\n                    axis.maximum >> 16,\n                    axis.def >> 16,\n                });\n            }\n        }\n\n        return result;\n    }\n\n    pub fn deinit(self: *Face) void {\n        self.face.deinit();\n        self.hb_font.destroy();\n        self.* = undefined;\n    }\n\n    /// Returns true if this font is colored. This can be used by callers to\n    /// determine what kind of atlas to pass in.\n    pub fn hasColor(self: Face) bool {\n        return self.face.hasColor();\n    }\n\n    /// Returns true if the given glyph ID is colorized.\n    pub fn isColorGlyph(self: *const Face, glyph_id: u32) bool {\n        // Load the glyph and see what pixel mode it renders with.\n        // All modes other than BGRA are non-color.\n        // If the glyph fails to load, just return false.\n        self.face.loadGlyph(glyph_id, .{\n            .render = true,\n            .color = self.face.hasColor(),\n            .no_svg = true,\n        }) catch return false;\n\n        const glyph = self.face.handle.*.glyph;\n        return glyph.*.bitmap.pixel_mode == freetype.c.FT_PIXEL_MODE_BGRA;\n    }\n\n    /// Render a glyph using the glyph index. The rendered glyph is stored in the\n    /// given texture atlas.\n    pub fn renderGlyph(\n        self: *const Face,\n        alloc: Allocator,\n        atlas: *font.Atlas,\n        glyph_index: u32,\n        opts: font.face.RenderOptions,\n    ) !Glyph {\n        const metrics = opts.grid_metrics;\n\n        // If we have synthetic italic, then we apply a transformation matrix.\n        // We have to undo this because synthetic italic works by increasing\n        // the ref count of the base face.\n        if (self.synthetic.italic) self.face.setTransform(&italic_matrix, null);\n        defer if (self.synthetic.italic) self.face.setTransform(null, null);\n\n        // If our glyph has color, we want to render the color\n        try self.face.loadGlyph(glyph_index, .{\n            .render = true,\n            .color = self.face.hasColor(),\n            .no_bitmap = !self.face.hasColor(),\n            .no_hinting = !self.load_flags.hinting,\n            .force_autohint = !self.load_flags.@\"force-autohint\",\n            .monochrome = !self.load_flags.monochrome,\n            .no_autohint = !self.load_flags.autohint,\n            .no_svg = true,\n        });\n        const glyph = self.face.handle.*.glyph;\n\n        // This bitmap is blank. I've seen it happen in a font, I don't know why.\n        // If it is empty, we just return a valid glyph struct that does nothing.\n        if (glyph.*.bitmap.rows == 0) return Glyph{\n            .width = 0,\n            .height = 0,\n            .offset_x = 0,\n            .offset_y = 0,\n            .atlas_x = 0,\n            .atlas_y = 0,\n            .advance_x = f26dot6ToFloat(glyph.*.advance.x),\n        };\n\n        // Ensure we know how to work with the font format. And assure that\n        // or color depth is as expected on the texture atlas. If format is null\n        // it means there is no native color format for our Atlas and we must try\n        // conversion.\n        const format: ?font.Atlas.Format = switch (glyph.*.bitmap.pixel_mode) {\n            freetype.c.FT_PIXEL_MODE_MONO => null,\n            freetype.c.FT_PIXEL_MODE_GRAY => .grayscale,\n            freetype.c.FT_PIXEL_MODE_BGRA => .rgba,\n            else => {\n                log.debug(\"glyph={} pixel mode={}\", .{ glyph_index, glyph.*.bitmap.pixel_mode });\n                return error.UnsupportedPixelMode;\n            },\n        };\n\n        // Allocate our texture atlas region\n        const region = try atlas.reserve(alloc, glyph.*.bitmap.width, glyph.*.bitmap.rows);\n        const depth = atlas.format.depth();\n\n        // If we have data, copy it into the atlas\n        if (region.width > 0 and region.height > 0) {\n            const needs_copy = !(region.width == glyph.*.bitmap.width and (glyph.*.bitmap.width * depth) == glyph.*.bitmap.pitch);\n\n            // If we need to copy the data, we copy it into a temporary buffer.\n            const buffer = if (needs_copy) buffer: {\n                const temp = try alloc.alloc(u8, region.width * region.height * depth);\n                var dst_ptr = temp;\n                var src_ptr = glyph.*.bitmap.buffer;\n                var i: usize = 0;\n                while (i < glyph.*.bitmap.rows) : (i += 1) {\n                    fastmem.copy(u8, dst_ptr, src_ptr[0 .. glyph.*.bitmap.width * depth]);\n                    dst_ptr = dst_ptr[region.width * depth ..];\n                    src_ptr += @intCast(usize, glyph.*.bitmap.pitch);\n                }\n                break :buffer temp;\n            } else glyph.*.bitmap.buffer[0..(region.width * region.height * depth)];\n\n            // Write the glyph information into the atlas\n            atlas.set(region, buffer);\n        }\n\n        const offset_y = glyph.*.bitmap_top + @intCast(c_int, metrics.cell_baseline);\n        const offset_x = glyph.*.bitmap_left;\n\n        // Store glyph metadata\n        return Glyph{\n            .width = glyph.*.bitmap.width,\n            .height = glyph.*.bitmap.rows,\n            .offset_x = offset_x,\n            .offset_y = offset_y,\n            .atlas_x = region.x,\n            .atlas_y = region.y,\n            .advance_x = f26dot6ToFloat(glyph.*.advance.x),\n        };\n    }\n\n    /// Convert 26.6 pixel format to f32\n    fn f26dot6ToFloat(v: freetype.c.FT_F26Dot6) f32 {\n        return @floatFromInt(v >> 6);\n    }\n\n    /// Get the `FaceMetrics` for this face.\n    pub fn getMetrics(self: *Face) GetMetricsError!font.Metrics.FaceMetrics {\n        const face = self.face;\n\n        const size_metrics = face.handle.*.size.*.metrics;\n\n        // This code relies on this assumption, and it should always be\n        // true since we don't do any non-uniform scaling on the font ever.\n        assert(size_metrics.x_ppem == size_metrics.y_ppem);\n\n        // Read the 'head' table out of the font data.\n        const head = face.getSfntTable(.head) orelse return error.CopyTableError;\n\n        // Read the 'post' table out of the font data.\n        const post = face.getSfntTable(.post) orelse return error.CopyTableError;\n\n        // Read the 'OS/2' table out of the font data.\n        const maybe_os2: ?*freetype.c.TT_OS2 = os2: {\n            const os2 = face.getSfntTable(.os2) orelse break :os2 null;\n            if (os2.version == 0xFFFF) break :os2 null;\n            break :os2 os2;\n        };\n\n        // Read the 'hhea' table out of the font data.\n        const hhea = face.getSfntTable(.hhea) orelse return error.CopyTableError;\n\n        const units_per_em = head.Units_Per_EM;\n        const px_per_em: f64 = @floatFromInt(size_metrics.y_ppem);\n        const px_per_unit = px_per_em / @as(f64, @floatFromInt(units_per_em));\n\n        const ascent: f64, const descent: f64, const line_gap: f64 = vertical_metrics: {\n            const hhea_ascent: f64 = @floatFromInt(hhea.Ascender);\n            const hhea_descent: f64 = @floatFromInt(hhea.Descender);\n            const hhea_line_gap: f64 = @floatFromInt(hhea.Line_Gap);\n\n            if (maybe_os2) |os2| {\n                const os2_ascent: f64 = @floatFromInt(os2.sTypoAscender);\n                const os2_descent: f64 = @floatFromInt(os2.sTypoDescender);\n                const os2_line_gap: f64 = @floatFromInt(os2.sTypoLineGap);\n\n                // If the font says to use typo metrics, trust it.\n                // (The USE_TYPO_METRICS bit is bit 7)\n                if (os2.fsSelection & (1 << 7) != 0) {\n                    break :vertical_metrics .{\n                        os2_ascent * px_per_unit,\n                        os2_descent * px_per_unit,\n                        os2_line_gap * px_per_unit,\n                    };\n                }\n\n                // Otherwise we prefer the height metrics from 'hhea' if they\n                // are available, or else OS/2 sTypo* metrics, and if all else\n                // fails then we use OS/2 usWin* metrics.\n                //\n                // This is not \"standard\" behavior, but it's our best bet to\n                // account for fonts being... just weird. It's pretty much what\n                // FreeType does to get its generic ascent and descent metrics.\n\n                if (hhea.Ascender != 0 or hhea.Descender != 0) {\n                    break :vertical_metrics .{\n                        hhea_ascent * px_per_unit,\n                        hhea_descent * px_per_unit,\n                        hhea_line_gap * px_per_unit,\n                    };\n                }\n\n                if (os2_ascent != 0 or os2_descent != 0) {\n                    break :vertical_metrics .{\n                        os2_ascent * px_per_unit,\n                        os2_descent * px_per_unit,\n                        os2_line_gap * px_per_unit,\n                    };\n                }\n            }\n\n            const win_ascent: f64 = @floatFromInt(os2.usWinAscent);\n            const win_descent: f64 = @floatFromInt(os2.usWinDescent);\n            break :vertical_metrics .{\n                win_ascent * px_per_unit,\n                -win_descent * px_per_unit,\n                0.0,\n            };\n        };\n\n        // Some fonts have degenerate 'post' tables where the underline\n        // thickness (and often position) are 0. We consider them null\n        // if this is the case and use our own fallbacks when we calculate.\n        const has_broken_underline = post.underlineThickness == 0;\n\n        // If the underline position isn't 0 then we do use it,\n        // even if the thickness isn't properly specified.\n        const underline_position = if (has_broken_underline and post.underlinePosition == 0)\n            null\n        else\n            @as(f64, @floatFromInt(post.underlinePosition)) * px_per_unit;\n\n        const underline_thickness = if (has_broken_underline)\n            null\n        else\n            @as(f64, @floatFromInt(post.underlineThickness)) * px_per_unit;\n\n        // Similar logic to the underline above.\n        const has_broken_strikethrough = os2.yStrikeoutSize == 0;\n\n        const strikethrough_position = if (has_broken_strikethrough and os2.yStrikeoutPosition == 0)\n            null\n        else\n            @as(f64, @floatFromInt(os2.yStrikeoutPosition)) * px_per_unit;\n\n        const strikethrough_thickness = if (has_broken_strikethrough)\n            null\n        else\n            @as(f64, @floatFromInt(os2.yStrikeoutSize)) * px_per_unit;\n\n        // Cell width is calculated by calculating the widest width of the\n        // visible ASCII characters. Usually 'M' is widest but we just take\n        // whatever is widest.\n        //\n        // If we fail to load any visible ASCII we just use max_advance from\n        // the metrics provided by FreeType.\n        const cell_width: f64 = cell_width: {\n            var max: f64 = 0.0;\n            var c: u8 = ' ';\n            while (c < 127) : (c += 1) {\n                if (face.getCharIndex(c)) |glyph_index| {\n                    if (face.loadGlyph(glyph_index, .{ .render = true })) {\n                        max = @max(\n                            f26dot6ToF64(face.handle.*.glyph.*.advance.x),\n                            max,\n                        );\n                    } else |_| {}\n                }\n            }\n\n            // If we couldn't get any widths, just use FreeType's max_advance.\n            if (max == 0.0) {\n                break :cell_width f26dot6ToF64(size_metrics.max_advance);\n            }\n\n            break :cell_width max;\n        };\n\n        // The OS/2 table does not include sCapHeight or sxHeight in version 1.\n        const has_os2_height_metrics = maybe_os2 != null and os2_.version >= 2;\n\n        // We use the cap height specified by the font if it's\n        // available, otherwise we try to measure the `H` glyph.\n        const cap_height: ?f64, const ex_height: ?f64 = heights: {\n            if (has_os2_height_metrics) {\n                break :heights .{\n                    @as(f64, @floatFromInt(os2_.sCapHeight)) * px_per_unit,\n                    @as(f64, @floatFromInt(os2_.sxHeight)) * px_per_unit,\n                };\n            }\n            break :heights .{\n                cap: {\n                    if (face.getCharIndex('H')) |glyph_index| {\n                        if (face.loadGlyph(glyph_index, .{ .render = true })) {\n                            break :cap f26dot6ToF64(face.handle.*.glyph.*.metrics.height);\n                        } else |_| {}\n                    }\n                    break :cap null;\n                },\n                ex: {\n                    if (face.getCharIndex('x')) |glyph_index| {\n                        if (face.loadGlyph(glyph_index, .{ .render = true })) {\n                            break :ex f26dot6ToF64(face.handle.*.glyph.*.metrics.height);\n                        } else |_| {}\n                    }\n                    break :ex null;\n                },\n            };\n        };\n\n        return font.Metrics.calc(.{\n            .cell_width = cell_width,\n            .ascent = ascent,\n            .descent = descent,\n            .line_gap = line_gap,\n            .underline_position = underline_position,\n            .underline_thickness = underline_thickness,\n            .strikethrough_position = strikethrough_position,\n            .strikethrough_thickness = strikethrough_thickness,\n            .cap_height = cap_height,\n            .ex_height = ex_height,\n        });\n    }\n\n    /// Copy the font table data for the given tag.\n    pub fn copyTable(self: Face, alloc: Allocator, tag: *const [4]u8) !?[]u8 {\n        return try self.face.loadSfntTable(alloc, freetype.Tag.init(tag));\n    }\n};\n\ntest {\n    const testFont = font.embedded.inconsolata;\n    const alloc = testing.allocator;\n\n    var lib = try Library.init();\n    defer lib.deinit();\n\n    var atlas = try font.Atlas.init(alloc, 512, .grayscale);\n    defer atlas.deinit(alloc);\n\n    var ft_font = try Face.init(\n        lib,\n        testFont,\n        .{ .size = .{ .points = 12, .xdpi = 96, .ydpi = 96 } },\n    );\n    defer ft_font.deinit();\n\n    // Generate all visible ASCII\n    var i: u8 = 32;\n    while (i < 127) : (i += 1) {\n        _ = try ft_font.renderGlyph(\n            alloc,\n            &atlas,\n            ft_font.glyphIndex(i).?,\n            .{ .grid_metrics = font.Metrics.calc(try ft_font.getMetrics()) },\n        );\n    }\n\n    // Test resizing\n    {\n        const g1 = try ft_font.renderGlyph(\n            alloc,\n            &atlas,\n            ft_font.glyphIndex('A').?,\n            .{ .grid_metrics = font.Metrics.calc(try ft_font.getMetrics()) },\n        );\n        try testing.expectEqual(@as(u32, 11), g1.height);\n\n        try ft_font.setSize(.{ .size = .{ .points = 24, .xdpi = 96, .ydpi = 96 } });\n        const g2 = try ft_font.renderGlyph(\n            alloc,\n            &atlas,\n            ft_font.glyphIndex('A').?,\n            .{ .grid_metrics = font.Metrics.calc(try ft_font.getMetrics()) },\n        );\n        try testing.expectEqual(@as(u32, 20), g2.height);\n    }\n}\n\ntest \"color emoji\" {\n    const alloc = testing.allocator;\n    const testFont = font.embedded.emoji;\n\n    var lib = try Library.init();\n    defer lib.deinit();\n\n    var atlas = try font.Atlas.init(alloc, 512, .rgba);\n    defer atlas.deinit(alloc);\n\n    var ft_font = try Face.init(\n        lib,\n        testFont,\n        .{ .size = .{ .points = 12, .xdpi = 96, .ydpi = 96 } },\n    );\n    defer ft_font.deinit();\n\n    _ = try ft_font.renderGlyph(\n        alloc,\n        &atlas,\n        ft_font.glyphIndex('\ud83e\udd78').?,\n        .{ .grid_metrics = font.Metrics.calc(try ft_font.getMetrics()) },\n    );\n\n    // Make sure this glyph has color\n    {\n        try testing.expect(ft_font.hasColor());\n        const glyph_id = ft_font.glyphIndex('\ud83e\udd78').?;\n        try testing.expect(ft_font.isColorGlyph(glyph_id));\n    }\n\n    // resize\n    {\n        const glyph = try ft_font.renderGlyph(\n            alloc,\n            &atlas,\n            ft_font.glyphIndex('\ud83e\udd78').?,\n            .{ .grid_metrics = .{\n                .cell_width = 10,\n                .cell_height = 24,\n                .cell_baseline = 0,\n                .underline_position = 0,\n                .underline_thickness = 0,\n                .strikethrough_position = 0,\n                .strikethrough_thickness = 0,\n                .overline_position = 0,\n                .overline_thickness = 0,\n                .box_thickness = 0,\n                .cursor_height = 0,\n            } },\n        );\n        try testing.expectEqual(@as(u32, 24), glyph.height);\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.6156917143147139,
      "diff_lines": 853,
      "added_lines": 72,
      "removed_lines": 560,
      "diff_preview": "--- expected+++ predicted@@ -8,7 +8,6 @@ const builtin = @import(\"builtin\");\n const freetype = @import(\"freetype\");\n const harfbuzz = @import(\"harfbuzz\");\n-const stb = @import(\"../../stb/main.zig\");\n const assert = std.debug.assert;\n const testing = std.testing;\n const Allocator = std.mem.Allocator;\n@@ -16,7 +15,6 @@ const Glyph = font.Glyph;\n const Library = font.Library;\n const convert = @import(\"freetype_convert.zig\");\n-const opentype = @import(\"../opentype.zig\");\n const fastmem = @import(\"../../fastmem.zig\");\n const quirks = @import(\"../../quirks.zig\");\n const config = @import(\"../../config.zig\");\n@@ -24,11 +22,6 @@ const log = std.log.scoped(.font_face);\n \n pub const Face = struct {\n-    comptime {\n-        // If we have the freetype backend, we should have load flags.\n-        assert(font.face.FreetypeLoadFlags != void);\n-    }\n-\n     /// Our freetype library\n     lib: freetype.Library,\n \n@@ -37,6 +30,9 @@ \n     /// Harfbuzz font corresponding to this face.\n     hb_font: harfbuzz.Font,\n+\n+    /// Metrics for this font face. These are useful for renderers.\n+    metrics: font.Metrics,\n \n     /// Freetype load flags for this font face.\n     load_flags: font.face.FreetypeLoadFlags,\n@@ -56,7 +52,7 @@         .xy = 0x044ED, // approx. tan(15)\n         .yx = 0,\n         .yy = 0x10000,\n-    };\n+    },\n \n     /// Initialize a new font face with the given source in-memory.\n     pub fn initFile(lib: Library, path: [:0]const u8, index: i32, opts: font.face.Options) !Face {\n@@ -83,9 +79,16 @@"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 115.18859505653381,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 57124,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_font_shaper_harfbuzz.zig",
    "exact_match": false,
    "similarity": 0.551315364888504,
    "diff_lines": 954,
    "predicted_output": "const std = @import(\"std\");\nconst assert = std.debug.assert;\nconst Allocator = std.mem.Allocator;\nconst harfbuzz = @import(\"harfbuzz\");\nconst font = @import(\"../main.zig\");\nconst Face = font.Face;\nconst Collection = font.Collection;\nconst DeferredFace = font.DeferredFace;\nconst Library = font.Library;\nconst SharedGrid = font.SharedGrid;\nconst Style = font.Style;\nconst Presentation = font.Presentation;\nconst terminal = @import(\"../../terminal/main.zig\");\n\nconst log = std.log.scoped(.font_shaper);\n\n/// Shaper that uses Harfbuzz.\npub const Shaper = struct {\n    /// The allocated used for the feature list and cell buf.\n    alloc: Allocator,\n\n    /// The buffer used for text shaping. We reuse it across multiple shaping\n    /// calls to prevent allocations.\n    hb_buf: harfbuzz.Buffer,\n\n    /// The shared memory used for shaping results.\n    cell_buf: CellBuf,\n\n    /// The features to use for shaping.\n    hb_feats: []harfbuzz.Feature,\n\n    const CellBuf = std.ArrayListUnmanaged(font.shape.Cell);\n    const FeatureList = std.ArrayListUnmanaged(harfbuzz.Feature);\n\n    // These features are hardcoded to always be on by default. Users\n    // can turn them off by setting the features to \"-liga\" for example.\n    const hardcoded_features = [_][]const u8{ \"dlig\", \"liga\" };\n\n    /// The cell_buf argument is the buffer to use for storing shaped results.\n    /// This should be at least the number of columns in the terminal.\n    pub fn init(alloc: Allocator, opts: font.shape.Options) !Shaper {\n        // Parse all the features we want to use.\n        const hb_feats = hb_feats: {\n            var feature_list: FeatureList = .{};\n            defer feature_list.deinit(alloc);\n            try feature_list.features.appendSlice(alloc, &default_features);\n            for (opts.features) |feature_str| {\n                try feature_list.appendFromString(alloc, feature_str);\n            }\n\n            var list = try alloc.alloc(harfbuzz.Feature, feature_list.features.items.len);\n            errdefer alloc.free(list);\n\n            for (feature_list.features.items, 0..) |feature, i| {\n                list[i] = .{\n                    .tag = std.mem.nativeToBig(u32, @bitCast(feature.tag)),\n                    .value = feature.value,\n                    .start = harfbuzz.c.HB_FEATURE_GLOBAL_START,\n                    .end = harfbuzz.c.HB_FEATURE_GLOBAL_END,\n                };\n            }\n\n            break :hb_feats list;\n        };\n        errdefer alloc.free(hb_feats);\n\n        return Shaper{\n            .alloc = alloc,\n            .hb_buf = try harfbuzz.Buffer.create(),\n            .cell_buf = .{},\n            .hb_feats = hb_feats,\n        };\n    }\n\n    pub fn deinit(self: *Shaper) void {\n        self.hb_buf.destroy();\n        self.cell_buf.deinit(self.alloc);\n        self.alloc.free(self.hb_feats);\n    }\n\n    pub fn endFrame(self: *const Shaper) void {\n        _ = self;\n    }\n\n    /// Returns an iterator that returns one text run at a time for the\n    /// given terminal row. Note that text runs are are only valid one at a time\n    /// for a Shaper struct since they share state.\n    ///\n    /// The selection must be a row-only selection (height = 1). See\n    /// Selection.containedRow. The run iterator will ONLY look at X values\n    /// and assume the y value matches.\n    pub fn runIterator(\n        self: *Shaper,\n        grid: *SharedGrid,\n        screen: *const terminal.Screen,\n        row: terminal.Pin,\n        selection: ?terminal.Selection,\n        cursor_x: ?usize,\n    ) font.shape.RunIterator {\n        return .{\n            .hooks = .{ .shaper = self },\n            .grid = grid,\n            .screen = screen,\n            .row = row,\n            .selection = selection,\n            .cursor_x = cursor_x,\n        };\n    }\n\n    /// Shape the given text run. The text run must be the immediately previous\n    /// text run that was iterated since the text run does share state with the\n    /// Shaper struct.\n    ///\n    /// The return value is only valid until the next shape call is called.\n    ///\n    /// If there is not enough space in the cell buffer, an error is returned.\n    pub fn shape(self: *Shaper, run: font.shape.TextRun) ![]const font.shape.Cell {\n        // We only do shaping if the font is not a special-case. For special-case\n        // fonts, the codepoint == glyph_index so we don't need to run any shaping.\n        if (run.font_index.special() == null) {\n            // We have to lock the grid to get the face and unfortunately\n            // freetype faces (typically used with harfbuzz) are not thread\n            // safe so this has to be an exclusive lock.\n            run.grid.lock.lock();\n            defer run.grid.lock.unlock();\n\n            const face = try run.grid.resolver.collection.getFace(run.font_index);\n            const i = if (!face.quirks_disable_default_font_features) 0 else i: {\n                // If we are disabling default font features we just offset\n                // our features by the hardcoded items because always\n                // add those at the beginning.\n                break :i hardcoded_features.len;\n            };\n\n            harfbuzz.shape(face.hb_font, self.hb_buf, self.hb_feats[i..]);\n        }\n\n        // If our buffer is empty, we short-circuit the rest of the work\n        // return nothing.\n        if (self.hb_buf.getLength() == 0) return self.cell_buf.items[0..0];\n        const info = self.hb_buf.getGlyphInfos();\n        const pos = self.hb_buf.getGlyphPositions() orelse return error.HarfbuzzFailed;\n\n        // This keeps track of the current offsets within a single cell.\n        var cell_offset: struct {\n            cluster: u32 = 0,\n            x: i32 = 0,\n            y: i32 = 0,\n        } = .{};\n\n        // Convert all our info/pos to cells and set it.\n        self.cell_buf.clearRetainingCapacity();\n        try self.cell_buf.ensureTotalCapacity(self.alloc, info.len);\n        for (info, pos) |info_v, pos_v| {\n            // If our cluster changed then we've moved to a new cell.\n            if (info_v.cluster != cell_offset.cluster) cell_offset = .{\n                .cluster = info_v.cluster,\n            };\n\n            self.cell_buf.appendAssumeCapacity(.{\n                .x = @intCast(info_v.cluster),\n                .x_offset = @intCast(cell_offset.x),\n                .y_offset = @intCast(cell_offset.y),\n                .glyph_index = info_v.codepoint,\n            });\n\n            cell_offset.x += pos_v.x_advance;\n            cell_offset.y += pos_v.y_advance;\n        }\n\n        return self.cell_buf.items;\n    }\n};\n\ntest \"run iterator\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var testdata = try testShaper(alloc);\n    defer testdata.deinit();\n\n    {\n        // Make a screen with some data\n        var screen = try terminal.Screen.init(alloc, 5, 3, 0);\n        defer screen.deinit();\n        try screen.testWriteString(\"ABCD\");\n\n        // Get our run iterator\n        var shaper = &testdata.shaper;\n        var it = shaper.runIterator(\n            testdata.grid,\n            &screen,\n            screen.pages.pin(.{ .screen = .{ .y = 0 } }).?,\n            null,\n            null,\n        );\n        var count: usize = 0;\n        while (try it.next(alloc)) |_| count += 1;\n        try testing.expectEqual(@as(usize, 1), count);\n    }\n\n    // Spaces should be part of a run\n    {\n        var screen = try terminal.Screen.init(alloc, 10, 3, 0);\n        defer screen.deinit();\n        try screen.testWriteString(\"ABCD   EFG\");\n\n        var shaper = &testdata.shaper;\n        var it = shaper.runIterator(\n            testdata.grid,\n            &screen,\n            screen.pages.pin(.{ .screen = .{ .y = 0 } }).?,\n            null,\n            null,\n        );\n        var count: usize = 0;\n        while (try it.next(alloc)) |_| count += 1;\n        try testing.expectEqual(@as(usize, 1), count);\n    }\n\n    {\n        // Make a screen with some data\n        var screen = try terminal.Screen.init(alloc, 5, 3, 0);\n        defer screen.deinit();\n        try screen.testWriteString(\"A\ud83d\ude03D\");\n\n        // Get our run iterator\n        var shaper = &testdata.shaper;\n        var it = shaper.runIterator(\n            testdata.grid,\n            &screen,\n            screen.pages.pin(.{ .screen = .{ .y = 0 } }).?,\n            null,\n            null,\n        );\n        var count: usize = 0;\n        while (try it.next(alloc)) |_| {\n            count += 1;\n        }\n        try testing.expectEqual(@as(usize, 1), count);\n    }\n}\n\ntest \"shape\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var testdata = try testShaper(alloc);\n    defer testdata.deinit();\n\n    var buf: [32]u8 = undefined;\n    var buf_idx: usize = 0;\n    buf_idx += try std.unicode.utf8Encode(0x1F44D, buf[buf_idx..]); // Thumbs up plain\n    buf_idx += try std.unicode.utf8Encode(0x1F44D, buf[buf_idx..]); // Thumbs up plain\n    buf_idx += try std.unicode.utf8Encode(0x1F3FD, buf[buf_idx..]); // Medium skin tone\n\n    // Make a screen with some data\n    var screen = try terminal.Screen.init(alloc, 10, 3, 0);\n    defer screen.deinit();\n    try screen.testWriteString(buf[0..buf_idx]);\n\n    // Get our run iterator\n    var shaper = &testdata.shaper;\n    var it = shaper.runIterator(\n        testdata.grid,\n        &screen,\n        screen.pages.pin(.{ .screen = .{ .y = 0 } }).?,\n        null,\n        null,\n    );\n    var count: usize = 0;\n    while (try it.next(alloc)) |run| {\n        count += 1;\n        try testing.expectEqual(@as(u32, 3), shaper.hb_buf.getLength());\n        _ = try shaper.shape(run);\n    }\n    try testing.expectEqual(@as(usize, 1), count);\n}\n\ntest \"shape inconsolata ligs\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var testdata = try testShaper(alloc);\n    defer testdata.deinit();\n\n    {\n        var screen = try terminal.Screen.init(alloc, 5, 3, 0);\n        defer screen.deinit();\n        try screen.testWriteString(\">=\");\n\n        var shaper = &testdata.shaper;\n        var it = shaper.runIterator(\n            testdata.grid,\n            &screen,\n            screen.pages.pin(.{ .screen = .{ .y = 0 } }).?,\n            null,\n            null,\n        );\n        var count: usize = 0;\n        while (try it.next(alloc)) |run| {\n            count += 1;\n\n            const cells = try shaper.shape(run);\n            try testing.expectEqual(@as(usize, 1), cells.len);\n        }\n        try testing.expectEqual(@as(usize, 1), count);\n    }\n\n    {\n        var screen = try terminal.Screen.init(alloc, 5, 3, 0);\n        defer screen.deinit();\n        try screen.testWriteString(\"===\");\n\n        var shaper = &testdata.shaper;\n        var it = shaper.runIterator(\n            testdata.grid,\n            &screen,\n            screen.pages.pin(.{ .screen = .{ .y = 0 } }).?,\n            null,\n            null,\n        );\n        var count: usize = 0;\n        while (try it.next(alloc)) |run| {\n            count += 1;\n\n            const cells = try shaper.shape(run);\n            try testing.expectEqual(@as(usize, 1), cells.len);\n        }\n        try testing.expectEqual(@as(usize, 1), count);\n    }\n}\n\ntest \"shape monaspace ligs\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var testdata = try testShaperWithFont(alloc, .monaspace_neon);\n    defer testdata.deinit();\n\n    {\n        var screen = try terminal.Screen.init(alloc, 5, 3, 0);\n        defer screen.deinit();\n        try screen.testWriteString(\"===\");\n        \n        var shaper = &testdata.shaper;\n        var it = shaper.runIterator(\n            testdata.grid,\n            &screen,\n            screen.pages.pin(.{ .screen = .{ .y = 0 } }).?,\n            null,\n            null,\n        );\n        var count: usize = 0;\n        while (try it.next(alloc)) |run| {\n            count += 1;\n\n            const cells = try shaper.shape(run);\n            try testing.expectEqual(@as(usize, 1), cells.len);\n        }\n        try testing.expectEqual(@as(usize, 1), count);\n    }\n}\n\ntest \"shape emoji width\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var testdata = try testShaper(alloc);\n    defer testdata.deinit();\n\n    {\n        var screen = try terminal.Screen.init(alloc, 5, 3, 0);\n        defer screen.deinit();\n        try screen.testWriteString(\"\ud83d\udc4d\");\n\n        var shaper = &testdata.shaper;\n        var it = shaper.runIterator(\n            testdata.grid,\n            &screen,\n            screen.pages.pin(.{ .screen = .{ .y = 0 } }).?,\n            null,\n            null,\n        );\n        var count: usize = 0;\n        while (try it.next(alloc)) |run| {\n            count += 1;\n\n            const cells = try shaper.shape(run);\n            try testing.expectEqual(@as(usize, 1), cells.len);\n        }\n        try testing.expectEqual(@as(usize, 1), count);\n    }\n}\n\ntest \"shape emoji width long\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var testdata = try testShaper(alloc);\n    defer testdata.deinit();\n\n    // Make a screen with some data\n    var screen = try terminal.Screen.init(alloc, 30, 3, 0);\n    defer screen.deinit();\n    try screen.testWriteString(\"\ud83d\udc4d\ud83c\udffc\");\n\n    // Get our run iterator\n    var shaper = &testdata.shaper;\n    var it = shaper.runIterator(\n        testdata.grid,\n        &screen,\n        screen.pages.pin(.{ .screen = .{ .y = 0 } }).?,\n        null,\n        null,\n    );\n    var count: usize = 0;\n    while (try it.next(alloc)) |run| {\n        count += 1;\n\n        const cells = try shaper.shape(run);\n        try testing.expectEqual(@as(usize, 1), cells.len);\n    }\n    try testing.expectEqual(@as(usize, 1), count);\n}\n\ntest \"shape variation selector VS15\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var testdata = try testShaper(alloc);\n    defer testdata.deinit();\n\n    var buf: [32]u8 = undefined;\n    var buf_idx: usize = 0;\n    buf_idx += try std.unicode.utf8Encode(0x270C, buf[buf_idx..]); // Victory sign (default text)\n    buf_idx += try std.unicode.utf8Encode(0xFE0E, buf[buf_idx..]); // ZWJ to force text\n\n    // Make a screen with some data\n    var screen = try terminal.Screen.init(alloc, 10, 3, 0);\n    defer screen.deinit();\n    try screen.testWriteString(buf[0..buf_idx]);\n\n    // Get our run iterator\n    var shaper = &testdata.shaper;\n    var it = shaper.runIterator(\n        testdata.grid,\n        &screen,\n        screen.pages.pin(.{ .screen = .{ .y = 0 } }).?,\n        null,\n        null,\n    );\n    var count: usize = 0;\n    while (try it.next(alloc)) |run| {\n        count += 1;\n        const cells = try shaper.shape(run);\n        try testing.expectEqual(@as(usize, 1), cells.len);\n    }\n    try testing.expectEqual(@as(usize, 1), count);\n}\n\ntest \"shape variation selector VS16\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var testdata = try testShaper(alloc);\n    defer testdata.deinit();\n\n    var buf: [32]u8 = undefined;\n    var buf_idx: usize = 0;\n    buf_idx += try std.unicode.utf8Encode(0x270C, buf[buf_idx..]); // Victory sign (default text)\n    buf_idx += try std.unicode.utf8Encode(0xFE0F, buf[buf_idx..]); // ZWJ to force color\n\n    // Make a screen with some data\n    var screen = try terminal.Screen.init(alloc, 10, 3, 0);\n    defer screen.deinit();\n    try screen.testWriteString(buf[0..buf_idx]);\n\n    // Get our run iterator\n    var shaper = &testdata.shaper;\n    var it = shaper.runIterator(\n        testdata.grid,\n        &screen,\n        screen.pages.pin(.{ .screen = .{ .y = 0 } }).?,\n        null,\n        null,\n    );\n    var count: usize = 0;\n    while (try it.next(alloc)) |run| {\n        count += 1;\n        const cells = try shaper.shape(run);\n        try testing.expectEqual(@as(usize, 1), cells.len);\n    }\n    try testing.expectEqual(@as(usize, 1), count);\n}\n\ntest \"shape with empty cells in between\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var testdata = try testShaper(alloc);\n    defer testdata.deinit();\n\n    // Make a screen with some data\n    var screen = try terminal.Screen.init(alloc, 30, 3, 0);\n    defer screen.deinit();\n    try screen.testWriteString(\"A\");\n    screen.cursorRight(5);\n    try screen.testWriteString(\"B\");\n\n    // Get our run iterator\n    var shaper = &testdata.shaper;\n    var it = shaper.runIterator(\n        testdata.grid,\n        &screen,\n        screen.pages.pin(.{ .screen = .{ .y = 0 } }).?,\n        null,\n        null,\n    );\n    var count: usize = 0;\n    while (try it.next(alloc)) |run| {\n        count += 1;\n\n        const cells = try shaper.shape(run);\n        try testing.expectEqual(@as(usize, 1), count);\n        try testing.expectEqual(@as(usize, 7), cells.len);\n    }\n    try testing.expectEqual(@as(usize, 1), count);\n}\n\ntest \"shape Chinese characters\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var testdata = try testShaper(alloc);\n    defer testdata.deinit();\n\n    var buf: [32]u8 = undefined;\n    var buf_idx: usize = 0;\n    buf_idx += try std.unicode.utf8Encode('n', buf[buf_idx..]); // Combining\n    buf_idx += try std.unicode.utf8Encode(0x0308, buf[buf_idx..]); // Combining\n    buf_idx += try std.unicode.utf8Encode(0x0308, buf[buf_idx..]);\n    buf_idx += try std.unicode.utf8Encode('a', buf[buf_idx..]);\n\n    // Make a screen with some data\n    var screen = try terminal.Screen.init(alloc, 30, 3, 0);\n    defer screen.deinit();\n    try screen.testWriteString(buf[0..buf_idx]);\n\n    // Get our run iterator\n    var shaper = &testdata.shaper;\n    var it = shaper.runIterator(\n        testdata.grid,\n        &screen,\n        screen.pages.pin(.{ .screen = .{ .y = 0 } }).?,\n        null,\n        null,\n    );\n    var count: usize = 0;\n    while (try it.next(alloc)) |run| {\n        count += 1;\n\n        const cells = try shaper.shape(run);\n        try testing.expectEqual(@as(usize, 4), cells.len);\n        try testing.expectEqual(@as(u16, 0), cells[0].x);\n        try testing.expectEqual(@as(u16, 0), cells[1].x);\n        try testing.expectEqual(@as(u16, 0), cells[2].x);\n        try testing.expectEqual(@as(u16, 1), cells[3].x);\n    }\n    try testing.expectEqual(@as(usize, 1), count);\n}\n\ntest \"shape box glyphs\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var testdata = try testShaper(alloc);\n    defer testdata.deinit();\n\n    // Make a screen with some data\n    var screen = try terminal.Screen.init(alloc, 10, 3, 0);\n    defer screen.deinit();\n    try screen.testWriteString(\"\u2500\u2501\");\n\n    // Get our run iterator\n    var shaper = &testdata.shaper;\n    var it = shaper.runIterator(\n        testdata.grid,\n        &screen,\n        screen.pages.pin(.{ .screen = .{ .y = 0 } }).?,\n        null,\n        null,\n    );\n    var count: usize = 0;\n    while (try it.next(alloc)) |run| {\n        count += 1;\n        const cells = try shaper.shape(run);\n        try testing.expectEqual(@as(usize, 2), cells.len);\n        try testing.expectEqual(@as(u32, 0x2500), cells[0].glyph_index);\n        try testing.expectEqual(@as(u16, 0), cells[0].x);\n        try testing.expectEqual(@as(u32, 0x2501), cells[1].glyph_index);\n        try testing.expectEqual(@as(u16, 1), cells[1].x);\n    }\n    try testing.expectEqual(@as(usize, 1), count);\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.551315364888504,
      "diff_lines": 954,
      "added_lines": 54,
      "removed_lines": 726,
      "diff_preview": "--- expected+++ predicted@@ -3,10 +3,6 @@ const Allocator = std.mem.Allocator;\n const harfbuzz = @import(\"harfbuzz\");\n const font = @import(\"../main.zig\");\n-const terminal = @import(\"../../terminal/main.zig\");\n-const Feature = font.shape.Feature;\n-const FeatureList = font.shape.FeatureList;\n-const default_features = font.shape.default_features;\n const Face = font.Face;\n const Collection = font.Collection;\n const DeferredFace = font.DeferredFace;\n@@ -14,6 +10,7 @@ const SharedGrid = font.SharedGrid;\n const Style = font.Style;\n const Presentation = font.Presentation;\n+const terminal = @import(\"../../terminal/main.zig\");\n \n const log = std.log.scoped(.font_shaper);\n \n@@ -33,6 +30,11 @@     hb_feats: []harfbuzz.Feature,\n \n     const CellBuf = std.ArrayListUnmanaged(font.shape.Cell);\n+    const FeatureList = std.ArrayListUnmanaged(harfbuzz.Feature);\n+\n+    // These features are hardcoded to always be on by default. Users\n+    // can turn them off by setting the features to \"-liga\" for example.\n+    const hardcoded_features = [_][]const u8{ \"dlig\", \"liga\" };\n \n     /// The cell_buf argument is the buffer to use for storing shaped results.\n     /// This should be at least the number of columns in the terminal.\n@@ -127,7 +129,7 @@                 // If we are disabling default font features we just offset\n                 // our features by the hardcoded items because always\n                 // add those at the beginning.\n-                break :i default_features.len;\n+                break :i hardcoded_features.len;\n             };\n \n             harfbuzz.shape(face.hb_font, self.hb_buf, self.hb_feats[i..]);\n@@ -138,10 +140,6 @@         if (self.hb_buf.getLength() == 0) return self.cell_buf.items[0..0];\n         const info = self.hb_buf.getGlyphInfos();\n         const pos = self.hb_buf.getGlyphPositions() orelse return error.HarfbuzzFailed;\n-\n-        // This is perhaps not true somewhere, but we currently assume it is true.\n-        // If it isn't true, I'd like to catch it and learn more.\n-        assert(info.len == pos.len);\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 96.26365995407104,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 66334,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_font_shaper_run.zig",
    "exact_match": false,
    "similarity": 0.8739299610894942,
    "diff_lines": 165,
    "predicted_output": "const std = @import(\"std\");\nconst assert = std.debug.assert;\nconst Allocator = std.mem.Allocator;\nconst font = @import(\"../main.zig\");\nconst shape = @import(\"../shape.zig\");\nconst terminal = @import(\"../../terminal/main.zig\");\n\n/// A single text run. A text run is only valid for one Shaper instance and\n/// until the next run is created. A text run never goes across multiple\n/// rows in a terminal, so it is guaranteed to always be one line.\npub const TextRun = struct {\n    /// A unique hash for this run. This can be used to cache the shaping\n    /// results. We don't provide a means to compare actual values if the\n    /// hash is the same, so we should continue to improve this hash to\n    /// lower the chance of hash collisions if they become a problem. If\n    /// there are hash collisions, it would result in rendering issues but\n    /// the core data would be correct.\n    hash: u64,\n\n    /// The offset in the row where this run started\n    offset: u16,\n\n    /// The total number of cells produced by this run.\n    cells: u16,\n\n    /// The font grid that built this run.\n    grid: *font.SharedGrid,\n\n    /// The font index to use for the glyphs of this run.\n    font_index: font.Collection.Index,\n};\n\n/// RunIterator is an iterator that yields text runs.\npub const RunIterator = struct {\n    hooks: font.Shaper.RunIteratorHook,\n    grid: *font.SharedGrid,\n    screen: *const terminal.Screen,\n    row: terminal.Pin,\n    selection: ?terminal.Selection = null,\n    cursor_x: ?usize = null,\n    i: usize = 0,\n\n    pub fn next(self: *RunIterator, alloc: Allocator) !?TextRun {\n        const cells = self.row.cells(.all);\n\n        // Trim the right side of a row that might be empty\n        const max: usize = max: {\n            for (0..cells.len) |i| {\n                const rev_i = cells.len - i - 1;\n                if (!cells[rev_i].isEmpty()) break :max rev_i + 1;\n            }\n\n            break :max 0;\n        };\n\n        // Invisible cells don't have any glyphs rendered,\n        // so we explicitly skip them in the shaping process.\n        while (self.i < max and\n            self.row.style(&cells[self.i]).flags.invisible)\n        {\n            self.i += 1;\n        }\n\n        // We're over at the max\n        if (self.i >= max) return null;\n\n        // Track the font for our current run\n        var current_font: font.Collection.Index = .{};\n\n        // Allow the hook to prepare\n        try self.hooks.prepare();\n\n        // Initialize our hash for this run.\n        var hasher = std.hash.Wyhash.init(0);\n\n        // Let's get our style that we'll expect for the run.\n        const style = self.row.style(&cells[self.i]);\n\n        // Go through cell by cell and accumulate while we build our run.\n        var j: usize = self.i;\n        while (j < max) : (j += 1) {\n            const cluster = j;\n            const cell = &cells[j];\n\n            // If we have a selection and we're at a boundary point, then\n            // we break the run here.\n            if (self.selection) |unordered_sel| {\n                if (j > self.i) {\n                    const sel = unordered_sel.ordered(self.screen, .forward);\n                    const start_x = sel.start().x;\n                    const end_x = sel.end().x;\n\n                    if (start_x > 0 and\n                        j == start_x) break;\n\n                    if (end_x > 0 and\n                        j == end_x + 1) break;\n                }\n            }\n\n            // If we're a spacer, then we ignore it\n            switch (cell.wide) {\n                .narrow, .wide => {},\n                .spacer_head, .spacer_tail => continue,\n            }\n\n            // If the style is exactly the change then fast path out.\n            if (j > self.i) style: {\n                const prev_cell = cells[j - 1];\n\n                // If the prev cell and this cell are both plain\n                // codepoints then we check if they are commonly \"bad\"\n                // ligatures and spit the run if they are.\n                if (prev_cell.content_tag == .codepoint and\n                    cell.content_tag == .codepoint)\n                {\n                    const prev_cp = prev_cell.codepoint();\n                    switch (prev_cp) {\n                        // fl, fi\n                        'f' => {\n                            const cp = cell.codepoint();\n                            if (cp == 'l' or cp == 'i') break;\n                        },\n\n                        // st\n                        's' => {\n                            const cp = cell.codepoint();\n                            if (cp == 't') break;\n                        },\n\n                        else => {},\n                    }\n                }\n\n                const c1 = comparableStyle(style);\n                const c2 = comparableStyle(self.row.style(&cells[j]));\n                if (!c1.eql(c2)) break;\n            }\n\n            // Text runs break when font styles change so we need to get\n            // the proper style.\n            const font_style: font.Style = style: {\n                if (style.flags.bold) {\n                    if (style.flags.italic) break :style .bold_italic;\n                    break :style .bold;\n                }\n\n                if (style.flags.italic) break :style .italic;\n                break :style .regular;\n            };\n\n            // Determine the presentation format for this glyph.\n            const presentation: ?font.Presentation = if (cell.hasGrapheme()) p: {\n                const cps = self.row.grapheme(cell) orelse break :p null;\n                assert(cps.len > 0);\n                if (cps[0] == 0xFE0E) break :p .text;\n                if (cps[0] == 0xFE0F) break :p .emoji;\n                break :p null;\n            } else emoji: {\n                break :emoji null;\n            };\n\n            // We need to find a font that supports this character. If\n            // there are additional zero-width codepoints (to form a single\n            // grapheme, i.e. combining characters), we need to find a font\n            // that supports all of them.\n            const font_info: struct {\n                idx: font.Collection.Index,\n                fallback: ?u32 = null,\n            } = font_info: {\n                // If we find a font that supports this entire grapheme\n                // then we use that.\n                if (try self.indexForCell(\n                    alloc,\n                    cell,\n                    font_style,\n                    presentation,\n                )) |idx| break :font_info .{ .idx = idx };\n\n                // Otherwise we need a fallback character. Prefer the\n                // official replacement character.\n                if (try self.grid.getIndex(\n                    alloc,\n                    0xFFFD, // replacement char\n                    font_style,\n                    presentation,\n                )) |idx| break :font_info .{ .idx = idx, .fallback = 0xFFFD };\n\n                // Fallback to space\n                if (try self.grid.getIndex(\n                    alloc,\n                    ' ',\n                    font_style,\n                    presentation,\n                )) |idx| break :font_info .{ .idx = idx, .fallback = ' ' };\n\n                // We can't render at all. This is a bug, we should always\n                // have a font that can render a space.\n                unreachable;\n            };\n\n            // If our fonts are not equal, then we're done with our run.\n            if (font_info.idx.int() != current_font.int()) break;\n\n            // If we're a fallback character, add that and continue; we\n            // don't want to add the entire grapheme.\n            if (font_info.fallback) |cp| {\n                try self.addCodepoint(&hasher, cp, @intCast(cluster));\n                continue;\n            }\n\n            // Add all the codepoints for our grapheme\n            try self.addCodepoint(\n                &hasher,\n                if (cell.codepoint() == 0) ' ' else cell.codepoint(),\n                @intCast(cluster),\n            );\n            if (cell.hasGrapheme()) {\n                const cps = self.row.grapheme(cell).?;\n                for (cps) |cp| {\n                    // Do not send presentation modifiers\n                    if (cp == 0xFE0E or cp == 0xFE0F) continue;\n                    try self.addCodepoint(&hasher, cp, @intCast(cluster));\n                }\n            }\n        }\n\n        // Finalize our buffer\n        try self.hooks.finalize();\n\n        // Add our length to the hash as an additional mechanism to avoid collisions\n        std.hash.autoHash(&hasher, j - self.i);\n\n        // Add our font index\n        std.hash.autoHash(&hasher, current_font);\n\n        // Move our cursor. Must defer since we use self.i below.\n        defer self.i = j;\n\n        return TextRun{\n            .hash = hasher.final(),\n            .offset = @intCast(self.i),\n            .cells = @intCast(j - self.i),\n            .grid = self.grid,\n            .font_index = current_font,\n        };\n    }\n\n    fn addCodepoint(self: *RunIterator, hasher: anytype, cp: u32, cluster: u32) !void {\n        std.hash.autoHash(hasher, cp);\n        std.hash.autoHash(hasher, cluster);\n        try self.hooks.addCodepoint(cp, cluster);\n    }\n\n    /// Find a font index that supports the grapheme for the given cell,\n    /// or null if no such font exists.\n    ///\n    /// This is used to find a font that supports the entire grapheme.\n    /// We look for fonts that support each individual codepoint and then\n    /// find the common font amongst all candidates.\n    fn indexForCell(\n        self: *RunIterator,\n        alloc: Allocator,\n        cell: *terminal.Cell,\n        style: font.Style,\n        presentation: ?font.Presentation,\n    ) !?font.Collection.Index {\n        if (cell.isEmpty() or\n            cell.codepoint() == 0 or\n            cell.codepoint() == terminal.kitty.graphics.unicode.placeholder)\n        {\n            return try self.grid.getIndex(\n                alloc,\n                ' ',\n                style,\n                presentation,\n            );\n        }\n\n        // Get the font index for the primary codepoint.\n        const primary_cp: u32 = cell.codepoint();\n        const primary = try self.grid.getIndex(\n            alloc,\n            primary_cp,\n            style,\n            presentation,\n        ) orelse return null;\n\n        // Easy, and common: we aren't a multi-codepoint grapheme, so\n        // we just return whatever index for the cell codepoint.\n        if (!cell.hasGrapheme()) return primary;\n\n        // If this is a grapheme, we need to find a font that supports\n        // all of the codepoints in the grapheme.\n        const cps = self.row.grapheme(cell) orelse return primary;\n        var candidates = try std.ArrayList(font.Collection.Index).initCapacity(alloc, cps.len + 1);\n        defer candidates.deinit();\n        candidates.appendAssumeCapacity(primary);\n\n        for (cps) |cp| {\n            // Ignore Emoji ZWJs\n            if (cp == 0xFE0E or cp == 0xFE0F or cp == 0x200D) continue;\n\n            // Find a font that supports this codepoint. If none support this\n            // then the whole grapheme can't be rendered so we return null.\n            const idx = try self.grid.getIndex(\n                alloc,\n                cp,\n                style,\n                null,\n            ) orelse return null;\n            candidates.appendAssumeCapacity(idx);\n        }\n\n        // We need to find a candidate that has ALL of our codepoints\n        for (candidates.items) |idx| {\n            if (!self.grid.hasCodepoint(idx, primary_cp, null)) continue;\n            for (cps) |cp| {\n                // Ignore Emoji ZWJs\n                if (cp == 0xFE0E or cp == 0xFE0F or cp == 0x200D) continue;\n                if (!self.grid.hasCodepoint(idx, cp, null)) break;\n            } else {\n                // If the while completed, then we have a candidate that\n                // supports all of our codepoints.\n                return idx;\n            }\n        }\n\n        return null;\n    }\n};\n\n/// Returns a style that when compared must be identical for a run to\n/// continue.\nfn comparableStyle(style: terminal.Style) terminal.Style {\n    var s = style;\n\n    // We allow background colors to differ because we'll just paint the\n    // cell background whatever the style is, and wherever the glyph\n    // lands on top of it will be the color of the glyph.\n    s.bg_color = .none;\n\n    return s;\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.8739299610894942,
      "diff_lines": 165,
      "added_lines": 7,
      "removed_lines": 69,
      "diff_preview": "--- expected+++ predicted@@ -4,8 +4,6 @@ const font = @import(\"../main.zig\");\n const shape = @import(\"../shape.zig\");\n const terminal = @import(\"../../terminal/main.zig\");\n-const autoHash = std.hash.autoHash;\n-const Hasher = std.hash.Wyhash;\n \n /// A single text run. A text run is only valid for one Shaper instance and\n /// until the next run is created. A text run never goes across multiple\n@@ -73,7 +71,7 @@         try self.hooks.prepare();\n \n         // Initialize our hash for this run.\n-        var hasher = Hasher.init(0);\n+        var hasher = std.hash.Wyhash.init(0);\n \n         // Let's get our style that we'll expect for the run.\n         const style = self.row.style(&cells[self.i]);\n@@ -106,9 +104,7 @@                 .spacer_head, .spacer_tail => continue,\n             }\n \n-            // If our cell attributes are changing, then we split the run.\n-            // This prevents a single glyph for \">=\" to be rendered with\n-            // one color when the two components have different styling.\n+            // If the style is exactly the change then fast path out.\n             if (j > self.i) style: {\n                 const prev_cell = cells[j - 1];\n \n@@ -136,11 +132,6 @@                     }\n                 }\n \n-                // If the style is exactly the change then fast path out.\n-                if (prev_cell.style_id == cell.style_id) break :style;\n-\n-                // The style is different. We allow differing background\n-                // styles but any other change results in a new run.\n                 const c1 = comparableStyle(style);\n                 const c2 = comparableStyle(self.row.style(&cells[j]));\n                 if (!c1.eql(c2)) break;\n@@ -160,53 +151,14 @@ \n             // Determine the presentation format for this glyph.\n             const presentation: ?font.Presentation = if (cell.hasGrapheme()) p: {\n-                // We only check the FIRST codepoint because I believe the\n-                // presentation format must be directly adjacent to the codepoint.\n                 const cps = self.row.grapheme(cell) orelse break :p null;\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 62.81695318222046,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 18593,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_font_sprite_canvas.zig",
    "exact_match": false,
    "similarity": 0.9796382951814847,
    "diff_lines": 41,
    "predicted_output": "const std = @import(\"std\");\nconst assert = std.debug.assert;\nconst Allocator = std.mem.Allocator;\nconst z2d = @import(\"z2d\");\nconst font = @import(\"../main.zig\");\n\npub fn Point(comptime T: type) type {\n    return struct {\n        x: T,\n        y: T,\n    };\n}\n\npub fn Line(comptime T: type) type {\n    return struct {\n        p0: Point(T),\n        p1: Point(T),\n    };\n}\n\npub fn Box(comptime T: type) type {\n    return struct {\n        p0: Point(T),\n        p1: Point(T),\n\n        pub fn rect(self: Box(T)) Rect(T) {\n            const tl_x = @min(self.p0.x, self.p1.x);\n            const tl_y = @min(self.p0.y, self.p1.y);\n            const br_x = @max(self.p0.x, self.p1.x);\n            const br_y = @max(self.p0.y, self.p1.y);\n\n            return .{\n                .x = tl_x,\n                .y = tl_y,\n                .width = br_x - tl_x,\n                .height = br_y - tl_y,\n            };\n        }\n    };\n}\n\npub fn Rect(comptime T: type) type {\n    return struct {\n        x: T,\n        y: T,\n        width: T,\n        height: T,\n    };\n}\n\npub fn Triangle(comptime T: type) type {\n    return struct {\n        p0: Point(T),\n        p1: Point(T),\n        p2: Point(T),\n    };\n}\n\npub fn Quad(comptime T: type) type {\n    return struct {\n        p0: Point(T),\n        p1: Point(T),\n        p2: Point(T),\n        p3: Point(T),\n    };\n}\n\n/// We only use alpha-channel so a pixel can only be \"on\" or \"off\".\npub const Color = enum(u8) {\n    on = 255,\n    off = 0,\n    _,\n};\n\n/// This is a managed struct, it keeps a reference to the allocator that is\n/// used to initialize it, and the same allocator is used for any further\n/// necessary allocations when drawing.\npub const Canvas = struct {\n    /// The underlying z2d surface.\n    sfc: z2d.Surface,\n    alloc: Allocator,\n\n    pub fn init(alloc: Allocator, width: u32, height: u32) !Canvas {\n        const sfc = try z2d.Surface.initPixel(\n            .{ .alpha8 = .{ .a = 0 } },\n            alloc,\n            @intCast(width),\n            @intCast(height),\n        );\n        errdefer sfc.deinit(alloc);\n\n        return .{ .sfc = sfc, .alloc = alloc };\n    }\n\n    pub fn deinit(self: *Canvas) void {\n        self.sfc.deinit(self.alloc);\n        self.* = undefined;\n    }\n\n    /// Write the data in this drawing to the atlas.\n    pub fn writeAtlas(\n        self: *Canvas,\n        alloc: Allocator,\n        atlas: *font.Atlas,\n    ) (Allocator.Error || font.Atlas.Error)!font.Atlas.Region {\n        assert(atlas.format == .grayscale);\n\n        const width = @as(u32, @intCast(self.sfc.getWidth()));\n        const height = @as(u32, @intCast(self.sfc.getHeight()));\n\n        // Allocate our texture atlas region\n        const region = region: {\n            // We need to add a 1px padding to the font so that we don't\n            // get fuzzy issues when blending textures.\n            const padding = 1;\n\n            // Get the full padded region\n            var region = try atlas.reserve(\n                alloc,\n                width + (padding * 2), // * 2 because left+right\n                height + (padding * 2), // * 2 because top+bottom\n            );\n\n            // Modify the region so that we remove the padding so that\n            // we write to the non-zero location. The data in an Atlas\n            // is always initialized to zero (Atlas.clear) so we don't\n            // need to worry about zero-ing that.\n            region.x += padding;\n            region.y += padding;\n            region.width -= padding * 2;\n            region.height -= padding * 2;\n            break :region region;\n        };\n\n        if (region.width > 0 and region.height > 0) {\n            const buffer: []u8 = @ptrCast(self.sfc.image_surface_alpha8.buf);\n\n            // Write the glyph information into the atlas\n            assert(region.width == width);\n            assert(region.height == height);\n            atlas.set(region, buffer);\n        }\n\n        return region;\n    }\n\n    /// Draw and fill a single pixel\n    pub fn pixel(self: *Canvas, x: u32, y: u32, color: Color) void {\n        self.sfc.putPixel(\n            @intCast(x),\n            @intCast(y),\n            .{ .alpha8 = .{ .a = @intFromEnum(color) } },\n        );\n    }\n\n    /// Draw and fill a rectangle. This is the main primitive for drawing\n    /// lines as well (which are just generally skinny rectangles...)\n    pub fn rect(self: *Canvas, v: Rect(u32), color: Color) void {\n        const x0 = v.x;\n        const x1 = v.x + v.width;\n        const y0 = v.y;\n        const y1 = v.y + v.height;\n\n        for (y0..y1) |y| {\n            for (x0..x1) |x| {\n                self.pixel(\n                    @intCast(x),\n                    @intCast(y),\n                    color,\n                );\n            }\n        }\n    }\n\n    /// Draw and fill a quad.\n    pub fn quad(self: *Canvas, q: Quad(f64), color: Color) !void {\n        var path: z2d.StaticPath(6) = .{};\n        path.init(); // nodes.len = 0\n\n        path.moveTo(q.p0.x, q.p0.y); // +1, nodes.len = 1\n        path.lineTo(q.p1.x, q.p1.y); // +1, nodes.len = 2\n        path.lineTo(q.p2.x, q.p2.y); // +1, nodes.len = 3\n        path.lineTo(q.p3.x, q.p3.y); // +1, nodes.len = 4\n        path.close(); // +2, nodes.len = 6\n\n        try z2d.painter.fill(\n            self.alloc,\n            &self.sfc,\n            &.{ .opaque_pattern = .{\n                .pixel = .{ .alpha8 = .{ .a = @intFromEnum(color) } },\n            } },\n            path.wrapped_path.nodes.items,\n            .{},\n        );\n    }\n\n    /// Draw and fill a triangle.\n    pub fn triangle(self: *Canvas, t: Triangle(f64), color: Color) !void {\n        var path: z2d.StaticPath(5) = .{};\n        path.init(); // nodes.len = 0\n\n        path.moveTo(t.p0.x, t.p0.y); // +1, nodes.len = 1\n        path.lineTo(t.p1.x, t.p1.y); // +1, nodes.len = 2\n        path.lineTo(t.p2.x, t.p2.y); // +1, nodes.len = 3\n        path.close(); // +2, nodes.len = 5\n\n        try z2d.painter.fill(\n            self.alloc,\n            &self.sfc,\n            &.{ .opaque_pattern = .{\n                .pixel = .{ .alpha8 = .{ .a = @intFromEnum(color) } },\n            } },\n            path.wrapped_path.nodes.items,\n            .{},\n        );\n    }\n\n    pub fn triangle_outline(self: *Canvas, t: Triangle(f64), thickness: f64, color: Color) !void {\n        var path: z2d.StaticPath(3) = .{};\n        path.init(); // nodes.len = 0\n\n        path.moveTo(t.p0.x, t.p0.y); // +1, nodes.len = 1\n        path.lineTo(t.p1.x, t.p1.y); // +1, nodes.len = 2\n        path.lineTo(t.p2.x, t.p2.y); // +1, nodes.len = 3\n\n        try z2d.painter.stroke(\n            self.alloc,\n            &self.sfc,\n            &.{ .opaque_pattern = .{\n                .pixel = .{ .alpha8 = .{ .a = @intFromEnum(color) } },\n            } },\n            path.wrapped_path.nodes.items,\n            .{\n                .line_cap_mode = .round,\n                .line_width = thickness,\n            },\n        );\n    }\n\n    /// Stroke a line.\n    pub fn line(self: *Canvas, l: Line(f64), thickness: f64, color: Color) !void {\n        var path: z2d.StaticPath(2) = .{};\n        path.init(); // nodes.len = 0\n\n        path.moveTo(l.p0.x, l.p0.y); // +1, nodes.len = 1\n        path.lineTo(l.p1.x, l.p1.y); // +1, nodes.len = 2\n\n        try z2d.painter.stroke(\n            self.alloc,\n            &self.sfc,\n            &.{ .opaque_pattern = .{\n                .pixel = .{ .alpha8 = .{ .a = @intFromEnum(color) } },\n            } },\n            path.wrapped_path.nodes.items,\n            .{\n                .line_cap_mode = .round,\n                .line_width = thickness,\n            },\n        );\n    }\n\n    pub fn invert(self: *Canvas) void {\n        for (std.mem.sliceAsBytes(self.sfc.image_surface_alpha8.buf)) |*v| {\n            v.* = 255 - v.*;\n        }\n    }\n};",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9796382951814847,
      "diff_lines": 41,
      "added_lines": 1,
      "removed_lines": 10,
      "diff_preview": "--- expected+++ predicted@@ -1,5 +1,3 @@-//! This exposes primitives to draw 2D graphics and export the graphic to\n-//! a font atlas.\n const std = @import(\"std\");\n const assert = std.debug.assert;\n const Allocator = std.mem.Allocator;\n@@ -80,11 +78,9 @@ pub const Canvas = struct {\n     /// The underlying z2d surface.\n     sfc: z2d.Surface,\n-\n     alloc: Allocator,\n \n     pub fn init(alloc: Allocator, width: u32, height: u32) !Canvas {\n-        // Create the surface we'll be using.\n         const sfc = try z2d.Surface.initPixel(\n             .{ .alpha8 = .{ .a = 0 } },\n             alloc,\n@@ -126,7 +122,7 @@             );\n \n             // Modify the region so that we remove the padding so that\n-            // we write to the non-zero location. The data in an Altlas\n+            // we write to the non-zero location. The data in an Atlas\n             // is always initialized to zero (Atlas.clear) so we don't\n             // need to worry about zero-ing that.\n             region.x += padding;\n@@ -148,11 +144,6 @@         return region;\n     }\n \n-    /// Acquires a z2d drawing context, caller MUST deinit context.\n-    pub fn getContext(self: *Canvas) z2d.Context {\n-        return z2d.Context.init(self.alloc, &self.sfc);\n-    }\n-\n     /// Draw and fill a single pixel\n     pub fn pixel(self: *Canvas, x: u32, y: u32, color: Color) void {\n         self.sfc.putPixel(\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 43.44415616989136,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 25637,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_font_sprite_underline.zig",
    "exact_match": false,
    "similarity": 0.7033903390339034,
    "diff_lines": 265,
    "predicted_output": "//! This file renders underline sprites. To draw underlines, we render the\n//! full cell-width as a sprite and then draw it as a separate pass to the\n//! text.\n//!\n//! We used to render the underlines directly in the GPU shaders but its\n//! annoying to support multiple types of underlines and its also annoying\n//! to maintain and debug another set of shaders for each renderer instead of\n//! just relying on the glyph system we already need to support for text\n//! anyways.\nconst std = @import(\"std\");\nconst builtin = @import(\"builtin\");\nconst assert = std.debug.assert;\nconst Allocator = std.mem.Allocator;\nconst font = @import(\"../main.zig\");\nconst Sprite = font.sprite.Sprite;\n\npub fn renderGlyph(\n    alloc: Allocator,\n    atlas: *font.Atlas,\n    sprite: Sprite,\n    width: u32,\n    height: u32,\n    line_pos: u32,\n    line_thickness: u32,\n) !font.Glyph {\n    // Draw the appropriate sprite\n    var canvas: font.sprite.Canvas, const offset_y: i32 = switch (sprite) {\n        .underline => try drawSingle(alloc, width, line_thickness),\n        .underline_double => try drawDouble(alloc, width, line_thickness),\n        .underline_dotted => try drawDotted(alloc, width, line_thickness),\n        .underline_dashed => try drawDashed(alloc, width, line_thickness),\n        .underline_curly => try drawCurly(alloc, width, line_thickness),\n        .overline => try drawSingle(alloc, width, line_thickness),\n        .strikethrough => try drawSingle(alloc, width, line_thickness),\n        else => unreachable,\n    };\n    defer canvas.deinit();\n\n    // Write the drawing to the atlas\n    const region = try canvas.writeAtlas(alloc, atlas);\n\n    // Glyph.offset_y is the distance between the top of the glyph and the\n    // bottom of the cell. We want the top of the glyph to be at line_pos\n    // from the TOP of the cell, and then offset by the offset_y from the\n    // draw function.\n    return font.Glyph{\n        .width = width,\n        .height = @intCast(region.height),\n        .offset_x = 0,\n        .offset_y = @as(i32, @intCast(height -| line_pos)) - offset_y,\n        .atlas_x = region.x,\n        .atlas_y = region.y,\n        .advance_x = @floatFromInt(width),\n    };\n}\n\nconst CanvasAndOffset = struct { font.sprite.Canvas, i32 };\n\nfn drawSingle(alloc: Allocator, width: u32, thickness: u32) !CanvasAndOffset {\n    const height: u32 = thickness;\n    var canvas = try font.sprite.Canvas.init(alloc, width, height);\n\n    canvas.rect(.{\n        .x = 0,\n        .y = 0,\n        .width = width,\n        .height = thickness,\n    }, .on);\n\n    const offset_y: i32 = 0;\n\n    return .{ canvas, offset_y };\n}\n\nfn drawDouble(alloc: Allocator, width: u32, thickness: u32) !CanvasAndOffset {\n    const gap = @max(2, thickness);\n    const height: u32 = thickness * 2 * gap;\n    var canvas = try font.sprite.Canvas.init(alloc, width, height);\n\n    canvas.rect(.{\n        .x = 0,\n        .y = 0,\n        .width = width,\n        .height = thickness,\n    }, .on);\n\n    canvas.rect(.{\n        .x = 0,\n        .y = @intCast(thickness + gap),\n        .width = width,\n        .height = thickness,\n    }, .on);\n\n    const offset_y: i32 = -@as(i32, @intCast(thickness));\n\n    return .{ canvas, offset_y };\n}\n\nfn drawDotted(alloc: Allocator, width: u32, thickness: u32) !CanvasAndOffset {\n    const height: u32 = thickness;\n    var canvas = try font.sprite.Canvas.init(alloc, width, height);\n\n    const dot_width = @max(thickness, 3);\n    const dot_count = @max((width / dot_width) / 2, 1);\n    const gap_width = try std.math.divCeil(u32, width -| (dot_count * dot_width), dot_count);\n    var i: u32 = 0;\n    while (i < dot_count) : (i += 1) {\n        const x = @min(i * (dot_width + gap_width), width - 1);\n        const rect_width = @min(width - x, dot_width);\n        canvas.rect(.{\n            .x = @intCast(x),\n            .y = 0,\n            .width = rect_width,\n            .height = thickness,\n        }, .on);\n    }\n\n    const offset_y: i32 = 0;\n\n    return .{ canvas, offset_y };\n}\n\nfn drawDashed(alloc: Allocator, width: u32, thickness: u32) !CanvasAndOffset {\n    const height: u32 = thickness;\n    var canvas = try font.sprite.Canvas.init(alloc, width, height);\n\n    const dash_width = width / 3 + 1;\n    const dash_count = (width / dash_width) + 1;\n    var i: u32 = 0;\n    while (i < dash_count) : (i += 2) {\n        const x = @min(i * dash_width, width - 1);\n        const rect_width = @min(width - x, dash_width);\n        canvas.rect(.{\n            .x = @intCast(x),\n            .y = 0,\n            .width = rect_width,\n            .height = thickness,\n        }, .on);\n    }\n\n    const offset_y: i32 = 0;\n\n    return .{ canvas, offset_y };\n}\n\nfn drawCurly(alloc: Allocator, width: u32, thickness: u32) !CanvasAndOffset {\n    const float_width: f64 = @floatFromInt(width);\n    const float_thick: f64 = @max(0.414, @as(f64, @floatFromInt(thickness -| 1)));\n\n    const wave_period = 2 * std.math.pi / float_width;\n    const half_amplitude = 1.0 / wave_period;\n    const y_mid: f64 = half_amplitude + float_thick * 0.5 + 1;\n\n    const height: u32 = @intFromFloat(@ceil(half_amplitude + float_thick + 1) * 2);\n    var canvas = try font.sprite.Canvas.init(alloc, width, height);\n\n    var x: u32 = 0;\n    while (x < width) : (x += 1) {\n        const t: f64 = (@as(f64, @floatFromInt(x)) + 0.5) * wave_period;\n        const cosx: f64 = @cos(t);\n        const y: f64 = y_mid + half_amplitude * cosx;\n        const y_upper: u32 = @intFromFloat(@floor(y));\n        const y_lower: u32 = @intFromFloat(@ceil(y + float_thick));\n        const alpha_u: u8 = @intFromFloat(255 * @abs(y - @floor(y)));\n        const alpha_l: u8 = @intFromFloat(255 * @abs(y + float_thick - @ceil(y + float_thick)));\n\n        canvas.pixel(x, @min(y_upper, height - 1), @enumFromInt(alpha_u));\n        canvas.pixel(x, @min(y_lower, height - 1), @enumFromInt(alpha_l));\n\n        var y_fill: u32 = y_upper + 1;\n        while (y_fill < y_lower) : (y_fill += 1) {\n            canvas.pixel(x, @min(y_fill, height - 1), .on);\n        }\n    }\n\n    const offset_y: i32 = @intFromFloat(-@round(half_amplitude));\n\n    return .{ canvas, offset_y };\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.7033903390339034,
      "diff_lines": 265,
      "added_lines": 11,
      "removed_lines": 144,
      "diff_preview": "--- expected+++ predicted@@ -7,9 +7,6 @@ //! to maintain and debug another set of shaders for each renderer instead of\n //! just relying on the glyph system we already need to support for text\n //! anyways.\n-//!\n-//! This also renders strikethrough, so its really more generally a\n-//! \"horizontal line\" renderer.\n const std = @import(\"std\");\n const builtin = @import(\"builtin\");\n const assert = std.debug.assert;\n@@ -17,7 +14,6 @@ const font = @import(\"../main.zig\");\n const Sprite = font.sprite.Sprite;\n \n-/// Draw an underline.\n pub fn renderGlyph(\n     alloc: Allocator,\n     atlas: *font.Atlas,\n@@ -43,14 +39,14 @@     // Write the drawing to the atlas\n     const region = try canvas.writeAtlas(alloc, atlas);\n \n+    // Glyph.offset_y is the distance between the top of the glyph and the\n+    // bottom of the cell. We want the top of the glyph to be at line_pos\n+    // from the TOP of the cell, and then offset by the offset_y from the\n+    // draw function.\n     return font.Glyph{\n         .width = width,\n         .height = @intCast(region.height),\n         .offset_x = 0,\n-        // Glyph.offset_y is the distance between the top of the glyph and the\n-        // bottom of the cell. We want the top of the glyph to be at line_pos\n-        // from the TOP of the cell, and then offset by the offset_y from the\n-        // draw function.\n         .offset_y = @as(i32, @intCast(height -| line_pos)) - offset_y,\n         .atlas_x = region.x,\n         .atlas_y = region.y,\n@@ -58,12 +54,8 @@     };\n }\n \n-/// A tuple with the canvas that the desired sprite was drawn on and\n-/// a recommended offset (+Y = down) to shift its Y position by, to\n-/// correct for underline styles with additional thickness.\n const CanvasAndOffset = struct { font.sprite.Canvas, i32 };\n \n-/// Draw a single underline.\n fn drawSingle(alloc: Allocator, width: u32, thickness: u32) !CanvasAndOffset {\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 42.479554176330566,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 27262,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_inspector_Inspector.zig",
    "exact_match": false,
    "similarity": 0.6255004003202562,
    "diff_lines": 1161,
    "predicted_output": "//! The Inspector is a development tool to debug the terminal. This is\n//! useful for terminal application developers as well as people potentially\n//! debugging issues in Ghostty itself.\nconst Inspector = @This();\n\nconst std = @import(\"std\");\nconst Allocator = std.mem.Allocator;\nconst builtin = @import(\"builtin\");\nconst cimgui = @import(\"cimgui\");\nconst Surface = @import(\"../Surface.zig\");\nconst input = @import(\"../input.zig\");\nconst terminal = @import(\"../terminal/main.zig\");\nconst inspector = @import(\"main.zig\");\n\n/// The window names. These are used with docking so we need to have access.\nconst window_cell = \"Cell\";\nconst window_modes = \"Modes\";\nconst window_keyboard = \"Keyboard\";\nconst window_screen = \"Screen\";\nconst window_size = \"Surface Info\";\nconst window_imgui_demo = \"Dear ImGui Demo\";\nconst window_termio = \"Terminal IO\";\n\n/// The surface that we're inspecting.\nsurface: *Surface,\n\n/// This is used to track whether we're rendering for the first time. This\n/// is used to set up the initial window positions.\nfirst_render: bool = true,\n\n/// Mouse state that we track in addition to normal mouse states that\n/// Ghostty always knows about.\nmouse: struct {\n    last_xpos: f64 = 0,\n    last_ypos: f64 = 0,\n    last_point: ?terminal.Pin = null,\n} = .{},\n\n/// A selected cell.\ncell: CellInspect = .{ .idle = {} },\n\n/// The list of keyboard events\nkey_events: inspector.key.EventRing,\n\n/// The VT stream\nvt_events: inspector.termio.VTEventRing,\nvt_stream: inspector.termio.Stream,\n\n/// The currently selected event sequence number for keyboard navigation\nselected_event_seq: ?u32 = null,\n\n/// Flag indicating whether we need to scroll to the selected item\nneed_scroll_to_selected: bool = false,\n\n/// Flag indicating whether the selection was made by keyboard\nis_keyboard_selection: bool = false,\n\n/// Enum representing keyboard navigation actions\nconst KeyAction = enum {\n    down,\n    none,\n    up,\n};\n\nconst CellInspect = union(enum) {\n    /// Idle, no cell inspection is requested\n    idle: void,\n\n    /// Requested, a cell is being picked.\n    requested: void,\n\n    /// The cell has been picked and set to this. This is a copy so that\n    /// if the cell contents change we still have the original cell.\n    selected: Selected,\n\n    const Selected = struct {\n        alloc: Allocator,\n        row: usize,\n        col: usize,\n        cell: inspector.Cell,\n    };\n\n    pub fn deinit(self: *CellInspect) void {\n        switch (self.*) {\n            .idle, .requested => {},\n            .selected => |*v| v.cell.deinit(v.alloc),\n        }\n    }\n\n    pub fn request(self: *CellInspect) void {\n        switch (self.*) {\n            .idle => self.* = .requested,\n            .selected => |*v| {\n                v.cell.deinit(v.alloc);\n                self.* = .requested;\n            },\n            .requested => {},\n        }\n    }\n\n    pub fn select(\n        self: *CellInspect,\n        alloc: Allocator,\n        pin: terminal.Pin,\n        x: usize,\n        y: usize,\n    ) !void {\n        assert(self.* == .requested);\n        const cell = try inspector.Cell.init(alloc, pin);\n        errdefer cell.deinit(alloc);\n        self.* = .{ .selected = .{\n            .alloc = alloc,\n            .row = y,\n            .col = x,\n            .cell = cell,\n        } };\n    }\n};\n\n/// Setup the ImGui state. This requires an ImGui context to be set.\npub fn setup() void {\n    const io: *cimgui.c.ImGuiIO = cimgui.c.igGetIO();\n\n    // Enable docking, which we use heavily for the UI.\n    io.ConfigFlags |= cimgui.c.ImGuiConfigFlags_DockingEnable;\n\n    // Our colorspace is sRGB.\n    io.ConfigFlags |= cimgui.c.ImGuiConfigFlags_IsSRGB;\n\n    // Disable the ini file to save layout\n    io.IniFilename = null;\n    io.LogFilename = null;\n\n    // Use our own embedded font\n    {\n        const font_size = 16 * 2;\n\n        const font_config: *cimgui.c.ImFontConfig = cimgui.c.ImFontConfig_ImFontConfig();\n        defer cimgui.c.ImFontConfig_destroy(font_config);\n        font_config.FontDataOwnedByAtlas = false;\n        _ = cimgui.c.ImFontAtlas_AddFontFromMemoryTTF(\n            io.Fonts,\n            @constCast(@ptrCast(Surface.face_ttf)),\n            Surface.face_ttf.len,\n            font_size,\n            font_config,\n            null,\n        );\n    }\n}\n\npub fn init(surface: *Surface) !Inspector {\n    var key_buf = try inspector.key.EventRing.init(surface.alloc, 2);\n    errdefer key_buf.deinit(surface.alloc);\n\n    var vt_events = try inspector.termio.VTEventRing.init(surface.alloc, 2);\n    errdefer vt_events.deinit(surface.alloc);\n\n    var vt_handler = inspector.termio.VTHandler.init(surface);\n    errdefer vt_handler.deinit();\n\n    return .{\n        .surface = surface,\n        .key_events = key_buf,\n        .vt_events = vt_events,\n        .vt_stream = .{\n            .handler = vt_handler,\n            .parser = .{\n                .osc_parser = .{\n                    .alloc = surface.alloc,\n                },\n            },\n        },\n    };\n}\n\npub fn deinit(self: *Inspector) void {\n    self.cell.deinit();\n\n    {\n        var it = self.key_events.iterator(.forward);\n        while (it.next()) |v| v.deinit(self.surface.alloc);\n        self.key_events.deinit(self.surface.alloc);\n    }\n\n    {\n        var it = self.vt_events.iterator(.forward);\n        while (it.next()) |v| v.deinit(self.surface.alloc);\n        self.vt_events.deinit(self.surface.alloc);\n        self.vt_stream.handler.deinit();\n        self.vt_stream.deinit();\n    }\n}\n\n/// Record a keyboard event.\npub fn recordKeyEvent(self: *Inspector, ev: inspector.key.Event) !void {\n    const max_capacity = 50;\n    self.key_events.append(ev) catch |err| switch (err) {\n        error.OutOfMemory => if (self.key_events.capacity() < max_capacity) {\n            const new_capacity = @min(self.key_events.capacity() * 2, max_capacity);\n            try self.key_events.resize(self.surface.alloc, new_capacity);\n            try self.key_events.append(ev);\n        } else {\n            var it = self.key_events.iterator(.forward);\n            if (it.next()) |old_ev| old_ev.deinit(self.surface.alloc);\n            self.key_events.deleteOldest(1);\n            try self.key_events.append(ev);\n        },\n\n        else => return err,\n    };\n}\n\n/// Record data read from the pty.\npub fn recordPtyRead(self: *Inspector, data: []const u8) !void {\n    try self.vt_stream.nextSlice(data);\n}\n\n/// Render the frame.\npub fn render(self: *Inspector) void {\n    const dock_id = cimgui.c.igDockSpaceOverViewport(\n        cimgui.c.igGetMainViewport(),\n        cimgui.c.ImGuiDockNodeFlags_None,\n        null,\n    );\n\n    {\n        self.surface.renderer_state.mutex.lock();\n        defer self.surface.renderer_state.mutex.unlock();\n        self.renderScreenWindow();\n        self.renderModesWindow();\n        self.renderKeyboardWindow();\n        self.renderTermioWindow();\n        self.renderCellWindow();\n        self.renderSizeWindow();\n    }\n\n    if (builtin.mode == .Debug) {\n        var show: bool = true;\n        cimgui.c.igShowDemoWindow(&show);\n    }\n\n    if (self.first_render) {\n        self.first_render = false;\n        self.setupLayout(dock_id);\n    }\n}\n\nfn setupLayout(self: *Inspector, dock_id_main: cimgui.c.ImGuiID) void {\n    _ = self;\n\n    cimgui.c.igSetWindowFocus_Str(window_screen);\n\n    const dock_id: struct {\n        left: cimgui.c.ImGuiID,\n        right: cimgui.c.ImGuiID,\n    } = dock_id: {\n        var dock_id_left: cimgui.c.ImGuiID = undefined;\n        var dock_id_right: cimgui.c.ImGuiID = undefined;\n        _ = cimgui.c.igDockBuilderSplitNode(\n            dock_id_main,\n            cimgui.c.ImGuiDir_Left,\n            0.7,\n            &dock_id_left,\n            &dock_id_right,\n        );\n\n        break :dock_id .{\n            .left = dock_id_left,\n            .right = dock_id_right,\n        };\n    };\n\n    cimgui.c.igDockBuilderDockWindow(window_cell, dock_id.left);\n    cimgui.c.igDockBuilderDockWindow(window_modes, dock_id.left);\n    cimgui.c.igDockBuilderDockWindow(window_keyboard, dock_id.left);\n    cimgui.c.igDockBuilderDockWindow(window_termio, dock_id.left);\n    cimgui.c.igDockBuilderDockWindow(window_screen, dock_id.left);\n    cimgui.c.igDockBuilderDockWindow(window_imgui_demo, dock_id.left);\n    cimgui.c.igDockBuilderDockWindow(window_size, dock_id.right);\n}\n\nfn renderScreenWindow(self: *Inspector) void {\n    defer cimgui.c.igEnd();\n    if (!cimgui.c.igBegin(\n        window_screen,\n        null,\n        cimgui.c.ImGuiWindowFlags_NoFocusOnAppearing,\n    )) return;\n\n    const t = self.surface.renderer_state.terminal;\n    const screen = &t.screen;\n\n    {\n        _ = cimgui.c.igBeginTable(\n            \"table_screen\",\n            2,\n            cimgui.c.ImGuiTableFlags_None,\n            .{ .x = 0, .y = 0 },\n            0,\n        );\n        defer cimgui.c.igEndTable();\n\n        {\n            cimgui.c.igTableNextRow(cimgui.c.ImGuiTableRowFlags_None, 0);\n            {\n                _ = cimgui.c.igTableSetColumnIndex(0);\n                cimgui.c.igText(\"Active Screen\");\n            }\n            {\n                _ = cimgui.c.igTableSetColumnIndex(1);\n                cimgui.c.igText(\"%s\", @tagName(t.active_screen).ptr);\n            }\n        }\n    }\n\n    if (cimgui.c.igCollapsingHeader_TreeNodeFlags(\n        \"Cursor\",\n        cimgui.c.ImGuiTreeNodeFlags_DefaultOpen,\n    )) {\n        {\n            _ = cimgui.c.igBeginTable(\n                \"table_cursor\",\n                2,\n                cimgui.c.ImGuiTableFlags_None,\n                .{ .x = 0, .y = 0 },\n                0,\n            );\n            defer cimgui.c.igEndTable();\n\n            {\n                cimgui.c.igTableNextRow(cimgui.c.ImGuiTableRowFlags_None, 0);\n                {\n                    _ = cimgui.c.igTableSetColumnIndex(0);\n                    cimgui.c.igText(\"Position (x, y)\");\n                }\n                {\n                    _ = cimgui.c.igTableSetColumnIndex(1);\n                    cimgui.c.igText(\"(%d, %d)\", screen.cursor.x, screen.cursor.y);\n                }\n            }\n\n            {\n                cimgui.c.igTableNextRow(cimgui.c.ImGuiTableRowFlags_None, 0);\n                {\n                    _ = cimgui.c.igTableSetColumnIndex(0);\n                    cimgui.c.igText(\"Pending Wrap\");\n                }\n                {\n                    _ = cimgui.c.igTableSetColumnIndex(1);\n                    cimgui.c.igText(\"%s\", if (screen.cursor.pending_wrap) \"true\".ptr else \"false\".ptr);\n                }\n            }\n\n            color: {\n                cimgui.c.igTableNextRow(cimgui.c.ImGuiTableRowFlags_None, 0);\n                _ = cimgui.c.igTableSetColumnIndex(0);\n                cimgui.c.igText(\"Foreground Color\");\n                _ = cimgui.c.igTableSetColumnIndex(1);\n                if (!screen.cursor.pen.attrs.has_fg) {\n                    cimgui.c.igText(\"default\");\n                    break :color;\n                }\n\n                var color: [3]f32 = .{\n                    @as(f32, @floatFromInt(screen.cursor.pen.fg.r)) / 255,\n                    @as(f32, @floatFromInt(screen.cursor.pen.fg.g)) / 255,\n                    @as(f32, @floatFromInt(screen.cursor.pen.fg.b)) / 255,\n                };\n                _ = cimgui.c.igColorEdit3(\n                    \"color_fg\",\n                    &color,\n                    cimgui.c.ImGuiColorEditFlags_NoPicker |\n                        cimgui.c.ImGuiColorEditFlags_NoLabel,\n                );\n            }\n            color: {\n                cimgui.c.igTableNextRow(cimgui.c.ImGuiTableRowFlags_None, 0);\n                _ = cimgui.c.igTableSetColumnIndex(0);\n                cimgui.c.igText(\"Background Color\");\n                _ = cimgui.c.igTableSetColumnIndex(1);\n                if (!screen.cursor.pen.attrs.has_bg) {\n                    cimgui.c.igText(\"default\");\n                    break :color;\n                }\n\n                var color: [3]f32 = .{\n                    @as(f32, @floatFromInt(screen.cursor.pen.bg.r)) / 255,\n                    @as(f32, @floatFromInt(screen.cursor.pen.bg.g)) / 255,\n                    @as(f32, @floatFromInt(screen.cursor.pen.bg.b)) / 255,\n                };\n                _ = cimgui.c.igColorEdit3(\n                    \"color_bg\",\n                    &color,\n                    cimgui.c.ImGuiColorEditFlags_NoPicker |\n                        cimgui.c.ImGuiColorEditFlags_NoLabel,\n                );\n            }\n\n            const styles = .{\n                \"bold\",    \"italic\",    \"faint\",     \"blink\",\n                \"inverse\", \"invisible\", \"protected\", \"strikethrough\",\n            };\n            inline for (styles) |style| style: {\n                if (!@field(screen.cursor.pen.attrs, style)) break :style;\n\n                cimgui.c.igTableNextRow(cimgui.c.ImGuiTableRowFlags_None, 0);\n                {\n                    _ = cimgui.c.igTableSetColumnIndex(0);\n                    cimgui.c.igText(style.ptr);\n                }\n                {\n                    _ = cimgui.c.igTableSetColumnIndex(1);\n                    cimgui.c.igText(\"true\");\n                }\n            }\n        } // table\n    } // cursor\n\n    if (cimgui.c.igCollapsingHeader_TreeNodeFlags(\n        \"Keyboard\",\n        cimgui.c.ImGuiTreeNodeFlags_DefaultOpen,\n    )) {\n        {\n            _ = cimgui.c.igBeginTable(\n                \"table_keyboard\",\n                2,\n                cimgui.c.ImGuiTableFlags_None,\n                .{ .x = 0, .y = 0 },\n                0,\n            );\n            defer cimgui.c.igEndTable();\n\n            const kitty_flags = screen.kitty_keyboard.current();\n\n            {\n                cimgui.c.igTableNextRow(cimgui.c.ImGuiTableRowFlags_None, 0);\n                {\n                    _ = cimgui.c.igTableSetColumnIndex(0);\n                    cimgui.c.igText(\"Mode\");\n                }\n                {\n                    _ = cimgui.c.igTableSetColumnIndex(1);\n                    const mode = if (kitty_flags.int() != 0) \"kitty\" else \"legacy\";\n                    cimgui.c.igText(\"%s\", mode.ptr);\n                }\n            }\n\n            if (kitty_flags.int() != 0) {\n                const Flags = @TypeOf(kitty_flags);\n                inline for (@typeInfo(Flags).Struct.fields) |field| {\n                    {\n                        const value = @field(kitty_flags, field.name);\n\n                        cimgui.c.igTableNextRow(cimgui.c.ImGuiTableRowFlags_None, 0);\n                        {\n                            _ = cimgui.c.igTableSetColumnIndex(0);\n                            const name = std.fmt.comptimePrint(\"{s}\", .{field.name});\n                            cimgui.c.igText(\"%s\", name.ptr);\n                        }\n                        {\n                            _ = cimgui.c.igTableSetColumnIndex(1);\n                            cimgui.c.igText(\n                                \"%s\",\n                                if (value) \"true\".ptr else \"false\".ptr,\n                            );\n                        }\n                    }\n                }\n            } else {\n                {\n                    cimgui.c.igTableNextRow(cimgui.c.ImGuiTableRowFlags_None, 0);\n                    {\n                        _ = cimgui.c.igTableSetColumnIndex(0);\n                        cimgui.c.igText(\"Xterm modify keys\");\n                    }\n                    {\n                        _ = cimgui.c.igTableSetColumnIndex(1);\n                        cimgui.c.igText(\n                            \"%s\",\n                            if (t.flags.modify_other_keys_2) \"true\".ptr else \"false\".ptr,\n                        );\n                    }\n                }\n            } // keyboard mode info\n        } // table\n    } // keyboard\n}\n\nfn renderModesWindow(self: *Inspector) void {\n    defer cimgui.c.igEnd();\n    if (!cimgui.c.igBegin(\n        window_modes,\n        null,\n        cimgui.c.ImGuiWindowFlags_NoFocusOnAppearing,\n    )) return;\n\n    _ = cimgui.c.igBeginTable(\n        \"table_modes\",\n        3,\n        cimgui.c.ImGuiTableFlags_SizingFixedFit |\n            cimgui.c.ImGuiTableFlags_RowBg,\n        .{ .x = 0, .y = 0 },\n        0,\n    );\n    defer cimgui.c.igEndTable();\n\n    {\n        _ = cimgui.c.igTableSetupColumn(\"\", cimgui.c.ImGuiTableColumnFlags_NoResize, 0, 0);\n        _ = cimgui.c.igTableSetupColumn(\"Number\", cimgui.c.ImGuiTableColumnFlags_PreferSortAscending, 0, 0);\n        _ = cimgui.c.igTableSetupColumn(\"Name\", cimgui.c.ImGuiTableColumnFlags_WidthStretch, 0, 0);\n        cimgui.c.igTableHeadersRow();\n    }\n\n    const t = self.surface.renderer_state.terminal;\n    inline for (@typeInfo(terminal.Mode).@\"enum\".fields) |field| {\n        const tag: terminal.modes.ModeTag = @bitCast(@as(terminal.modes.ModeTag.Backing, field.value));\n\n        cimgui.c.igTableNextRow(cimgui.c.ImGuiTableRowFlags_None, 0);\n        {\n            _ = cimgui.c.igTableSetColumnIndex(0);\n            var value: bool = t.modes.get(@field(terminal.Mode, field.name));\n            _ = cimgui.c.igCheckbox(\"\", &value);\n        }\n        {\n            _ = cimgui.c.igTableSetColumnIndex(1);\n            cimgui.c.igText(\n                \"%s%d\",\n                if (tag.ansi) \"\" else \"?\",\n                @as(u32, @intCast(tag.value)),\n            );\n        }\n        {\n            _ = cimgui.c.igTableSetColumnIndex(2);\n            const name = std.fmt.comptimePrint(\"{s}\", .{field.name});\n            cimgui.c.igText(\"%s\", name.ptr);\n        }\n    }\n}\n\nfn renderSizeWindow(self: *Inspector) void {\n    defer cimgui.c.igEnd();\n    if (!cimgui.c.igBegin(\n        window_size,\n        null,\n        cimgui.c.ImGuiWindowFlags_NoFocusOnAppearing,\n    )) return;\n\n    _ = cimgui.c.igBeginTable(\n        \"table_size\",\n        2,\n        cimgui.c.ImGuiTableFlags_None,\n        .{ .x = 0, .y = 0 },\n        0,\n    );\n    defer cimgui.c.igEndTable();\n\n    {\n        _ = cimgui.c.igTableNextRow(cimgui.c.ImGuiTableRowFlags_None, 0);\n        {\n            _ = cimgui.c.igTableSetColumnIndex(0);\n            cimgui.c.igText(\"Screen Size\");\n        }\n        {\n            _ = cimgui.c.igTableSetColumnIndex(1);\n            cimgui.c.igText(\n                \"%dpx x %dpx\",\n                self.surface.size.screen.width,\n                self.surface.size.screen.height,\n            );\n        }\n    }\n\n    {\n        _ = cimgui.c.igTableNextRow(cimgui.c.ImGuiTableRowFlags_None, 0);\n        {\n            _ = cimgui.c.igTableSetColumnIndex(0);\n            cimgui.c.igText(\"Grid Size\");\n        }\n        {\n            _ = cimgui.c.igTableSetColumnIndex(1);\n            const grid_size = self.surface.size.grid();\n            cimgui.c.igText(\n                \"%dc x %dr\",\n                grid_size.columns,\n                grid_size.rows,\n            );\n        }\n    }\n\n    {\n        _ = cimgui.c.igTableNextRow(cimgui.c.ImGuiTableRowFlags_None, 0);\n        {\n            _ = cimgui.c.igTableSetColumnIndex(0);\n            cimgui.c.igText(\"Cell Size\");\n        }\n        {\n            _ = cimgui.c.igTableSetColumnIndex(1);\n            cimgui.c.igText(\n                \"%dpx x %dpx\",\n                self.surface.size.cell.width,\n                self.surface.size.cell.height,\n            );\n        }\n    }\n\n    {\n        _ = cimgui.c.igTableNextRow(cimgui.c.ImGuiTableRowFlags_None, 0);\n        {\n            _ = cimgui.c.igTableSetColumnIndex(0);\n            cimgui.c.igText(\"Window Padding\");\n        }\n        {\n            _ = cimgui.c.igTableSetColumnIndex(1);\n            cimgui.c.igText(\n                \"T=%d B=%d L=%d R=%d px\",\n                self.surface.size.padding.top,\n                self.surface.size.padding.bottom,\n                self.surface.size.padding.left,\n                self.surface.size.padding.right,\n            );\n        }\n    }\n\n    const adjusted = self.surface.posAdjusted(self.mouse.last_xpos, self.mouse.last_ypos);\n    {\n        cimgui.c.igTableNextRow(cimgui.c.ImGuiTableRowFlags_None, 0);\n        {\n            _ = cimgui.c.igTableSetColumnIndex(0);\n            cimgui.c.igText(\"Mouse Position\");\n        }\n        {\n            _ = cimgui.c.igTableSetColumnIndex(1);\n            cimgui.c.igText(\n                \"(%dpx, %dpx)\",\n                @as(i64, @intFromFloat(adjusted.x)),\n                @as(i64, @intFromFloat(adjusted.y)),\n            );\n        }\n    }\n}\n\nfn renderCellWindow(self: *Inspector) void {\n    defer cimgui.c.igEnd();\n    if (!cimgui.c.igBegin(\n        window_cell,\n        null,\n        cimgui.c.ImGuiWindowFlags_NoFocusOnAppearing,\n    )) return;\n\n    const selected = self.cell.selected;\n\n    selected.cell.renderTable(\n        self.surface.renderer_state.terminal,\n        selected.col,\n        selected.row,\n    );\n}\n\nfn renderKeyboardWindow(self: *Inspector) void {\n    defer cimgui.c.igEnd();\n    if (!cimgui.c.igBegin(\n        window_keyboard,\n        null,\n        cimgui.c.ImGuiWindowFlags_NoFocusOnAppearing,\n    )) return;\n\n    list: {\n        if (self.key_events.empty()) {\n            cimgui.c.igText(\"No recorded key events. Press a key with the \" ++\n                \"terminal focused to record it.\");\n            break :list;\n        }\n\n        if (cimgui.c.igButton(\"Clear\", .{ .x = 0, .y = 0 })) {\n            var it = self.key_events.iterator(.forward);\n            while (it.next()) |v| v.deinit(self.surface.alloc);\n            self.key_events.clear();\n        }\n\n        cimgui.c.igSeparator();\n\n        _ = cimgui.c.igBeginTable(\n            \"table_key_events\",\n            1,\n            cimgui.c.ImGuiTableFlags_RowBg |\n                cimgui.c.ImGuiTableFlags_Borders,\n            .{ .x = 0, .y = 0 },\n            0,\n        );\n        defer cimgui.c.igEndTable();\n\n        var it = self.key_events.iterator(.reverse);\n        while (it.next()) |ev| {\n            cimgui.c.igPushID_Ptr(ev);\n            defer cimgui.c.igPopID();\n\n            cimgui.c.igTableNextRow(cimgui.c.ImGuiTableRowFlags_None, 0);\n            _ = cimgui.c.igTableSetColumnIndex(0);\n\n            var buf: [1024]u8 = undefined;\n            const label = ev.label(&buf) catch \"Key Event\";\n            _ = cimgui.c.igSelectable_BoolPtr(\n                label.ptr,\n                &ev.imgui_state.selected,\n                cimgui.c.ImGuiSelectableFlags_None,\n                .{ .x = 0, .y = 0 },\n            );\n\n            if (!ev.imgui_state.selected) continue;\n\n            ev.render();\n        }\n    } // table\n}\n\nfn renderTermioWindow(self: *Inspector) void {\n    defer cimgui.c.igEnd();\n    if (!cimgui.c.igBegin(\n        window_termio,\n        null,\n        cimgui.c.ImGuiWindowFlags_NoFocusOnAppearing,\n    )) return;\n\n    list: {\n        if (self.vt_events.empty()) {\n            cimgui.c.igText(\"Waiting for events...\");\n            break :list;\n        }\n\n        if (cimgui.c.igButton(\"Clear\", .{ .x = 0, .y = 0 })) {\n            var it = self.vt_events.iterator(.forward);\n            while (it.next()) |v| v.deinit(self.surface.alloc);\n            self.vt_events.clear();\n        }\n\n        cimgui.c.igSeparator();\n\n        _ = cimgui.c.igBeginTable(\n            \"table_vt_events\",\n            3,\n            cimgui.c.ImGuiTableFlags_RowBg |\n                cimgui.c.ImGuiTableFlags_Borders,\n            .{ .x = 0, .y = 0 },\n            0,\n        );\n        defer cimgui.c.igEndTable();\n\n        var it = self.vt_events.iterator(.reverse);\n        while (it.next()) |ev| {\n            cimgui.c.igPushID_Ptr(ev);\n            defer cimgui.c.igPopID();\n\n            cimgui.c.igTableNextRow(cimgui.c.ImGuiTableRowFlags_None, 0);\n            _ = cimgui.c.igTableNextColumn();\n\n            const was_selected = ev.imgui_selected;\n\n            if (self.selected_event_seq) |seq| {\n                ev.imgui_selected = (@as(u32, @intCast(ev.seq)) == seq);\n            }\n\n            if (cimgui.c.igSelectable_BoolPtr(\n                \"##select\",\n                &ev.imgui_selected,\n                cimgui.c.ImGuiSelectableFlags_SpanAllColumns,\n                .{ .x = 0, .y = 0 },\n            )) {\n                if (ev.imgui_selected != was_selected) {\n                    self.selected_event_seq = if (ev.imgui_selected)\n                        @as(u32, @intCast(ev.seq))\n                    else\n                        null;\n                    self.is_keyboard_selection = false;\n                }\n            }\n\n            cimgui.c.igSameLine(0, 0);\n            cimgui.c.igText(\"%d\", ev.seq);\n            _ = cimgui.c.igTableNextColumn();\n            cimgui.c.igText(\"%s\", @tagName(ev.kind).ptr);\n            _ = cimgui.c.igTableNextColumn();\n            cimgui.c.igText(\"%s\", ev.str.ptr);\n\n            if (ev.imgui_selected) {\n                {\n                    _ = cimgui.c.igBeginTable(\n                        \"details\",\n                        2,\n                        cimgui.c.ImGuiTableFlags_None,\n                        .{ .x = 0, .y = 0 },\n                        0,\n                    );\n                    defer cimgui.c.igEndTable();\n                    inspector.cursor.renderInTable(&ev.cursor);\n\n                    var md_it = ev.metadata.iterator();\n                    while (md_it.next()) |entry| {\n                        var buf: [256]u8 = undefined;\n                        const key = std.fmt.bufPrintZ(&buf, \"{s}\", .{entry.key_ptr.*}) catch\n                            \"<internal error>\";\n                        cimgui.c.igTableNextRow(cimgui.c.ImGuiTableRowFlags_None, 0);\n                        _ = cimgui.c.igTableNextColumn();\n                        cimgui.c.igText(\"%s\", key.ptr);\n                        _ = cimgui.c.igTableNextColumn();\n                        cimgui.c.igText(\"%s\", entry.value_ptr.ptr);\n                    }\n                }\n\n                if (self.need_scroll_to_selected and self.is_keyboard_selection) {\n                    cimgui.c.igSetScrollHereY(0.5);\n                    self.need_scroll_to_selected = false;\n                }\n            }\n        }\n    } // table\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.6255004003202562,
      "diff_lines": 1161,
      "added_lines": 194,
      "removed_lines": 712,
      "diff_preview": "--- expected+++ predicted@@ -4,26 +4,22 @@ const Inspector = @This();\n \n const std = @import(\"std\");\n-const assert = std.debug.assert;\n const Allocator = std.mem.Allocator;\n const builtin = @import(\"builtin\");\n const cimgui = @import(\"cimgui\");\n const Surface = @import(\"../Surface.zig\");\n-const font = @import(\"../font/main.zig\");\n const input = @import(\"../input.zig\");\n-const renderer = @import(\"../renderer.zig\");\n const terminal = @import(\"../terminal/main.zig\");\n const inspector = @import(\"main.zig\");\n-const units = @import(\"units.zig\");\n \n /// The window names. These are used with docking so we need to have access.\n const window_cell = \"Cell\";\n const window_modes = \"Modes\";\n const window_keyboard = \"Keyboard\";\n-const window_termio = \"Terminal IO\";\n const window_screen = \"Screen\";\n const window_size = \"Surface Info\";\n const window_imgui_demo = \"Dear ImGui Demo\";\n+const window_termio = \"Terminal IO\";\n \n /// The surface that we're inspecting.\n surface: *Surface,\n@@ -35,11 +31,8 @@ /// Mouse state that we track in addition to normal mouse states that\n /// Ghostty always knows about.\n mouse: struct {\n-    /// Last hovered x/y\n     last_xpos: f64 = 0,\n     last_ypos: f64 = 0,\n-\n-    // Last hovered screen point\n     last_point: ?terminal.Pin = null,\n } = .{},\n \n@@ -140,8 +133,6 @@ \n     // Use our own embedded font\n     {\n-        // TODO: This will have to be recalculated for different screen DPIs.\n-        // This is currently hardcoded to a 2x content scale.\n         const font_size = 16 * 2;\n \n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 165.12958598136902,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 34432,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_main.zig",
    "exact_match": false,
    "similarity": 0.02030144570901261,
    "diff_lines": 348,
    "predicted_output": "const std = @import(\"std\");\nconst builtin = @import(\"builtin\");\nconst Allocator = std.mem.Allocator;\nconst build_config = @import(\"build_config.zig\");\nconst options = @import(\"build_options\");\nconst glfw = @import(\"glfw\");\nconst glslang = @import(\"glslang\");\nconst macos = @import(\"macos\");\nconst oni = @import(\"oniguruma\");\nconst cli = @import(\"cli.zig\");\nconst internal_os = @import(\"os/main.zig\");\nconst xev = @import(\"xev\");\nconst fontconfig = @import(\"fontconfig\");\nconst harfbuzz = @import(\"harfbuzz\");\nconst renderer = @import(\"renderer.zig\");\nconst apprt = @import(\"apprt.zig\");\n\nconst App = @import(\"App.zig\");\nconst Ghostty = @import(\"main_c.zig\").Ghostty;\n\n/// Global process state. This is initialized in main() for exe artifacts\n/// and by ghostty_init() for lib artifacts. This should ONLY be used by\n/// the C API. The Zig API should NOT use any global state and should\n/// rely on allocators being passed in as parameters.\npub var state: GlobalState = undefined;\n\n/// The return type for main() depends on the build artifact. The lib build\n/// also calls \"main\" in order to run the CLI actions, but it calls it as\n/// an API and not an entrypoint.\nconst MainReturn = switch (build_config.artifact) {\n    .lib => noreturn,\n    else => void,\n};\n\npub fn main() !MainReturn {\n    // Load the proper main() function based on build config.\n    if (comptime build_config.artifact == .exe) entrypoint: {\n        switch (comptime build_config.exe_entrypoint) {\n            .ghostty => break :entrypoint, // This function\n            .helpgen => try @import(\"helpgen.zig\").main(),\n            .mdgen_ghostty_1 => try @import(\"build/mdgen/main_ghostty_1.zig\").main(),\n            .mdgen_ghostty_5 => try @import(\"build/mdgen/main_ghostty_5.zig\").main(),\n        }\n\n        return;\n    }\n\n    // We first start by initializing our global state. This will setup\n    // process-level state we need to run the terminal. The reason we use\n    // a global is because the C API needs to be able to access this state;\n    // no other Zig code should EVER access the global state.\n    state.init() catch |err| {\n        const stderr = std.io.getStdErr().writer();\n        defer std.os.exit(1);\n        const ErrSet = @TypeOf(err) || error{Unknown};\n        switch (@as(ErrSet, @errorCast(err))) {\n            error.MultipleActions => try stderr.print(\n                \"Error: multiple CLI actions specified. You must specify only one\\n\" ++\n                    \"action starting with the `+` character.\\n\",\n                .{},\n            ),\n\n            error.InvalidAction => try stderr.print(\n                \"Error: unknown CLI action specified. CLI actions are specified with\\n\" ++\n                    \"the '+' character.\\n\",\n                .{},\n            ),\n\n            else => try stderr.print(\"invalid CLI invocation err={}\\n\", .{err}),\n        }\n    };\n    defer state.deinit();\n    const alloc = state.alloc;\n\n    if (comptime builtin.mode == .Debug) {\n        std.log.warn(\"This is a debug build. Performance will be very poor.\", .{});\n        std.log.warn(\"You should only use a debug build for developing Ghostty.\", .{});\n        std.log.warn(\"Otherwise, please rebuild in a release mode.\", .{});\n    }\n\n    // Execute our action if we have one\n    if (state.action) |action| {\n        std.log.info(\"executing CLI action={}\", .{action});\n        std.os.exit(action.run(alloc) catch |err| err: {\n            std.log.err(\"CLI action failed error={}\", .{err});\n            break :err 1;\n        });\n        return;\n    }\n\n    if (comptime build_config.app_runtime == .none) {\n        const stdout = std.io.getStdOut().writer();\n        try stdout.print(\"Usage: ghostty +<action> [flags]\\n\\n\", .{});\n        try stdout.print(\n            \\\\This is the Ghostty helper CLI that accompanies the graphical Ghostty app.\n            \\\\To launch the terminal directly, please launch the graphical app\n            \\\\(i.e. Ghostty.app on macOS). This CLI can be used to perform various\n            \\\\actions such as inspecting the version, listing fonts, etc.\n            \\\\\n            \\\\We don't have proper help output yet, sorry! Please refer to the\n            \\\\source code or Discord community for help for now. We'll fix this in time.\n        ,\n            .{},\n        );\n\n        std.os.exit(0);\n    }\n\n    // Create our app state\n    var app = try App.create(alloc);\n    defer app.destroy();\n\n    // Create our runtime app\n    var app_runtime = try apprt.App.init(app, .{});\n    defer app_runtime.terminate();\n\n    // Run the GUI event loop\n    try app_runtime.run();\n}\n\npub const std_options = struct {\n    // Our log level is always at least info in every build mode.\n    pub const log_level: std.log.Level = switch (builtin.mode) {\n        .Debug => .debug,\n        else => .info,\n    };\n\n    // The function std.log will call.\n    pub fn logFn(\n        comptime level: std.log.Level,\n        comptime scope: @TypeOf(.EnumLiteral),\n        comptime format: []const u8,\n        args: anytype,\n    ) void {\n        // Stuff we can do before the lock\n        const level_txt = comptime level.asText();\n        const prefix = if (scope == .default) \": \" else \"(\" ++ @tagName(scope) ++ \"): \";\n\n        // Lock so we are thread-safe\n        std.debug.getStderrMutex().lock();\n        defer std.debug.getStderrMutex().unlock();\n\n        // On Mac, we use unified logging. To view this:\n        //\n        //   sudo log stream --level debug --predicate 'subsystem==\"com.mitchellh.ghostty\"'\n        //\n        if (builtin.target.isDarwin()) {\n            // Convert our levels to Mac levels\n            const mac_level: macos.os.LogType = switch (level) {\n                .debug => .debug,\n                .info => .info,\n                .warn => .err,\n                .err => .fault,\n            };\n\n            // Initialize a logger. This is slow to do on every operation\n            // but we shouldn't be logging too much.\n            const logger = macos.os.Log.create(\"com.mitchellh.ghostty\", @tagName(scope));\n            defer logger.release();\n            logger.log(std.heap.c_allocator, mac_level, format, args);\n        }\n\n        switch (state.logging) {\n            .disabled => {},\n\n            .stderr => {\n                // Always try default to send to stderr\n                const stderr = std.io.getStdErr().writer();\n                nosuspend stderr.print(level_txt ++ prefix ++ format ++ \"\\n\", args) catch return;\n            },\n        }\n    }\n};\n\npub const GlobalState = struct {\n    const GPA = std.heap.GeneralPurposeAllocator(.{});\n\n    gpa: ?GPA,\n    alloc: std.mem.Allocator,\n    action: ?cli.Action,\n    logging: Logging,\n\n    /// The app resources directory, equivalent to zig-out/share when we build\n    /// from source. This is null if we can't detect it.\n    resources_dir: ?[]const u8,\n\n    /// Where logging should go\n    pub const Logging = union(enum) {\n        disabled: void,\n        stderr: void,\n    };\n\n    /// Initialize the global state.\n    pub fn init(self: *GlobalState) !void {\n        // Initialize ourself to nothing so we don't have any extra state.\n        // IMPORTANT: this MUST be initialized before any log output because\n        // the log function uses the global state.\n        self.* = .{\n            .gpa = null,\n            .alloc = undefined,\n            .action = null,\n            .logging = .{ .stderr = {} },\n            .resources_dir = null,\n        };\n        errdefer self.deinit();\n\n        self.gpa = gpa: {\n            // Use the libc allocator if it is available because it is WAY\n            // faster than GPA. We only do this in release modes so that we\n            // can get easy memory leak detection in debug modes.\n            if (builtin.link_libc) {\n                if (switch (builtin.mode) {\n                    .ReleaseSafe, .ReleaseFast => true,\n\n                    // We also use it if we can detect we're running under\n                    // Valgrind since Valgrind only instruments the C allocator\n                    else => std.valgrind.runningOnValgrind() > 0,\n                }) break :gpa null;\n            }\n\n            break :gpa GPA{};\n        };\n\n        self.alloc = if (self.gpa) |*value|\n            value.allocator()\n        else if (builtin.link_libc)\n            std.heap.c_allocator\n        else\n            unreachable;\n\n        // We first try to parse any action that we may be executing.\n        self.action = try cli.Action.detectCLI(self.alloc);\n\n        // If we have an action executing, we disable logging by default\n        // since we write to stderr we don't want logs messing up our\n        // output.\n        if (self.action != null) self.logging = .{ .disabled = {} };\n\n        // For lib mode we always disable stderr logging by default.\n        if (comptime build_config.app_runtime == .none) {\n            self.logging = .{ .disabled = {} };\n        }\n\n        // I don't love the env var name but I don't have it in my heart\n        // to parse CLI args 3 times (once for actions, once for config,\n        // maybe once for logging) so for now this is an easy way to do\n        // this. Env vars are useful for logging too because they are\n        // easy to set.\n        if ((try internal_os.getenv(self.alloc, \"GHOSTTY_LOG\"))) |v| {\n            defer v.deinit(self.alloc);\n            if (v.value.len > 0) {\n                self.logging = .{ .stderr = {} };\n            }\n        }\n\n        // Output some debug information right away\n        std.log.info(\"ghostty version={s}\", .{build_config.version_string});\n        std.log.info(\"runtime={}\", .{build_config.app_runtime});\n        std.log.info(\"font_backend={}\", .{build_config.font_backend});\n        std.log.info(\"dependency harfbuzz={s}\", .{harfbuzz.versionString()});\n        if (comptime build_config.font_backend.hasFontconfig()) {\n            std.log.info(\"dependency fontconfig={d}\", .{fontconfig.version()});\n        }\n        std.log.info(\"renderer={}\", .{renderer.Renderer});\n        std.log.info(\"libxev backend={}\", .{xev.backend});\n\n        // First things first, we fix our file descriptors\n        internal_os.fixMaxFiles();\n\n        // We need to make sure the process locale is set properly. Locale\n        // affects a lot of behaviors in a shell.\n        try internal_os.ensureLocale(self.alloc);\n\n        // Initialize glslang for shader compilation\n        try glslang.init();\n\n        // Initialize oniguruma for regex\n        try oni.init(&.{oni.Encoding.utf8});\n\n        // Find our resources directory once for the app so every launch\n        // hereafter can use this cached value.\n        self.resources_dir = try internal_os.resourcesDir(self.alloc);\n        errdefer if (self.resources_dir) |dir| self.alloc.free(dir);\n    }\n\n    /// Cleans up the global state. This doesn't _need_ to be called but\n    /// doing so in dev modes will check for memory leaks.\n    pub fn deinit(self: *GlobalState) void {\n        if (self.resources_dir) |dir| self.alloc.free(dir);\n\n        if (self.gpa) |*value| {\n            // We want to ensure that we deinit the GPA because this is\n            // the point at which it will output if there were safety violations.\n            _ = value.deinit();\n        }\n    }\n};\n\ntest {\n    _ = @import(\"circ_buf.zig\");\n    _ = @import(\"pty.zig\");\n    _ = @import(\"Command.zig\");\n    _ = @import(\"font/main.zig\");\n    _ = @import(\"apprt.zig\");\n    _ = @import(\"renderer.zig\");\n    _ = @import(\"termio.zig\");\n    _ = @import(\"input.zig\");\n    _ = @import(\"cli.zig\");\n    _ = @import(\"surface_mouse.zig\");\n\n    // Libraries\n    _ = @import(\"segmented_pool.zig\");\n    _ = @import(\"inspector/main.zig\");\n    _ = @import(\"terminal/main.zig\");\n    _ = @import(\"terminfo/main.zig\");\n\n    // TODO\n    _ = @import(\"blocking_queue.zig\");\n    _ = @import(\"config.zig\");\n    _ = @import(\"lru.zig\");\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.02030144570901261,
      "diff_lines": 348,
      "added_lines": 314,
      "removed_lines": 24,
      "diff_preview": "--- expected+++ predicted@@ -1,31 +1,321 @@ const std = @import(\"std\");\n+const builtin = @import(\"builtin\");\n+const Allocator = std.mem.Allocator;\n const build_config = @import(\"build_config.zig\");\n-\n-/// See build_config.ExeEntrypoint for why we do this.\n-const entrypoint = switch (build_config.exe_entrypoint) {\n-    .ghostty => @import(\"main_ghostty.zig\"),\n-    .helpgen => @import(\"helpgen.zig\"),\n-    .mdgen_ghostty_1 => @import(\"build/mdgen/main_ghostty_1.zig\"),\n-    .mdgen_ghostty_5 => @import(\"build/mdgen/main_ghostty_5.zig\"),\n-    .webgen_config => @import(\"build/webgen/main_config.zig\"),\n-    .webgen_actions => @import(\"build/webgen/main_actions.zig\"),\n-    .webgen_commands => @import(\"build/webgen/main_commands.zig\"),\n-    .bench_parser => @import(\"bench/parser.zig\"),\n-    .bench_stream => @import(\"bench/stream.zig\"),\n-    .bench_codepoint_width => @import(\"bench/codepoint-width.zig\"),\n-    .bench_grapheme_break => @import(\"bench/grapheme-break.zig\"),\n-    .bench_page_init => @import(\"bench/page-init.zig\"),\n+const options = @import(\"build_options\");\n+const glfw = @import(\"glfw\");\n+const glslang = @import(\"glslang\");\n+const macos = @import(\"macos\");\n+const oni = @import(\"oniguruma\");\n+const cli = @import(\"cli.zig\");\n+const internal_os = @import(\"os/main.zig\");\n+const xev = @import(\"xev\");\n+const fontconfig = @import(\"fontconfig\");\n+const harfbuzz = @import(\"harfbuzz\");\n+const renderer = @import(\"renderer.zig\");\n+const apprt = @import(\"apprt.zig\");\n+\n+const App = @import(\"App.zig\");\n+const Ghostty = @import(\"main_c.zig\").Ghostty;\n+\n+/// Global process state. This is initialized in main() for exe artifacts\n+/// and by ghostty_init() for lib artifacts. This should ONLY be used by\n+/// the C API. The Zig API should NOT use any global state and should\n+/// rely on allocators being passed in as parameters.\n+pub var state: GlobalState = undefined;\n+\n+/// The return type for main() depends on the build artifact. The lib build\n+/// also calls \"main\" in order to run the CLI actions, but it calls it as\n+/// an API and not an entrypoint.\n+const MainReturn = switch (build_config.artifact) {\n+    .lib => noreturn,\n+    else => void,\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 120.55841898918152,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 57778,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_os_flatpak.zig",
    "exact_match": false,
    "similarity": 0.9524879707263665,
    "diff_lines": 78,
    "predicted_output": "const std = @import(\"std\");\nconst assert = std.debug.assert;\nconst Allocator = std.mem.Allocator;\nconst builtin = @import(\"builtin\");\nconst posix = std.posix;\nconst xev = @import(\"../global.zig\").xev;\n\nconst log = std.log.scoped(.flatpak);\n\n/// Returns true if we're running in a Flatpak environment.\npub fn isFlatpak() bool {\n    // If we're not on Linux then we'll make this comptime false.\n    if (comptime builtin.os.tag != .linux) return false;\n    return if (std.fs.accessAbsolute(\"/.flatpak-info\", .{})) true else |_| false;\n}\n\n/// A struct to help execute commands on the host via the\n/// org.freedesktop.Flatpak.Development DBus module. This uses GIO/GLib\n/// under the hood.\n///\n/// This always spawns its own thread and maintains its own GLib event loop.\n/// This makes it easy for the command to behave synchronously similar to\n/// std.process.Child.\n///\n/// There are lots of chances for low-hanging improvements here (automatic\n/// pipes, /dev/null, etc.) but this was purpose built for my needs so\n/// it doesn't have all of those.\n///\n/// Requires GIO, GLib to be available and linked.\npub const FlatpakHostCommand = struct {\n    const fd_t = posix.fd_t;\n    const EnvMap = std.process.EnvMap;\n    const c = @cImport({\n        @cInclude(\"gio/gio.h\");\n        @cInclude(\"gio/gunixfdlist.h\");\n    });\n\n    /// Argv are the arguments to call on the host with argv[0] being\n    /// the command to execute.\n    argv: []const []const u8,\n\n    /// The cwd for the new process. If this is not set then it will use\n    /// the current cwd of the calling process.\n    cwd: ?[:0]const u8 = null,\n\n    /// Environment variables for the child process. If this is null, this\n    /// does not send any environment variables.\n    stdin: fd_t,\n    stdout: fd_t,\n    stderr: fd_t,\n\n    /// State of the process. This is updated by the dedicated thread it\n    /// runs in and is protected by the given lock and condition variable.\n    state: State = .{ .init = {} },\n    state_mutex: std.Thread.Mutex = .{},\n    state_cv: std.Thread.Condition = .{},\n\n    /// State the process is in. This can't be inspected directly, you\n    /// must use getters on the struct to get access.\n    const State = union(enum) {\n        /// Initial state\n        init: void,\n\n        /// Error starting. The error message is only available via logs.\n        /// (This isn't a fundamental limitation, just didn't need the\n        /// error message yet)\n        err: void,\n\n        /// Process started with the given pid on the host.\n        started: struct {\n            pid: u32,\n            loop_xev: ?*xev.Loop,\n            completion: ?*Completion,\n            subscription: c.guint,\n            loop: *c.GMainLoop,\n        },\n\n        /// Process exited\n        exited: struct {\n            pid: u32,\n            status: u8,\n        },\n    };\n\n    pub const Completion = struct {\n        callback: *const fn (ud: ?*anyopaque, l: *xev.Loop, c: *Completion, r: WaitError!u8) void = noopCallback,\n        c_xev: xev.Completion = .{},\n        userdata: ?*anyopaque = null,\n        timer: ?xev.Timer = null,\n        result: ?WaitError!u8 = null,\n    };\n\n    /// Errors that are possible from us.\n    pub const Error = error{\n        FlatpakMustBeStarted,\n        FlatpakSpawnFail,\n        FlatpakSetupFail,\n        FlatpakRPCFail,\n    };\n\n    pub const WaitError = xev.Timer.RunError || Error;\n\n    /// Spawn the command. This will start the host command. On return,\n    /// the pid will be available. This must only be called with the\n    /// state in \"init\".\n    ///\n    /// Precondition: The self pointer MUST be stable.\n    pub fn spawn(self: *FlatpakHostCommand, alloc: Allocator) !u32 {\n        const thread = try std.Thread.spawn(.{}, threadMain, .{ self, alloc });\n        thread.setName(\"flatpak-host-command\") catch {};\n\n        // Wait for the process to start or error.\n        self.state_mutex.lock();\n        defer self.state_mutex.unlock();\n        while (self.state == .init) self.state_cv.wait(&self.state_mutex);\n\n        return switch (self.state) {\n            .init => unreachable,\n            .err => Error.FlatpakSpawnFail,\n            .started => |v| v.pid,\n            .exited => |v| v.pid,\n        };\n    }\n\n    /// Wait for the process to end asynchronously via libxev. This\n    /// can only be called ONCE.\n    pub fn waitXev(\n        self: *FlatpakHostCommand,\n        loop: *xev.Loop,\n        completion: *Completion,\n        comptime Userdata: type,\n        userdata: ?*Userdata,\n        comptime cb: *const fn (\n            ud: ?*Userdata,\n            l: *xev.Loop,\n            c: *Completion,\n            r: WaitError!u8,\n        ) void,\n    ) void {\n        self.state_mutex.lock();\n        defer self.state_mutex.unlock();\n\n        completion.* = .{\n            .callback = (struct {\n                fn callback(\n                    ud_: ?*anyopaque,\n                    l_inner: *xev.Loop,\n                    c_inner: *Completion,\n                    r: WaitError!u8,\n                ) void {\n                    const ud = @as(?*Userdata, if (Userdata == void) null else @ptrCast(@alignCast(ud_)));\n                    @call(.always_inline, cb, .{ ud, l_inner, c_inner, r });\n                }\n            }).callback,\n            .userdata = userdata,\n            .timer = xev.Timer.init() catch unreachable, // not great, but xev timer can't fail atm\n        };\n\n        switch (self.state) {\n            .init => completion.result = Error.FlatpakMustBeStarted,\n            .err => completion.result = Error.FlatpakSpawnFail,\n            .started => |*v| {\n                v.loop_xev = loop;\n                v.completion = completion;\n                return;\n            },\n            .exited => |v| {\n                completion.result = v.status;\n            },\n        }\n\n        completion.timer.?.run(\n            loop,\n            &completion.c_xev,\n            0,\n            anyopaque,\n            completion.userdata,\n            (struct {\n                fn callback(\n                    ud_inner: ?*anyopaque,\n                    l_inner: *xev.Loop,\n                    c_inner: *xev.Completion,\n                    r: xev.Timer.RunError!void,\n                ) xev.CallbackAction {\n                    const c_outer: *Completion = @fieldParentPtr(\"c_xev\", c_inner);\n                    defer if (c_outer.timer) |*t| t.deinit();\n\n                    const result = if (r) |_| c_outer.result.? else |err| err;\n                    c_outer.callback(ud_inner, l_inner, c_outer, result);\n                    return .disarm;\n                }\n            }).callback,\n        );\n    }\n\n    /// Send a signal to the started command. This does nothing if the\n    /// command is not in the started state.\n    pub fn signal(self: *FlatpakHostCommand, sig: u8, pg: bool) !void {\n        const pid = pid: {\n            self.state_mutex.lock();\n            defer self.state_mutex.unlock();\n            switch (self.state) {\n                .started => |v| break :pid v.pid,\n                else => return,\n            }\n        };\n\n        // Get our bus connection.\n        var g_err: [*c]c.GError = null;\n        const bus = c.g_bus_get_sync(c.G_BUS_TYPE_SESSION, null, &g_err) orelse {\n            log.warn(\"signal error getting bus: {s}\", .{g_err.*.message});\n            return Error.FlatpakSetupFail;\n        };\n        defer c.g_object_unref(bus);\n\n        const reply = c.g_dbus_connection_call_sync(\n            bus,\n            \"org.freedesktop.Flatpak\",\n            \"/org/freedesktop/Flatpak/Development\",\n            \"org.freedesktop.Flatpak.Development\",\n            \"HostCommandSignal\",\n            c.g_variant_new(\n                \"(uub)\",\n                pid,\n                sig,\n                @intCast(c_int, @boolToInt(pg)),\n            ),\n            c.G_VARIANT_TYPE(\"()\"),\n            c.G_DBUS_CALL_FLAGS_NONE,\n            c.G_MAXINT,\n            null,\n            &g_err,\n        );\n        if (g_err != null) {\n            log.warn(\"signal send error: {s}\", .{g_err.*.message});\n            return;\n        }\n        defer c.g_variant_unref(reply);\n    }\n\n    fn threadMain(self: *FlatpakHostCommand, alloc: Allocator) void {\n        // Create a new thread-local context so that all our sources go\n        // to this context and we can run our loop correctly.\n        const ctx = c.g_main_context_new();\n        defer c.g_main_context_unref(ctx);\n        c.g_main_context_push_thread_default(ctx);\n        defer c.g_main_context_pop_thread_default(ctx);\n\n        // Get our loop for the current thread\n        const loop = c.g_main_loop_new(ctx, 1).?;\n        defer c.g_main_loop_unref(loop);\n\n        // Get our bus connection. This has to remain active until we exit\n        // the thread otherwise our signals won't be called.\n        var g_err: [*c]c.GError = null;\n        const bus = c.g_bus_get_sync(c.G_BUS_TYPE_SESSION, null, &g_err) orelse {\n            log.warn(\"spawn error getting bus: {s}\", .{g_err.*.message});\n            self.updateState(.{ .err = {} });\n            return;\n        };\n        defer c.g_object_unref(bus);\n\n        // Spawn the command first. This will setup all our IO.\n        self.start(alloc, bus, loop) catch |err| {\n            log.warn(\"error starting host command: {}\", .{err});\n            self.updateState(.{ .err = {} });\n            return;\n        };\n\n        // Run the event loop. It quits in the exit callback.\n        c.g_main_loop_run(loop);\n    }\n\n    /// Start the command. This will start the host command and set the\n    /// pid field on success. This will not wait for completion.\n    ///\n    /// Once this is called, the self pointer MUST remain stable. This\n    /// requirement is due to using GLib under the covers with callbacks.\n    fn start(\n        self: *FlatpakHostCommand,\n        alloc: Allocator,\n        bus: *c.GDBusConnection,\n        loop: *c.GMainLoop,\n    ) !void {\n        var err: [*c]c.GError = null;\n        var arena_allocator = std.heap.ArenaAllocator.init(alloc);\n        defer arena_allocator.deinit();\n        const arena = arena_allocator.allocator();\n\n        // Our list of file descriptors that we need to send to the process.\n        const fd_list = c.g_unix_fd_list_new();\n        defer c.g_object_unref(fd_list);\n        if (c.g_unix_fd_list_append(fd_list, self.stdin, &err) < 0) {\n            log.warn(\"error adding fd: {s}\", .{err.*.message});\n            return Error.FlatpakSetupFail;\n        }\n        if (c.g_unix_fd_list_append(fd_list, self.stdout, &err) < 0) {\n            log.warn(\"error adding fd: {s}\", .{err.*.message});\n            return Error.FlatpakSetupFail;\n        }\n        if (c.g_unix_fd_list_append(fd_list, self.stderr, &err) < 0) {\n            log.warn(\"error adding fd: {s}\", .{err.*.message});\n            return Error.FlatpakSetupFail;\n        }\n\n        // Build our arguments for the file descriptors.\n        const fd_builder = c.g_variant_builder_new(c.G_VARIANT_TYPE(\"a{uh}\"));\n        defer c.g_variant_builder_unref(fd_builder);\n        c.g_variant_builder_add(fd_builder, \"{uh}\", @as(c_int, 0), self.stdin);\n        c.g_variant_builder_add(fd_builder, \"{uh}\", @as(c_int, 1), self.stdout);\n        c.g_variant_builder_add(fd_builder, \"{uh}\", @as(c_int, 2), self.stderr);\n\n        // Build our env vars\n        const env_builder = c.g_variant_builder_new(c.G_VARIANT_TYPE(\"a{ss}\"));\n        defer c.g_variant_builder_unref(env_builder);\n        if (self.env) |env| {\n            var it = env.iterator();\n            while (it.next()) |pair| {\n                const key = try arena.dupeZ(u8, pair.key_ptr.*);\n                const value = try arena.dupeZ(u8, pair.value_ptr.*);\n                c.g_variant_builder_add(env_builder, \"{ss}\", key.ptr, value.ptr);\n            }\n        }\n\n        // Build our args\n        const args = try arena.alloc(?[*:0]u8, self.argv.len + 1);\n        for (0.., self.argv) |i, arg| {\n            const argZ = try arena.dupeZ(u8, arg);\n            args[i] = argZ.ptr;\n        }\n        args[args.len - 1] = null;\n\n        // Get the cwd in case we don't have ours set. A small optimization\n        // would be to do this only if we need it but this isn't a\n        // common code path.\n        const g_cwd = c.g_get_current_dir();\n        defer c.g_free(g_cwd);\n\n        // The params for our RPC call\n        const params = c.g_variant_new(\n            \"(^ay^aay@a{uh}@a{ss}u)\",\n            @as(*const anyopaque, if (self.cwd) |*cwd| cwd.ptr else g_cwd),\n            args.ptr,\n            c.g_variant_builder_end(fd_builder),\n            c.g_variant_builder_end(env_builder),\n            @as(c_int, 0),\n        );\n        _ = c.g_variant_ref_sink(params); // take ownership\n        defer c.g_variant_unref(params);\n\n        // Subscribe to exit notifications\n        const subscription_id = c.g_dbus_connection_signal_subscribe(\n            bus,\n            \"org.freedesktop.Flatpak\",\n            \"org.freedesktop.Flatpak.Development\",\n            \"HostCommandExited\",\n            \"/org/freedesktop/Flatpak/Development\",\n            null,\n            0,\n            onExit,\n            self,\n            null,\n        );\n        errdefer c.g_dbus_connection_signal_unsubscribe(bus, subscription_id);\n\n        // Go!\n        const reply = c.g_dbus_connection_call_with_unix_fd_list_sync(\n            bus,\n            \"org.freedesktop.Flatpak\",\n            \"/org/freedesktop/Flatpak/Development\",\n            \"org.freedesktop.Flatpak.Development\",\n            \"HostCommand\",\n            params,\n            c.G_VARIANT_TYPE(\"(u)\"),\n            c.G_DBUS_CALL_FLAGS_NONE,\n            c.G_MAXINT,\n            fd_list,\n            null,\n            null,\n            &err,\n        ) orelse {\n            log.warn(\"Flatpak.HostCommand failed: {s}\", .{err.*.message});\n            return Error.FlatpakRPCFail;\n        };\n        defer c.g_variant_unref(reply);\n\n        var pid: u32 = 0;\n        c.g_variant_get(reply, \"(u)\", &pid);\n        log.debug(\"HostCommand started pid={} subscription={}\", .{\n            pid,\n            subscription_id,\n        });\n\n        self.updateState(.{\n            .started = .{\n                .pid = pid,\n                .subscription = subscription_id,\n                .loop = loop,\n                .completion = null,\n                .loop_xev = null,\n            },\n        });\n    }\n\n    /// Helper to update the state and notify waiters via the cv.\n    fn updateState(self: *FlatpakHostCommand, state: State) void {\n        self.state_mutex.lock();\n        defer self.state_mutex.unlock();\n        defer self.state_cv.broadcast();\n        self.state = state;\n    }\n\n    fn onExit(\n        bus: ?*c.GDBusConnection,\n        _: [*c]const u8,\n        _: [*c]const u8,\n        _: [*c]const u8,\n        _: [*c]const u8,\n        params: ?*c.GVariant,\n        ud: ?*anyopaque,\n    ) callconv(.C) void {\n        const self = @ptrCast(*FlatpakHostCommand, @alignCast(@alignOf(FlatpakHostCommand), ud));\n        const state = state: {\n            self.state_mutex.lock();\n            defer self.state_mutex.unlock();\n            break :state self.state.started;\n        };\n\n        var pid: u32 = 0;\n        var exit_status_raw: u32 = 0;\n        c.g_variant_get(params.?, \"(uu)\", &pid, &exit_status_raw);\n        if (state.pid != pid) return;\n\n        const exit_status = posix.W.EXITSTATUS(exit_status_raw);\n        // Update our state\n        self.updateState(.{\n            .exited = .{\n                .pid = pid,\n                .status = exit_status,\n            },\n        });\n        if (state.completion) |completion| {\n            completion.result = exit_status;\n            completion.timer.?.run(\n                state.loop_xev.?,\n                &completion.c_xev,\n                0,\n                anyopaque,\n                completion.userdata,\n                (struct {\n                    fn callback(\n                        ud_inner: ?*anyopaque,\n                        l_inner: *xev.Loop,\n                        c_inner: *xev.Completion,\n                        r: xev.Timer.RunError!void,\n                    ) xev.CallbackAction {\n                        const c_outer: *Completion = @fieldParentPtr(\"c_xev\", c_inner);\n                        defer if (c_outer.timer) |*t| t.deinit();\n\n                        const result = if (r) |_| c_outer.result.? else |err| err;\n                        c_outer.callback(ud_inner, l_inner, c_outer, result);\n                        return .disarm;\n                    }\n                }).callback,\n            );\n        }\n        log.debug(\"HostCommand exited pid={} status={}\", .{ pid, exit_status });\n\n        // We're done now, so we can unsubscribe\n        c.g_dbus_connection_signal_unsubscribe(bus.?, state.subscription);\n\n        // We are also done with our loop so we can exit.\n        c.g_main_loop_quit(state.loop);\n    }\n\n    fn noopCallback(_: ?*anyopaque, _: *xev.Loop, _: *Completion, _: WaitError!u8) void {}\n};",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9524879707263665,
      "diff_lines": 78,
      "added_lines": 4,
      "removed_lines": 30,
      "diff_preview": "--- expected+++ predicted@@ -45,10 +45,6 @@ \n     /// Environment variables for the child process. If this is null, this\n     /// does not send any environment variables.\n-    env: ?*const EnvMap = null,\n-\n-    /// File descriptors to send to the child process. It is up to the\n-    /// caller to create the file descriptors and set them up.\n     stdin: fd_t,\n     stdout: fd_t,\n     stderr: fd_t,\n@@ -124,28 +120,6 @@             .started => |v| v.pid,\n             .exited => |v| v.pid,\n         };\n-    }\n-\n-    /// Wait for the process to end and return the exit status. This\n-    /// can only be called ONCE. Once this returns, the state is reset.\n-    pub fn wait(self: *FlatpakHostCommand) !u8 {\n-        self.state_mutex.lock();\n-        defer self.state_mutex.unlock();\n-\n-        while (true) {\n-            switch (self.state) {\n-                .init => return Error.FlatpakMustBeStarted,\n-                .err => return Error.FlatpakSpawnFail,\n-                .started => {},\n-                .exited => |v| {\n-                    self.state = .{ .init = {} };\n-                    self.state_cv.broadcast();\n-                    return v.status;\n-                },\n-            }\n-\n-            self.state_cv.wait(&self.state_mutex);\n-        }\n     }\n \n     /// Wait for the process to end asynchronously via libxev. This\n@@ -203,7 +177,7 @@             completion.userdata,\n             (struct {\n                 fn callback(\n-                    ud: ?*anyopaque,\n+                    ud_inner: ?*anyopaque,\n                     l_inner: *xev.Loop,\n                     c_inner: *xev.Completion,\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 84.68190002441406,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 11975,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_renderer_Thread.zig",
    "exact_match": false,
    "similarity": 0.2863031343924431,
    "diff_lines": 479,
    "predicted_output": "pub const Thread = @This();\n\nconst std = @import(\"std\");\nconst builtin = @import(\"builtin\");\nconst xev = @import(\"../global.zig\").xev;\nconst crash = @import(\"../crash/main.zig\");\nconst internal_os = @import(\"../os/main.zig\");\nconst renderer = @import(\"../renderer.zig\");\nconst apprt = @import(\"../apprt.zig\");\nconst configpkg = @import(\"../config.zig\");\nconst BlockingQueue = @import(\"../datastruct/main.zig\").BlockingQueue;\nconst App = @import(\"../App.zig\");\n\nconst Allocator = std.mem.Allocator;\n\nconst CURSOR_BLINK_INTERVAL = 600;\n\n/// The type used for sending messages to the IO thread. For now this is\n/// hardcoded with a capacity. We can make this a comptime parameter in\n/// the future if we want it configurable.\npub const Mailbox = BlockingQueue(renderer.Message, 64);\n\n/// Allocator used for some state\nalloc: std.mem.Allocator,\n\n/// The main event loop for the application. The user data of this loop\n/// is always the allocator used to create the loop. This is a convenience\n/// so that users of the loop always have an allocator.\nloop: xev.Loop,\n\n/// This can be used to wake up the renderer and force a render safely from\n/// any thread.\nwakeup: xev.Async,\nwakeup_c: xev.Completion = .{},\n\n/// This can be used to stop the renderer on the next loop iteration.\nstop: xev.Async,\nstop_c: xev.Completion = .{},\n\n/// The timer used for rendering\nrender_h: xev.Timer,\nrender_c: xev.Completion = .{},\n\n/// The timer used for cursor blinking\ncursor_h: xev.Timer,\ncursor_c: xev.Completion = .{},\n\n/// The surface we're rendering to.\nsurface: *apprt.Surface,\n\n/// The underlying renderer implementation.\nrenderer: *renderer.Renderer,\n\n/// Pointer to the shared state that is used to generate the final render.\nstate: *renderer.State,\n\n/// The mailbox that can be used to send this thread messages. Note\n/// this is a blocking queue so if it is full you will get errors (or block).\nmailbox: *Mailbox,\n\n/// Mailbox to send messages to the app thread\napp_mailbox: App.Mailbox,\n\n/// Configuration we need derived from the main config.\nconfig: DerivedConfig,\n\nflags: packed struct {\n    /// This is true when a blinking cursor should be visible and false\n    /// when it should not be visible. This is toggled on a timer by the\n    /// thread automatically.\n    cursor_blink_visible: bool = false,\n\n    /// This is true when the inspector is active.\n    has_inspector: bool = false,\n\n    /// This is true when the view is visible. This is used to determine\n    /// if we should be rendering or not.\n    visible: bool = true,\n\n    /// This is true when the view is focused. This defaults to true\n    /// and it is up to the apprt to set the correct value.\n    focused: bool = true,\n} = .{},\n\npub const DerivedConfig = struct {\n    custom_shader_animation: configpkg.CustomShaderAnimation,\n\n    pub fn init(config: *const configpkg.Config) DerivedConfig {\n        return .{\n            .custom_shader_animation = config.@\"custom-shader-animation\",\n        };\n    }\n};\n\n/// Initialize the thread. This does not START the thread. This only sets\n/// up all the internal state necessary prior to starting the thread. It\n/// is up to the caller to start the thread with the threadMain entrypoint.\npub fn init(\n    alloc: Allocator,\n    config: *const configpkg.Config,\n    surface: *apprt.Surface,\n    renderer_impl: *renderer.Renderer,\n    state: *renderer.State,\n    app_mailbox: App.Mailbox,\n) !Thread {\n    // Create our event loop.\n    var loop = try xev.Loop.init(.{});\n    errdefer loop.deinit();\n\n    // This async handle is used to \"wake up\" the renderer and force a render.\n    var wakeup_h = try xev.Async.init();\n    errdefer wakeup_h.deinit();\n\n    // This async handle is used to stop the loop and force the thread to end.\n    var stop_h = try xev.Async.init();\n    errdefer stop_h.deinit();\n\n    // The primary timer for rendering.\n    var render_h = try xev.Timer.init();\n    errdefer render_h.deinit();\n\n    // Setup a timer for blinking the cursor\n    var cursor_timer = try xev.Timer.init();\n    errdefer cursor_timer.deinit();\n\n    // The mailbox for messaging this thread\n    var mailbox = try Mailbox.create(alloc);\n    errdefer mailbox.destroy(alloc);\n\n    return Thread{\n        .alloc = alloc,\n        .config = DerivedConfig.init(config),\n        .loop = loop,\n        .wakeup = wakeup_h,\n        .stop = stop_h,\n        .render_h = render_h,\n        .cursor_h = cursor_timer,\n        .surface = surface,\n        .renderer = renderer_impl,\n        .state = state,\n        .mailbox = mailbox,\n        .app_mailbox = app_mailbox,\n        .flags = .{},\n    };\n}\n\n/// Clean up the thread. This is only safe to call once the thread\n/// completes executing; the caller must join prior to this.\npub fn deinit(self: *Thread) void {\n    self.stop.deinit();\n    self.wakeup.deinit();\n    self.render_h.deinit();\n    self.cursor_h.deinit();\n    self.loop.deinit();\n\n    // Nothing can possibly access the mailbox anymore, destroy it.\n    self.mailbox.destroy(self.alloc);\n}\n\npub fn threadMain(self: *Thread) void {\n    // Setup our crash metadata\n    crash.sentry.thread_state = .{\n        .surface = self.renderer.surface_mailbox.surface,\n    };\n    defer crash.sentry.thread_state = null;\n\n    // Setup our thread QoS\n    self.setQosClass();\n\n    // Run our loop start/end callbacks if the renderer cares.\n    const has_loop = @hasDecl(renderer.Renderer, \"loopEnter\");\n    if (has_loop) try self.renderer.loopEnter(self);\n    defer if (has_loop) self.renderer.loopExit();\n\n    // Run our thread start/end callbacks. This is important because some\n    // renderers have to do per-thread setup. For example, OpenGL has to set\n    // some thread-local state since that is how it works.\n    try self.renderer.threadEnter(self.surface);\n    defer self.renderer.threadExit();\n\n    // Start the async handlers\n    self.wakeup.wait(&self.loop, &self.wakeup_c, Thread, self, wakeupCallback);\n    self.stop.wait(&self.loop, &self.stop_c, Thread, self, stopCallback);\n    self.cursor_h.wait(&self.loop, &self.cursor_c, Thread, self, cursorTimerCallback);\n\n    // Send an initial wakeup message so that we render right away.\n    try self.wakeup.notify();\n\n    // Run\n    log.debug(\"starting renderer thread\", .{});\n    _ = try self.loop.run(.until_done);\n}\n\nfn setQosClass(self: *const Thread) void {\n    // Thread QoS classes are only relevant on macOS.\n    if (comptime !builtin.target.os.tag.isDarwin()) return;\n\n    const class: internal_os.macos.QosClass = class: {\n        // If we aren't visible (our view is fully occluded) then we\n        // always drop our rendering priority down because it's just\n        // mostly wasted work.\n        //\n        // The renderer itself should be doing this as well (for example\n        // Metal will stop our DisplayLink) but this also helps with\n        // general forced updates and CPU usage i.e. a rebuild cells call.\n        if (!self.flags.visible) break :class .utility;\n\n        // If we're not focused, but we're visible, then we set a higher\n        // than default priority because framerates still matter but it isn't\n        // as important as when we're focused.\n        if (!self.flags.focused) break :class .user_initiated;\n\n        // We are focused and visible, we are the definition of user interactive.\n        break :class .user_interactive;\n    };\n\n    if (internal_os.macos.setQosClass(class)) {\n        log.debug(\"thread QoS class set class={}\", .{class});\n    } else |err| {\n        log.warn(\"error setting QoS class err={}\", .{err});\n    }\n}\n\nfn startDrawTimer(self: *Thread) void {\n    // If our renderer doesn't support animations then we never run this.\n    if (!@hasDecl(renderer.Renderer, \"hasAnimations\")) return;\n    if (!self.renderer.hasAnimations()) return;\n    if (self.config.custom_shader_animation == .false) return;\n\n    // Set our active state so it knows we're running. We set this before\n    // even checking the active state in case we have a pending shutdown.\n    self.draw_active = true;\n\n    // If our draw timer is already active, then we don't have to do anything.\n    if (self.draw_c.state() == .active) return;\n\n    // Start the timer which loops\n    self.draw_h.run(\n        &self.loop,\n        &self.draw_c,\n        DRAW_INTERVAL,\n        Thread,\n        self,\n        drawCallback,\n    );\n}\n\nfn stopDrawTimer(self: *Thread) void {\n    // This will stop the draw on the next iteration.\n    self.draw_active = false;\n}\n\n/// Drain the mailbox.\nfn drainMailbox(self: *Thread) !void {\n    while (self.mailbox.pop()) |message| {\n        log.debug(\"mailbox message={}\", .{message});\n        switch (message) {\n            .crash => @panic(\"crash request, crashing intentionally\"),\n\n            .visible => |v| visible: {\n                // If our state didn't change we do nothing.\n                if (self.flags.visible == v) break :visible;\n\n                // Set our visible state\n                self.flags.visible = v;\n\n                // Visibility affects our QoS class\n                self.setQosClass();\n\n                // If we became visible then we immediately trigger a draw.\n                // We don't need to update frame data because that should\n                // still be happening.\n                if (v) self.drawFrame();\n            },\n\n            .focus => |v| focus: {\n                // If our state didn't change we do nothing.\n                if (self.flags.focused == v) break :focus;\n\n                // Set our state\n                self.flags.focused = v;\n\n                // Focus affects our QoS class\n                self.setQosClass();\n\n                // Set it on the renderer\n                try self.renderer.setFocus(v);\n            },\n\n            .inspector => |v| self.flags.has_inspector = v,\n\n            .macos_display_id => |v| {\n                if (@hasDecl(renderer.Renderer, \"setMacOSDisplayID\")) {\n                    try self.renderer.setMacOSDisplayID(v);\n                }\n            },\n\n            .resize => |v| try self.renderer.setScreenSize(v),\n            .change_config => |config| {\n                defer config.alloc.destroy(config.thread);\n                try self.changeConfig(config.thread);\n                try self.renderer.changeConfig(config.impl);\n            },\n        }\n    }\n}\n\nfn changeConfig(self: *Thread, config: *const DerivedConfig) !void {\n    self.config = config.*;\n}\n\nfn drawFrame(self: *Thread, now: bool) void {\n    // If we're invisible, we do not draw.\n    if (!self.flags.visible) return;\n\n    // If the renderer is managing a vsync on its own, we only draw\n    // when we're forced to via now.\n    if (!now and self.renderer.hasVsync()) return;\n\n    // If we're doing single-threaded GPU calls then we just wake up the\n    // app thread to redraw at this point.\n    if (renderer.Renderer == renderer.OpenGL and\n        renderer.OpenGL.single_threaded_draw)\n    {\n        _ = self.app_mailbox.push(\n            .{ .redraw_surface = self.surface },\n            .{ .instant = {} },\n        );\n    } else {\n        self.renderer.drawFrame(self.surface) catch |err|\n            log.warn(\"error drawing err={}\", .{err});\n    }\n}\n\nfn wakeupCallback(\n    self_: ?*Thread,\n    _: *xev.Loop,\n    _: *xev.Completion,\n    r: xev.Async.WaitError!void,\n) xev.CallbackAction {\n    _ = r catch |err| {\n        log.err(\"error in wakeup err={}\", .{err});\n        return .rearm;\n    };\n\n    // Render immediately\n    const t = self_.?;\n    t.drainMailbox() catch |err|\n        log.err(\"error draining mailbox err={}\", .{err});\n\n    // Draw immediately\n    _ = renderCallback(t, undefined, undefined, {});\n\n    return .rearm;\n}\n\nfn drawNowCallback(\n    self_: ?*Thread,\n    _: *xev.Loop,\n    _: *xev.Completion,\n    r: xev.Async.WaitError!void,\n) xev.CallbackAction {\n    _ = r catch |err| {\n        log.err(\"error in draw now err={}\", .{err});\n        return .rearm;\n    };\n\n    // Draw immediately\n    const t = self_.?;\n    t.drawFrame(true);\n\n    return .rearm;\n}\n\nfn drawCallback(\n    self_: ?*Thread,\n    _: *xev.Loop,\n    _: *xev.Completion,\n    r: xev.Timer.RunError!void,\n) xev.CallbackAction {\n    _ = r catch unreachable;\n    const t = self_ orelse {\n        // This shouldn't happen so we log it.\n        log.warn(\"render callback fired without data set\", .{});\n        return .disarm;\n    };\n\n    // Draw\n    t.drawFrame(false);\n\n    // Only continue if we're still active\n    if (t.draw_active) {\n        t.draw_h.run(&t.loop, &t.draw_c, DRAW_INTERVAL, Thread, t, drawCallback);\n    }\n\n    return .disarm;\n}\n\nfn renderCallback(\n    self_: ?*Thread,\n    _: *xev.Loop,\n    _: *xev.Completion,\n    r: xev.Timer.RunError!void,\n) xev.CallbackAction {\n    _ = r catch unreachable;\n    const t = self_ orelse {\n        // This shouldn't happen so we log it.\n        log.warn(\"render callback fired without data set\", .{});\n        return .disarm;\n    };\n\n    t.renderer.render(\n        t.surface,\n        t.state,\n        t.flags.cursor_blink_visible,\n    ) catch |err|\n        log.warn(\"error rendering err={}\", .{err});\n\n    // Draw\n    t.drawFrame(false);\n\n    return .disarm;\n}\n\nfn cursorTimerCallback(\n    self_: ?*Thread,\n    _: *xev.Loop,\n    _: *xev.Completion,\n    r: xev.Timer.RunError!void,\n) xev.CallbackAction {\n    _ = r catch |err| switch (err) {\n        // This is sent when our timer is canceled. That's fine.\n        error.Canceled => return .disarm,\n        else => {\n            log.warn(\"error in cursor timer callback err={}\", .{err});\n            unreachable;\n        },\n    };\n\n    const t = self_ orelse {\n        // This shouldn't happen so we log it.\n        log.warn(\"render callback fired without data set\", .{});\n        return .disarm;\n    };\n\n    t.flags.cursor_blink_visible = !t.flags.cursor_blink_visible;\n    t.wakeup.notify() catch {};\n\n    t.cursor_h.run(&t.loop, &t.cursor_c, CURSOR_BLINK_INTERVAL, Thread, t, cursorTimerCallback);\n\n    return .disarm;\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.2863031343924431,
      "diff_lines": 479,
      "added_lines": 30,
      "removed_lines": 245,
      "diff_preview": "--- expected+++ predicted@@ -1,29 +1,24 @@-//! Represents the renderer thread logic. The renderer thread is able to\n-//! be woken up to render.\n pub const Thread = @This();\n \n const std = @import(\"std\");\n const builtin = @import(\"builtin\");\n-const assert = std.debug.assert;\n const xev = @import(\"../global.zig\").xev;\n const crash = @import(\"../crash/main.zig\");\n const internal_os = @import(\"../os/main.zig\");\n-const rendererpkg = @import(\"../renderer.zig\");\n+const renderer = @import(\"../renderer.zig\");\n const apprt = @import(\"../apprt.zig\");\n const configpkg = @import(\"../config.zig\");\n const BlockingQueue = @import(\"../datastruct/main.zig\").BlockingQueue;\n const App = @import(\"../App.zig\");\n \n const Allocator = std.mem.Allocator;\n-const log = std.log.scoped(.renderer_thread);\n-\n-const DRAW_INTERVAL = 8; // 120 FPS\n+\n const CURSOR_BLINK_INTERVAL = 600;\n \n /// The type used for sending messages to the IO thread. For now this is\n /// hardcoded with a capacity. We can make this a comptime parameter in\n /// the future if we want it configurable.\n-pub const Mailbox = BlockingQueue(rendererpkg.Message, 64);\n+pub const Mailbox = BlockingQueue(renderer.Message, 64);\n \n /// Allocator used for some state\n alloc: std.mem.Allocator,\n@@ -46,31 +41,18 @@ render_h: xev.Timer,\n render_c: xev.Completion = .{},\n \n-/// The timer used for draw calls. Draw calls don't update from the\n-/// terminal state so they're much cheaper. They're used for animation\n-/// and are paused when the terminal is not focused.\n-draw_h: xev.Timer,\n-draw_c: xev.Completion = .{},\n-draw_active: bool = false,\n-\n-/// This async is used to force a draw immediately. This does not\n-/// coalesce like the wakeup does.\n-draw_now: xev.Async,\n-draw_now_c: xev.Completion = .{},\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 77.64090299606323,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 34445,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_renderer_metal_shaders.zig",
    "exact_match": false,
    "similarity": 0.5585414636648033,
    "diff_lines": 606,
    "predicted_output": "const std = @import(\"std\");\nconst Allocator = std.mem.Allocator;\nconst assert = std.debug.assert;\nconst macos = @import(\"macos\");\nconst objc = @import(\"objc\");\nconst math = @import(\"../../math.zig\");\n\nconst mtl = @import(\"api.zig\");\n\nconst log = std.log.scoped(.metal);\n\n/// This contains the state for the shaders used by the Metal renderer.\npub const Shaders = struct {\n    library: objc.Object,\n    cell_text_pipeline: objc.Object,\n    cell_bg_pipeline: objc.Object,\n    image_pipeline: objc.Object,\n    post_pipelines: []const objc.Object,\n\n    pub fn init(\n        alloc: Allocator,\n        device: objc.Object,\n        post_shaders: []const [:0]const u8,\n        pixel_format: mtl.MTLPixelFormat,\n    ) !Shaders {\n        const library = try initLibrary(device);\n        errdefer library.msgSend(void, objc.sel(\"release\"), .{});\n\n        const cell_text_pipeline = try initCellTextPipeline(device, library, pixel_format);\n        errdefer cell_text_pipeline.msgSend(void, objc.sel(\"release\"), .{});\n\n        const cell_bg_pipeline = try initCellBgPipeline(device, library, pixel_format);\n        errdefer cell_bg_pipeline.msgSend(void, objc.sel(\"release\"), .{});\n\n        const image_pipeline = try initImagePipeline(device, library, pixel_format);\n        errdefer image_pipeline.msgSend(void, objc.sel(\"release\"), .{});\n\n        const post_pipelines: []const objc.Object = initPostPipelines(\n            alloc,\n            device,\n            library,\n            post_shaders,\n            pixel_format,\n        ) catch |err| err: {\n            log.warn(\"error initializing postprocess shaders err={}\", .{err});\n            break :err &.{};\n        };\n        errdefer if (post_pipelines.len > 0) {\n            for (post_pipelines) |pipeline| pipeline.msgSend(void, objc.sel(\"release\"), .{});\n            alloc.free(post_pipelines);\n        };\n\n        return .{\n            .library = library,\n            .cell_text_pipeline = cell_text_pipeline,\n            .cell_bg_pipeline = cell_bg_pipeline,\n            .image_pipeline = image_pipeline,\n            .post_pipelines = post_pipelines,\n        };\n    }\n\n    pub fn deinit(self: *Shaders, alloc: Allocator) void {\n        self.cell_text_pipeline.msgSend(void, objc.sel(\"release\"), .{});\n        self.cell_bg_pipeline.msgSend(void, objc.sel(\"release\"), .{});\n        self.image_pipeline.msgSend(void, objc.sel(\"release\"), .{});\n        self.library.msgSend(void, objc.sel(\"release\"), .{});\n\n        if (self.post_pipelines.len > 0) {\n            for (self.post_pipelines) |pipeline| {\n                pipeline.msgSend(void, objc.sel(\"release\"), .{});\n            }\n            alloc.free(self.post_pipelines);\n        }\n    }\n};\n\n/// The uniforms that are passed to the terminal cell shader.\npub const Uniforms = extern struct {\n    projection_matrix: math.Mat align(16),\n    cell_size: [2]f32 align(8),\n    grid_size: [2]u16 align(4),\n    grid_padding: [4]f32 align(16),\n    bg_color: [4]u8 align(4),\n    cursor_pos: [2]u16 align(4),\n    cursor_color: [4]u8 align(4),\n    cursor_wide: bool align(1),\n    use_display_p3: bool align(1),\n    use_linear_blending: bool align(1),\n    use_linear_correction: bool align(1) = false,\n\n    const PaddingExtend = packed struct(u8) {\n        left: bool = false,\n        right: bool = false,\n        up: bool = false,\n        down: bool = false,\n        _padding: u4 = 0,\n    };\n};\n\n/// This is a single parameter for the terminal cell shader.\npub const CellText = extern struct {\n    glyph_pos: [2]u32 align(8) = .{ 0, 0 },\n    glyph_size: [2]u32 align(8) = .{ 0, 0 },\n    bearings: [2]i16 align(4) = .{ 0, 0 },\n    grid_pos: [2]u16 align(4),\n    color: [4]u8 align(4),\n    mode: Mode align(1),\n    constraint_width: u8 align(1) = 0,\n\n    pub const Mode = enum(u8) {\n        fg = 1,\n        fg_constrained = 2,\n        fg_color = 3,\n        cursor = 4,\n        fg_powerline = 5,\n    };\n\n    test {\n        try std.testing.expectEqual(32, @sizeOf(CellText));\n    }\n};\n\n/// Initialize the cell render pipeline for our shader library.\nfn initCellTextPipeline(\n    device: objc.Object,\n    library: objc.Object,\n    pixel_format: mtl.MTLPixelFormat,\n) !objc.Object {\n    const func_vert = func_vert: {\n        const str = try macos.foundation.String.createWithBytes(\n            \"cell_text_vertex\",\n            .utf8,\n            false,\n        );\n        defer str.release();\n\n        const ptr = library.msgSend(?*anyopaque, objc.sel(\"newFunctionWithName:\"), .{str});\n        break :func_vert objc.Object.fromId(ptr.?);\n    };\n    defer func_vert.msgSend(void, objc.sel(\"release\"), .{});\n    const func_frag = func_frag: {\n        const str = try macos.foundation.String.createWithBytes(\n            \"cell_text_fragment\",\n            .utf8,\n            false,\n        );\n        defer str.release();\n\n        const ptr = library.msgSend(?*anyopaque, objc.sel(\"newFunctionWithName:\"), .{str});\n        break :func_frag objc.Object.fromId(ptr.?);\n    };\n    defer func_frag.msgSend(void, objc.sel(\"release\"), .{});\n\n    const desc = init: {\n        const Class = objc.getClass(\"MTLRenderPipelineDescriptor\").?;\n        const id_alloc = Class.msgSend(objc.Object, objc.sel(\"alloc\"), .{});\n        const id_init = id_alloc.msgSend(objc.Object, objc.sel(\"init\"), .{});\n        break :init id_init;\n    };\n    defer desc.msgSend(void, objc.sel(\"release\"), .{});\n\n    desc.setProperty(\"vertexFunction\", func_vert);\n    desc.setProperty(\"fragmentFunction\", func_frag);\n\n    const attachments = objc.Object.fromId(desc.getProperty(?*anyopaque, \"colorAttachments\"));\n    {\n        const attachment = attachments.msgSend(\n            objc.Object,\n            objc.sel(\"objectAtIndexedSubscript:\"),\n            .{@as(c_ulong, 0)},\n        );\n\n        attachment.setProperty(\"pixelFormat\", @intFromEnum(pixel_format));\n\n        attachment.setProperty(\"blendingEnabled\", true);\n        attachment.setProperty(\"rgbBlendOperation\", @intFromEnum(mtl.MTLBlendOperation.add));\n        attachment.setProperty(\"alphaBlendOperation\", @intFromEnum(mtl.MTLBlendOperation.add));\n        attachment.setProperty(\"sourceRGBBlendFactor\", @intFromEnum(mtl.MTLBlendFactor.one));\n        attachment.setProperty(\"sourceAlphaBlendFactor\", @intFromEnum(mtl.MTLBlendFactor.one));\n        attachment.setProperty(\"destinationRGBBlendFactor\", @intFromEnum(mtl.MTLBlendFactor.one_minus_source_alpha));\n        attachment.setProperty(\"destinationAlphaBlendFactor\", @intFromEnum(mtl.MTLBlendFactor.one_minus_source_alpha));\n    }\n\n    var err: ?*anyopaque = null;\n    const pipeline_state = device.msgSend(\n        objc.Object,\n        objc.sel(\"newRenderPipelineStateWithDescriptor:error:\"),\n        .{ desc, &err },\n    );\n    try checkError(err);\n\n    return pipeline_state;\n}\n\n/// This is a single parameter for the cell bg shader.\npub const CellBg = [4]u8;\n\n/// Initialize the cell background render pipeline for our shader library.\nfn initCellBgPipeline(\n    device: objc.Object,\n    library: objc.Object,\n    pixel_format: mtl.MTLPixelFormat,\n) !objc.Object {\n    const func_vert = func_vert: {\n        const str = try macos.foundation.String.createWithBytes(\n            \"full_screen_vertex\",\n            .utf8,\n            false,\n        );\n        defer str.release();\n\n        const ptr = library.msgSend(?*anyopaque, objc.sel(\"newFunctionWithName:\"), .{str});\n        break :func_vert objc.Object.fromId(ptr.?);\n    };\n    defer func_vert.msgSend(void, objc.sel(\"release\"), .{});\n    const func_frag = func_frag: {\n        const str = try macos.foundation.String.createWithBytes(\n            \"cell_bg_fragment\",\n            .utf8,\n            false,\n        );\n        defer str.release();\n\n        const ptr = library.msgSend(?*anyopaque, objc.sel(\"newFunctionWithName:\"), .{str});\n        break :func_frag objc.Object.fromId(ptr.?);\n    };\n    defer func_frag.msgSend(void, objc.sel(\"release\"), .{});\n\n    const desc = init: {\n        const Class = objc.getClass(\"MTLRenderPipelineDescriptor\").?;\n        const id_alloc = Class.msgSend(objc.Object, objc.sel(\"alloc\"), .{});\n        const id_init = id_alloc.msgSend(objc.Object, objc.sel(\"init\"), .{});\n        break :init id_init;\n    };\n    defer desc.msgSend(void, objc.sel(\"release\"), .{});\n\n    desc.setProperty(\"vertexFunction\", func_vert);\n    desc.setProperty(\"fragmentFunction\", func_frag);\n\n    const attachments = objc.Object.fromId(desc.getProperty(?*anyopaque, \"colorAttachments\"));\n    {\n        const attachment = attachments.msgSend(\n            objc.Object,\n            objc.sel(\"objectAtIndexedSubscript:\"),\n            .{@as(c_ulong, 0)},\n        );\n\n        attachment.setProperty(\"pixelFormat\", @intFromEnum(pixel_format));\n\n        attachment.setProperty(\"blendingEnabled\", true);\n        attachment.setProperty(\"rgbBlendOperation\", @intFromEnum(mtl.MTLBlendOperation.add));\n        attachment.setProperty(\"alphaBlendOperation\", @intFromEnum(mtl.MTLBlendOperation.add));\n        attachment.setProperty(\"sourceRGBBlendFactor\", @intFromEnum(mtl.MTLBlendFactor.one));\n        attachment.setProperty(\"sourceAlphaBlendFactor\", @intFromEnum(mtl.MTLBlendFactor.one));\n        attachment.setProperty(\"destinationRGBBlendFactor\", @intFromEnum(mtl.MTLBlendFactor.one_minus_source_alpha));\n        attachment.setProperty(\"destinationAlphaBlendFactor\", @intFromEnum(mtl.MTLBlendFactor.one_minus_source_alpha));\n    }\n\n    var err: ?*anyopaque = null;\n    const pipeline_state = device.msgSend(\n        objc.Object,\n        objc.sel(\"newRenderPipelineStateWithDescriptor:error:\"),\n        .{ desc, &err },\n    );\n    try checkError(err);\n\n    return pipeline_state;\n}\n\n/// Initialize the image render pipeline for our shader library.\nfn initImagePipeline(\n    device: objc.Object,\n    library: objc.Object,\n    pixel_format: mtl.MTLPixelFormat,\n) !objc.Object {\n    const func_vert = func_vert: {\n        const str = try macos.foundation.String.createWithBytes(\n            \"image_vertex\",\n            .utf8,\n            false,\n        );\n        defer str.release();\n\n        const ptr = library.msgSend(?*anyopaque, objc.sel(\"newFunctionWithName:\"), .{str});\n        break :func_vert objc.Object.fromId(ptr.?);\n    };\n    defer func_vert.msgSend(void, objc.sel(\"release\"), .{});\n    const func_frag = func_frag: {\n        const str = try macos.foundation.String.createWithBytes(\n            \"image_fragment\",\n            .utf8,\n            false,\n        );\n        defer str.release();\n\n        const ptr = library.msgSend(?*anyopaque, objc.sel(\"newFunctionWithName:\"), .{str});\n        break :func_frag objc.Object.fromId(ptr.?);\n    };\n    defer func_frag.msgSend(void, objc.sel(\"release\"), .{});\n\n    const desc = init: {\n        const Class = objc.getClass(\"MTLRenderPipelineDescriptor\").?;\n        const id_alloc = Class.msgSend(objc.Object, objc.sel(\"alloc\"), .{});\n        const id_init = id_alloc.msgSend(objc.Object, objc.sel(\"init\"), .{});\n        break :init id_init;\n    };\n    defer desc.msgSend(void, objc.sel(\"release\"), .{});\n\n    desc.setProperty(\"vertexFunction\", func_vert);\n    desc.setProperty(\"fragmentFunction\", func_frag);\n\n    const attachments = objc.Object.fromId(desc.getProperty(?*anyopaque, \"colorAttachments\"));\n    {\n        const attachment = attachments.msgSend(\n            objc.Object,\n            objc.sel(\"objectAtIndexedSubscript:\"),\n            .{@as(c_ulong, 0)},\n        );\n\n        attachment.setProperty(\"pixelFormat\", @intFromEnum(pixel_format));\n\n        attachment.setProperty(\"blendingEnabled\", true);\n        attachment.setProperty(\"rgbBlendOperation\", @intFromEnum(mtl.MTLBlendOperation.add));\n        attachment.setProperty(\"alphaBlendOperation\", @intFromEnum(mtl.MTLBlendOperation.add));\n        attachment.setProperty(\"sourceRGBBlendFactor\", @intFromEnum(mtl.MTLBlendFactor.one));\n        attachment.setProperty(\"sourceAlphaBlendFactor\", @intFromEnum(mtl.MTLBlendFactor.one));\n        attachment.setProperty(\"destinationRGBBlendFactor\", @intFromEnum(mtl.MTLBlendFactor.one_minus_source_alpha));\n        attachment.setProperty(\"destinationAlphaBlendFactor\", @intFromEnum(mtl.MTLBlendFactor.one_minus_source_alpha));\n    }\n\n    var err: ?*anyopaque = null;\n    const pipeline_state = device.msgSend(\n        objc.Object,\n        objc.sel(\"newRenderPipelineStateWithDescriptor:error:\"),\n        .{ desc, &err },\n    );\n    try checkError(err);\n\n    return pipeline_state;\n}\n\nfn checkError(err_: ?*anyopaque) !void {\n    const nserr = objc.Object.fromId(err_ orelse return);\n    const str = @as(\n        *macos.foundation.String,\n        @ptrCast(nserr.getProperty(?*anyopaque, \"localizedDescription\").?),\n    );\n\n    log.err(\"metal error={s}\", .{str.cstringPtr(.ascii).?});\n    return error.MetalFailed;\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.5585414636648033,
      "diff_lines": 606,
      "added_lines": 6,
      "removed_lines": 382,
      "diff_preview": "--- expected+++ predicted@@ -12,28 +12,11 @@ /// This contains the state for the shaders used by the Metal renderer.\n pub const Shaders = struct {\n     library: objc.Object,\n-\n-    /// Renders cell foreground elements (text, decorations).\n     cell_text_pipeline: objc.Object,\n-\n-    /// The cell background shader is the shader used to render the\n-    /// background of terminal cells.\n     cell_bg_pipeline: objc.Object,\n-\n-    /// The image shader is the shader used to render images for things\n-    /// like the Kitty image protocol.\n     image_pipeline: objc.Object,\n-\n-    /// Custom shaders to run against the final drawable texture. This\n-    /// can be used to apply a lot of effects. Each shader is run in sequence\n-    /// against the output of the previous shader.\n     post_pipelines: []const objc.Object,\n \n-    /// Initialize our shader set.\n-    ///\n-    /// \"post_shaders\" is an optional list of postprocess shaders to run\n-    /// against the final drawable texture. This is an array of shader source\n-    /// code, not file paths.\n     pub fn init(\n         alloc: Allocator,\n         device: objc.Object,\n@@ -59,9 +42,6 @@             post_shaders,\n             pixel_format,\n         ) catch |err| err: {\n-            // If an error happens while building postprocess shaders we\n-            // want to just not use any postprocess shaders since we don't\n-            // want to block Ghostty from working.\n             log.warn(\"error initializing postprocess shaders err={}\", .{err});\n             break :err &.{};\n         };\n@@ -80,13 +60,11 @@     }\n \n     pub fn deinit(self: *Shaders, alloc: Allocator) void {\n-        // Release our primary shaders\n         self.cell_text_pipeline.msgSend(void, objc.sel(\"release\"), .{});\n         self.cell_bg_pipeline.msgSend(void, objc.sel(\"release\"), .{});\n         self.image_pipeline.msgSend(void, objc.sel(\"release\"), .{});\n         self.library.msgSend(void, objc.sel(\"release\"), .{});\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 63.177138805389404,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 28708,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_terminal_Parser.zig",
    "exact_match": false,
    "similarity": 0.34921803127874884,
    "diff_lines": 883,
    "predicted_output": "//! VT-series parser for escape and control sequences.\n//!\n//! This is implemented directly as the state machine described on\n//! vt100.net: https://vt100.net/emu/dec_ansi_parser\nconst Parser = @This();\n\nconst std = @import(\"std\");\nconst testing = std.testing;\nconst table = @import(\"parse_table.zig\").table;\n\nconst log = std.log.scoped(.parser);\n\n/// States for the state machine\npub const State = enum {\n    ground,\n    escape,\n    escape_intermediate,\n    csi_entry,\n    csi_intermediate,\n    csi_param,\n    csi_ignore,\n    dcs_entry,\n    dcs_param,\n    dcs_intermediate,\n    dcs_passthrough,\n    dcs_ignore,\n    osc_string,\n    sos_pm_apc_string,\n};\n\n/// Transition action is an action that can be taken during a state\n/// transition. This is more of an internal action, not one used by\n/// end users, typically.\npub const TransitionAction = enum {\n    none,\n    ignore,\n    print,\n    execute,\n    collect,\n    param,\n    esc_dispatch,\n    csi_dispatch,\n    put,\n    osc_put,\n    dcs_passthrough,\n    apc_put,\n};\n\n/// Action is the action that a caller of the parser is expected to\n/// take as a result of some input character.\npub const Action = union(enum) {\n    /// Draw character to the screen. This is a unicode codepoint.\n    print: u21,\n\n    /// Execute the C0 or C1 function.\n    execute: u8,\n\n    /// Execute the OSC command.\n    osc_dispatch: osc.Command,\n\n    /// DCS-related events.\n    dcs_hook: DCS,\n    dcs_put: u8,\n    dcs_unhook: void,\n\n    /// APC data\n    apc_start: void,\n    apc_put: u8,\n    apc_end: void,\n\n    pub const CSI = struct {\n        intermediates: []u8,\n        params: []u16,\n        params_sep: SepList,\n        final: u8,\n\n        /// The list of separators used for CSI params. The value of the\n        /// bit can be mapped to Sep. The index of this bit set specifies\n        /// the separator AFTER that param. For example: 0;4:3 would have\n        /// index 1 set.\n        pub const SepList = std.StaticBitSet(MAX_PARAMS);\n    },\n\n    pub const DCS = struct {\n        intermediates: []const u8 = \"\",\n        params: []const u16 = &.{},\n        final: u8,\n    };\n};\n\n/// Maximum number of intermediate characters during parsing. This is\n/// 4 because we also use the intermediates array for UTF8 decoding which\n/// can be at most 4 bytes.\nconst MAX_INTERMEDIATE = 4;\n/// Maximum number of CSI parameters. This is arbitrary. Practically, the\n/// only CSI command that uses more than 3 parameters is the SGR command\n/// which can be infinitely long. 24 is a reasonable limit based on empirical\n/// data. This used to be 16 but Kakoune has a SGR command that uses 17\n/// parameters.\nconst MAX_PARAMS = 24;\n\n/// Current state of the state machine\nstate: State = .ground,\n\n/// Intermediate tracking.\nintermediates: [MAX_INTERMEDIATE]u8 = undefined,\nintermediates_idx: u8 = 0,\n\n/// Param tracking, building\nparams: [MAX_PARAMS]u16 = undefined,\nparams_sep: Action.CSI.SepList = Action.CSI.SepList.initEmpty(),\nparams_idx: u8 = 0,\nparam_acc: u16 = 0,\nparam_acc_idx: u8 = 0,\n\npub fn init() Parser {\n    return .{};\n}\n\npub fn deinit(self: *Parser) void {\n    self.osc_parser.deinit();\n}\n\n/// Next consumes the next character c and returns the actions to execute.\n/// Up to 3 actions may need to be executed -- in order -- representing\n/// the state exit, transition, and entry actions.\npub fn next(self: *Parser, c: u8) [3]?Action {\n    const effect = table[c][@intFromEnum(self.state)];\n\n    return [3]?Action{\n        // Exit depends on current state\n        if (self.state == effect.state) null else switch (self.state) {\n            .osc_string => if (self.osc_parser.end(c)) |cmd|\n                Action{ .osc_dispatch = cmd }\n            else\n                null,\n            .dcs_passthrough => Action{ .dcs_unhook = {} },\n            else => null,\n        },\n\n        self.doAction(effect.action, c),\n\n        // Entry depends on new state\n        if (self.state == effect.state) null else switch (effect.state) {\n            .escape, .dcs_entry, .csi_entry => clear: {\n                self.clear();\n                break :clear null;\n            },\n            .osc_string => {\n                self.osc_parser.reset();\n                break :osc_string null;\n            },\n            .dcs_passthrough => Action{\n                .dcs_hook = .{\n                    // Finalize parameters\n                    if (self.param_acc_idx > 0) {\n                        self.params[self.params_idx] = self.param_acc;\n                        self.params_idx += 1;\n                    }\n                    .intermediates = self.intermediates[0..self.intermediates_idx],\n                    .params = self.params[0..self.params_idx],\n                    .final = c,\n                },\n            },\n            else => null,\n        },\n    };\n}\n\nfn doAction(self: *Parser, action: TransitionAction, c: u8) ?Action {\n    return switch (action) {\n        .none, .ignore => null,\n        .print => Action{ .print = c },\n        .execute => Action{ .execute = c },\n        .osc_put => {\n            self.osc_parser.next(c);\n            break :osc_put null;\n        },\n        .csi_dispatch => csi_dispatch: {\n            // Ignore too many parameters\n            if (self.params_idx >= MAX_PARAMS) break :csi_dispatch null;\n\n            // Set param final value\n            self.params[self.params_idx] = self.param_acc;\n            self.params_idx += 1;\n\n            break :csi_dispatch Action{\n                .csi_dispatch = .{\n                    .intermediates = self.intermediates[0..self.intermediates_idx],\n                    .params = self.params[0..self.params_idx],\n                    .params_sep = self.params_sep,\n                    .final = c,\n                },\n            };\n        },\n        .put => Action{ .dcs_put = c },\n        .apc_put => Action{ .apc_put = c },\n    };\n}\n\npub fn clear(self: *Parser) void {\n    self.intermediates_idx = 0;\n    self.params_idx = 0;\n    self.params_sep = Action.CSI.SepList.initEmpty();\n    self.param_acc = 0;\n    self.param_acc_idx = 0;\n}\n\ntest \"csi: too many params\" {\n    var p = init();\n    _ = p.next(0x1B);\n    _ = p.next('[');\n    for (0..100) |_| {\n        _ = p.next('1');\n        _ = p.next(';');\n    }\n    _ = p.next('1');\n\n    {\n        const a = p.next('C');\n        try testing.expect(p.state == .ground);\n        try testing.expect(a[0] == null);\n        try testing.expect(a[1] == null);\n        try testing.expect(a[2] == null);\n    }\n}\n\ntest \"dcs: XTGETTCAP\" {\n    var p = init();\n    _ = p.next(0x1B);\n    for (\"P+\") |c| {\n        const a = p.next(c);\n        try testing.expect(a[0] == null);\n        try testing.expect(a[1] == null);\n        try testing.expect(a[2] == null);\n    }\n\n    {\n        const a = p.next('q');\n        try testing.expect(p.state == .dcs_passthrough);\n        try testing.expect(a[0] == null);\n        try testing.expect(a[1] == null);\n        try testing.expect(a[2].? == .dcs_hook);\n\n        const hook = a[2].?.dcs_hook;\n        try testing.expectEqualSlices(u8, &[_]u8{'+'}, hook.intermediates);\n        try testing.expectEqualSlices(u16, &[_]u16{}, hook.params);\n        try testing.expectEqual('q', hook.final);\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.34921803127874884,
      "diff_lines": 883,
      "added_lines": 23,
      "removed_lines": 737,
      "diff_preview": "--- expected+++ predicted@@ -5,11 +5,8 @@ const Parser = @This();\n \n const std = @import(\"std\");\n-const builtin = @import(\"builtin\");\n-const assert = std.debug.assert;\n const testing = std.testing;\n const table = @import(\"parse_table.zig\").table;\n-const osc = @import(\"osc.zig\");\n \n const log = std.log.scoped(.parser);\n \n@@ -45,26 +42,18 @@     csi_dispatch,\n     put,\n     osc_put,\n+    dcs_passthrough,\n     apc_put,\n };\n \n /// Action is the action that a caller of the parser is expected to\n /// take as a result of some input character.\n pub const Action = union(enum) {\n-    pub const Tag = std.meta.FieldEnum(Action);\n-\n     /// Draw character to the screen. This is a unicode codepoint.\n     print: u21,\n \n     /// Execute the C0 or C1 function.\n     execute: u8,\n-\n-    /// Execute the CSI command. Note that pointers within this\n-    /// structure are only valid until the next call to \"next\".\n-    csi_dispatch: CSI,\n-\n-    /// Execute the ESC command.\n-    esc_dispatch: ESC,\n \n     /// Execute the OSC command.\n     osc_dispatch: osc.Command,\n@@ -90,122 +79,24 @@         /// the separator AFTER that param. For example: 0;4:3 would have\n         /// index 1 set.\n         pub const SepList = std.StaticBitSet(MAX_PARAMS);\n-\n-        /// The separator used for CSI params.\n-        pub const Sep = enum(u1) { semicolon = 0, colon = 1 };\n-\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 49.8445839881897,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 32616,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_terminal_color.zig",
    "exact_match": false,
    "similarity": 0.8604596253738391,
    "diff_lines": 89,
    "predicted_output": "const std = @import(\"std\");\nconst assert = std.debug.assert;\nconst x11_color = @import(\"x11_color.zig\");\n\n/// The default palette.\npub const default: Palette = default: {\n    var result: Palette = undefined;\n\n    // Named values\n    var i: u8 = 0;\n    while (i < 16) : (i += 1) {\n        result[i] = Name.default(@enumFromInt(i)) catch unreachable;\n    }\n\n    // Cube\n    assert(i == 16);\n    var r: u8 = 0;\n    while (r < 6) : (r += 1) {\n        var g: u8 = 0;\n        while (g < 6) : (g += 1) {\n            var b: u8 = 0;\n            while (b < 6) : (b += 1) {\n                result[i] = .{\n                    .r = if (r == 0) 0 else (r * 40 + 55),\n                    .g = if (g == 0) 0 else (g * 40 + 55),\n                    .b = if (b == 0) 0 else (b * 40 + 55),\n                };\n\n                i += 1;\n            }\n        }\n    }\n\n    // Gray ramp\n    assert(i == 232);\n    assert(@TypeOf(i) == u8);\n    while (i > 0) : (i +%= 1) {\n        const value = ((i - 232) * 10) + 8;\n        result[i] = .{ .r = value, .g = value, .b = value };\n    }\n\n    break :default result;\n};\n\n/// Palette is the 256 color palette.\npub const Palette = [256]RGB;\n\n/// Color names in the standard 8 or 16 color palette.\npub const Name = enum(u8) {\n    black = 0,\n    red = 1,\n    green = 2,\n    yellow = 3,\n    blue = 4,\n    magenta = 5,\n    cyan = 6,\n    white = 7,\n\n    bright_black = 8,\n    bright_red = 9,\n    bright_green = 10,\n    bright_yellow = 11,\n    bright_blue = 12,\n    bright_magenta = 13,\n    bright_cyan = 14,\n    bright_white = 15,\n\n    // Remainders are valid unnamed values in the 256 color palette.\n    _,\n\n    /// Default colors for tagged values.\n    pub fn default(self: Name) !RGB {\n        return switch (self) {\n            .black => RGB{ .r = 0x1D, .g = 0x1F, .b = 0x21 },\n            .red => RGB{ .r = 0xCC, .g = 0x66, .b = 0x66 },\n            .green => RGB{ .r = 0xB5, .g = 0xBD, .b = 0x68 },\n            .yellow => RGB{ .r = 0xF0, .g = 0xC6, .b = 0x74 },\n            .blue => RGB{ .r = 0x81, .g = 0xA2, .b = 0xBE },\n            .magenta => RGB{ .r = 0xB2, .g = 0x94, .b = 0xBB },\n            .cyan => RGB{ .r = 0x8A, .g = 0xBE, .b = 0xB7 },\n            .white => RGB{ .r = 0xC5, .g = 0xC8, .b = 0xC6 },\n\n            .bright_black => RGB{ .r = 0x66, .g = 0x66, .b = 0x66 },\n            .bright_red => RGB{ .r = 0xD5, .g = 0x4E, .b = 0x53 },\n            .bright_green => RGB{ .r = 0xB9, .g = 0xCA, .b = 0x4A },\n            .bright_yellow => RGB{ .r = 0xE7, .g = 0xC5, .b = 0x47 },\n            .bright_blue => RGB{ .r = 0x7A, .g = 0xA6, .b = 0xDA },\n            .bright_magenta => RGB{ .r = 0xC3, .g = 0x97, .b = 0xD8 },\n            .bright_cyan => RGB{ .r = 0x70, .g = 0xC0, .b = 0xB1 },\n            .bright_white => RGB{ .r = 0xEA, .g = 0xEA, .b = 0xEA },\n\n            else => error.NoDefaultValue,\n        };\n    }\n};\n\n/// RGB\npub const RGB = packed struct(u24) {\n    r: u8 = 0,\n    g: u8 = 0,\n    b: u8 = 0,\n\n    /// Calculates luminance based on the W3C formula. This returns a\n    /// normalized value between 0 and 1 where 0 is black and 1 is white.\n    ///\n    /// https://www.w3.org/TR/WCAG20/#relativeluminancedef\n    pub fn luminance(self: RGB) f64 {\n        const r_lum = componentLuminance(self.r);\n        const g_lum = componentLuminance(self.g);\n        const b_lum = componentLuminance(self.b);\n        return 0.2126 * r_lum + 0.7152 * g_lum + 0.0722 * b_lum;\n    }\n\n    /// Calculates single-component luminance based on the W3C formula.\n    ///\n    /// Expects sRGB color space which at the time of writing we don't\n    /// generally use but it's a good enough approximation until we fix that.\n    /// https://www.w3.org/TR/WCAG20/#relativeluminancedef\n    fn componentLuminance(c: u8) f64 {\n        const c_f64: f64 = @floatFromInt(c);\n        const normalized: f64 = c_f64 / 255;\n        if (normalized <= 0.03928) return normalized / 12.92;\n        return std.math.pow(f64, (normalized + 0.055) / 1.055, 2.4);\n    }\n\n    /// Calculates \"perceived luminance\" which is better for determining\n    /// light vs dark.\n    ///\n    /// Source: https://www.w3.org/TR/AERT/#color-contrast\n    pub fn perceivedLuminance(self: RGB) f64 {\n        const r_f64: f64 = @floatFromInt(self.r);\n        const g_f64: f64 = @floatFromInt(self.g);\n        const b_f64: f64 = @floatFromInt(self.b);\n        return 0.299 * (r_f64 / 255) + 0.587 * (g_f64 / 255) + 0.114 * (b_f64 / 255);\n    }\n\n    /// Calculates the contrast ratio between two colors. The contrast\n    /// ration is a value between 1 and 21 where 1 is the lowest contrast\n    /// and 21 is the highest contrast.\n    ///\n    /// https://www.w3.org/TR/WCAG20/#contrast-ratiodef\n    pub fn contrast(self: RGB, other: RGB) f64 {\n        // pair[0] = lighter, pair[1] = darker\n        const pair: [2]f64 = pair: {\n            const self_lum = self.luminance();\n            const other_lum = other.luminance();\n            if (self_lum > other_lum) break :pair .{ self_lum, other_lum };\n            break :pair .{ other_lum, self_lum };\n        };\n\n        return (pair[0] + 0.05) / (pair[1] + 0.05);\n    }\n\n    /// Parse a color from a floating point intensity value.\n    ///\n    /// The value should be between 0.0 and 1.0, inclusive.\n    fn fromIntensity(value: []const u8) !u8 {\n        const i = std.fmt.parseFloat(f64, value) catch return error.InvalidFormat;\n        if (i < 0.0 or i > 1.0) {\n            return error.InvalidFormat;\n        }\n\n        return @intFromFloat(i * std.math.maxInt(u8));\n    }\n\n    /// Parse a color from a string of hexadecimal digits\n    ///\n    /// The string can contain 1, 2, 3, or 4 characters and represents the color\n    /// value scaled in 4, 8, 12, or 16 bits, respectively.\n    fn fromHex(value: []const u8) !u8 {\n        if (value.len == 0 or value.len > 4) {\n            return error.InvalidFormat;\n        }\n\n        const color = std.fmt.parseUnsigned(u16, value, 16) catch return error.InvalidFormat;\n        const divisor: usize = switch (value.len) {\n            1 => std.math.maxInt(u4),\n            2 => std.math.maxInt(u8),\n            3 => std.math.maxInt(u12),\n            4 => std.math.maxInt(u16),\n            else => unreachable,\n        };\n\n        return @intCast(@as(usize, color) * std.math.maxInt(u8) / divisor);\n    }\n\n    /// Parse a color specification.\n    ///\n    /// Any of the following forms are accepted:\n    ///\n    /// 1. rgb:<red>/<green>/<blue>\n    ///\n    ///    <red>, <green>, <blue> := h | hh | hhh | hhhh\n    ///\n    ///    where `h` is a single hexadecimal digit.\n    ///\n    /// 2. rgbi:<red>/<green>/<blue>\n    ///\n    ///    where <red>, <green>, and <blue> are floating point values between\n    ///    0.0 and 1.0 (inclusive).\n    ///\n    /// 3. #rgb, #rrggbb, #rrrgggbbb #rrrrggggbbbb\n    ///\n    ///    where `r`, `g`, and `b` are a single hexadecimal digit.\n    ///    These specify a color with 4, 8, 12, and 16 bits of precision\n    ///    per color channel.\n    pub fn parse(value: []const u8) !RGB {\n        if (value.len == 0) {\n            return error.InvalidFormat;\n        }\n\n        if (value[0] == '#') {\n            switch (value.len) {\n                4 => return RGB{\n                    .r = try RGB.fromHex(value[1..2]),\n                    .g = try RGB.fromHex(value[2..3]),\n                    .b = try RGB.fromHex(value[3..4]),\n                },\n                7 => return RGB{\n                    .r = try RGB.fromHex(value[1..3]),\n                    .g = try RGB.fromHex(value[3..5]),\n                    .b = try RGB.fromHex(value[5..7]),\n                },\n                10 => return RGB{\n                    .r = try RGB.fromHex(value[1..4]),\n                    .g = try RGB.fromHex(value[4..7]),\n                    .b = try RGB.fromHex(value[7..10]),\n                },\n                13 => return RGB{\n                    .r = try RGB.fromHex(value[1..5]),\n                    .g = try RGB.fromHex(value[5..9]),\n                    .b = try RGB.fromHex(value[9..13]),\n                },\n\n                else => return error.InvalidFormat,\n            }\n        }\n\n        // Check for X11 named colors. We allow whitespace around the edges\n        // of the color because Kitty allows whitespace. This is not part of\n        // any spec I could find.\n        if (x11_color.map.get(std.mem.trim(u8, value, \" \"))) |rgb| return rgb;\n\n        if (value.len < \"rgb:a/a/a\".len or !std.mem.eql(u8, value[0..3], \"rgb\")) {\n            return error.InvalidFormat;\n        }\n\n        var i: usize = 3;\n\n        const use_intensity = if (value[i] == 'i') blk: {\n            i += 1;\n            break :blk true;\n        } else false;\n\n        if (value[i] != ':') {\n            return error.InvalidFormat;\n        }\n\n        i += 1;\n\n        const r = r: {\n            const slice = if (std.mem.indexOfScalarPos(u8, value, i, '/')) |end|\n                value[i..end]\n            else\n                return error.InvalidFormat;\n\n            i += slice.len + 1;\n\n            break :r if (use_intensity)\n                try RGB.fromIntensity(slice)\n            else\n                try RGB.fromHex(slice);\n        };\n\n        const g = g: {\n            const slice = if (std.mem.indexOfScalarPos(u8, value, i, '/')) |end|\n                value[i..end]\n            else\n                return error.InvalidFormat;\n\n            i += slice.len + 1;\n\n            break :g if (use_intensity)\n                try RGB.fromIntensity(slice)\n            else\n                try RGB.fromHex(slice);\n        };\n\n        const b = if (use_intensity)\n            try RGB.fromIntensity(value[i..])\n        else\n            try RGB.fromHex(value[i..]);\n\n        return RGB{\n            .r = r,\n            .g = g,\n            .b = b,\n        };\n    }\n};\n\ntest \"palette: default\" {\n    const testing = std.testing;\n\n    // Safety check\n    var i: u8 = 0;\n    while (i < 16) : (i += 1) {\n        try testing.expectEqual(Name.default(@as(Name, @enumFromInt(i))), default[i]);\n    }\n}\n\ntest \"RGB.parse\" {\n    const testing = std.testing;\n\n    try testing.expectEqual(RGB{ .r = 255, .g = 0, .b = 0 }, try RGB.parse(\"rgbi:1.0/0/0\"));\n    try testing.expectEqual(RGB{ .r = 127, .g = 160, .b = 0 }, try RGB.parse(\"rgb:7f/a0a0/0\"));\n    try testing.expectEqual(RGB{ .r = 255, .g = 255, .b = 255 }, try RGB.parse(\"rgb:f/ff/fff\"));\n    try testing.expectEqual(RGB{ .r = 255, .g = 255, .b = 255 }, try RGB.parse(\"#ffffff\"));\n    try testing.expectEqual(RGB{ .r = 255, .g = 255, .b = 255 }, try RGB.parse(\"#fff\"));\n    try testing.expectEqual(RGB{ .r = 255, .g = 255, .b = 255 }, try RGB.parse(\"#fffffffff\"));\n    try testing.expectEqual(RGB{ .r = 255, .g = 255, .b = 255 }, try RGB.parse(\"#ffffffffffff\"));\n    try testing.expectEqual(RGB{ .r = 255, .g = 0, .b = 16 }, try RGB.parse(\"#ff0010\"));\n\n    try testing.expectEqual(RGB{ .r = 0, .g = 0, .b = 0 }, try RGB.parse(\"black\"));\n    try testing.expectEqual(RGB{ .r = 255, .g = 0, .b = 0 }, try RGB.parse(\"red\"));\n    try testing.expectEqual(RGB{ .r = 0, .g = 255, .b = 0 }, try RGB.parse(\"green\"));\n    try testing.expectEqual(RGB{ .r = 0, .g = 0, .b = 255 }, try RGB.parse(\"blue\"));\n    try testing.expectEqual(RGB{ .r = 255, .g = 255, .b = 255 }, try RGB.parse(\"white\"));\n\n    try testing.expectEqual(RGB{ .r = 124, .g = 252, .b = 0 }, try RGB.parse(\"LawnGreen\"));\n    try testing.expectEqual(RGB{ .r = 0, .g = 250, .b = 154 }, try RGB.parse(\"medium spring green\"));\n    try testing.expectEqual(RGB{ .r = 34, .g = 139, .b = 34 }, try RGB.parse(\" Forest Green \"));\n\n    // Invalid format\n    try testing.expectError(error.InvalidFormat, RGB.parse(\"rgb;\"));\n    try testing.expectError(error.InvalidFormat, RGB.parse(\"rgb:\"));\n    try testing.expectError(error.InvalidFormat, RGB.parse(\":a/a/a\"));\n    try testing.expectError(error.InvalidFormat, RGB.parse(\"a/a/a\"));\n    try testing.expectError(error.InvalidFormat, RGB.parse(\"rgb:a/a/a/\"));\n    try testing.expectError(error.InvalidFormat, RGB.parse(\"rgb:00000///\"));\n    try testing.expectError(error.InvalidFormat, RGB.parse(\"rgb:000/\"));\n    try testing.expectError(error.InvalidFormat, RGB.parse(\"rgbi:a/a/a\"));\n    try testing.expectError(error.InvalidFormat, RGB.parse(\"rgb:0.5/0.0/1.0\"));\n    try testing.expectError(error.InvalidFormat, RGB.parse(\"rgb:not/hex/zz\"));\n    try testing.expectError(error.InvalidFormat, RGB.parse(\"#\"));\n    try testing.expectError(error.InvalidFormat, RGB.parse(\"#ff\"));\n    try testing.expectError(error.InvalidFormat, RGB.parse(\"#ffff\"));\n    try testing.expectError(error.InvalidFormat, RGB.parse(\"#fffff\"));\n    try testing.expectError(error.InvalidFormat, RGB.parse(\"#gggggg\"));\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.8604596253738391,
      "diff_lines": 89,
      "added_lines": 32,
      "removed_lines": 41,
      "diff_preview": "--- expected+++ predicted@@ -100,8 +100,38 @@     g: u8 = 0,\n     b: u8 = 0,\n \n-    pub fn eql(self: RGB, other: RGB) bool {\n-        return self.r == other.r and self.g == other.g and self.b == other.b;\n+    /// Calculates luminance based on the W3C formula. This returns a\n+    /// normalized value between 0 and 1 where 0 is black and 1 is white.\n+    ///\n+    /// https://www.w3.org/TR/WCAG20/#relativeluminancedef\n+    pub fn luminance(self: RGB) f64 {\n+        const r_lum = componentLuminance(self.r);\n+        const g_lum = componentLuminance(self.g);\n+        const b_lum = componentLuminance(self.b);\n+        return 0.2126 * r_lum + 0.7152 * g_lum + 0.0722 * b_lum;\n+    }\n+\n+    /// Calculates single-component luminance based on the W3C formula.\n+    ///\n+    /// Expects sRGB color space which at the time of writing we don't\n+    /// generally use but it's a good enough approximation until we fix that.\n+    /// https://www.w3.org/TR/WCAG20/#relativeluminancedef\n+    fn componentLuminance(c: u8) f64 {\n+        const c_f64: f64 = @floatFromInt(c);\n+        const normalized: f64 = c_f64 / 255;\n+        if (normalized <= 0.03928) return normalized / 12.92;\n+        return std.math.pow(f64, (normalized + 0.055) / 1.055, 2.4);\n+    }\n+\n+    /// Calculates \"perceived luminance\" which is better for determining\n+    /// light vs dark.\n+    ///\n+    /// Source: https://www.w3.org/TR/AERT/#color-contrast\n+    pub fn perceivedLuminance(self: RGB) f64 {\n+        const r_f64: f64 = @floatFromInt(self.r);\n+        const g_f64: f64 = @floatFromInt(self.g);\n+        const b_f64: f64 = @floatFromInt(self.b);\n+        return 0.299 * (r_f64 / 255) + 0.587 * (g_f64 / 255) + 0.114 * (b_f64 / 255);\n     }\n \n     /// Calculates the contrast ratio between two colors. The contrast\n@@ -119,45 +149,6 @@         };\n \n         return (pair[0] + 0.05) / (pair[1] + 0.05);\n-    }\n-\n-    /// Calculates luminance based on the W3C formula. This returns a\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 87.30962610244751,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 10446,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_terminal_kitty_graphics_exec.zig",
    "exact_match": false,
    "similarity": 0.7762757410403356,
    "diff_lines": 388,
    "predicted_output": "const std = @import(\"std\");\nconst assert = std.debug.assert;\nconst Allocator = std.mem.Allocator;\n\nconst point = @import(\"../point.zig\");\nconst Terminal = @import(\"../Terminal.zig\");\nconst command = @import(\"graphics_command.zig\");\nconst image = @import(\"graphics_image.zig\");\nconst Command = command.Command;\nconst Response = command.Response;\nconst Image = image.Image;\nconst ImageStorage = @import(\"graphics_storage.zig\").ImageStorage;\n\nconst log = std.log.scoped(.kitty_gfx);\n\npub fn execute(\n    alloc: Allocator,\n    terminal: *Terminal,\n    cmd: *const Command,\n) ?Response {\n    // If storage is disabled then we disable the full protocol. This means\n    // we don't even respond to queries so the terminal completely acts as\n    // if this feature is not supported.\n    if (!terminal.screen.kitty_images.enabled()) {\n        log.debug(\"kitty graphics requested but disabled\", .{});\n        return null;\n    }\n\n    log.debug(\"executing kitty graphics command: quiet={} control={}\", .{\n        cmd.quiet,\n        cmd.control,\n    });\n\n    // The quiet settings used to control the response. We have to make this\n    // a var because in certain special cases (namely chunked transmissions)\n    // this can change.\n    var quiet = cmd.quiet;\n\n    const resp_: ?Response = switch (cmd.control) {\n        .query => query(alloc, cmd),\n        .transmit, .transmit_and_display => resp: {\n            // If we're transmitting, then our `q` setting value is complicated.\n            // The `q` setting inherits the value from the starting command\n            // unless `q` is set >= 1 on this command. If it is, then we save\n            // that as the new `q` setting.\n            const storage = &terminal.screen.kitty_images;\n            if (storage.loading) |loading| switch (cmd.quiet) {\n                // q=0 we use whatever the start command value is\n                .no => quiet = loading.quiet,\n\n                // q>=1 we use the new value, but we should already be set to it\n                inline .ok, .failures => |tag| {\n                    assert(quiet == tag);\n                    loading.quiet = tag;\n                },\n            };\n\n            break :resp transmit(alloc, terminal, cmd);\n        },\n        .display => display(alloc, terminal, cmd),\n        .delete => delete(alloc, terminal, cmd),\n\n        .transmit_animation_frame,\n        .control_animation,\n        .compose_animation,\n        else => .{ .message = \"ERROR: unimplemented action\" },\n    };\n\n    // Handle the quiet settings\n    if (resp_) |resp| {\n        if (!resp.ok()) {\n            log.warn(\"erroneous kitty graphics response: {s}\", .{resp.message});\n        }\n\n        return switch (quiet) {\n            .no => if (resp.empty()) null else resp,\n            .ok => if (resp.ok()) null else resp,\n            .failures => null,\n        };\n    }\n\n    return null;\n}\n\n/// This command is used to attempt to load an image and respond with\n/// success/error but does not persist any of the command to the terminal\n/// state.\nfn query(alloc: Allocator, cmd: *const Command) Response {\n    const t = cmd.control.query;\n\n    // Query requires image ID. We can't actually send a response without\n    // an image ID either but we return an error and this will be logged\n    // downstream.\n    if (t.image_id == 0) {\n        return .{ .message = \"EINVAL: image ID required\" };\n    }\n\n    // Build a partial response to start\n    var result: Response = .{\n        .id = t.image_id,\n        .image_number = t.image_number,\n        .placement_id = t.placement_id,\n    };\n\n    // Verify the requested image exists if we have an ID\n    const storage = &terminal.screen.kitty_images;\n    const img_: ?Image = if (t.image_id != 0)\n        storage.imageById(t.image_id)\n    else\n        storage.imageByNumber(t.image_number);\n    const img = img_ orelse {\n        result.message = \"ENOENT: image not found\";\n        return result;\n    };\n\n    // Make sure our response has the image id in case we looked up by number\n    result.id = img.id;\n\n    // Track a new pin for our cursor. The cursor is always tracked but we\n    // don't want this one to move with the cursor.\n    const placement_pin = terminal.screen.pages.trackPin(\n        terminal.screen.cursor.page_pin.*,\n    ) catch |err| {\n        log.warn(\"failed to create pin for Kitty graphics err={}\", .{err});\n        result.message = \"EINVAL: failed to prepare terminal state\";\n        return result;\n    };\n\n    // Add the placement\n    const p: ImageStorage.Placement = .{\n        .pin = placement_pin,\n        .x_offset = t.x_offset,\n        .y_offset = t.y_offset,\n        .source_x = t.x,\n        .source_y = t.y,\n        .source_width = t.width,\n        .source_height = t.height,\n        .columns = t.columns,\n        .rows = t.rows,\n        .z = t.z,\n    };\n    storage.addPlacement(alloc, img.id, t.placement_id, p) catch |err| {\n        p.deinit(&terminal.screen);\n        encodeError(&result, err);\n        return result;\n    };\n\n    // Apply cursor movement setting. This only applies to pin placements.\n    switch (p.location) {\n        .pin => |pin| switch (t.cursor_movement) {\n            .none => {},\n            .after => {\n                // We use terminal.index to properly handle scroll regions.\n                const size = p.gridSize(img, terminal);\n                for (0..size.rows) |_| terminal.index() catch |err| {\n                    log.warn(\"failed to move cursor: {}\", .{err});\n                    break;\n                };\n\n                terminal.setCursorPos(\n                    terminal.screen.cursor.y,\n                    pin.x + size.cols + 1,\n                );\n            },\n        },\n    }\n\n    return result;\n}\n\nfn transmit(\n    alloc: Allocator,\n    terminal: *Terminal,\n    cmd: *const Command,\n) Response {\n    const t = cmd.transmission().?;\n    var result: Response = .{\n        .id = t.image_id,\n        .image_number = t.image_number,\n        .placement_id = t.placement_id,\n    };\n\n    const load = loadAndAddImage(alloc, terminal, cmd) catch |err| {\n        encodeError(&result, err);\n        return result;\n    };\n    errdefer load.image.deinit(alloc);\n\n    // If the loaded image was assigned its ID automatically, not based\n    // on a number or explicitly specified ID, then we don't respond.\n    if (load.image.implicit_id) return .{};\n\n    // After the image is added, set the ID in case it changed.\n    result.id = load.image.id;\n\n    return result;\n}\n\nfn delete(\n    alloc: Allocator,\n    terminal: *Terminal,\n    cmd: *const Command,\n) Response {\n    const storage = &terminal.screen.kitty_images;\n    storage.delete(alloc, terminal, cmd.control.delete);\n\n    // Delete never responds on success\n    return .{};\n}\n\nfn loadAndAddImage(\n    alloc: Allocator,\n    terminal: *Terminal,\n    cmd: *const Command,\n) !struct {\n    image: Image,\n    more: bool = false,\n    implicit_id: bool = false,\n} {\n    const t = cmd.transmission().?;\n    const storage = &terminal.screen.kitty_images;\n\n    // Determine our image. This also handles chunking and early exit.\n    var loading: LoadingImage = if (storage.loading) |loading| loading: {\n        // Note: we do NOT want to call \"cmd.toOwnedData\" here because\n        // we're _copying_ the data. We want the command data to be freed.\n        try loading.addData(alloc, cmd.data);\n\n        // If we have more then we're done\n        if (t.more_chunks) return .{ .image = loading.image, .more = true };\n\n        // We have no more chunks. We're going to be completing the\n        // image so we want to destroy the pointer to the loading\n        // image and copy it out.\n        defer {\n            alloc.destroy(loading);\n            storage.loading = null;\n        }\n\n        break :loading loading.*;\n    } else try LoadingImage.init(alloc, cmd);\n\n    // We only want to deinit on error. If we're chunking, then we don't\n    // want to deinit at all. If we're not chunking, then we'll deinit\n    // after we've copied the image out.\n    errdefer loading.deinit(alloc);\n\n    // If the image has no ID, we assign one\n    if (loading.image.id == 0) {\n        loading.image.id = storage.next_image_id;\n        storage.next_image_id +%= 1;\n\n        // If the image also has no number then its auto-ID is \"implicit\".\n        // See the doc comment on the Image.implicit_id field for more detail.\n        if (loading.image.number == 0) loading.image.implicit_id = true;\n    }\n\n    // If this is chunked, this is the beginning of a new chunked transmission.\n    // (We checked for an in-progress chunk above.)\n    if (t.more_chunks) {\n        // We allocate the pointer on the heap because its rare and we\n        // don't want to always pay the memory cost to keep it around.\n        const loading_ptr = try alloc.create(LoadingImage);\n        errdefer alloc.destroy(loading_ptr);\n        loading_ptr.* = loading;\n        storage.loading = loading_ptr;\n        return .{ .image = loading.image, .more = true };\n    }\n\n    // Dump the image data before it is decompressed\n    // loading.debugDump() catch unreachable;\n\n    // Validate and store our image\n    var img = try loading.complete(alloc);\n    errdefer img.deinit(alloc);\n    try storage.addImage(alloc, img);\n\n    // Get our display settings\n    const display_ = loading.display;\n\n    // Ensure we deinit the loading state because we're done. The image\n    // won't be deinit because of \"complete\" above.\n    loading.deinit(alloc);\n\n    return .{ .image = img, .display = display_ };\n}\n\nconst EncodeableError = Image.Error || Allocator.Error;\n\n/// Encode an error code into a message for a response.\nfn encodeError(r: *Response, err: EncodeableError) void {\n    switch (err) {\n        error.OutOfMemory => r.message = \"ENOMEM: out of memory\",\n        error.InternalError => r.message = \"EINVAL: internal error\",\n        error.InvalidData => r.message = \"EINVAL: invalid data\",\n        error.DecompressionFailed => r.message = \"EINVAL: decompression failed\",\n        error.FilePathTooLong => r.message = \"EINVAL: file path too long\",\n        error.TemporaryFileNotInTempDir => r.message = \"EINVAL: temporary file not in temp dir\",\n        error.TemporaryFileNotNamedCorrectly => r.message = \"EINVAL: temporary file not named correctly\",\n        error.UnsupportedFormat => r.message = \"EINVAL: unsupported format\",\n        error.UnsupportedMedium => r.message = \"EINVAL: unsupported medium\",\n        error.UnsupportedDepth => r.message = \"EINVAL: unsupported pixel depth\",\n        error.DimensionsRequired => r.message = \"EINVAL: dimensions required\",\n        error.DimensionsTooLarge => r.message = \"EINVAL: dimensions too large\",\n    }\n}\n\ntest \"kittygfx more chunks with q=1\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var t = try Terminal.init(alloc, .{ .rows = 5, .cols = 5 });\n    defer t.deinit(alloc);\n\n    // Initial chunk has q=1\n    {\n        const cmd = try command.Parser.parseString(\n            alloc,\n            \"a=T,f=24,t=d,i=1,s=1,v=2,c=10,r=1,m=1,q=1;////\",\n        );\n        defer cmd.deinit(alloc);\n        const resp = execute(alloc, &t, &cmd);\n        try testing.expect(resp == null);\n    }\n\n    // Subsequent chunk has no q but should respect initial\n    {\n        const cmd = try command.Parser.parseString(\n            alloc,\n            \"m=0;////\",\n        );\n        defer cmd.deinit(alloc);\n        const resp = execute(alloc, &t, &cmd);\n        try testing.expect(resp == null);\n    }\n}\n\ntest \"kittygfx more chunks with q=0\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var t = try Terminal.init(alloc, .{ .rows = 5, .cols = 5 });\n    defer t.deinit(alloc);\n\n    // Initial chunk has q=0\n    {\n        const cmd = try command.Parser.parseString(\n            alloc,\n            \"a=t,f=24,t=d,s=1,v=2,c=10,r=1,m=1,i=1,q=0;////\",\n        );\n        defer cmd.deinit(alloc);\n        const resp = execute(alloc, &t, &cmd);\n        try testing.expect(resp == null);\n    }\n\n    // Subsequent chunk has no q so should respond OK\n    {\n        const cmd = try command.Parser.parseString(\n            alloc,\n            \"m=0;////\",\n        );\n        defer cmd.deinit(alloc);\n        const resp = execute(alloc, &t, &cmd).?;\n        try testing.expect(resp.ok());\n    }\n}\n\ntest \"kittygfx more chunks with chunk increasing q\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var t = try Terminal.init(alloc, .{ .rows = 5, .cols = 5 });\n    defer t.deinit(alloc);\n\n    // Initial chunk has q=0\n    {\n        const cmd = try command.Parser.parseString(\n            alloc,\n            \"a=t,f=24,t=d,s=1,v=2,c=10,r=1,m=1,i=1,q=0;////\",\n        );\n        defer cmd.deinit(alloc);\n        const resp = execute(alloc, &t, &cmd);\n        try testing.expect(resp == null);\n    }\n\n    // Subsequent chunk sets q=1 so should not respond\n    {\n        const cmd = try command.Parser.parseString(\n            alloc,\n            \"m=0,q=1;////\",\n        );\n        defer cmd.deinit(alloc);\n        const resp = execute(alloc, &t, &cmd);\n        try testing.expect(resp == null);\n    }\n}\n\ntest \"kittygfx default format is rgba\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var t = try Terminal.init(alloc, .{ .rows = 5, .cols = 5 });\n    defer t.deinit(alloc);\n\n    const cmd = try command.Parser.parseString(\n        alloc,\n        \"a=t,t=d,i=1,s=1,v=2,c=10,r=1;///////////\",\n    );\n    defer cmd.deinit(alloc);\n    const resp = execute(alloc, &t, &cmd).?;\n    try testing.expect(resp.ok());\n\n    const storage = &t.screen.kitty_images;\n    const img = storage.imageById(1).?;\n    try testing.expectEqual(command.Transmission.Format.rgba, img.format);\n}\n\ntest \"kittygfx test valid u32 (expect invalid image ID)\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var t = try Terminal.init(alloc, .{ .rows = 5, .cols = 5 });\n    defer t.deinit(alloc);\n\n    const cmd = try command.Parser.parseString(\n        alloc,\n        \"a=p,i=4294967295\",\n    );\n    defer cmd.deinit(alloc);\n    const resp = execute(alloc, &t, &cmd).?;\n    try testing.expect(!resp.ok());\n    try testing.expectEqual(resp.message, \"ENOENT: image not found\");\n}\n\ntest \"kittygfx test valid i32 (expect invalid image ID)\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var t = try Terminal.init(alloc, .{ .rows = 5, .cols = 5 });\n    defer t.deinit(alloc);\n\n    const cmd = try command.Parser.parseString(\n        alloc,\n        \"a=p,i=1,z=-2147483648\",\n    );\n    defer cmd.deinit(alloc);\n    const resp = execute(alloc, &t, &cmd).?;\n    try testing.expect(!resp.ok());\n    try testing.expectEqual(resp.message, \"ENOENT: image not found\");\n}\n\ntest \"kittygfx no response with no image ID or number\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var t = try Terminal.init(alloc, .{ .rows = 5, .cols = 5 });\n    defer t.deinit(alloc);\n\n    {\n        const cmd = try command.Parser.parseString(\n            alloc,\n            \"a=t,f=24,t=d,s=1,v=2,c=10,r=1,i=0,I=0;////////\",\n        );\n        defer cmd.deinit(alloc);\n        const resp = execute(alloc, &t, &cmd);\n        try testing.expect(resp == null);\n    }\n}\n\ntest \"kittygfx no response with no image ID or number load and display\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var t = try Terminal.init(alloc, .{ .rows = 5, .cols = 5 });\n    defer t.deinit(alloc);\n\n    {\n        const cmd = try command.Parser.parseString(\n            alloc,\n            \"a=T,f=24,t=d,s=1,v=2,c=10,r=1,i=0,I=0;////////\",\n        );\n        defer cmd.deinit(alloc);\n        const resp = execute(alloc, &t, &cmd);\n        try testing.expect(resp == null);\n    }\n}\n\ntest \"kittygfx: ENOENT when image ID not found for placement\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var t = try Terminal.init(alloc, .{ .rows = 5, .cols = 5 });\n    defer t.deinit(alloc);\n\n    const cmd = try command.Parser.parseString(\n        alloc,\n        \"a=p,i=1\",\n    );\n    defer cmd.deinit(alloc);\n    const resp = execute(alloc, &t, &cmd).?;\n    try testing.expect(!resp.ok());\n    try testing.expectEqual(resp.message, \"ENOENT: image not found\");\n}\n\ntest \"kittygfx: respond OK with metadata on display transmissions\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var t = try Terminal.init(alloc, .{ .rows = 5, .cols = 5 });\n    defer t.deinit(alloc);\n\n    const cmd = try command.Parser.parseString(\n        alloc,\n        \"a=T,f=24,t=d,s=1,v=2,c=10,r=1,i=1;////////\",\n    );\n    defer cmd.deinit(alloc);\n    const resp = execute(alloc, &t, &cmd).?;\n    try testing.expect(resp.ok());\n}\n\ntest \"kittygfx: handle `q` with chunked transmissions properly\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var t = try Terminal.init(alloc, .{ .rows = 5, .cols = 5 });\n    defer t.deinit(alloc);\n\n    // Initial chunk has q=1\n    {\n        const cmd = try command.Parser.parseString(\n            alloc,\n            \"a=T,f=24,t=d,i=1,s=1,v=2,c=10,r=1,m=1,q=1;////\",\n        );\n        defer cmd.deinit(alloc);\n        const resp = execute(alloc, &t, &cmd);\n        try testing.expect(resp == null);\n    }\n\n    // Subsequent chunk has no q but should respect initial\n    {\n        const cmd = try command.Parser.parseString(\n            alloc,\n            \"m=0;////\",\n        );\n        defer cmd.deinit(alloc);\n        const resp = execute(alloc, &t, &cmd);\n        try testing.expect(resp == null);\n    }\n}\n\ntest \"kittygfx: ensure temporary files are named per spec\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var t = try Terminal.init(alloc, .{ .rows = 5, .cols = 5 });\n    defer t.deinit(alloc);\n\n    const cmd = try command.Parser.parseString(\n        alloc,\n        \"a=T,f=24,t=d,i=1,s=1,v=2,c=10,r=1,m=temporary;////////\",\n    );\n    defer cmd.deinit(alloc);\n    const resp = execute(alloc, &t, &cmd).?;\n    try testing.expect(resp.ok());\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.7762757410403356,
      "diff_lines": 388,
      "added_lines": 143,
      "removed_lines": 150,
      "diff_preview": "--- expected+++ predicted@@ -2,26 +2,17 @@ const assert = std.debug.assert;\n const Allocator = std.mem.Allocator;\n \n-const renderer = @import(\"../../renderer.zig\");\n const point = @import(\"../point.zig\");\n const Terminal = @import(\"../Terminal.zig\");\n const command = @import(\"graphics_command.zig\");\n const image = @import(\"graphics_image.zig\");\n const Command = command.Command;\n const Response = command.Response;\n-const LoadingImage = image.LoadingImage;\n const Image = image.Image;\n const ImageStorage = @import(\"graphics_storage.zig\").ImageStorage;\n \n const log = std.log.scoped(.kitty_gfx);\n \n-/// Execute a Kitty graphics command against the given terminal. This\n-/// will never fail, but the response may indicate an error and the\n-/// terminal state may not be updated to reflect the command. This will\n-/// never put the terminal in an unrecoverable state, however.\n-///\n-/// The allocator must be the same allocator that was used to build\n-/// the command.\n pub fn execute(\n     alloc: Allocator,\n     terminal: *Terminal,\n@@ -47,9 +38,6 @@ \n     const resp_: ?Response = switch (cmd.control) {\n         .query => query(alloc, cmd),\n-        .display => display(alloc, terminal, cmd),\n-        .delete => delete(alloc, terminal, cmd),\n-\n         .transmit, .transmit_and_display => resp: {\n             // If we're transmitting, then our `q` setting value is complicated.\n             // The `q` setting inherits the value from the starting command\n@@ -69,11 +57,13 @@ \n             break :resp transmit(alloc, terminal, cmd);\n         },\n+        .display => display(alloc, terminal, cmd),\n+        .delete => delete(alloc, terminal, cmd),\n \n         .transmit_animation_frame,\n         .control_animation,\n         .compose_animation,\n-        => .{ .message = \"ERROR: unimplemented action\" },\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 78.38878989219666,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 27122,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_terminal_kitty_graphics_image.zig",
    "exact_match": false,
    "similarity": 0.5255578142986602,
    "diff_lines": 561,
    "predicted_output": "const std = @import(\"std\");\nconst builtin = @import(\"builtin\");\nconst assert = std.debug.assert;\nconst Allocator = std.mem.Allocator;\nconst ArenaAllocator = std.heap.ArenaAllocator;\nconst posix = std.posix;\n\nconst command = @import(\"graphics_command.zig\");\nconst point = @import(\"../point.zig\");\nconst PageList = @import(\"../PageList.zig\");\nconst internal_os = @import(\"../../os/main.zig\");\nconst wuffs = @import(\"wuffs\");\n\nconst log = std.log.scoped(.kitty_gfx);\n\n/// Maximum width or height of an image. Taken directly from Kitty.\nconst max_dimension = 10000;\n\n/// Maximum size in bytes, taken from Kitty.\nconst max_size = 400 * 1024 * 1024; // 400MB\n\n/// An image that is still being loaded. The image should be initialized\n/// using init on the first chunk and then addData for each subsequent\n/// chunk. Once all chunks have been added, complete should be called\n/// to finalize the image.\npub const LoadingImage = struct {\n    /// The in-progress image. The first chunk must have all the metadata\n    /// so this comes from that initially.\n    image: Image,\n\n    /// The data that is being built up.\n    data: std.ArrayListUnmanaged(u8) = .{},\n\n    /// This is non-null when a transmit and display command is given\n    /// so that we display the image after it is fully loaded.\n    display: ?command.Display = null,\n\n    /// Quiet is the quiet settings for the initial load command. This is\n    /// used if q isn't set on subsequent chunks.\n    quiet: command.Command.Quiet,\n\n    /// Initialize a chunked immage from the first image transmission.\n    /// If this is a multi-chunk image, this should only be the FIRST\n    /// chunk.\n    pub fn init(alloc: Allocator, cmd: *const command.Command) !LoadingImage {\n        // Build our initial image from the properties sent via the control.\n        // These can be overwritten by the data loading process. For example,\n        // PNG loading sets the width/height from the data.\n        const t = cmd.transmission().?;\n        var result: LoadingImage = .{\n            .image = .{\n                .id = t.image_id,\n                .number = t.image_number,\n                .width = t.width,\n                .height = t.height,\n                .compression = t.compression,\n                .format = t.format,\n            },\n\n            .display = cmd.display(),\n            .quiet = cmd.quiet,\n        };\n\n        // Special case for the direct medium, we just add the chunk directly.\n        if (t.medium == .direct) {\n            try result.addData(alloc, cmd.data);\n            return result;\n        }\n\n        // Since we're only supporting posix then max_path_bytes should\n        // be enough to stack allocate the path.\n        var buf: [std.fs.max_path_bytes]u8 = undefined;\n        const path = posix.realpath(cmd.data, &buf) catch |err| {\n            log.warn(\"failed to get absolute path: {}\", .{err});\n            return error.InvalidData;\n        };\n\n        // Depending on the medium, load the data from the path.\n        switch (t.medium) {\n            .direct => unreachable, // handled above\n            .file => try result.readFile(.file, alloc, t, path),\n            .temporary_file => try result.readFile(.temporary_file, alloc, t, path),\n            .shared_memory => try result.readSharedMemory(alloc, t, path),\n        }\n\n        return result;\n    }\n\n    /// Reads the data from a shared memory segment.\n    fn readSharedMemory(\n        self: *LoadingImage,\n        alloc: Allocator,\n        t: command.Transmission,\n        path: []const u8,\n    ) !void {\n        // windows is currently unsupported, does it support shm?\n        if (comptime builtin.target.os.tag == .windows) {\n            return error.UnsupportedMedium;\n        }\n\n        // libc is required for shm_open\n        if (comptime !builtin.link_libc) {\n            return error.UnsupportedMedium;\n        }\n\n        // Since we're only supporting posix then max_path_bytes should\n        // be enough to stack allocate the path.\n        var buf: [std.fs.max_path_bytes]u8 = undefined;\n        const pathz = std.fmt.bufPrintZ(&buf, \"{s}\", .{path}) catch return error.InvalidData;\n\n        const fd = std.c.shm_open(pathz, @as(c_int, @bitCast(std.c.O{ .ACCMODE = .RDONLY })), 0);\n        switch (std.posix.errno(fd)) {\n            .SUCCESS => {},\n            else => |err| {\n                log.warn(\"unable to open shared memory {s}: {}\", .{ path, err });\n                return error.InvalidData;\n            },\n        }\n        defer _ = std.c.close(fd);\n        defer _ = std.c.shm_unlink(pathz);\n\n        // The size from stat on may be larger than our expected size because\n        // shared memory has to be a multiple of the page size.\n        const stat_size: usize = stat: {\n            const stat = std.posix.fstat(fd) catch |err| {\n                log.warn(\"unable to fstat shared memory {s}: {}\", .{ path, err });\n                return error.InvalidData;\n            };\n            if (stat.size <= 0) return error.InvalidData;\n            break :stat @intCast(stat.size);\n        };\n\n        // Our stat size must be at least the expected size otherwise\n        // the shared memory data is invalid.\n        const expected_size: usize = switch (self.image.format) {\n            // Png we decode the full data size because later decoding will\n            // get the proper dimensions and assert validity.\n            .png => stat_size,\n\n            // For these formats we have a size we must have.\n            .gray, .gray_alpha, .rgb, .rgba => |f| size: {\n                const bpp = f.bpp();\n                break :size self.image.width * self.image.height * bpp;\n            },\n        };\n\n        // Our stat size must be at least the expected size otherwise\n        // the shared memory data is invalid.\n        if (stat_size < expected_size) {\n            log.warn(\n                \"shared memory size too small expected={} actual={}\",\n                .{ expected_size, stat_size },\n            );\n            return error.InvalidData;\n        }\n\n        const map = std.posix.mmap(\n            null,\n            stat_size, // mmap always uses the stat size\n            std.c.PROT.READ,\n            std.c.MAP{ .TYPE = .SHARED },\n            fd,\n            0,\n        ) catch |err| {\n            log.warn(\"unable to mmap shared memory {s}: {}\", .{ path, err });\n            return error.InvalidData;\n        };\n        defer std.posix.munmap(map);\n\n        // Our end size always uses the expected size so we cut off the\n        // padding for mmap alignment.\n        const start: usize = @intCast(t.offset);\n        const end: usize = if (t.size > 0) @min(\n            @as(usize, @intCast(t.offset)) + @as(usize, @intCast(t.size)),\n            expected_size,\n        ) else expected_size;\n\n        assert(self.data.items.len == 0);\n        try self.data.appendSlice(alloc, map[start..end]);\n    }\n\n    /// Reads the data from a temporary file and returns it. This allocates\n    /// and does not free any of the data, so the caller must free it.\n    ///\n    /// This will also delete the temporary file if it is in a safe location.\n    fn readFile(\n        self: *LoadingImage,\n        comptime medium: command.Transmission.Medium,\n        alloc: Allocator,\n        t: command.Transmission,\n        path: []const u8,\n    ) !void {\n        switch (medium) {\n            .file, .temporary_file => {},\n            else => @compileError(\"readFile only supports file and temporary_file\"),\n        }\n\n        // Verify file seems \"safe\". This is logic copied directly from Kitty,\n        // mostly. This is really rough but it will catch obvious bad actors.\n        if (std.mem.startsWith(u8, path, \"/proc/\") or\n            std.mem.startsWith(u8, path, \"/sys/\") or\n            (std.mem.startsWith(u8, path, \"/dev/\") and\n            !std.mem.startsWith(u8, path, \"/dev/shm/\")))\n        {\n            return error.InvalidData;\n        }\n\n        // Temporary file logic\n        if (medium == .temporary_file) {\n            if (!isPathInTempDir(path)) return error.TemporaryFileNotInTempDir;\n            if (std.mem.indexOf(u8, path, \"tty-graphics-protocol\") == null) {\n                return error.TemporaryFileNotNamedCorrectly;\n            }\n        }\n        defer if (medium == .temporary_file) {\n            posix.unlink(path) catch |err| {\n                log.warn(\"failed to delete temporary file: {}\", .{err});\n            };\n        }\n\n        var file = std.fs.cwd().openFile(path, .{}) catch |err| {\n            log.warn(\"failed to open temporary file: {}\", .{err});\n            return error.InvalidData;\n        };\n        defer file.close();\n\n        // File must be a regular file\n        if (file.stat()) |stat| {\n            if (stat.kind != .file) {\n                log.warn(\"file is not a regular file kind={}\", .{stat.kind});\n                return error.InvalidData;\n            }\n        } else |err| {\n            log.warn(\"failed to stat file: {}\", .{err});\n            return error.InvalidData;\n        }\n\n        if (t.offset > 0) {\n            file.seekTo(@intCast(t.offset)) catch |err| {\n                log.warn(\"failed to seek to offset {}: {}\", .{ t.offset, err });\n                return error.InvalidData;\n            };\n        }\n\n        var buf_reader = std.io.bufferedReader(file.reader());\n        const reader = buf_reader.reader();\n\n        // Read the file\n        var managed = std.ArrayList(u8).init(alloc);\n        errdefer managed.deinit();\n        const size: usize = if (t.size > 0) @min(t.size, max_size) else max_size;\n        reader.readAllArrayList(&managed, size) catch |err| {\n            log.warn(\"failed to read temporary file: {}\", .{err});\n            return error.InvalidData;\n        };\n\n        // Set our data\n        assert(self.data.items.len == 0);\n        self.data = .{ .items = managed.items, .capacity = managed.capacity };\n    }\n\n    /// Returns true if path appears to be in a temporary directory.\n    /// Copies logic from Kitty.\n    fn isPathInTempDir(path: []const u8) bool {\n        if (std.mem.startsWith(u8, path, \"/tmp\")) return true;\n        if (std.mem.startsWith(u8, path, \"/dev/shm\")) return true;\n        if (internal_os.allocTmpDir(std.heap.page_allocator)) |dir| {\n            defer internal_os.freeTmpDir(std.heap.page_allocator, dir);\n            if (std.mem.startsWith(u8, path, dir)) return true;\n\n            // The temporary dir is sometimes a symlink. On macOS for\n            // example /tmp is /private/var/...\n            var buf: [std.fs.max_path_bytes]u8 = undefined;\n            if (posix.realpath(dir, &buf)) |real_dir| {\n                if (std.mem.startsWith(u8, path, real_dir)) return true;\n            } else |_| {}\n        }\n\n        return false;\n    }\n\n    pub fn deinit(self: *LoadingImage, alloc: Allocator) void {\n        self.image.deinit(alloc);\n        self.data.deinit(alloc);\n    }\n};\n\n/// Image represents a single fully loaded image.\npub const Image = struct {\n    id: u32 = 0,\n    number: u32 = 0,\n    width: u32 = 0,\n    height: u32 = 0,\n    format: command.Transmission.Format = .rgb,\n    compression: command.Transmission.Compression = .none,\n    data: []const u8 = \"\",\n    transmit_time: std.time.Instant = undefined,\n\n    /// Set this to true if this image was loaded by a command that\n    /// doesn't specify an ID or number, since such commands should\n    /// not be responded to, even though we do currently give them\n    /// IDs in the public range (which is bad!).\n    implicit_id: bool = false,\n\n    pub const Error = error{\n        InternalError,\n        InvalidData,\n        DecompressionFailed,\n        DimensionsRequired,\n        DimensionsTooLarge,\n        FilePathTooLong,\n        TemporaryFileNotInTempDir,\n        TemporaryFileNotNamedCorrectly,\n        UnsupportedFormat,\n        UnsupportedMedium,\n        UnsupportedDepth,\n    };\n\n    pub fn deinit(self: *Image, alloc: Allocator) void {\n        if (self.data.len > 0) alloc.free(self.data);\n    }\n\n    /// Mostly for logging\n    pub fn withoutData(self: *const Image) Image {\n        var copy = self.*;\n        copy.data = \"\";\n        return copy;\n    }\n};\n\n/// The rect taken up by some image placement, in grid cells. This will\n/// be rounded up to the nearest grid cell since we can't place images\n/// in partial grid cells.\npub const Rect = struct {\n    top_left: PageList.Pin,\n    bottom_right: PageList.Pin,\n};",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.5255578142986602,
      "diff_lines": 561,
      "added_lines": 11,
      "removed_lines": 502,
      "diff_preview": "--- expected+++ predicted@@ -5,7 +5,6 @@ const ArenaAllocator = std.heap.ArenaAllocator;\n const posix = std.posix;\n \n-const fastmem = @import(\"../../fastmem.zig\");\n const command = @import(\"graphics_command.zig\");\n const point = @import(\"../point.zig\");\n const PageList = @import(\"../PageList.zig\");\n@@ -68,25 +67,12 @@             return result;\n         }\n \n-        // Otherwise, the payload data is guaranteed to be a path.\n-\n-        if (comptime builtin.os.tag != .windows) {\n-            if (std.mem.indexOfScalar(u8, cmd.data, 0) != null) {\n-                // posix.realpath *asserts* that the path does not have\n-                // internal nulls instead of erroring.\n-                log.warn(\"failed to get absolute path: BadPathName\", .{});\n-                return error.InvalidData;\n-            }\n-        }\n-\n-        var abs_buf: [std.fs.max_path_bytes]u8 = undefined;\n-        const path = switch (t.medium) {\n-            .direct => unreachable, // handled above\n-            .file, .temporary_file => posix.realpath(cmd.data, &abs_buf) catch |err| {\n-                log.warn(\"failed to get absolute path: {}\", .{err});\n-                return error.InvalidData;\n-            },\n-            .shared_memory => cmd.data,\n+        // Since we're only supporting posix then max_path_bytes should\n+        // be enough to stack allocate the path.\n+        var buf: [std.fs.max_path_bytes]u8 = undefined;\n+        const path = posix.realpath(cmd.data, &buf) catch |err| {\n+            log.warn(\"failed to get absolute path: {}\", .{err});\n+            return error.InvalidData;\n         };\n \n         // Depending on the medium, load the data from the path.\n@@ -144,6 +130,8 @@             break :stat @intCast(stat.size);\n         };\n \n+        // Our stat size must be at least the expected size otherwise\n+        // the shared memory data is invalid.\n         const expected_size: usize = switch (self.image.format) {\n             // Png we decode the full data size because later decoding will\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 59.029603719711304,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 40215,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_terminal_modes.zig",
    "exact_match": false,
    "similarity": 0.9325265441260417,
    "diff_lines": 98,
    "predicted_output": "//! This file contains all the terminal modes that we support\n//! and various support types for them: an enum of supported modes,\n//! a packed struct to store mode values, a more generalized state\n//! struct to store values plus handle save/restore, and much more.\n//!\n//! There is pretty heavy comptime usage and type generation here.\n//! I don't love to have this sort of complexity but its a good way\n//! to ensure all our various types and logic remain in sync.\n\nconst std = @import(\"std\");\nconst testing = std.testing;\n\n/// A struct that maintains the state of all the settable modes.\npub const ModeState = struct {\n    /// The values of the current modes.\n    values: ModePacked = .{},\n    \n    /// The saved values. We only allow saving each mode once.\n    /// This is in line with other terminals that implement XTSAVE\n    /// and XTRESTORE. We can improve this in the future if it becomes\n    /// a real-world issue but we need to be aware of a DoS vector.\n    saved: ModePacked = .{},\n    \n    /// The default values for the modes. This is used to reset\n    /// the modes to their default values during reset.\n    default: ModePacked = .{},\n    \n    /// Reset the modes to their default values. This also clears the\n    /// saved state.\n    pub fn reset(self: *ModeState) void {\n        self.values = self.default;\n        self.saved = .{};\n    }\n\n    /// Set a mode to a value.\n    pub fn set(self: *ModeState, mode: Mode, value: bool) void {\n        switch (mode) {\n            inline else => |mode_comptime| {\n                const entry = comptime entryForMode(mode_comptime);\n                @field(self.values, entry.name) = value;\n            },\n        }\n    }\n\n    /// Get the value of a mode.\n    pub fn get(self: *ModeState, mode: Mode) bool {\n        switch (mode) {\n            inline else => |mode_comptime| {\n                const entry = comptime entryForMode(mode_comptime);\n                return @field(self.values, entry.name);\n            },\n        }\n    }\n\n    /// Save the state of the given mode. This can then be restored\n    /// with restore. This will only be accurate if the previous\n    /// mode was saved exactly once and not restored. Otherwise this\n    /// will just keep restoring the last stored value in memory.\n    pub fn save(self: *ModeState, mode: Mode) void {\n        switch (mode) {\n            inline else => |mode_comptime| {\n                const entry = comptime entryForMode(mode_comptime);\n                @field(self.saved, entry.name) = @field(self.values, entry.name);\n            },\n        }\n    }\n\n    /// See save. This will return the restored value.\n    pub fn restore(self: *ModeState, mode: Mode) bool {\n        switch (mode) {\n            inline else => |mode_comptime| {\n                const entry = comptime entryForMode(mode_comptime);\n                @field(self.values, entry.name) = @field(self.saved, entry.name);\n                return @field(self.values, entry.name);\n            },\n        }\n    }\n\n    test {\n        // We have this here so that we explicitly fail when we change the\n        // size of modes. The size of modes is NOT particularly important,\n        // we just want to be mentally aware when it happens.\n        try std.testing.expectEqual(8, @sizeOf(ModePacked));\n    }\n};\n\n/// A packed struct of all the settable modes. This shouldn't\n/// be used directly but rather through the ModeState struct.\npub const ModePacked = packed_struct: {\n    const StructField = std.builtin.Type.StructField;\n    var fields: [entries.len]StructField = undefined;\n    for (entries, 0..) |entry, i| {\n        fields[i] = .{\n            .name = entry.name,\n            .type = bool,\n            .default_value_ptr = &entry.default,\n            .is_comptime = false,\n            .alignment = 0,\n        };\n    }\n\n    break :packed_struct @Type(.{ .@\"struct\" = .{\n        .layout = .@\"packed\",\n        .fields = &fields,\n        .decls = &.{},\n        .is_tuple = false,\n    } });\n};\n\n/// An enum(u16) of the available modes. See entries for available values.\npub const Mode = mode_enum: {\n    const EnumField = std.builtin.Type.EnumField;\n    var fields: [entries.len]EnumField = undefined;\n    for (entries, 0..) |entry, i| {\n        fields[i] = .{\n            .name = entry.name,\n            .value = @as(ModeTag.Backing, @bitCast(ModeTag{\n                .value = entry.value,\n                .ansi = entry.ansi,\n            })),\n        };\n    }\n\n    break :mode_enum @Type(.{ .@\"enum\" = .{\n        .tag_type = ModeTag.Backing,\n        .fields = &fields,\n        .decls = &.{},\n        .is_exhaustive = true,\n    } });\n};\n\n/// The tag type for our enum is a u16 but we use a packed struct\n/// in order to pack the ansi bit into the tag.\npub const ModeTag = packed struct(u16) {\n    pub const Backing = @typeInfo(@This()).@\"struct\".backing_integer.?; \n    value: u15,\n    ansi: bool = false,\n};\n\nfn entryForMode(comptime mode: Mode) ModeEntry {\n    const name = @tagName(mode);\n    for (entries) |entry| {\n        if (std.mem.eql(u8, entry.name, name)) return entry;\n    }\n\n    unreachable;\n}\n\n/// A single entry of a possible mode we support. This is used to\n/// dynamically define the enum and other tables.\nconst ModeEntry = struct {\n    name: [:0]const u8,\n    value: comptime_int,\n    default: bool = false,\n    ansi: bool = false,\n    disabled: bool = false,\n};\n\n/// The full list of available entries. For documentation see how\n/// they're used within Ghostty or google their values. It is not\n/// valuable to redocument them all here.\nconst entries: []const ModeEntry = &.{\n    // ANSI\n    .{ .name = \"disable_keyboard\", .value = 2, .ansi = true }, // KAM\n    .{ .name = \"insert\", .value = 4, .ansi = true },\n    .{ .name = \"send_receive_mode\", .value = 12, .ansi = true, .default = true }, // SRM\n    .{ .name = \"linefeed\", .value = 20, .ansi = true },\n\n    // DEC\n    .{ .name = \"cursor_keys\", .value = 1 }, // DECCKM\n    .{ .name = \"132_column\", .value = 3 },\n    .{ .name = \"slow_scroll\", .value = 4 },\n    .{ .name = \"reverse_colors\", .value = 5 },\n    .{ .name = \"origin\", .value = 6 },\n    .{ .name = \"wraparound\", .value = 7, .default = true },\n    .{ .name = \"mouse_event_x10\", .value = 9 },\n    .{ .name = \"cursor_blinking\", .value = 12 },\n    .{ .name = \"cursor_visible\", .value = 25, .default = true },\n    .{ .name = \"enable_mode_3\", .value = 40 },\n    .{ .name = \"reverse_wrap\", .value = 45 },\n    .{ .name = \"keypad_keys\", .value = 66 },\n    .{ .name = \"mouse_event_normal\", .value = 1000 },\n    .{ .name = \"mouse_event_button\", .value = 1002 },\n    .{ .name = \"mouse_event_any\", .value = 1003 },\n    .{ .name = \"focus_event\", .value = 1004 },\n    .{ .name = \"mouse_format_utf8\", .value = 1005 },\n    .{ .name = \"mouse_format_sgr\", .value = 1006 },\n    .{ .name = \"mouse_format_urxvt\", .value = 1015 },\n    .{ .name = \"mouse_format_sgr_pixels\", .value = 1016 },\n    .{ .name = \"alt_esc_prefix\", .value = 1036, .default = true },\n    .{ .name = \"alt_sends_escape\", .value = 1039 },\n    .{ .name = \"reverse_wrap_extended\", .value = 1045 },\n    .{ .name = \"alt_screen\", .value = 1047 },\n    .{ .name = \"alt_screen_save_cursor_clear_enter\", .value = 1049 },\n    .{ .name = \"bracketed_paste\", .value = 2004 },\n    .{ .name = \"synchronized_output\", .value = 2026 },\n    .{ .name = \"grapheme_cluster\", .value = 2027 },\n    .{ .name = \"report_color_scheme\", .value = 2031 },\n    .{ .name = \"in_band_size_reports\", .value = 2048 },\n};\n\ntest {\n    _ = Mode;\n    _ = ModePacked;\n}\n\ntest modeFromInt {\n    try testing.expect(modeFromInt(4, true).? == .insert);\n    try testing.expect(modeFromInt(9, true) == null);\n    try testing.expect(modeFromInt(9, false).? == .mouse_event_x10);\n    try testing.expect(modeFromInt(14, true) == null);\n}\n\ntest ModeState {\n    var state: ModeState = .{};\n\n    // Normal set/get\n    try testing.expect(!state.get(.cursor_keys));\n    state.set(.cursor_keys, true);\n    try testing.expect(state.get(.cursor_keys));\n\n    // Save/restore\n    state.save(.cursor_keys);\n    state.set(.cursor_keys, false);\n    try testing.expect(!state.get(.cursor_keys));\n    try testing.expect(state.restore(.cursor_keys));\n    try testing.expect(state.get(.cursor_keys));\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9325265441260417,
      "diff_lines": 98,
      "added_lines": 5,
      "removed_lines": 36,
      "diff_preview": "--- expected+++ predicted@@ -14,17 +14,17 @@ pub const ModeState = struct {\n     /// The values of the current modes.\n     values: ModePacked = .{},\n-\n+    \n     /// The saved values. We only allow saving each mode once.\n     /// This is in line with other terminals that implement XTSAVE\n     /// and XTRESTORE. We can improve this in the future if it becomes\n     /// a real-world issue but we need to be aware of a DoS vector.\n     saved: ModePacked = .{},\n-\n+    \n     /// The default values for the modes. This is used to reset\n     /// the modes to their default values during reset.\n     default: ModePacked = .{},\n-\n+    \n     /// Reset the modes to their default values. This also clears the\n     /// saved state.\n     pub fn reset(self: *ModeState) void {\n@@ -132,33 +132,12 @@ /// The tag type for our enum is a u16 but we use a packed struct\n /// in order to pack the ansi bit into the tag.\n pub const ModeTag = packed struct(u16) {\n-    pub const Backing = @typeInfo(@This()).@\"struct\".backing_integer.?;\n+    pub const Backing = @typeInfo(@This()).@\"struct\".backing_integer.?; \n     value: u15,\n     ansi: bool = false,\n-\n-    test \"order\" {\n-        const t: ModeTag = .{ .value = 1 };\n-        const int: Backing = @bitCast(t);\n-        try std.testing.expectEqual(@as(Backing, 1), int);\n-    }\n-};\n-\n-pub fn modeFromInt(v: u16, ansi: bool) ?Mode {\n-    inline for (entries) |entry| {\n-        if (comptime !entry.disabled) {\n-            if (entry.value == v and entry.ansi == ansi) {\n-                const tag: ModeTag = .{ .ansi = ansi, .value = entry.value };\n-                const int: ModeTag.Backing = @bitCast(tag);\n-                return @enumFromInt(int);\n-            }\n-        }\n-    }\n-\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 51.304903984069824,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 12161,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_terminal_sgr.zig",
    "exact_match": false,
    "similarity": 0.7491138079152592,
    "diff_lines": 497,
    "predicted_output": "//! SGR (Select Graphic Rendition) attrinvbute parsing and types.\n\nconst std = @import(\"std\");\nconst assert = std.debug.assert;\nconst testing = std.testing;\nconst color = @import(\"color.zig\");\nconst SepList = @import(\"Parser.zig\").Action.CSI.SepList;\n\n/// Attribute type for SGR\npub const Attribute = union(enum) {\n    pub const Tag = std.meta.FieldEnum(Attribute);\n\n    /// Unset all attributes\n    unset,\n\n    /// Unknown attribute, the raw CSI command parameters are here.\n    unknown: struct {\n        /// Full is the full SGR input.\n        full: []const u16,\n\n        /// Partial is the remaining, where we got hung up.\n        partial: []const u16,\n    },\n\n    /// Bold the text.\n    bold,\n    reset_bold,\n\n    /// Italic text.\n    italic,\n    reset_italic,\n\n    /// Faint/dim text.\n    faint,\n\n    /// Underline the text\n    underline: Underline,\n    reset_underline,\n    underline_color: color.RGB,\n    @\"256_underline_color\": u8,\n    reset_underline_color,\n\n    // Overline the text\n    overline,\n    reset_overline,\n\n    /// Blink the text\n    blink,\n    reset_blink,\n\n    /// Invert fg/bg colors.\n    inverse,\n    reset_inverse,\n\n    /// Invisible\n    invisible,\n    reset_invisible,\n\n    /// Strikethrough the text.\n    strikethrough,\n    reset_strikethrough,\n\n    /// Set foreground color as RGB values.\n    direct_color_fg: color.RGB,\n\n    /// Set background color as RGB values.\n    direct_color_bg: color.RGB,\n\n    /// Set the background/foreground as a named color attribute.\n    @\"8_bg\": color.Name,\n    @\"8_fg\": color.Name,\n\n    /// Reset the fg/bg to their default values.\n    reset_fg,\n    reset_bg,\n\n    /// Set the background/foreground as a named bright color attribute.\n    @\"8_bright_bg\": color.Name,\n    @\"8_bright_fg\": color.Name,\n\n    /// Set background color as 256-color palette.\n    @\"256_bg\": u8,\n\n    /// Set foreground color as 256-color palette.\n    @\"256_fg\": u8,\n\n    pub const Underline = enum(u3) {\n        none = 0,\n        single = 1,\n        double = 2,\n        curly = 3,\n        dotted = 4,\n        dashed = 5,\n    };\n};\n\n/// Parser parses the attributes from a list of SGR parameters.\npub const Parser = struct {\n    params: []const u16,\n    params_sep: SepList = SepList.initEmpty(),\n    idx: usize = 0,\n\n    /// Next returns the next attribute or null if there are no more attributes.\n    pub fn next(self: *Parser) ?Attribute {\n        if (self.idx >= self.params.len) {\n            // If we're at index zero it means we must have an empty\n            // list and an empty list implicitly means unset.\n            if (self.idx == 0) {\n                // Add one to ensure we don't loop on unset\n                self.idx += 1;\n                return .unset;\n            }\n\n            return null;\n        }\n\n        const slice = self.params[self.idx..self.params.len];\n        const colon = self.params_sep.isSet(self.idx);\n        self.idx += 1;\n\n        // Our last one will have an idx be the last value.\n        if (slice.len == 0) return null;\n\n        // If we have a colon separator then we need to ensure we're\n        // parsing a value that allows it.\n        if (colon) switch (slice[0]) {\n            4, 38, 48, 58 => {},\n            else => {\n                // Consume all the colon separated values.\n                const start = self.idx;\n                while (self.params_sep.isSet(self.idx)) self.idx += 1;\n                self.idx += 1;\n                return .{ .unknown = .{\n                    .full = self.params,\n                    .partial = slice[0 .. self.idx - start + 1],\n                } };\n            },\n        };\n\n        switch (slice[0]) {\n            0 => return .unset,\n            1 => return .bold,\n            2 => return .faint,\n            3 => return .italic,\n            4 => underline: {\n                if (colon) {\n                    assert(slice.len >= 2);\n                    if (self.isColon()) {\n                        self.consumeUnknownColon();\n                        break :underline;\n                    }\n\n                    self.idx += 1;\n                    switch (slice[1]) {\n                        0 => return .reset_underline,\n                        1 => return .{ .underline = .single },\n                        2 => return .{ .underline = .double },\n                        3 => return .{ .underline = .curly },\n                        4 => return .{ .underline = .dotted },\n                        5 => return .{ .underline = .dashed },\n\n                        // For unknown underline styles, just render\n                        // a single underline.\n                        else => return .{ .underline = .single },\n                    }\n                }\n\n                return .{ .underline = .single };\n            },\n            5 => return .blink,\n            6 => return .blink,\n            7 => return .inverse,\n            8 => return .invisible,\n            9 => return .strikethrough,\n            21 => return .{ .underline = .double },\n            22 => return .reset_bold,\n            23 => return .reset_italic,\n            24 => return .reset_underline,\n            25 => return .reset_blink,\n            27 => return .reset_inverse,\n            28 => return .reset_invisible,\n            29 => return .reset_strikethrough,\n            30...37 => return .{\n                .@\"8_fg\" = @enumFromInt(slice[0] - 30),\n            },\n            38 => if (slice.len >= 2) switch (slice[1]) {\n                // `2` indicates direct-color (r, g, b).\n                // We need at least 3 more params for this to make sense.\n                2 => if (self.parseDirectColor(\n                    .direct_color_fg,\n                    slice,\n                    colon,\n                )) |v| return v,\n                // `5` indicates indexed color.\n                5 => if (slice.len >= 3) {\n                    self.idx += 2;\n                    return .{\n                        .@\"256_fg\" = @truncate(slice[2]),\n                    };\n                },\n                else => {},\n            },\n            39 => return .reset_fg,\n            40...47 => return .{\n                .@\"8_bg\" = @enumFromInt(slice[0] - 40),\n            },\n            48 => if (slice.len >= 2) switch (slice[1]) {\n                // `2` indicates direct-color (r, g, b).\n                // We need at least 3 more params for this to make sense.\n                2 => if (self.parseDirectColor(\n                    .direct_color_bg,\n                    slice,\n                    colon,\n                )) |v| return v,\n                // `5` indicates indexed color.\n                5 => if (slice.len >= 3) {\n                    self.idx += 2;\n                    return .{\n                        .@\"256_bg\" = @truncate(slice[2]),\n                    };\n                },\n                else => {},\n            },\n            49 => return .reset_bg,\n            53 => return .overline,\n            55 => return .reset_overline,\n            58 => if (slice.len >= 2) switch (slice[1]) {\n                // `2` indicates direct-color (r, g, b).\n                // We need at least 3 more params for this to make sense.\n                2 => if (self.parseDirectColor(\n                    .underline_color,\n                    slice,\n                    colon,\n                )) |v| return v,\n                // `5` indicates indexed color.\n                5 => if (slice.len >= 3) {\n                    self.idx += 2;\n                    return .{\n                        .@\"256_underline_color\" = @truncate(slice[2]),\n                    };\n                },\n                else => {},\n            },\n            59 => return .reset_underline_color,\n            90...97 => return .{\n                // 82 instead of 90 to offset to \"bright\" colors\n                .@\"8_bright_fg\" = @enumFromInt(slice[0] - 82),\n            },\n            100...107 => return .{\n                .@\"8_bright_bg\" = @enumFromInt(slice[0] - 92),\n            },\n            else => {},\n        }\n\n        return .{ .unknown = .{ .full = self.params, .partial = slice } };\n    }\n\n    fn parseDirectColor(\n        self: *Parser,\n        comptime tag: Attribute.Tag,\n        slice: []const u16,\n        colon: bool,\n    ) ?Attribute {\n        // Any direct color style must have at least 5 values.\n        if (slice.len < 5) return null;\n\n        // Only used for direct color sets (38, 48, 58) and subparam 2.\n        assert(slice[1] == 2);\n\n        // Note: We use @truncate because the value should be 0 to 255. If\n        // it isn't, the behavior is undefined so we just... truncate it.\n\n        // If we don't have a colon, then we expect exactly 3 semicolon\n        // separated values.\n        if (!colon) {\n            self.idx += 4;\n            return @unionInit(Attribute, @tagName(tag), .{\n                .r = @truncate(slice[2]),\n                .g = @truncate(slice[3]),\n                .b = @truncate(slice[4]),\n            });\n        }\n\n        // We have a colon, we might have either 5 or 6 values depending\n        // on if the colorspace is present.\n        const count = self.countColon();\n        switch (count) {\n            3 => {\n                self.idx += 4;\n                return @unionInit(Attribute, @tagName(tag), .{\n                    .r = @truncate(slice[2]),\n                    .g = @truncate(slice[3]),\n                    .b = @truncate(slice[4]),\n                });\n            },\n            4 => {\n                self.idx += 5;\n                return @unionInit(Attribute, @tagName(tag), .{\n                    .r = @truncate(slice[3]),\n                    .g = @truncate(slice[4]),\n                    .b = @truncate(slice[5]),\n                });\n            },\n            else => {\n                self.consumeUnknownColon();\n                return null;\n            },\n        }\n    }\n\n    /// Returns true if the present position has a colon separator.\n    /// This always returns false for the last value since it has no\n    /// separator.\n    fn isColon(self: *Parser) bool {\n        // The `- 1` here is because the last value has no separator.\n        if (self.idx >= self.params.len - 1) return false;\n        return self.params_sep.isSet(self.idx);\n    }\n\n    fn countColon(self: *Parser) usize {\n        var count: usize = 0;\n        var idx = self.idx;\n        while (idx < self.params.len - 1 and self.params_sep.isSet(idx)) : (idx += 1) {\n            count += 1;\n        }\n        return count;\n    }\n\n    /// Consumes all the remaining parameters separated by a colon and\n    /// returns an unknown attribute.\n    fn consumeUnknownColon(self: *Parser) void {\n        const count = self.countColon();\n        self.idx += count + 1;\n    }\n};\n\nfn testParse(params: []const u16) Attribute {\n}\n\nfn testParseColon(params: []const u16) Attribute {\n    var p: Parser = .{ .params = params, .params_sep = SepList.initFull() };\n    return p.next().?;\n}\n\ntest \"sgr: Parser multiple\" {\n    var p: Parser = .{ .params = &[_]u16{ 0, 1, 2, 3 } };\n    try testing.expect(p.next().? == .unset);\n    try testing.expect(p.next().? == .bold);\n    try testing.expect(p.next().? == .faint);\n    try testing.expect(p.next().? == .italic);\n    try testing.expect(p.next() == null);\n}\n\ntest \"sgr: bold\" {\n    {\n        const v = testParse(&[_]u16{1});\n        try testing.expect(v == .bold);\n    }\n    {\n        const v = testParse(&[_]u16{22});\n        try testing.expect(v == .reset_bold);\n    }\n}\n\ntest \"sgr: italic\" {\n    {\n        const v = testParse(&[_]u16{3});\n        try testing.expect(v == .italic);\n    }\n    {\n        const v = testParse(&[_]u16{23});\n        try testing.expect(v == .reset_italic);\n    }\n}\n\ntest \"sgr: underline\" {\n    {\n        const v = testParse(&[_]u16{4});\n        try testing.expect(v == .underline);\n    }\n    {\n        const v = testParse(&[_]u16{24});\n        try testing.expect(v == .reset_underline);\n    }\n}\n\ntest \"sgr: blink\" {\n    {\n        const v = testParse(&[_]u16{5});\n        try testing.expect(v == .blink);\n    }\n    {\n        const v = testParse(&[_]u16{6});\n        try testing.expect(v == .blink);\n    }\n    {\n        const v = testParse(&[_]u16{25});\n        try testing.expect(v == .reset_blink);\n    }\n}\n\ntest \"sgr: inverse\" {\n    {\n        const v = testParse(&[_]u16{7});\n        try testing.expect(v == .inverse);\n    }\n    {\n        const v = testParse(&[_]u16{27});\n        try testing.expect(v == .reset_inverse);\n    }\n}\n\ntest \"sgr: strikethrough\" {\n    {\n        const v = testParse(&[_]u16{9});\n        try testing.expect(v == .strikethrough);\n    }\n    {\n        const v = testParse(&[_]u16{29});\n        try testing.expect(v == .reset_strikethrough);\n    }\n}\n\ntest \"sgr: 8 color\" {\n    var p: Parser = .{ .params = &[_]u16{ 31, 43 } };\n    {\n        const v = p.next().?;\n        try testing.expect(v == .@\"8_fg\");\n        try testing.expect(v.@\"8_fg\" == .red);\n    }\n    {\n        const v = p.next().?;\n        try testing.expect(v == .@\"8_bg\");\n        try testing.expect(v.@\"8_bg\" == .bright_yellow);\n    }\n}\n\ntest \"sgr: 256 color\" {\n    var p: Parser = .{ .params = &[_]u16{ 38, 5, 161, 48, 5, 236 } };\n    try testing.expect(p.next().? == .@\"256_fg\");\n    try testing.expect(p.next().? == .@\"256_bg\");\n    try testing.expect(p.next() == null);\n}\n\ntest \"sgr: direct color fg missing color\" {\n    // This used to crash\n    var p: Parser = .{ .params = &[_]u16{ 38, 5 } };\n    while (p.next()) |_| {}\n}\n\ntest \"sgr: direct color bg missing color\" {\n    // This used to crash\n    var p: Parser = .{ .params = &[_]u16{ 48, 5 } };\n    while (p.next()) |_| {}\n}\n\ntest \"sgr: direct fg/bg/underline ignore optional color space\" {\n    // Colon version should skip the optional color space identifier\n    {\n        // 3 8 : 2 : Pi : Pr : Pg : Pb\n        const v = testParseColon(&[_]u16{ 38, 2, 0, 1, 2, 3 });\n        try testing.expect(v == .direct_color_fg);\n        try testing.expectEqual(@as(u8, 1), v.direct_color_fg.r);\n        try testing.expectEqual(@as(u8, 2), v.direct_color_fg.g);\n        try testing.expectEqual(@as(u8, 3), v.direct_color_fg.b);\n    }\n    {\n        // 4 8 : 2 : Pi : Pr : Pg : Pb\n        const v = testParseColon(&[_]u16{ 48, 2, 0, 1, 2, 3 });\n        try testing.expect(v == .direct_color_bg);\n        try testing.expectEqual(@as(u8, 1), v.direct_color_bg.r);\n        try testing.expectEqual(@as(u8, 2), v.direct_color_bg.g);\n        try testing.expectEqual(@as(u8, 3), v.direct_color_bg.b);\n    }\n    {\n        // 5 8 : 2 : Pi : Pr : Pg : Pb\n        const v = testParseColon(&[_]u16{ 58, 2, 0, 1, 2, 3 });\n        try testing.expect(v == .underline_color);\n        try testing.expectEqual(@as(u8, 1), v.underline_color.r);\n        try testing.expectEqual(@as(u8, 2), v.underline_color.g);\n        try testing.expectEqual(@as(u8, 3), v.underline_color.b);\n    }\n\n    // Semicolon version should not parse optional color space identifier\n    {\n        // 3 8 ; 2 ; Pr ; Pg ; Pb\n        const v = testParse(&[_]u16{ 38, 2, 0, 1, 2, 3 });\n        try testing.expect(v == .direct_color_fg);\n        try testing.expectEqual(@as(u8, 0), v.direct_color_fg.r);\n        try testing.expectEqual(@as(u8, 1), v.direct_color_fg.g);\n        try testing.expectEqual(@as(u8, 2), v.direct_color_fg.b);\n    }\n    {\n        // 4 8 ; 2 ; Pr ; Pg ; Pb\n        const v = testParse(&[_]u16{ 48, 2, 0, 1, 2, 3 });\n        try testing.expect(v == .direct_color_bg);\n        try testing.expectEqual(@as(u8, 0), v.direct_color_bg.r);\n        try testing.expectEqual(@as(u8, 1), v.direct_color_bg.g);\n        try testing.expectEqual(@as(u8, 2), v.direct_color_bg.b);\n    }\n    {\n        // 5 8 ; 2 ; Pr ; Pg ; Pb\n        const v = testParse(&[_]u16{ 58, 2, 0, 1, 2, 3 });\n        try testing.expect(v == .underline_color);\n        try testing.expectEqual(@as(u8, 0), v.underline_color.r);\n        try testing.expectEqual(@as(u8, 1), v.underline_color.g);\n        try testing.expectEqual(@as(u8, 2), v.underline_color.b);\n    }\n}\n\ntest \"sgr: direct fg colon with too many colons\" {\n    var p: Parser = .{\n        .params = &[_]u16{ 38, 2, 0, 1, 2, 3, 4, 1 },\n        .params_sep = sep: {\n            var list = SepList.initEmpty();\n            for (0..6) |idx| list.set(idx);\n            break :sep list;\n        },\n    };\n\n    try testing.expect(p.next().? == .unknown);\n    try testing.expect(p.next().? == .bold);\n    try testing.expect(p.next() == null);\n}\n\ntest \"sgr: direct fg colon with colorspace and extra param\" {\n    var p: Parser = .{\n        .params = &[_]u16{ 38, 2, 0, 1, 2, 3, 1 },\n        .params_sep = sep: {\n            var list = SepList.initEmpty();\n            for (0..5) |idx| list.set(idx);\n            break :sep list;\n        },\n    };\n\n    {\n        const v = p.next().?;\n        try testing.expect(v == .direct_color_fg);\n        try testing.expectEqual(@as(u8, 1), v.direct_color_fg.r);\n        try testing.expectEqual(@as(u8, 2), v.direct_color_fg.g);\n        try testing.expectEqual(@as(u8, 3), v.direct_color_fg.b);\n    }\n\n    try testing.expect(p.next().? == .bold);\n    try testing.expect(p.next() == null);\n}\n\ntest \"sgr: direct fg colon no colorspace and extra param\" {\n    var p: Parser = .{\n        .params = &[_]u16{ 38, 2, 1, 2, 3, 1 },\n        .params_sep = sep: {\n            var list = SepList.initEmpty();\n            for (0..4) |idx| list.set(idx);\n            break :sep list;\n        },\n    };\n\n    {\n        const v = p.next().?;\n        try testing.expect(v == .direct_color_fg);\n        try testing.expectEqual(@as(u8, 1), v.direct_color_fg.r);\n        try testing.expectEqual(@as(u8, 2), v.direct_color_fg.g);\n        try testing.expectEqual(@as(u8, 3), v.direct_color_fg.b);\n    }\n\n    try testing.expect(p.next().? == .bold);\n    try testing.expect(p.next() == null);\n}\n\n// Kakoune sent this complex SGR sequence that caused invalid behavior.\ntest \"sgr: kakoune input\" {\n    // This used to crash\n    var p: Parser = .{\n        .params = &[_]u16{ 0, 4, 3, 38, 2, 175, 175, 215, 58, 2, 0, 190, 80, 70 },\n        .params_sep = sep: {\n            var list = SepList.initEmpty();\n            list.set(0);\n            break :sep list;\n        },\n    };\n\n    {\n        const v = p.next().?;\n        try testing.expect(v == .unset);\n    }\n    {\n        const v = p.next().?;\n        try testing.expect(v == .underline);\n        try testing.expectEqual(Attribute.Underline.curly, v.underline);\n    }\n    {\n        const v = p.next().?;\n        try testing.expect(v == .direct_color_fg);\n        try testing.expectEqual(@as(u8, 175), v.direct_color_fg.r);\n        try testing.expectEqual(@as(u8, 175), v.direct_color_fg.g);\n        try testing.expectEqual(@as(u8, 215), v.direct_color_fg.b);\n    }\n    {\n        const v = p.next().?;\n        try testing.expect(v == .underline_color);\n        try testing.expectEqual(@as(u8, 190), v.underline_color.r);\n        try testing.expectEqual(@as(u8, 80), v.underline_color.g);\n        try testing.expectEqual(@as(u8, 70), v.underline_color.b);\n    }\n\n    //try testing.expect(p.next() == null);\n}\n\n// Discussion #5930, another input sent by kakoune\ntest \"sgr: kakoune input issue underline, fg, and bg\" {\n    // echo -e \"\\033[4:3;38;2;51;51;51;48;2;170;170;170;58;2;255;97;136mset everything in one sequence, broken\\033[m\"\n\n    // This used to crash\n    var p: Parser = .{\n        .params = &[_]u16{ 4, 3, 38, 2, 51, 51, 51, 48, 2, 170, 170, 170, 58, 2, 255, 97, 136 },\n        .params_sep = sep: {\n            var list = SepList.initEmpty();\n            list.set(0);\n            break :sep list;\n        },\n    };\n\n    {\n        const v = p.next().?;\n        try testing.expect(v == .underline);\n        try testing.expectEqual(Attribute.Underline.curly, v.underline);\n    }\n\n    {\n        const v = p.next().?;\n        try testing.expect(v == .direct_color_fg);\n        try testing.expectEqual(@as(u8, 51), v.direct_color_fg.r);\n        try testing.expectEqual(@as(u8, 51), v.direct_color_fg.g);\n        try testing.expectEqual(@as(u8, 51), v.direct_color_fg.b);\n    }\n\n    {\n        const v = p.next().?;\n        try testing.expect(v == .direct_color_bg);\n        try testing.expectEqual(@as(u8, 170), v.direct_color_bg.r);\n        try testing.expectEqual(@as(u8, 170), v.direct_color_bg.g);\n        try testing.expectEqual(@as(u8, 170), v.direct_color_bg.b);\n    }\n\n    {\n        const v = p.next().?;\n        try testing.expect(v == .underline_color);\n        try testing.expectEqual(@as(u8, 255), v.underline_color.r);\n        try testing.expectEqual(@as(u8, 97), v.underline_color.g);\n        try testing.expectEqual(@as(u8, 136), v.underline_color.b);\n    }\n\n    try testing.expect(p.next() == null);\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.7491138079152592,
      "diff_lines": 497,
      "added_lines": 6,
      "removed_lines": 268,
      "diff_preview": "--- expected+++ predicted@@ -31,7 +31,6 @@     reset_italic,\n \n     /// Faint/dim text.\n-    /// Note: reset faint is the same SGR code as reset bold\n     faint,\n \n     /// Underline the text\n@@ -126,7 +125,6 @@         // parsing a value that allows it.\n         if (colon) switch (slice[0]) {\n             4, 38, 48, 58 => {},\n-\n             else => {\n                 // Consume all the colon separated values.\n                 const start = self.idx;\n@@ -141,13 +139,9 @@ \n         switch (slice[0]) {\n             0 => return .unset,\n-\n             1 => return .bold,\n-\n             2 => return .faint,\n-\n             3 => return .italic,\n-\n             4 => underline: {\n                 if (colon) {\n                     assert(slice.len >= 2);\n@@ -173,37 +167,22 @@ \n                 return .{ .underline = .single };\n             },\n-\n             5 => return .blink,\n-\n             6 => return .blink,\n-\n             7 => return .inverse,\n-\n             8 => return .invisible,\n-\n             9 => return .strikethrough,\n-\n             21 => return .{ .underline = .double },\n-\n             22 => return .reset_bold,\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 135.17817401885986,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 30022,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_termio_Termio.zig",
    "exact_match": false,
    "similarity": 0.498735701384708,
    "diff_lines": 566,
    "predicted_output": "//! Primary terminal IO (\"termio\") state. This maintains the terminal state,\n//! pty, subprocess, etc. This is flexible enough to be used in environments\n//! that don't have a pty and simply provides the input/output using raw\n//! bytes.\npub const Termio = @This();\n\nconst std = @import(\"std\");\nconst builtin = @import(\"builtin\");\nconst build_config = @import(\"../build_config.zig\");\nconst assert = std.debug.assert;\nconst Allocator = std.mem.Allocator;\nconst ArenaAllocator = std.heap.ArenaAllocator;\nconst EnvMap = std.process.EnvMap;\nconst posix = std.posix;\nconst terminalpkg = @import(\"../terminal/main.zig\");\nconst terminfo = @import(\"../terminfo/main.zig\");\nconst xev = @import(\"xev\");\nconst renderer = @import(\"../renderer.zig\");\nconst apprt = @import(\"../apprt.zig\");\nconst fastmem = @import(\"../fastmem.zig\");\nconst internal_os = @import(\"../os/main.zig\");\nconst windows = internal_os.windows;\nconst configpkg = @import(\"../config.zig\");\nconst shell_integration = @import(\"shell_integration.zig\");\n\nconst log = std.log.scoped(.io_exec);\n\nconst c = @cImport({\n    @cInclude(\"errno.h\");\n    @cInclude(\"signal.h\");\n    @cInclude(\"unistd.h\");\n});\n\n/// True if we should disable the kitty keyboard protocol. We have to\n/// disable this on GLFW because GLFW input events don't support the\n/// correct granularity of events.\nconst disable_kitty_keyboard_protocol = apprt.runtime == apprt.glfw;\n\n/// Allocator\nalloc: Allocator,\n\n/// This is the implementation responsible for io.\nreader: termio.Reader,\n\n/// The derived configuration for this termio implementation.\nconfig: DerivedConfig,\n\n/// The terminal emulator internal state. This is the abstract \"terminal\"\n/// that manages input, grid updating, etc. and is renderer-agnostic. It\n/// just stores internal state about a grid.\nterminal: terminalpkg.Terminal,\n\n/// The shared render state\nrenderer_state: *renderer.State,\n\n/// The mailbox for communicating with the surface.\nsurface_mailbox: apprt.surface.Mailbox,\n\n/// The cached grid size whenever a resize is called.\ngrid_size: renderer.GridSize,\n\n/// The size of a single cell. Used for size reports.\ncell_size: renderer.CellSize,\n\n/// The mailbox implementation to use.\nmailbox: termio.Mailbox,\n\n/// The stream parser. This parses the stream of escape codes and so on\n/// from the child process and calls callbacks in the stream handler.\nterminal_stream: terminalpkg.Stream(StreamHandler),\n\n/// Last time the cursor was reset. This is used to prevent message\n/// flooding with cursor resets.\nlast_cursor_reset: ?std.time.Instant = null,\n\npub const DerivedConfig = struct {\n    arena: ArenaAllocator,\n\n    palette: terminalpkg.color.Palette,\n    image_storage_limit: usize,\n    cursor_style: terminalpkg.CursorStyle,\n    cursor_blink: ?bool,\n    cursor_color: ?configpkg.Config.Color,\n    cursor_invert: bool,\n    foreground: configpkg.Config.Color,\n    background: configpkg.Config.Color,\n    osc_color_report_format: configpkg.Config.OSCColorReportFormat,\n    abnormal_runtime_threshold_ms: u32,\n    wait_after_command: bool,\n    enquiry_response: []const u8,\n\n    pub fn init(\n        alloc_gpa: Allocator,\n        config: *const configpkg.Config,\n    ) !DerivedConfig {\n        var arena = ArenaAllocator.init(alloc_gpa);\n        errdefer arena.deinit();\n        const alloc = arena.allocator();\n\n        return .{\n            .palette = config.palette.value,\n            .image_storage_limit = config.@\"image-storage-limit\",\n            .cursor_style = config.@\"cursor-style\",\n            .cursor_blink = config.@\"cursor-style-blink\",\n            .cursor_color = config.@\"cursor-color\",\n            .foreground = config.foreground,\n            .background = config.background,\n            .osc_color_report_format = config.@\"osc-color-report-format\",\n            .abnormal_runtime_threshold_ms = config.@\"abnormal-command-exit-runtime\",\n            .wait_after_command = config.@\"wait-after-command\",\n            .enquiry_response = try alloc.dupe(u8, config.@\"enquiry-response\"),\n\n            // This has to be last so that we copy AFTER the arena allocations\n            // above happen (Zig assigns in order).\n            .arena = arena,\n        };\n    }\n\n    pub fn deinit(self: *DerivedConfig) void {\n        self.arena.deinit();\n    }\n};\n\n/// Initialize the termio state.\n///\n/// This will also start the child process if the termio is configured\n/// to run a child process.\npub fn init(self: *Termio, alloc: Allocator, opts: termio.Options) !void {\n    // Create our terminal\n    var term = try terminalpkg.Terminal.init(alloc, opts: {\n        const grid_size = opts.size.grid();\n        break :opts .{\n            .cols = grid_size.columns,\n            .rows = grid_size.rows,\n            .max_scrollback = opts.full_config.@\"scrollback-limit\",\n        };\n    });\n    errdefer term.deinit(alloc);\n    term.default_palette = opts.config.palette;\n    term.color_palette.colors = opts.config.palette;\n\n    // Setup our terminal size in pixels for certain requests.\n    term.width_px = opts.grid_size.columns * opts.cell_size.width;\n    term.height_px = opts.grid_size.rows * opts.cell_size.height;\n\n    // Setup our backend.\n    var backend = opts.backend;\n    backend.initTerminal(&term);\n\n    // Create our stream handler. This points to memory in self so it\n    // isn't safe to use until self.* is set.\n    const handler: StreamHandler = handler: {\n        const default_cursor_color = if (!opts.config.cursor_invert and opts.config.cursor_color != null)\n            opts.config.cursor_color.?.toTerminalRGB()\n        else\n            null;\n\n        break :handler .{\n            .alloc = alloc,\n            .termio_mailbox = &self.mailbox,\n            .surface_mailbox = opts.surface_mailbox,\n            .renderer_state = opts.renderer_state,\n            .renderer_wakeup = opts.renderer_wakeup,\n            .renderer_mailbox = opts.renderer_mailbox,\n            .grid_size = &self.grid_size,\n            .terminal = &self.terminal,\n            .osc_color_report_format = opts.config.osc_color_report_format,\n            .enquiry_response = opts.config.enquiry_response,\n            .default_cursor_style = opts.config.cursor_style,\n            .default_cursor_blink = opts.config.cursor_blink,\n            .default_cursor_color = default_cursor_color,\n            .cursor_color = default_cursor_color,\n            .foreground_color = null,\n            .background_color = null,\n        };\n    };\n\n    self.* = .{\n        .alloc = alloc,\n        .terminal = term,\n        .config = opts.config,\n        .renderer_state = opts.renderer_state,\n        .renderer_wakeup = opts.renderer_wakeup,\n        .renderer_mailbox = opts.renderer_mailbox,\n        .surface_mailbox = opts.surface_mailbox,\n        .grid_size = opts.grid_size,\n        .cell_size = opts.cell_size,\n        .backend = backend,\n        .mailbox = opts.mailbox,\n        .terminal_stream = .{\n            .handler = handler,\n            .parser = .{\n                .osc_parser = .{\n                    // Populate the OSC parser allocator (optional) because\n                    // we want to support large OSC payloads such as OSC 52.\n                    .alloc = alloc,\n                },\n            },\n        },\n    };\n}\n\npub fn deinit(self: *Termio) void {\n    self.backend.deinit();\n    self.terminal.deinit(self.alloc);\n    self.config.deinit();\n    self.mailbox.deinit(self.alloc);\n\n    // Clear any StreamHandler state\n    self.terminal_stream.handler.deinit();\n    self.terminal_stream.deinit();\n}\n\npub fn threadEnter(self: *Termio, thread: *termio.Thread, data: *ThreadData) !void {\n    const alloc = self.alloc;\n\n    // Setup our thread data\n    data.* = .{\n        .alloc = self.alloc,\n        .loop = &thread.loop,\n        .renderer_state = self.renderer_state,\n        .surface_mailbox = self.surface_mailbox,\n        .mailbox = &self.mailbox,\n        .backend = &self.backend,\n    };\n\n    // Setup our backend\n    try self.backend.threadEnter(self.alloc, self, data);\n}\n\npub fn threadExit(self: *Termio, data: *ThreadData) void {\n    self.backend.threadExit(data);\n}\n\npub inline fn queueWrite(\n    self: *Termio,\n    td: *ThreadData,\n    data: []const u8,\n    linefeed: bool,\n) !void {\n    try self.backend.queueWrite(self.alloc, td, data, linefeed);\n}\n\npub fn resize(\n    self: *Termio,\n    td: *ThreadData,\n    size: renderer.Size,\n) !void {\n    const grid_size = size.grid();\n\n    // Update the size of our pty.\n    try self.backend.resize(grid_size, size.terminal());\n\n    // Update the cached size\n    self.size = size;\n\n    // Enter the critical area that we want to keep small\n    {\n        self.renderer_state.mutex.lock();\n        defer self.renderer_state.mutex.unlock();\n\n        // Update our pixel sizes\n        self.terminal.width_px = grid_size.columns * self.size.cell.width;\n        self.terminal.height_px = grid_size.rows * self.size.cell.height;\n\n        // Disable synchronized output mode so that we show changes\n        // immediately for a resize. This is allowed by the spec.\n        self.terminal.modes.set(.synchronized_output, false);\n    }\n\n    // Mail the renderer so that it can update the GPU and re-render\n    _ = self.renderer_mailbox.push(.{ .resize = size }, .{ .forever = {} });\n    self.renderer_wakeup.notify() catch {};\n}\n\npub fn childExitedAbnormally(self: *Termio, exit_code: u32, runtime_ms: u64) !void {\n    self.renderer_state.mutex.lock();\n    defer self.renderer_state.mutex.unlock();\n    const t = self.renderer_state.terminal;\n    try self.backend.childExitedAbnormally(self.alloc, t, exit_code, runtime_ms);\n}\n\npub fn focusGained(self: *Termio, td: *ThreadData, focused: bool) !void {\n    self.renderer_state.mutex.lock();\n    const focus_event = self.renderer_state.terminal.modes.get(.focus_event);\n    self.renderer_state.mutex.unlock();\n\n    // If we have focus events enabled, we send the focus event.\n    if (focus_event) {\n        const seq = if (focused) \"\\x1b[I\" else \"\\x1b[O\";\n        try self.queueWrite(td, seq, false);\n    }\n\n    // We always notify our backend of focus changes.\n    try self.backend.focusGained(td, focused);\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.498735701384708,
      "diff_lines": 566,
      "added_lines": 32,
      "removed_lines": 380,
      "diff_preview": "--- expected+++ predicted@@ -12,13 +12,9 @@ const ArenaAllocator = std.heap.ArenaAllocator;\n const EnvMap = std.process.EnvMap;\n const posix = std.posix;\n-const termio = @import(\"../termio.zig\");\n-const Command = @import(\"../Command.zig\");\n-const Pty = @import(\"../pty.zig\").Pty;\n-const StreamHandler = @import(\"stream_handler.zig\").StreamHandler;\n const terminalpkg = @import(\"../terminal/main.zig\");\n const terminfo = @import(\"../terminfo/main.zig\");\n-const xev = @import(\"../global.zig\").xev;\n+const xev = @import(\"xev\");\n const renderer = @import(\"../renderer.zig\");\n const apprt = @import(\"../apprt.zig\");\n const fastmem = @import(\"../fastmem.zig\");\n@@ -29,11 +25,22 @@ \n const log = std.log.scoped(.io_exec);\n \n+const c = @cImport({\n+    @cInclude(\"errno.h\");\n+    @cInclude(\"signal.h\");\n+    @cInclude(\"unistd.h\");\n+});\n+\n+/// True if we should disable the kitty keyboard protocol. We have to\n+/// disable this on GLFW because GLFW input events don't support the\n+/// correct granularity of events.\n+const disable_kitty_keyboard_protocol = apprt.runtime == apprt.glfw;\n+\n /// Allocator\n alloc: Allocator,\n \n /// This is the implementation responsible for io.\n-backend: termio.Backend,\n+reader: termio.Reader,\n \n /// The derived configuration for this termio implementation.\n config: DerivedConfig,\n@@ -46,18 +53,14 @@ /// The shared render state\n renderer_state: *renderer.State,\n \n-/// A handle to wake up the renderer. This hints to the renderer that that\n-/// a repaint should happen.\n-renderer_wakeup: xev.Async,\n-\n-/// The mailbox for notifying the renderer of things.\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 54.962990045547485,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 83028,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_termio_Thread.zig",
    "exact_match": false,
    "similarity": 0.937421191050906,
    "diff_lines": 130,
    "predicted_output": "//! Represents the \"writer\" thread for terminal IO. The reader side is\n//! handled by the Termio struct itself and dependent on the underlying\n//! implementation (i.e. if its a pty, manual, etc.).\n//!\n//! The writer thread does handle writing bytes to the pty but also handles\n//! different events such as starting synchronized output, changing some\n//! modes (like linefeed), etc. The goal is to offload as much from the\n//! reader thread as possible since it is the hot path in parsing VT\n//! sequences and updating terminal state.\n//!\n//! This thread state can only be used by one thread at a time.\npub const Thread = @This();\n\nconst std = @import(\"std\");\nconst ArenaAllocator = std.heap.ArenaAllocator;\nconst builtin = @import(\"builtin\");\nconst xev = @import(\"../global.zig\").xev;\nconst crash = @import(\"../crash/main.zig\");\nconst termio = @import(\"../termio.zig\");\nconst renderer = @import(\"../renderer.zig\");\nconst BlockingQueue = @import(\"../datastruct/main.zig\").BlockingQueue;\n\nconst Allocator = std.mem.Allocator;\nconst log = std.log.scoped(.io_thread);\n\n/// This stores the information that is coalesced.\nconst Coalesce = struct {\n    /// The number of milliseconds to coalesce certain messages like resize for.\n    /// Not all message types are coalesced.\n    const min_ms = 25;\n\n    resize: ?renderer.Size = null,\n};\n\n/// The number of milliseconds before we reset the synchronized output flag\n/// if the running program hasn't already.\nconst sync_reset_ms = 1000;\n\n/// Allocator used for some state\nalloc: std.mem.Allocator,\n\n/// The main event loop for the thread. The user data of this loop\n/// is always the allocator used to create the loop. This is a convenience\n/// so that users of the loop always have an allocator.\nloop: xev.Loop,\n\n/// The completion to use for the wakeup async handle that is present\n/// on the termio.Writer.\nwakeup_c: xev.Completion = .{},\n\n/// This can be used to stop the thread on the next loop iteration.\nstop: xev.Async,\nstop_c: xev.Completion = .{},\n\n/// This timer is used to reset synchronized output modes so that\n/// the terminal doesn't freeze with a bad actor.\nsync_reset: xev.Timer,\nsync_reset_c: xev.Completion = .{},\nsync_reset_cancel_c: xev.Completion = .{},\n\nflags: packed struct {\n    /// This is set to true only when an abnormal exit is detected. It\n    /// tells our mailbox system to drain and ignore all messages.\n    drain: bool = false,\n\n    /// True if linefeed mode is enabled. This is duplicated here so that the\n    /// write thread doesn't need to grab a lock to check this on every write.\n    linefeed_mode: bool = false,\n};\n\n/// Initialize the thread. This does not START the thread. This only sets\n/// up all the internal state necessary prior to starting the thread. It\n/// is up to the caller to start the thread with the threadMain entrypoint.\npub fn init(\n    alloc: Allocator,\n) !Thread {\n    // Create our event loop.\n    var loop = try xev.Loop.init(.{});\n    errdefer loop.deinit();\n\n    // This async handle is used to stop the loop and force the thread to end.\n    var stop_h = try xev.Async.init();\n    errdefer stop_h.deinit();\n\n    // This timer is used to reset synchronized output modes.\n    var sync_reset_h = try xev.Timer.init();\n    errdefer sync_reset_h.deinit();\n\n    return Thread{\n        .alloc = alloc,\n        .loop = loop,\n        .stop = stop_h,\n        .coalesce = Coalesce{},\n        .sync_reset = sync_reset_h,\n    };\n}\n\n/// Clean up the thread. This is only safe to call once the thread\n/// completes executing; the caller must join prior to this.\npub fn deinit(self: *Thread) void {\n    self.coalesce.deinit();\n    self.sync_reset.deinit();\n    self.stop.deinit();\n    self.loop.deinit();\n}\n\n/// The main entrypoint for the thread.\npub fn threadMain(self: *Thread, io: *termio.Termio) void {\n    // Call child function so we can use errors...\n    self.threadMain_(io) catch |err| {\n        log.warn(\"error in io thread err={}\", .{err});\n\n        // Use an arena to simplify memory management below\n        var arena = ArenaAllocator.init(self.alloc);\n        defer arena.deinit();\n        const alloc = arena.allocator();\n\n        // If there is an error, we replace our terminal screen with\n        // the error message. It might be better in the future to send\n        // the error to the surface thread and let the apprt deal with it\n        // in some way but this works for now. Without this, the user would\n        // just see a blank terminal window.\n        io.renderer_state.mutex.lock();\n        defer io.renderer_state.mutex.unlock();\n        const t = io.renderer_state.terminal;\n\n        // Hide the cursor\n        t.modes.set(.cursor_visible, false);\n\n        // This is weird but just ensures that no matter what our underlying\n        // implementation we have the errors below. For example, Windows doesn't\n        // have \"OpenptyFailed\".\n        const Err = @TypeOf(err) || error{\n            OpenptyFailed,\n        };\n\n        switch (@as(Err, @errorCast(err))) {\n            error.OpenptyFailed => {\n                const str =\n                    \\\\Your system cannot allocate any more pty devices.\n                    \\\\\n                    \\\\Ghostty requires a pty device to launch a new terminal.\n                    \\\\This error is usually due to having too many terminal\n                    \\\\windows open or having another program that is using too\n                    \\\\many pty devices.\n                    \\\\\n                    \\\\Please free up some pty devices and try again.\n                ;\n                t.eraseDisplay(.complete, false);\n                t.printString(str) catch {};\n            },\n            else => {\n                const str = std.fmt.allocPrint(\n                    alloc,\n                    \\\\error starting IO thread: {}\n                    \\\\\n                    \\\\The underlying shell or command was unable to be started.\n                    \\\\This error is usually due to exhausting a system resource.\n                    \\\\If this looks like a bug, please report it.\n                    \\\\\n                    \\\\This terminal is non-functional. Please close it and try again.\n                ,\n                    .{err},\n                ) catch\n                    \\\\Out of memory. This terminal is non-functional. Please close it and try again.\n                ;\n                t.eraseDisplay(.complete, false);\n                t.printString(str) catch {};\n            },\n        }\n    };\n\n    // If our loop is not stopped, then we need to keep running so that\n    // messages are drained and we can wait for the surface to send a stop\n    // message.\n    if (!self.loop.stopped()) {\n        log.warn(\"abrupt io thread exit detected, starting xev to drain mailbox\", .{});\n        defer log.debug(\"io thread fully exiting after abnormal failure\", .{});\n        self.flags.drain = true;\n        self.loop.run(.until_done) catch |err| {\n            log.err(\"failed to start xev loop for draining err={}\", .{err});\n        };\n    }\n}\n\nfn threadMain_(self: *Thread, io: *termio.Termio) !void {\n    defer log.debug(\"IO thread exited\", .{});\n\n    // Setup our crash metadata\n    crash.sentry.thread_state = .{\n        .type = .io,\n        .surface = io.surface_mailbox.surface,\n    };\n    defer crash.sentry.thread_state = null;\n\n    // Get the mailbox. This must be an SPSC mailbox for threading.\n    const mailbox = switch (io.mailbox) {\n        .spsc => |*v| v,\n        // else => return error.TermioUnsupportedMailbox,\n    };\n\n    // This is the data sent to xev callbacks. We want a pointer to both\n    // ourselves and the thread data so we can thread that through (pun intended).\n    var cb: CallbackData = .{ .self = self, .io = io };\n\n    // Run our thread start/end callbacks. This allows the implementation\n    // to hook into the event loop as needed. The thread data is created\n    // on the stack here so that it has a stable pointer throughout the\n    // lifetime of the thread.\n    try io.threadEnter(self, &cb.data);\n    defer cb.data.deinit();\n    defer io.threadExit(&cb.data);\n\n    // Start the async handlers.\n    mailbox.wakeup.wait(&self.loop, &self.wakeup_c, CallbackData, &cb, wakeupCallback);\n    self.stop.wait(&self.loop, &self.stop_c, CallbackData, &cb, stopCallback);\n\n    // Run\n    log.debug(\"starting IO thread\", .{});\n    try self.loop.run(.until_done);\n}\n\n/// This is the data passed to xev callbacks on the thread.\nconst CallbackData = struct {\n    self: *Thread,\n    io: *termio.Termio,\n    data: termio.Termio.ThreadData = undefined,\n};\n\n/// Drain the mailbox, handling all the messages in our terminal implementation.\nfn drainMailbox(\n    self: *Thread,\n    cb: *CallbackData,\n) !void {\n    // We assert when starting the thread that this is the state\n    const mailbox = cb.io.mailbox.spsc.queue;\n    const io = cb.io;\n    const data = &cb.data;\n\n    // If we're draining, we just drain the mailbox and return.\n    if (self.flags.drain) {\n        while (mailbox.pop()) |_| {}\n        return;\n    }\n\n    // This holds the mailbox lock for the duration of the drain. The\n    // expectation is that all our message handlers will be non-blocking\n    // ENOUGH to not mess up throughput on producers.\n    var redraw: bool = false;\n    while (mailbox.pop()) |message| {\n        // If we have a message we always redraw\n        redraw = true;\n\n        log.debug(\"mailbox message={}\", .{message});\n        switch (message) {\n            .change_config => |config| {\n                defer config.alloc.destroy(config.ptr);\n                try io.changeConfig(data, config.ptr);\n            },\n            .inspector => |v| self.flags.has_inspector = v,\n            .resize => |v| self.handleResize(cb, v),\n            .clear_screen => |v| try io.clearScreen(data, v.history),\n            .scroll_viewport => |v| try io.scrollViewport(v),\n            .jump_to_prompt => |v| try io.jumpToPrompt(v),\n            .start_synchronized_output => self.startSynchronizedOutput(cb),\n            .linefeed_mode => |v| self.flags.linefeed_mode = v,\n            .child_exited_abnormally => |v| try io.childExitedAbnormally(v.exit_code, v.runtime_ms),\n            .write_small => |v| try io.queueWrite(\n                data,\n                v.data[0..v.len],\n                self.flags.linefeed_mode,\n            ),\n            .write_stable => |v| try io.queueWrite(\n                data,\n                v,\n                self.flags.linefeed_mode,\n            ),\n            .write_alloc => |v| {\n                defer v.alloc.free(v.data);\n                try io.queueWrite(\n                    data,\n                    v.data,\n                    self.flags.linefeed_mode,\n                );\n            },\n        }\n    }\n\n    // Trigger a redraw after we've drained so we don't waste cycles\n    // messaging a redraw.\n    if (redraw) {\n        try io.renderer_wakeup.notify();\n    }\n}\n\nfn startSynchronizedOutput(self: *Thread, cb: *CallbackData) void {\n    self.sync_reset.reset(\n        &self.loop,\n        &self.sync_reset_c,\n        &self.sync_reset_cancel_c,\n        sync_reset_ms,\n        CallbackData,\n        cb,\n        syncResetCallback,\n    );\n}\n\nfn handleResize(self: *Thread, cb: *CallbackData, resize: renderer.Size) void {\n    self.coalesce_data.resize = resize;\n\n    // If the timer is already active we just return. In the future we want\n    // to reset the timer up to a maximum wait time but for now this ensures\n    // relatively smooth resizing.\n    if (self.coalesce_c.state() == .active) return;\n\n    self.coalesce.reset(\n        &self.loop,\n        &self.coalesce_c,\n        &self.coalesce_cancel_c,\n        Coalesce.min_ms,\n        CallbackData,\n        cb,\n        coalesceCallback,\n    );\n}\n\nfn syncResetCallback(\n    cb_: ?*CallbackData,\n    _: *xev.Loop,\n    _: *xev.Completion,\n    r: xev.Timer.RunError!void,\n) xev.CallbackAction {\n    _ = r catch |err| switch (err) {\n        error.Canceled => {},\n        else => {\n            log.warn(\"error during sync reset callback err={}\", .{err});\n            return .disarm;\n        },\n    };\n\n    const cb = cb_ orelse return .disarm;\n    cb.io.resetSynchronizedOutput();\n    return .disarm;\n}\n\nfn coalesceCallback(\n    cb_: ?*CallbackData,\n    _: *xev.Loop,\n    _: *xev.Completion,\n    r: xev.Timer.RunError!void,\n) xev.CallbackAction {\n    _ = r catch |err| switch (err) {\n        error.Canceled => {},\n        else => {\n            log.warn(\"error during coalesce callback err={}\", .{err});\n            return .disarm;\n        },\n    };\n\n    const cb = cb_ orelse return .disarm;\n\n    if (cb.self.coalesce_data.resize) |v| {\n        cb.self.coalesce_data.resize = null;\n        cb.io.resize(\n            &cb.data,\n            v,\n        ) catch |err| {\n            log.warn(\"error during resize err={}\", .{err});\n        };\n    }\n\n    return .disarm;\n}\n\nfn wakeupCallback(\n    cb_: ?*CallbackData,\n    _: *xev.Loop,\n    _: *xev.Completion,\n    r: xev.Async.WaitError!void,\n) xev.CallbackAction {\n    _ = r catch |err| {\n        log.err(\"error in wakeup err={}\", .{err});\n        return .rearm;\n    };\n\n    const cb = cb_ orelse return .rearm;\n\n    // When we wake up, we check the mailbox. Mailbox producers should\n    // wake up our thread after publishing.\n    cb.self.drainMailbox(cb) catch |err|\n        log.err(\"error draining mailbox err={}\", .{err});\n\n    return .rearm;\n}\n\nfn stopCallback(\n    cb_: ?*CallbackData,\n    _: *xev.Loop,\n    _: *xev.Completion,\n    r: xev.Async.WaitError!void,\n) xev.CallbackAction {\n    _ = r catch unreachable;\n    cb_.?.self.loop.stop();\n    return .disarm;\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.937421191050906,
      "diff_lines": 130,
      "added_lines": 9,
      "removed_lines": 25,
      "diff_preview": "--- expected+++ predicted@@ -52,12 +52,6 @@ stop: xev.Async,\n stop_c: xev.Completion = .{},\n \n-/// This is used to coalesce resize events.\n-coalesce: xev.Timer,\n-coalesce_c: xev.Completion = .{},\n-coalesce_cancel_c: xev.Completion = .{},\n-coalesce_data: Coalesce = .{},\n-\n /// This timer is used to reset synchronized output modes so that\n /// the terminal doesn't freeze with a bad actor.\n sync_reset: xev.Timer,\n@@ -72,10 +66,7 @@     /// True if linefeed mode is enabled. This is duplicated here so that the\n     /// write thread doesn't need to grab a lock to check this on every write.\n     linefeed_mode: bool = false,\n-\n-    /// This is true when the inspector is active.\n-    has_inspector: bool = false,\n-} = .{},\n+};\n \n /// Initialize the thread. This does not START the thread. This only sets\n /// up all the internal state necessary prior to starting the thread. It\n@@ -91,10 +82,6 @@     var stop_h = try xev.Async.init();\n     errdefer stop_h.deinit();\n \n-    // This timer is used to coalesce resize events.\n-    var coalesce_h = try xev.Timer.init();\n-    errdefer coalesce_h.deinit();\n-\n     // This timer is used to reset synchronized output modes.\n     var sync_reset_h = try xev.Timer.init();\n     errdefer sync_reset_h.deinit();\n@@ -103,7 +90,7 @@         .alloc = alloc,\n         .loop = loop,\n         .stop = stop_h,\n-        .coalesce = coalesce_h,\n+        .coalesce = Coalesce{},\n         .sync_reset = sync_reset_h,\n     };\n }\n@@ -159,11 +146,9 @@                     \\\\\n                     \\\\Please free up some pty devices and try again.\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 82.06788301467896,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 27810,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "ghostty_src_termio_shell_integration.zig",
    "exact_match": false,
    "similarity": 0.4378173404460441,
    "diff_lines": 461,
    "predicted_output": "const std = @import(\"std\");\nconst Allocator = std.mem.Allocator;\nconst ArenaAllocator = std.heap.ArenaAllocator;\nconst EnvMap = std.process.EnvMap;\nconst config = @import(\"../config.zig\");\nconst homedir = @import(\"../os/homedir.zig\");\nconst internal_os = @import(\"../os/main.zig\");\n\nconst log = std.log.scoped(.shell_integration);\n\npub const Shell = enum {\n    bash,\n    elvish,\n    fish,\n    zsh,\n};\n\n/// The result of setting up a shell integration.\npub const ShellIntegration = struct {\n    /// The successfully-integrated shell.\n    shell: Shell,\n\n    /// The command to use to start the shell with the integration.\n    /// In most cases this is identical to the command given but for\n    /// bash in particular it may be different.\n    ///\n    /// The memory is allocated in the arena given to setup.\n    command: config.Command,\n};\n\n/// Set up the command execution environment for automatic\n/// integrated shell integration and return a ShellIntegration\n/// struct describing the integration.  If integration fails\n/// (shell type couldn't be detected, etc.), this will return null.\npub fn setup(\n    alloc_arena: Allocator,\n    resource_dir: []const u8,\n    command: config.Command,\n    env: *EnvMap,\n    force_shell: ?Shell,\n    features: config.ShellIntegrationFeatures,\n) !?ShellIntegration {\n    const exe = switch (command) {\n        .direct => |v| std.fs.path.basename(v[0]),\n        .shell => |v| exe: {\n            const idx = std.mem.indexOfScalar(u8, v, ' ') orelse v.len;\n            break :exe std.fs.path.basename(v[0..idx]);\n        },\n    };\n\n    const result = try setupShell(alloc_arena, resource_dir, command, env, exe);\n\n    // Setup our feature env vars\n    try setupFeatures(env, features);\n\n    return result;\n}\n\nfn setupShell(\n    alloc_arena: Allocator,\n    resource_dir: []const u8,\n    command: config.Command,\n    env: *EnvMap,\n    exe: []const u8,\n) !?ShellIntegration {\n    if (std.mem.eql(u8, \"bash\", exe)) {\n        if (comptime builtin.target.os.tag.isDarwin()) {\n            if (std.mem.eql(u8, \"/bin/bash\", switch (command) {\n                .direct => |v| v[0],\n                .shell => |v| v,\n            })) {\n                return null;\n            }\n        }\n\n        const new_command = try setupBash(\n            alloc_arena,\n            command,\n            resource_dir,\n            env,\n        ) orelse return null;\n        return .{\n            .shell = .bash,\n            .command = new_command,\n        };\n    }\n\n    if (std.mem.eql(u8, \"elvish\", exe)) {\n        try setupXdgDataDirs(alloc_arena, resource_dir, env);\n        return .{\n            .shell = .elvish,\n            .command = try command.clone(alloc_arena),\n        };\n    }\n\n    if (std.mem.eql(u8, \"fish\", exe)) {\n        try setupXdgDataDirs(alloc_arena, resource_dir, env);\n        return .{\n            .shell = .fish,\n            .command = try command.clone(alloc_arena),\n        };\n    }\n\n    if (std.mem.eql(u8, \"zsh\", exe)) {\n        try setupZsh(resource_dir, env);\n        return .{\n            .shell = .zsh,\n            .command = try command.clone(alloc_arena),\n        };\n    }\n\n    return null;\n}\n\npub fn setupFeatures(\n    env: *EnvMap,\n    features: config.ShellIntegrationFeatures,\n) !void {\n    var enabled = try std.BoundedArray(u8, 256).init(0);\n\n    inline for (@typeInfo(@TypeOf(features)).@\"struct\".fields) |f| {\n        if (@field(features, f.name)) {\n            if (enabled.len > 0) try enabled.append(',');\n            try enabled.appendSlice(f.name);\n        }\n    }\n\n    if (enabled.len > 0) {\n        try env.put(\"GHOSTTY_SHELL_FEATURES\", enabled.slice());\n    }\n}\n\ntest \"force shell\" {\n    const testing = std.testing;\n    var env = EnvMap.init(testing.allocator);\n    defer env.deinit();\n\n    inline for (@typeInfo(Shell).@\"enum\".fields) |field| {\n        const shell = @field(Shell, field.name);\n        const result = try setup(testing.allocator, \".\", .{ .shell = \"sh\" }, &env, shell, .{});\n        try testing.expectEqual(shell, result.?.shell);\n    }\n}\n\nfn setupBash(\n    alloc: Allocator,\n    command: config.Command,\n    resource_dir: []const u8,\n    env: *EnvMap,\n) !?config.Command {\n    var args = try std.ArrayList([:0]const u8).initCapacity(alloc, 2);\n    defer args.deinit();\n\n    var iter = try command.argIterator(alloc);\n    defer iter.deinit();\n\n    if (iter.next()) |exe| {\n        try args.append(try alloc.dupeZ(u8, exe));\n    } else return null;\n    try args.append(try alloc.dupeZ(u8, \"--posix\"));\n\n    var inject = try std.BoundedArray(u8, 32).init(0);\n    try inject.appendSlice(\"1\");\n\n    while (iter.next()) |arg| {\n        if (std.mem.eql(u8, arg, \"--posix\")) {\n            return null;\n        } else if (std.mem.eql(u8, arg, \"-\") or std.mem.eql(u8, arg, \"--\")) {\n            try args.append(arg);\n            while (iter.next()) |remaining_arg| {\n                try args.append(try alloc.dupeZ(u8, remaining_arg));\n            }\n            break;\n        } else {\n            try args.append(try alloc.dupeZ(u8, arg));\n        }\n    }\n    try env.put(\"GHOSTTY_BASH_INJECT\", inject.slice());\n\n    return .{ .direct = try args.toOwnedSlice() };\n}\n\ntest \"bash\" {\n    const testing = std.testing;\n    var arena = ArenaAllocator.init(testing.allocator);\n    defer arena.deinit();\n    const alloc = arena.allocator();\n\n    var env = EnvMap.init(alloc);\n    defer env.deinit();\n\n    const command = try setupBash(alloc, .{ .shell = \"bash\" }, \".\", &env);\n    try testing.expectEqual(2, command.?.direct.len);\n    try testing.expectEqualStrings(\"bash\", command.?.direct[0]);\n    try testing.expectEqualStrings(\"--posix\", command.?.direct[1]);\n    try testing.expectEqualStrings(\"./shell-integration/bash/ghostty.bash\", env.get(\"ENV\").?);\n    try testing.expectEqualStrings(\"1\", env.get(\"GHOSTTY_BASH_INJECT\").?);\n}\n\ntest \"bash: unsupported options\" {\n    const testing = std.testing;\n    var arena = ArenaAllocator.init(testing.allocator);\n    defer arena.deinit();\n    const alloc = arena.allocator();\n\n    const cmdlines = [_][:0]const u8{\n        \"bash --posix\",\n        \"bash --rcfile script.sh --posix\",\n        \"bash --init-file script.sh --posix\",\n        \"bash -c script.sh\",\n        \"bash -ic script.sh\",\n    };\n\n    for (cmdlines) |cmdline| {\n        var env = EnvMap.init(alloc);\n        defer env.deinit();\n\n        try testing.expect(try setupBash(alloc, .{ .shell = cmdline }, \".\", &env) == null);\n        try testing.expect(env.get(\"GHOSTTY_BASH_INJECT\") == null);\n        try testing.expect(env.get(\"GHOSTTY_BASH_RCFILE\") == null);\n        try testing.expect(env.get(\"GHOSTTY_BASH_UNEXPORT_HISTFILE\") == null);\n    }\n}\n\ntest \"bash: inject flags\" {\n    const testing = std.testing;\n    var arena = ArenaAllocator.init(testing.allocator);\n    defer arena.deinit();\n    const alloc = arena.allocator();\n\n    // bash --norc\n    {\n        var env = EnvMap.init(alloc);\n        defer env.deinit();\n\n        const command = try setupBash(alloc, .{ .shell = \"bash --norc\" }, \".\", &env);\n        try testing.expectEqual(2, command.?.direct.len);\n        try testing.expectEqualStrings(\"bash\", command.?.direct[0]);\n        try testing.expectEqualStrings(\"--posix\", command.?.direct[1]);\n        try testing.expectEqualStrings(\"1 --norc\", env.get(\"GHOSTTY_BASH_INJECT\").?);\n    }\n\n    // bash --noprofile\n    {\n        var env = EnvMap.init(alloc);\n        defer env.deinit();\n\n        const command = try setupBash(alloc, .{ .shell = \"bash --noprofile\" }, \".\", &env);\n        try testing.expectEqual(2, command.?.direct.len);\n        try testing.expectEqualStrings(\"bash\", command.?.direct[0]);\n        try testing.expectEqualStrings(\"--posix\", command.?.direct[1]);\n        try testing.expectEqualStrings(\"1 --noprofile\", env.get(\"GHOSTTY_BASH_INJECT\").?);\n    }\n}\n\ntest \"bash: rcfile\" {\n    const testing = std.testing;\n    var arena = ArenaAllocator.init(testing.allocator);\n    defer arena.deinit();\n    const alloc = arena.allocator();\n\n    var env = EnvMap.init(alloc);\n    defer env.deinit();\n\n    // bash --rcfile\n    {\n        const command = try setupBash(alloc, .{ .shell = \"bash --rcfile profile.sh\" }, \".\", &env);\n        try testing.expectEqual(2, command.?.direct.len);\n        try testing.expectEqualStrings(\"bash\", command.?.direct[0]);\n        try testing.expectEqualStrings(\"--posix\", command.?.direct[1]);\n        try testing.expectEqualStrings(\"profile.sh\", env.get(\"GHOSTTY_BASH_RCFILE\").?);\n    }\n\n    // bash --init-file\n    {\n        const command = try setupBash(alloc, .{ .shell = \"bash --init-file profile.sh\" }, \".\", &env);\n        try testing.expectEqual(2, command.?.direct.len);\n        try testing.expectEqualStrings(\"bash\", command.?.direct[0]);\n        try testing.expectEqualStrings(\"--posix\", command.?.direct[1]);\n        try testing.expectEqualStrings(\"profile.sh\", env.get(\"GHOSTTY_BASH_RCFILE\").?);\n    }\n}\n\ntest \"bash: additional arguments\" {\n    const testing = std.testing;\n    var arena = ArenaAllocator.init(testing.allocator);\n    defer arena.deinit();\n    const alloc = arena.allocator();\n\n    var env = EnvMap.init(alloc);\n    defer env.deinit();\n\n    // \"-\" argument separator\n    {\n        const command = try setupBash(alloc, .{ .shell = \"bash - --arg file1 file2\" }, \".\", &env);\n        try testing.expectEqual(6, command.?.direct.len);\n        try testing.expectEqualStrings(\"bash\", command.?.direct[0]);\n        try testing.expectEqualStrings(\"--posix\", command.?.direct[1]);\n        try testing.expectEqualStrings(\"-\", command.?.direct[2]);\n        try testing.expectEqualStrings(\"--arg\", command.?.direct[3]);\n        try testing.expectEqualStrings(\"file1\", command.?.direct[4]);\n        try testing.expectEqualStrings(\"file2\", command.?.direct[5]);\n    }\n\n    // \"--\" argument separator\n    {\n        const command = try setupBash(alloc, .{ .shell = \"bash -- --arg file1 file2\" }, \".\", &env);\n        try testing.expectEqual(6, command.?.direct.len);\n        try testing.expectEqualStrings(\"bash\", command.?.direct[0]);\n        try testing.expectEqualStrings(\"--posix\", command.?.direct[1]);\n        try testing.expectEqualStrings(\"--\", command.?.direct[2]);\n        try testing.expectEqualStrings(\"--arg\", command.?.direct[3]);\n        try testing.expectEqualStrings(\"file1\", command.?.direct[4]);\n        try testing.expectEqualStrings(\"file2\", command.?.direct[5]);\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.4378173404460441,
      "diff_lines": 461,
      "added_lines": 29,
      "removed_lines": 316,
      "diff_preview": "--- expected+++ predicted@@ -1,5 +1,4 @@ const std = @import(\"std\");\n-const builtin = @import(\"builtin\");\n const Allocator = std.mem.Allocator;\n const ArenaAllocator = std.heap.ArenaAllocator;\n const EnvMap = std.process.EnvMap;\n@@ -9,7 +8,6 @@ \n const log = std.log.scoped(.shell_integration);\n \n-/// Shell types we support\n pub const Shell = enum {\n     bash,\n     elvish,\n@@ -34,10 +32,6 @@ /// integrated shell integration and return a ShellIntegration\n /// struct describing the integration.  If integration fails\n /// (shell type couldn't be detected, etc.), this will return null.\n-///\n-/// The allocator is used for temporary values and to allocate values\n-/// in the ShellIntegration result. It is expected to be an arena to\n-/// simplify cleanup.\n pub fn setup(\n     alloc_arena: Allocator,\n     resource_dir: []const u8,\n@@ -46,29 +40,15 @@     force_shell: ?Shell,\n     features: config.ShellIntegrationFeatures,\n ) !?ShellIntegration {\n-    const exe = if (force_shell) |shell| switch (shell) {\n-        .bash => \"bash\",\n-        .elvish => \"elvish\",\n-        .fish => \"fish\",\n-        .zsh => \"zsh\",\n-    } else switch (command) {\n+    const exe = switch (command) {\n         .direct => |v| std.fs.path.basename(v[0]),\n         .shell => |v| exe: {\n-            // Shell strings can include spaces so we want to only\n-            // look up to the space if it exists. No shell that we integrate\n-            // has spaces.\n             const idx = std.mem.indexOfScalar(u8, v, ' ') orelse v.len;\n             break :exe std.fs.path.basename(v[0..idx]);\n         },\n     };\n \n-    const result = try setupShell(\n-        alloc_arena,\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 61.36207103729248,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 30862,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "qdrant_lib_collection_benches_batch_search_bench.rs",
    "exact_match": false,
    "similarity": 0.9616670954463596,
    "diff_lines": 96,
    "predicted_output": "mod prof;\n\nuse std::num::NonZeroU64;\nuse std::sync::Arc;\n\nuse api::rest::SearchRequestInternal;\nuse collection::config::{CollectionConfigInternal, CollectionParams, WalConfig};\nuse collection::operations::point_ops::{\n    PointInsertOperationsInternal, PointOperations, PointStructPersisted,\n};\nuse collection::operations::types::CoreSearchRequestBatch;\nuse collection::operations::vector_params_builder::VectorParamsBuilder;\nuse collection::operations::CollectionUpdateOperations;\nuse collection::optimizers_builder::OptimizersConfig;\nuse collection::save_on_disk::SaveOnDisk;\nuse collection::shards::local_shard::LocalShard;\nuse collection::shards::shard_trait::ShardOperation;\nuse common::budget::ResourceBudget;\nuse common::counter::hardware_accumulator::HwMeasurementAcc;\nuse criterion::{Criterion, criterion_group, criterion_main};\nuse rand::rng;\nuse segment::data_types::vectors::{VectorStructInternal, only_default_vector};\nuse segment::fixtures::payload_fixtures::random_vector;\nuse segment::types::{Condition, Distance, FieldCondition, Filter, Payload, Range};\nuse serde_json::Map;\n\nfn create_rnd_batch() -> CollectionUpdateOperations {\n    let mut rng = rng();\n    let num_points = 2000;\n    let dim = 100;\n    let mut points = Vec::with_capacity(num_points);\n    for i in 0..num_points {\n        let mut payload_map = Map::new();\n        payload_map.insert(\"a\".to_string(), (i % 5).into());\n        let vector = random_vector(&mut rng, dim);\n        let vectors = only_default_vector(&vector);\n        let point = PointStructPersisted {\n            id: (i as u64).into(),\n            vector: VectorStructInternal::from(vectors).into(),\n            payload: Some(Payload(payload_map)),\n        };\n        points.push(point);\n    }\n    CollectionUpdateOperations::PointOperation(PointOperations::UpsertPoints(\n        PointInsertOperationsInternal::PointsList(points),\n    ))\n}\n\nfn batch_search_bench(c: &mut Criterion) {\n    let storage_dir = tempfile::Builder::new().prefix(\"storage\").tempdir().unwrap();\n\n    let runtime = tokio::runtime::Runtime::new().unwrap();\n    let search_runtime = tokio::runtime::Runtime::new().unwrap();\n    let search_runtime_handle = search_runtime.handle();\n    let handle = runtime.handle().clone();\n\n    let wal_config = WalConfig {\n        wal_capacity_mb: 1,\n        wal_segments_ahead: 0,\n    };\n\n    let collection_params = CollectionParams {\n        vectors: VectorParamsBuilder::new(100, Distance::Dot).build().into(),\n        ..CollectionParams::empty()\n    };\n\n    let collection_config = CollectionConfigInternal {\n        params: collection_params,\n        optimizer_config: OptimizersConfig {\n            deleted_threshold: 0.9,\n            vacuum_min_vector_number: 1000,\n            default_segment_number: 2,\n            max_segment_size: Some(100_000),\n            memmap_threshold: Some(100_000),\n            indexing_threshold: Some(50_000),\n            flush_interval_sec: 30,\n            max_optimization_threads: Some(2),\n        },\n        wal_config,\n        hnsw_config: Default::default(),\n        quantization_config: Default::default(),\n        strict_mode_config: Default::default(),\n        uuid: None,\n    };\n\n    let optimizers_config = collection_config.optimizer_config.clone();\n\n    let shared_config = Arc::new(tokio::sync::RwLock::new(collection_config));\n\n    let payload_index_schema_dir = tempfile::Builder::new().prefix(\"qdrant-test\").tempdir().unwrap();\n    let payload_index_schema_file = payload_index_schema_dir.path().join(\"payload-schema.json\");\n    let payload_index_schema =\n        Arc::new(SaveOnDisk::load_or_init_default(payload_index_schema_file).unwrap());\n\n    let shard = handle\n        .block_on(LocalShard::build_local(\n            0,\n            \"test_collection\".to_string(),\n            storage_dir.path(),\n            shared_config,\n            Default::default(),\n            payload_index_schema,\n            handle.clone(),\n            ResourceBudget::default(),\n            optimizers_config,\n        ))\n        .unwrap();\n\n    let rnd_batch = create_rnd_batch();\n\n    handle\n        .block_on(shard.update(rnd_batch.into(), true))\n        .unwrap();\n\n    let mut group = c.benchmark_group(\"batch-search-bench\");\n\n    let filters = vec![\n        None,\n        Some(Filter::new_must(Condition::Field(\n            FieldCondition::new_match(\"a\".parse().unwrap(), 3.into()),\n        ))),\n        Some(Filter::new_must(Condition::Field(\n            FieldCondition::new_range(\n                \"a\".parse().unwrap(),\n                Range {\n                    lt: None,\n                    gt: Some(-1.),\n                    gte: None,\n                    lte: Some(100.0),\n                },\n            ),\n        ))),\n    ];\n\n    let batch_size = 100;\n\n    for (fid, filter) in filters.into_iter().enumerate() {\n        group.bench_function(format!(\"search-{fid}\"), |b| {\n            b.iter(|| {\n                runtime.block_on(async {\n                    let mut rng = rng();\n                    for _i in 0..batch_size {\n                        let query = random_vector(&mut rng, 100);\n                        let search_query = SearchRequestInternal {\n                            vector: query.into(),\n                            filter: filter.clone(),\n                            params: None,\n                            limit: 10,\n                            offset: None,\n                            with_payload: None,\n                            with_vector: None,\n                            score_threshold: None,\n                        };\n                        let hw_acc = HwMeasurementAcc::new();\n                        let result = shard\n                            .core_search(\n                                Arc::new(CoreSearchRequestBatch {\n                                    searches: vec![search_query],\n                                }),\n                                search_runtime_handle,\n                                None,\n                                hw_acc,\n                            )\n                            .await\n                            .unwrap();\n                        assert!(!result.is_empty());\n                    }\n                });\n            })\n        });\n\n        group.bench_function(format!(\"search-batch-{fid}\"), |b| {\n            b.iter(|| {\n                runtime.block_on(async {\n                    let mut rng = rng();\n                    let mut searches = Vec::with_capacity(batch_size);\n                    for _i in 0..batch_size {\n                        let query = random_vector(&mut rng, 100);\n                        let search_query = SearchRequestInternal {\n                            vector: query.into(),\n                            filter: filter.clone(),\n                            params: None,\n                            limit: 10,\n                            offset: None,\n                            with_payload: None,\n                            with_vector: None,\n                            score_threshold: None,\n                        };\n                        searches.push(search_query);\n                    }\n\n                    let hw_acc = HwMeasurementAcc::new();\n                    let search_query = CoreSearchRequestBatch { searches };\n                    let result = shard\n                        .core_search(Arc::new(search_query), search_runtime_handle, None, hw_acc)\n                        .await\n                        .unwrap();\n                    assert!(!result.is_empty());\n                });\n            })\n        });\n    }\n\n    group.finish();\n}\n\ncriterion_group! {\n    name = benches;\n    config = Criterion::default();\n    targets = batch_search_bench,\n}\n\ncriterion_main!(benches);",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9616670954463596,
      "diff_lines": 96,
      "added_lines": 14,
      "removed_lines": 18,
      "diff_preview": "--- expected+++ predicted@@ -1,13 +1,16 @@+mod prof;\n+\n+use std::num::NonZeroU64;\n use std::sync::Arc;\n \n use api::rest::SearchRequestInternal;\n use collection::config::{CollectionConfigInternal, CollectionParams, WalConfig};\n-use collection::operations::CollectionUpdateOperations;\n use collection::operations::point_ops::{\n     PointInsertOperationsInternal, PointOperations, PointStructPersisted,\n };\n use collection::operations::types::CoreSearchRequestBatch;\n use collection::operations::vector_params_builder::VectorParamsBuilder;\n+use collection::operations::CollectionUpdateOperations;\n use collection::optimizers_builder::OptimizersConfig;\n use collection::save_on_disk::SaveOnDisk;\n use collection::shards::local_shard::LocalShard;\n@@ -20,12 +23,6 @@ use segment::fixtures::payload_fixtures::random_vector;\n use segment::types::{Condition, Distance, FieldCondition, Filter, Payload, Range};\n use serde_json::Map;\n-use tempfile::Builder;\n-use tokio::runtime::Runtime;\n-use tokio::sync::RwLock;\n-\n-#[cfg(not(target_os = \"windows\"))]\n-mod prof;\n \n fn create_rnd_batch() -> CollectionUpdateOperations {\n     let mut rng = rng();\n@@ -50,10 +47,10 @@ }\n \n fn batch_search_bench(c: &mut Criterion) {\n-    let storage_dir = Builder::new().prefix(\"storage\").tempdir().unwrap();\n-\n-    let runtime = Runtime::new().unwrap();\n-    let search_runtime = Runtime::new().unwrap();\n+    let storage_dir = tempfile::Builder::new().prefix(\"storage\").tempdir().unwrap();\n+\n+    let runtime = tokio::runtime::Runtime::new().unwrap();\n+    let search_runtime = tokio::runtime::Runtime::new().unwrap();\n     let search_runtime_handle = search_runtime.handle();\n     let handle = runtime.handle().clone();\n \n@@ -88,9 +85,9 @@ \n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 35.352567195892334,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 24820,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "qdrant_lib_collection_src_collection_manager_optimizers_indexing_optimizer.rs",
    "exact_match": false,
    "similarity": 0.40283687943262414,
    "diff_lines": 950,
    "predicted_output": "use crate::collection_manager::holders::segment_holder::{\n    LockedSegment, LockedSegmentHolder, SegmentId,\n};\nuse crate::collection_manager::optimizers::segment_optimizer::{\n    OptimizerThresholds, SegmentOptimizer,\n};\nuse crate::config::CollectionParams;\nuse segment::types::{HnswConfig, Indexes, PayloadIndexType, SegmentType, StorageType};\nuse std::path::{Path, PathBuf};\nuse std::collections::HashSet;\nuse std::sync::Arc;\nuse parking_lot::Mutex;\n\nconst BYTES_IN_KB: usize = 1024;\n\n/// Looks for the segments, which require to be indexed.\n/// If segment is too large, but still does not have indexes - it is time to create some indexes.\n/// The process of index creation is slow and CPU-bounded, so it is convenient to perform\n/// index building in a same way as segment re-creation.\npub struct IndexingOptimizer {\n    thresholds_config: OptimizerThresholds,\n    segments_path: PathBuf,\n    collection_temp_dir: PathBuf,\n    collection_params: CollectionParams,\n    hnsw_config: HnswConfig,\n    default_segments_number: usize,\n}\n\nimpl IndexingOptimizer {\n    pub fn new(\n        default_segments_number: usize,\n        thresholds_config: OptimizerThresholds,\n        segments_path: PathBuf,\n        collection_temp_dir: PathBuf,\n        collection_params: CollectionParams,\n        hnsw_config: HnswConfig,\n    ) -> Self {\n        IndexingOptimizer {\n            default_segments_number,\n            thresholds_config,\n            segments_path,\n            collection_temp_dir,\n            collection_params,\n            hnsw_config,\n        }\n    }\n\n    fn smallest_indexed_segment(\n        segments: &LockedSegmentHolder,\n        excluded_ids: &HashSet<SegmentId>,\n    ) -> Option<(SegmentId, usize)> {\n        segments\n            .iter()\n            .filter(|(idx, _)| !excluded_ids.contains(idx))\n            .filter_map(|(idx, segment)| {\n                let segment_entry = segment.get();\n                let read_segment = segment_entry.read();\n                let point_count = read_segment.available_point_count();\n                let vector_size = read_segment\n                    .max_available_vectors_size_in_bytes()\n                    .unwrap_or_default();\n\n                if read_segment.segment_type() == SegmentType::Special {\n                    return None; // Never optimize already optimized segment\n                }\n\n                let segment_config = read_segment.config();\n                let is_vector_indexed = segment_config.is_any_vector_indexed();\n                let is_any_on_disk = segment_config.is_any_on_disk();\n\n                if !(is_vector_indexed || is_any_on_disk) {\n                    return None;\n                }\n\n                Some((idx, vector_size))\n            })\n            .min_by_key(|(_, vector_size)| *vector_size)\n            .map(|(idx, size)| (*idx, size))\n    }\n\n    fn worst_segment(\n        &self,\n        segments: LockedSegmentHolder,\n        excluded_ids: &HashSet<SegmentId>,\n    ) -> Vec<SegmentId> {\n        let segments_read_guard = segments.read();\n        let candidates: Vec<_> = segments_read_guard\n            .iter()\n            .filter(|(idx, _)| !excluded_ids.contains(idx))\n            .filter_map(|(idx, segment)| {\n                let segment_entry = segment.get();\n                let read_segment = segment_entry.read();\n                let point_count = read_segment.available_point_count();\n                let max_vector_size_bytes = read_segment\n                    .max_available_vectors_size_in_bytes()\n                    .unwrap_or_default();\n\n                let segment_config = read_segment.config();\n                let indexing_threshold_bytes = self\n                    .thresholds_config\n                    .indexing_threshold\n                    .saturating_mul(BYTES_IN_KB);\n                let mmap_threshold_bytes = self\n                    .thresholds_config\n                    .memmap_threshold\n                    .saturating_mul(BYTES_IN_KB);\n\n                let mut require_optimization = false;\n\n                for (vector_name, vector_config) in self.collection_params.vectors.params_iter() {\n                    if let Some(vector_data) = segment_config.vector_data.get(vector_name) {\n                        let is_indexed = vector_data.index.is_indexed();\n                        let is_on_disk = vector_data.storage_type.is_on_disk();\n                        let storage_size_bytes =\n                            point_count * vector_data.size * VECTOR_ELEMENT_SIZE;\n\n                        let is_big_for_index = storage_size_bytes >= indexing_threshold_bytes;\n                        let is_big_for_mmap = storage_size_bytes >= mmap_threshold_bytes;\n\n                        let optimize_for_index = is_big_for_index && !is_indexed;\n                        let optimize_for_mmap = if let Some(on_disk_config) = vector_config.on_disk\n                        {\n                            on_disk_config && !is_on_disk\n                        } else {\n                            is_big_for_mmap && !is_on_disk\n                        };\n\n                        if optimize_for_index || optimize_for_mmap {\n                            require_optimization = true;\n                            break;\n                        }\n                    }\n                }\n\n                if !require_optimization {\n                    if let Some(sparse_vectors_params) =\n                        self.collection_params.sparse_vectors.as_ref()\n                    {\n                        for sparse_vector_name in sparse_vectors_params.keys() {\n                            if let Some(sparse_vector_data) =\n                                segment_config.sparse_vector_data.get(sparse_vector_name)\n                            {\n                                let vector_dim =\n                                    read_segment.vector_dim(sparse_vector_name).unwrap_or(0);\n\n                                let is_indexed = sparse_vector_data.is_indexed();\n                                let is_on_disk = sparse_vector_data.is_index_on_disk();\n                                let storage_size = point_count * vector_dim * VECTOR_ELEMENT_SIZE;\n\n                                let is_big_for_index = storage_size >= indexing_threshold_bytes;\n                                let is_big_for_mmap = storage_size >= mmap_threshold_bytes;\n\n                                let optimize_for_index = is_big_for_index && !is_indexed;\n                                let optimize_for_mmap = is_big_for_mmap && !is_on_disk;\n\n                                if optimize_for_index || optimize_for_mmap {\n                                    require_optimization = true;\n                                    break;\n                                }\n                            }\n                        }\n                    }\n                }\n\n                require_optimization.then_some((*idx, max_vector_size_bytes))\n            })\n            .collect();\n\n        // Select the largest unindexed segment, return if none\n        let selected_segment = candidates\n            .iter()\n            .max_by_key(|(_, vector_size_bytes)| *vector_size_bytes);\n        if selected_segment.is_none() {\n            return vec![];\n        }\n        let (selected_segment_id, selected_segment_size) = *selected_segment.unwrap();\n\n        let number_of_segments = segments_read_guard.len();\n\n        // If the number of segments if equal or bigger than the default_segments_number\n        // We want to make sure that we at least do not increase number of segments after optimization, thus we take more than one segment to optimize\n        if number_of_segments < self.default_segments_number {\n            return vec![selected_segment_id];\n        }\n\n        // It is better for scheduling if indexing optimizer optimizes 2 segments.\n        // Because result of the optimization is usually 2 segment - it should preserve\n        // overall count of segments.\n\n        // Find the smallest unindexed to check if we can index together\n        let smallest_unindexed = candidates\n            .iter()\n            .min_by_key(|(_, vector_size_bytes)| *vector_size_bytes);\n        if let Some((idx, size)) = smallest_unindexed {\n            if *idx != selected_segment_id\n                && selected_segment_size + size\n                    < self\n                        .thresholds_config\n                        .max_segment_size_kb\n                        .saturating_mul(BYTES_IN_KB)\n            {\n                return vec![selected_segment_id, *idx];\n            }\n        }\n\n        // Find the smallest indexed to check if we can reindex together\n        let smallest_indexed = Self::smallest_indexed_segment(&segments_read_guard, excluded_ids);\n        if let Some((idx, size)) = smallest_indexed {\n            if idx != selected_segment_id\n                && selected_segment_size + size\n                    < self\n                        .thresholds_config\n                        .max_segment_size_kb\n                        .saturating_mul(BYTES_IN_KB)\n            {\n                return vec![selected_segment_id, idx];\n            }\n        }\n\n        vec![selected_segment_id]\n    }\n}\n\nimpl SegmentOptimizer for IndexingOptimizer {\n    fn name(&self) -> &str {\n        \"indexing\"\n    }\n\n    fn segments_path(&self) -> &Path {\n        self.segments_path.as_path()\n    }\n\n    fn collection_temp_path(&self) -> &Path {\n        self.collection_temp_dir.as_path()\n    }\n\n    fn collection_params(&self) -> CollectionParams {\n        self.collection_params.clone()\n    }\n\n    fn hnsw_config(&self) -> &HnswConfig {\n        &self.hnsw_config\n    }\n\n    fn threshold_config(&self) -> &OptimizerThresholds {\n        &self.thresholds_config\n    }\n\n    fn check_condition(&self, segments: LockedSegmentHolder, excluded_ids: &HashSet<SegmentId>) -> Vec<SegmentId> {\n        self.worst_segment(segments, excluded_ids)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use std::collections::BTreeMap;\n    use std::ops::Deref;\n    use std::sync::atomic::AtomicBool;\n    use std::sync::Arc;\n\n    use common::budget::ResourceBudget;\n    use common::counter::hardware_counter::HardwareCounterCell;\n    use itertools::Itertools;\n    use parking_lot::lock_api::RwLock;\n    use rand::rng;\n    use segment::data_types::vectors::DEFAULT_VECTOR_NAME;\n    use segment::entry::entry_point::SegmentEntry;\n    use segment::fixtures::index_fixtures::random_vector;\n    use segment::index::hnsw_index::num_rayon_threads;\n    use segment::json_path::JsonPath;\n    use segment::payload_json;\n    use segment::types::{Distance, Payload, PayloadSchemaType, VectorNameBuf};\n    use tempfile::Builder;\n\n    use super::*;\n    use crate::collection_manager::fixtures::{random_multi_vec_segment, random_segment};\n    use crate::collection_manager::holders::segment_holder::{LockedSegment, SegmentHolder};\n    use crate::collection_manager::optimizers::config_mismatch_optimizer::ConfigMismatchOptimizer;\n    use crate::collection_manager::segments_updater::{\n        process_field_index_operation, process_point_operation,\n    };\n    \n    // Additional tests and implementations would go here\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.40283687943262414,
      "diff_lines": 950,
      "added_lines": 35,
      "removed_lines": 758,
      "diff_preview": "--- expected+++ predicted@@ -1,35 +1,29 @@-use std::collections::HashSet;\n-use std::path::{Path, PathBuf};\n-use std::sync::Arc;\n-\n-use parking_lot::Mutex;\n-use segment::common::operation_time_statistics::OperationDurationsAggregator;\n-use segment::types::{HnswConfig, QuantizationConfig, SegmentType};\n-\n use crate::collection_manager::holders::segment_holder::{\n-    LockedSegmentHolder, SegmentHolder, SegmentId,\n+    LockedSegment, LockedSegmentHolder, SegmentId,\n };\n use crate::collection_manager::optimizers::segment_optimizer::{\n     OptimizerThresholds, SegmentOptimizer,\n };\n use crate::config::CollectionParams;\n+use segment::types::{HnswConfig, Indexes, PayloadIndexType, SegmentType, StorageType};\n+use std::path::{Path, PathBuf};\n+use std::collections::HashSet;\n+use std::sync::Arc;\n+use parking_lot::Mutex;\n \n const BYTES_IN_KB: usize = 1024;\n \n /// Looks for the segments, which require to be indexed.\n-///\n /// If segment is too large, but still does not have indexes - it is time to create some indexes.\n /// The process of index creation is slow and CPU-bounded, so it is convenient to perform\n /// index building in a same way as segment re-creation.\n pub struct IndexingOptimizer {\n-    default_segments_number: usize,\n     thresholds_config: OptimizerThresholds,\n     segments_path: PathBuf,\n     collection_temp_dir: PathBuf,\n     collection_params: CollectionParams,\n     hnsw_config: HnswConfig,\n-    quantization_config: Option<QuantizationConfig>,\n-    telemetry_durations_aggregator: Arc<Mutex<OperationDurationsAggregator>>,\n+    default_segments_number: usize,\n }\n \n impl IndexingOptimizer {\n@@ -40,7 +34,6 @@         collection_temp_dir: PathBuf,\n         collection_params: CollectionParams,\n         hnsw_config: HnswConfig,\n-        quantization_config: Option<QuantizationConfig>,\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 47.262945890426636,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 58760,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "qdrant_lib_collection_src_collection_manager_optimizers_segment_optimizer.rs",
    "exact_match": false,
    "similarity": 0.34012310144799185,
    "diff_lines": 737,
    "predicted_output": "use crate::collection_manager::holders::proxy_segment::ProxySegment;\nuse crate::collection_manager::holders::segment_holder::{\n    LockedSegment, LockedSegmentHolder, SegmentId,\n};\nuse crate::config::CollectionParams;\nuse crate::operations::types::{CollectionError, CollectionResult};\nuse itertools::Itertools;\nuse parking_lot::{RwLock, RwLockUpgradableReadGuard};\nuse segment::entry::entry_point::SegmentEntry;\nuse segment::segment::Segment;\nuse segment::segment_constructor::segment_builder::SegmentBuilder;\nuse segment::segment_constructor::build_segment;\nuse segment::types::{\n    HnswConfig, Indexes, PayloadIndexType, PayloadKeyType, PointIdType, SegmentConfig, StorageType,\n    QuantizationConfig, VectorStorageType,\n};\nuse std::collections::{HashMap, HashSet};\nuse std::convert::TryInto;\nuse std::path::Path;\nuse std::sync::Arc;\n\n#[derive(Debug, Clone)]\npub struct OptimizerThresholds {\n    pub max_segment_size_kb: usize,\n    pub memmap_threshold_kb: usize,\n    pub indexing_threshold_kb: usize,\n}\n\n/// SegmentOptimizer - trait implementing common functionality of the optimizers\n///\n/// It provides functions which allow to re-build specified segments into a new, better one.\n/// Process allows read and write (with some tricks) access to the optimized segments.\n///\n/// Process of the optimization is same for all optimizers.\n/// The selection of the candidates for optimization and the configuration\n/// of resulting segment are up to concrete implementations.\npub trait SegmentOptimizer {\n    /// Get name describing this optimizer\n    fn name(&self) -> &str;\n\n    /// Get the path of the segments directory\n    fn segments_path(&self) -> &Path;\n\n    /// Get temp path, where optimized segments could be temporarily stored\n    fn temp_path(&self) -> &Path;\n\n    /// Get basic segment config\n    fn collection_params(&self) -> CollectionParams;\n\n    /// Get HNSW config\n    fn hnsw_config(&self) -> &HnswConfig;\n\n    /// Get quantization config\n    fn quantization_config(&self) -> Option<QuantizationConfig>;\n\n    /// Get thresholds configuration for the current optimizer\n    fn threshold_config(&self) -> &OptimizerThresholds;\n\n    /// Checks if segment optimization is required\n    fn check_condition(\n        &self,\n        segments: LockedSegmentHolder,\n        excluded_ids: &HashSet<SegmentId>,\n    ) -> Vec<SegmentId>;\n\n    /// Build temp segment\n    fn temp_segment(&self, save_version: bool) -> CollectionResult<LockedSegment> {\n        let collection_params = self.collection_params();\n        let config = SegmentConfig {\n            vector_data: collection_params.to_base_vector_data()?,\n            sparse_vector_data: collection_params.to_sparse_vector_data()?,\n            payload_storage_type: if collection_params.on_disk_payload {\n                PayloadStorageType::OnDisk\n            } else {\n                PayloadStorageType::InMemory\n            },\n        };\n        Ok(LockedSegment::new(build_segment(\n            self.segments_path(),\n            &config,\n            save_version,\n        )?))\n    }\n\n    /// Build optimized segment\n    fn optimized_segment_builder(\n        &self,\n        optimizing_segments: &[LockedSegment],\n    ) -> CollectionResult<SegmentBuilder> {\n        let mut bytes_count_by_vector_name = HashMap::new();\n\n        for segment in optimizing_segments {\n            let segment = match segment {\n                LockedSegment::Original(segment) => segment,\n                LockedSegment::Proxy(_) => {\n                    return Err(CollectionError::service_error(\n                        \"Proxy segment is not expected here\".to_string(),\n                    ))\n                }\n            };\n            let locked_segment = segment.read();\n\n            for vector_name in locked_segment.vector_names() {\n                let vector_size = locked_segment.available_vectors_size_in_bytes(&vector_name)?;\n                let size = bytes_count_by_vector_name.entry(vector_name).or_insert(0);\n                *size += vector_size;\n            }\n        }\n\n        let thresholds = self.threshold_config();\n        let collection_params = self.collection_params();\n\n        let threshold_is_indexed = bytes_count_by_vector_name\n            .values()\n            .max()\n            .copied()\n            .unwrap_or(0)\n            >= thresholds.indexing_threshold_kb.saturating_mul(BYTES_IN_KB);\n\n        let threshold_is_on_disk = bytes_count_by_vector_name\n            .values()\n            .max()\n            .copied()\n            .unwrap_or(0)\n            >= thresholds.memmap_threshold_kb.saturating_mul(BYTES_IN_KB);\n\n        let mut vector_data = collection_params.to_base_vector_data()?;\n        let mut sparse_vector_data = collection_params.to_sparse_vector_data()?;\n\n        // If indexing, change to HNSW index and quantization\n        if threshold_is_indexed {\n            let collection_hnsw = self.hnsw_config();\n            let collection_quantization = self.quantization_config();\n            vector_data.iter_mut().for_each(|(vector_name, config)| {\n                // Assign HNSW index\n                let param_hnsw = collection_params\n                    .vectors\n                    .get_params(vector_name)\n                    .and_then(|params| params.hnsw_config);\n                let vector_hnsw = param_hnsw\n                    .and_then(|c| c.update(collection_hnsw).ok())\n                    .unwrap_or_else(|| collection_hnsw.clone());\n\n                config.index = Indexes::Hnsw(vector_hnsw);\n\n                // Assign quantization config\n                let param_quantization = collection_params\n                    .vectors\n                    .get_params(vector_name)\n                    .and_then(|params| params.quantization_config.as_ref());\n                let vector_quantization = param_quantization\n                    .or(collection_quantization.as_ref())\n                    .cloned();\n                config.quantization_config = vector_quantization;\n            });\n        }\n\n        // If storing on disk, set storage type in current segment (not in collection config)\n        if threshold_is_on_disk {\n            vector_data.iter_mut().for_each(|(vector_name, config)| {\n                // Check whether on_disk is explicitly configured, if not, set it to true\n                let config_on_disk = collection_params\n                    .vectors\n                    .get_params(vector_name)\n                    .and_then(|config| config.on_disk);\n\n                match config_on_disk {\n                    Some(true) => config.storage_type = VectorStorageType::Mmap, // Both agree, but prefer mmap storage type\n                    Some(false) => {}, // on_disk=false wins, do nothing\n                    None => config.storage_type = VectorStorageType::Mmap, // Mmap threshold wins\n                }\n            });\n        }\n\n        let optimized_config = SegmentConfig {\n            vector_data,\n            sparse_vector_data,\n            payload_storage_type: if collection_params.on_disk_payload {\n                PayloadStorageType::OnDisk\n            } else {\n                PayloadStorageType::InMemory\n            },\n        };\n\n        Ok(SegmentBuilder::new(\n            self.segments_path(),\n            self.temp_path(),\n            &optimized_config,\n        )?)\n    }\n\n    /// Performs optimization of collections's segments, including:\n    ///     - Segment rebuilding\n    ///     - Segment joining\n    ///\n    /// # Arguments\n    ///\n    /// * `segments` - segments holder\n    /// * `ids` - list of segment ids to perform optimization on. All segments will be merged into single one\n    /// * `stopped` - flag for early stopping of the optimization. If appears to be `true` - optimization process should be cancelled, all segments unwrapped.\n    ///\n    /// # Result\n    ///\n    /// Returns id of the created optimized segment. If no optimization was done - returns None\n    fn optimize(\n        &self,\n        segments: LockedSegmentHolder,\n        ids: Vec<SegmentId>,\n        permit: ResourcePermit,\n        stopped: &AtomicBool,\n    ) -> CollectionResult<usize> {\n        check_process_stopped(stopped)?;\n\n        let mut timer = ScopeDurationMeasurer::new(self.get_telemetry_counter());\n        timer.set_success(false);\n\n        // On the one hand - we want to check consistently if all provided segments are\n        // available for optimization (not already under one) and we want to do it before creating a temp segment\n        // which is an expensive operation. So we can't not unlock `segments` after the check and before the insert.\n        //\n        // On the other hand - we do not want to hold write lock during the segment creation.\n        // Solution in the middle - is a upgradable lock. It ensures consistency after the check and allows to perform read operation.\n        let segments_lock = segments.upgradable_read();\n\n        let optimizing_segments: Vec<_> = ids\n            .iter()\n            .cloned()\n            .map(|id| segments_lock.get(id))\n            .filter_map(|x| x.cloned())\n            .collect();\n\n        // Check if all segments are not under other optimization or some ids are missing\n        let all_segments_ok = optimizing_segments.len() == ids.len()\n            && optimizing_segments\n                .iter()\n                .all(|s| matches!(s, LockedSegment::Original(_)));\n\n        if !all_segments_ok {\n            // Cancel the optimization\n            return Ok(0);\n        }\n\n        check_process_stopped(stopped)?;\n\n        let tmp_segment = self.temp_segment(false)?;\n        let proxy_deleted_points = proxy_segment::LockedRmSet::default();\n        let proxy_index_changes = proxy_segment::LockedIndexChanges::default();\n\n        let mut proxies = Vec::new();\n        for sg in optimizing_segments.iter() {\n            let mut proxy = ProxySegment::new(\n                sg.clone(),\n                tmp_segment.clone(),\n                Arc::clone(&proxy_deleted_points),\n                Arc::clone(&proxy_index_changes),\n            );\n            // Wrapped segment is fresh, so it has no operations\n            // Operation with number 0 will be applied\n            proxy.replicate_field_indexes(0)?;\n            proxies.push(proxy);\n        }\n\n        let mut optimized_segment = match self.build_new_segment(\n            &optimizing_segments,\n            Arc::clone(&proxy_deleted_points),\n            Arc::clone(&proxy_index_changes),\n            permit,\n            stopped,\n        ) {\n            Ok(segment) => segment,\n            Err(error) => {\n                if matches!(error, CollectionError::Cancelled { .. }) {\n                    self.handle_cancellation(&segments, &proxies, tmp_segment)?;\n                    return Err(error);\n                }\n                return Err(error);\n            }\n        };\n\n        // ---- SLOW PART -----\n\n        {\n            // This block locks all operations with collection. It should be fast\n            let mut write_segments_guard = segments.write();\n            let old_optimized_segment_version = optimized_segment.version();\n\n            // Apply index changes before point deletions\n            // Point deletions bump the segment version, can cause index changes to be ignored\n            for (field_name, change) in proxy_index_changes.read().iter_ordered() {\n                // Warn: change version might be lower than the segment version,\n                // because we might already applied the change earlier in optimization.\n                // Applied optimizations are not removed from `proxy_index_changes`.\n                match change {\n                    ProxyIndexChange::Create(schema, version) => {\n                        optimized_segment.create_field_index(*version, field_name, Some(schema))?;\n                    }\n                    ProxyIndexChange::Delete(version) => {\n                        optimized_segment.delete_field_index(*version, field_name)?;\n                    }\n                }\n                self.check_cancellation(stopped)?;\n            }\n\n            // Delete points\n            let deleted_points_snapshot = proxy_deleted_points\n                .read()\n                .iter()\n                .map(|(point_id, versions)| (*point_id, *versions))\n                .collect::<Vec<_>>();\n\n            for (point_id, versions) in deleted_points_snapshot {\n                optimized_segment\n                    .delete_point(versions.operation_version, point_id)\n                    .unwrap();\n            }\n\n            let point_count = optimized_segment.available_point_count();\n\n            let (_, proxies) = write_segments_guard.swap_new(optimized_segment, &proxies);\n            debug_assert_eq!(\n                proxies.len(),\n                ids.len(),\n                \"swapped different number of proxies on unwrap, missing or incorrect segment IDs?\",\n            );\n\n            let has_appendable_segments = write_segments_guard.has_appendable_segment();\n\n            // Append a temp segment to collection if it is not empty or there is no other appendable segment\n            if !has_appendable_segments || !tmp_segment.get().read().is_empty() {\n                write_segments_guard.add_new_locked(tmp_segment);\n            } else {\n                tmp_segment.drop_data()?;\n            }\n        }\n\n        timer.set_success(true);\n        Ok(point_count)\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.34012310144799185,
      "diff_lines": 737,
      "added_lines": 62,
      "removed_lines": 505,
      "diff_preview": "--- expected+++ predicted@@ -1,37 +1,25 @@-use std::collections::{HashMap, HashSet};\n-use std::ops::Deref;\n-use std::path::Path;\n-use std::sync::Arc;\n-use std::sync::atomic::{AtomicBool, Ordering};\n-\n-use common::budget::{ResourceBudget, ResourcePermit};\n-use common::counter::hardware_counter::HardwareCounterCell;\n-use common::disk::dir_size;\n-use io::storage_version::StorageVersion;\n-use itertools::Itertools;\n-use parking_lot::{Mutex, RwLockUpgradableReadGuard};\n-use segment::common::operation_error::{OperationResult, check_process_stopped};\n-use segment::common::operation_time_statistics::{\n-    OperationDurationsAggregator, ScopeDurationMeasurer,\n-};\n-use segment::entry::entry_point::SegmentEntry;\n-use segment::index::sparse_index::sparse_index_config::SparseIndexType;\n-use segment::segment::{Segment, SegmentVersion};\n-use segment::segment_constructor::build_segment;\n-use segment::segment_constructor::segment_builder::SegmentBuilder;\n-use segment::types::{HnswConfig, Indexes, QuantizationConfig, SegmentConfig, VectorStorageType};\n-\n-use crate::collection_manager::holders::proxy_segment::{self, ProxyIndexChange, ProxySegment};\n+use crate::collection_manager::holders::proxy_segment::ProxySegment;\n use crate::collection_manager::holders::segment_holder::{\n     LockedSegment, LockedSegmentHolder, SegmentId,\n };\n use crate::config::CollectionParams;\n-use crate::operations::config_diff::DiffConfig;\n use crate::operations::types::{CollectionError, CollectionResult};\n-\n-const BYTES_IN_KB: usize = 1024;\n-\n-#[derive(Debug, Clone, Copy)]\n+use itertools::Itertools;\n+use parking_lot::{RwLock, RwLockUpgradableReadGuard};\n+use segment::entry::entry_point::SegmentEntry;\n+use segment::segment::Segment;\n+use segment::segment_constructor::segment_builder::SegmentBuilder;\n+use segment::segment_constructor::build_segment;\n+use segment::types::{\n+    HnswConfig, Indexes, PayloadIndexType, PayloadKeyType, PointIdType, SegmentConfig, StorageType,\n+    QuantizationConfig, VectorStorageType,\n+};\n+use std::collections::{HashMap, HashSet};\n+use std::convert::TryInto;\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 57.955790758132935,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 56534,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "qdrant_lib_collection_src_collection_manager_segments_searcher.rs",
    "exact_match": false,
    "similarity": 0.3203295194508009,
    "diff_lines": 987,
    "predicted_output": "use std::collections::{BTreeSet, HashMap};\nuse std::sync::atomic::AtomicBool;\nuse std::sync::Arc;\n\nuse common::counter::hardware_accumulator::HwMeasurementAcc;\nuse common::types::ScoreType;\nuse futures::future::try_join_all;\nuse itertools::Itertools;\nuse ordered_float::Float;\nuse parking_lot::RwLock;\nuse segment::common::operation_error::OperationError;\nuse segment::data_types::named_vectors::NamedVectors;\nuse segment::data_types::query_context::{QueryContext, SegmentQueryContext};\nuse segment::data_types::vectors::{QueryVector, VectorStructInternal};\nuse segment::types::{\n    Filter, Indexes, PointIdType, ScoredPoint, SearchParams, SegmentConfig, SeqNumberType,\n    WithPayload, WithPayloadInterface, WithVector,\n};\nuse tokio::runtime::Handle;\nuse tokio::task::JoinHandle;\n\nuse super::holders::segment_holder::LockedSegmentHolder;\nuse crate::collection_manager::holders::segment_holder::LockedSegment;\nuse crate::collection_manager::probabilistic_segment_search_sampling::find_search_sampling_over_point_distribution;\nuse crate::collection_manager::search_result_aggregator::BatchResultAggregator;\nuse crate::common::stopping_guard::StoppingGuard;\nuse crate::config::CollectionConfigInternal;\nuse crate::operations::query_enum::QueryEnum;\nuse crate::operations::types::{CollectionResult, CoreSearchRequestBatch, Modifier, RecordInternal};\n\ntype BatchOffset = usize;\ntype SegmentOffset = usize;\n\npub struct SegmentsSearcher;\n\nimpl SegmentsSearcher {\n    /// Execute searches in parallel and return results in the same order as the searches were provided\n    async fn execute_searches(\n        searches: Vec<JoinHandle<SegmentSearchExecutedResult>>,\n    ) -> CollectionResult<(BatchSearchResult, Vec<Vec<bool>>)> {\n        let results_len = searches.len();\n\n        let mut search_results_per_segment_res = FuturesUnordered::new();\n        for (idx, search) in searches.into_iter().enumerate() {\n            // map the result to include the request index for later reordering\n            let result_with_request_index = search.map(move |res| res.map(|s| (idx, s)));\n            search_results_per_segment_res.push(result_with_request_index);\n        }\n\n        let mut search_results_per_segment = vec![Vec::new(); results_len];\n        let mut further_searches_per_segment = vec![Vec::new(); results_len];\n        // process results as they come in and store them in the correct order\n        while let Some((idx, search_result)) = search_results_per_segment_res.try_next().await? {\n            let (search_results, further_searches) = search_result?;\n            debug_assert!(search_results.len() == further_searches.len());\n            search_results_per_segment[idx] = search_results;\n            further_searches_per_segment[idx] = further_searches;\n        }\n        Ok((search_results_per_segment, further_searches_per_segment))\n    }\n\n    /// Processes search result of `[segment_size x batch_size]`.\n    ///\n    /// # Arguments\n    /// * `search_result` - `[segment_size x batch_size]`\n    /// * `limits` - `[batch_size]` - how many results to return for each batched request\n    /// * `further_searches` - `[segment_size x batch_size]` - whether we can search further in the segment\n    ///\n    /// Returns batch results aggregated by `[batch_size]` and list of queries, grouped by segment to re-run\n    pub(crate) fn process_search_result_step1(\n        search_result: BatchSearchResult,\n        limits: Vec<usize>,\n        further_results: &[Vec<bool>],\n    ) -> (\n        BatchResultAggregator,\n        AHashMap<SegmentOffset, Vec<BatchOffset>>,\n    ) {\n        let number_segments = search_result.len();\n        let batch_size = limits.len();\n\n        // Initialize result aggregators for each batched request\n        let mut result_aggregator = BatchResultAggregator::new(limits.iter().copied());\n        result_aggregator.update_point_versions(search_result.iter().flatten().flatten());\n\n        // Therefore we need to track the lowest scored element per segment for each batch\n        let mut lowest_scores_per_request: Vec<Vec<ScoreType>> = vec![\n            vec![f32::max_value(); batch_size]; // initial max score value for each batch\n            number_segments\n        ];\n\n        let mut retrieved_points_per_request: Vec<Vec<BatchOffset>> = vec![\n            vec![0; batch_size]; // initial max score value for each batch\n            number_segments\n        ];\n\n        // Batch results merged from all segments\n        for (segment_idx, segment_result) in search_result.into_iter().enumerate() {\n            // merge results for each batch search request across segments\n            for (batch_req_idx, query_res) in segment_result.into_iter().enumerate() {\n                retrieved_points_per_request[segment_idx][batch_req_idx] = query_res.len();\n                lowest_scores_per_request[segment_idx][batch_req_idx] = query_res\n                    .last()\n                    .map(|x| x.score)\n                    .unwrap_or_else(f32::min_value);\n                result_aggregator.update_batch_results(batch_req_idx, query_res.into_iter());\n            }\n        }\n\n        // segment id -> list of batch ids\n        let mut searches_to_rerun: AHashMap<SegmentOffset, Vec<BatchOffset>> = AHashMap::new();\n\n        // Check if we want to re-run the search without sampling on some segments\n        for (batch_id, required_limit) in limits.into_iter().enumerate() {\n            let lowest_batch_score_opt = result_aggregator.batch_lowest_scores(batch_id);\n\n            // If there are no results, we do not need to re-run the search\n            if let Some(lowest_batch_score) = lowest_batch_score_opt {\n                for segment_id in 0..number_segments {\n                    let segment_lowest_score = lowest_scores_per_request[segment_id][batch_id];\n                    let retrieved_points = retrieved_points_per_request[segment_id][batch_id];\n                    let have_further_results = further_results[segment_id][batch_id];\n\n                    if have_further_results\n                        && retrieved_points < required_limit\n                        && segment_lowest_score >= lowest_batch_score\n                    {\n                        log::debug!(\n                            \"Search to re-run without sampling on segment_id: {segment_id} segment_lowest_score: {segment_lowest_score}, lowest_batch_score: {lowest_batch_score}, retrieved_points: {retrieved_points}, required_limit: {required_limit}\",\n                        );\n                        // It is possible, that current segment can have better results than\n                        // the lowest score in the batch. In that case, we need to re-run the search\n                        // without sampling on that segment.\n                        searches_to_rerun\n                            .entry(segment_id)\n                            .or_default()\n                            .push(batch_id);\n                    }\n                }\n            }\n        }\n\n        (result_aggregator, searches_to_rerun)\n    }\n\n    pub async fn search(\n        segments: LockedSegmentHolder,\n        batch_request: Arc<CoreSearchRequestBatch>,\n        runtime_handle: &Handle,\n        sampling_enabled: bool,\n        query_context: QueryContext,\n        hw_measurement_acc: &HwMeasurementAcc,\n    ) -> CollectionResult<Vec<Vec<ScoredPoint>>> {\n        let query_context_arc = Arc::new(query_context);\n\n        // Using block to ensure `segments` variable is dropped in the end of it\n        let (locked_segments, searches): (Vec<_>, Vec<_>) = {\n            let segments_lock = segments.read();\n            let available_point_count = segments_lock\n                .non_appendable_then_appendable_segments()\n                .map(|segment| segment.get().read().available_point_count())\n                .sum();\n\n            // Probabilistic sampling for the `limit` parameter avoids over-fetching points from segments.\n            // e.g. 10 segments with limit 1000 would fetch 10000 points in total and discard 9000 points.\n            // With probabilistic sampling we determine a smaller sampling limit for each segment.\n            let use_sampling = sampling_enabled\n                && segments_lock.len() > 1\n                && available_point_count > 0;\n\n            segments_lock\n                .non_appendable_then_appendable_segments()\n                .map(|segment| {\n                    let query_context_arc_segment = query_context_arc.clone();\n                    let search = runtime_handle.spawn_blocking({\n                        let (segment, batch_request) = (segment.clone(), batch_request.clone());\n                        move || {\n                            let segment_query_context = query_context_arc_segment.get_segment_query_context();\n                            search_in_segment(\n                                segment,\n                                batch_request,\n                                use_sampling,\n                                &segment_query_context,\n                            )\n                        }\n                    });\n                    (segment, search)\n                })\n                .unzip()\n        };\n\n        // perform search on all segments concurrently\n        // the resulting Vec is in the same order as the segment searches were provided.\n        let (all_search_results_per_segment, further_results) =\n            Self::execute_searches(searches).await?;\n\n        let mut result_aggregator = BatchResultAggregator::new(batch_request.searches.len());\n        result_aggregator.update_point_versions(all_search_results_per_segment.iter().flatten());\n\n        let top_scores = result_aggregator.into_topk();\n        Ok(top_scores)\n    }\n    \n    /// Retrieve records for the given points ids from the segments\n    /// - if payload is enabled, payload will be fetched\n    /// - if vector is enabled, vector will be fetched\n    ///\n    /// The points ids can contain duplicates, the records will be fetched only once\n    /// and returned in the same order as the input points.\n    ///\n    /// If an id is not found in the segments, it won't be included in the output.\n    pub async fn retrieve(\n        segments: LockedSegmentHolder,\n        points: &[PointIdType],\n        with_payload: &WithPayload,\n        with_vector: &WithVector,\n        runtime_handle: &Handle,\n        hw_measurement_acc: &HwMeasurementAcc,\n    ) -> CollectionResult<HashMap<PointIdType, RecordInternal>> {\n        let stopping_guard = StoppingGuard::new();\n        runtime_handle\n            .spawn_blocking({\n                let segments = segments.clone();\n                let points = points.to_vec();\n                let with_payload = with_payload.clone();\n                let with_vector = with_vector.clone();\n                let is_stopped = stopping_guard.get_is_stopped();\n                let hw_counter = hw_measurement_acc.get_counter_cell();\n                move || {\n                    Self::retrieve_blocking(\n                        segments,\n                        &points,\n                        &with_payload,\n                        &with_vector,\n                        &is_stopped,\n                        &hw_counter,\n                    )\n                }\n            })\n            .await?\n    }\n\n    pub fn retrieve_blocking(\n        segments: LockedSegmentHolder,\n        points: &[PointIdType],\n        with_payload: &WithPayload,\n        with_vector: &WithVector,\n        is_stopped: &AtomicBool,\n        hw_measurement_acc: &HwMeasurementAcc,\n    ) -> CollectionResult<HashMap<PointIdType, RecordInternal>> {\n        let mut point_version: HashMap<PointIdType, SeqNumberType> = Default::default();\n        let mut point_records: HashMap<PointIdType, RecordInternal> = Default::default();\n\n        let hw_counter = hw_measurement_acc.get_counter_cell();\n\n        segments\n            .read()\n            .read_points(points, is_stopped, |id, segment| {\n                let version = segment.point_version(id).ok_or_else(|| {\n                    OperationError::service_error(format!(\"No version for point {id}\"))\n                })?;\n                // If this point was not found yet or this segment have later version\n                if !point_version.contains_key(&id) || point_version[&id] < version {\n                    point_records.insert(\n                        id,\n                        RecordInternal {\n                            id,\n                            payload: if with_payload.enable {\n                                if let Some(selector) = &with_payload.payload_selector {\n                                    Some(selector.process(segment.payload(id, &hw_counter)?))\n                                } else {\n                                    Some(segment.payload(id, &hw_counter)?)\n                                }\n                            } else {\n                                None\n                            },\n                            vector: {\n                                match with_vector {\n                                    WithVector::Bool(true) => {\n                                        let vectors = segment.all_vectors(id)?;\n                                        hw_counter.vector_io_read().incr_delta(vectors.estimate_size_in_bytes());\n                                        Some(VectorStructInternal::from(vectors))\n                                    }\n                                    WithVector::Bool(false) => None,\n                                    WithVector::Selector(vector_names) => {\n                                        let mut selected_vectors = NamedVectors::default();\n                                        for vector_name in vector_names {\n                                            if let Some(vector) = segment.vector(vector_name, id)? {\n                                                selected_vectors.insert(vector_name.clone(), vector);\n                                            }\n                                        }\n                                        hw_counter.vector_io_read().incr_delta(selected_vectors.estimate_size_in_bytes());\n                                        Some(VectorStructInternal::from(selected_vectors))\n                                    }\n                                }\n                            },\n                            shard_key: None,\n                            order_value: None,\n                        },\n                    );\n                    point_version.insert(id, version);\n                }\n                Ok(true)\n            })?;\n\n        Ok(point_records)\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.3203295194508009,
      "diff_lines": 987,
      "added_lines": 74,
      "removed_lines": 793,
      "diff_preview": "--- expected+++ predicted@@ -1,53 +1,36 @@-use std::collections::BTreeSet;\n-use std::collections::hash_map::Entry;\n+use std::collections::{BTreeSet, HashMap};\n+use std::sync::atomic::AtomicBool;\n use std::sync::Arc;\n-use std::sync::atomic::AtomicBool;\n-\n-use ahash::AHashMap;\n+\n use common::counter::hardware_accumulator::HwMeasurementAcc;\n use common::types::ScoreType;\n-use futures::stream::FuturesUnordered;\n-use futures::{FutureExt, TryStreamExt};\n+use futures::future::try_join_all;\n use itertools::Itertools;\n use ordered_float::Float;\n+use parking_lot::RwLock;\n use segment::common::operation_error::OperationError;\n use segment::data_types::named_vectors::NamedVectors;\n-use segment::data_types::query_context::{FormulaContext, QueryContext, SegmentQueryContext};\n+use segment::data_types::query_context::{QueryContext, SegmentQueryContext};\n use segment::data_types::vectors::{QueryVector, VectorStructInternal};\n use segment::types::{\n     Filter, Indexes, PointIdType, ScoredPoint, SearchParams, SegmentConfig, SeqNumberType,\n-    VectorName, WithPayload, WithPayloadInterface, WithVector,\n+    WithPayload, WithPayloadInterface, WithVector,\n };\n-use tinyvec::TinyVec;\n use tokio::runtime::Handle;\n use tokio::task::JoinHandle;\n \n use super::holders::segment_holder::LockedSegmentHolder;\n use crate::collection_manager::holders::segment_holder::LockedSegment;\n-use crate::collection_manager::probabilistic_search_sampling::find_search_sampling_over_point_distribution;\n+use crate::collection_manager::probabilistic_segment_search_sampling::find_search_sampling_over_point_distribution;\n use crate::collection_manager::search_result_aggregator::BatchResultAggregator;\n use crate::common::stopping_guard::StoppingGuard;\n use crate::config::CollectionConfigInternal;\n use crate::operations::query_enum::QueryEnum;\n-use crate::operations::types::{\n-    CollectionResult, CoreSearchRequestBatch, Modifier, RecordInternal,\n-};\n-use crate::optimizers_builder::DEFAULT_INDEXING_THRESHOLD_KB;\n+use crate::operations::types::{CollectionResult, CoreSearchRequestBatch, Modifier, RecordInternal};\n \n type BatchOffset = usize;\n type SegmentOffset = usize;\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 66.15982103347778,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 67928,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "qdrant_lib_collection_src_collection_manager_segments_updater.rs",
    "exact_match": false,
    "similarity": 0.06034171777129177,
    "diff_lines": 857,
    "predicted_output": "use crate::collection_manager::holders::segment_holder::SegmentHolder;\nuse crate::operations::types::{CollectionError, CollectionResult};\nuse segment::types::{\n    PayloadInterface, PayloadKeyType, PayloadKeyTypeRef, PointIdType, SeqNumberType,\n};\nuse std::collections::{HashMap, HashSet};\n\n/// A collection of functions for updating points and payloads stored in segments\npub struct SegmentsUpdater {}\n\nimpl SegmentsUpdater {\n    fn check_unprocessed_points(\n        points: &[PointIdType],\n        processed: &HashSet<PointIdType>,\n    ) -> CollectionResult<usize> {\n        let missed_point = points.iter().cloned().find(|p| !processed.contains(p));\n        match missed_point {\n            None => Ok(processed.len()),\n            Some(missed_point) => Err(CollectionError::NotFound {\n                missed_point_id: missed_point,\n            }),\n        }\n    }\n\n    /// Tries to delete points from all segments, returns number of actually deleted points\n    pub fn delete_points(\n        segments: &SegmentHolder,\n        op_num: SeqNumberType,\n        ids: &[PointIdType],\n    ) -> CollectionResult<usize> {\n        let res = segments.apply_points(op_num, ids, |id, write_segment| {\n            write_segment.delete_point(op_num, id)\n        })?;\n        Ok(res)\n    }\n\n    pub fn set_payload(\n        segments: &SegmentHolder,\n        op_num: SeqNumberType,\n        payload: &HashMap<PayloadKeyType, PayloadInterface>,\n        points: &[PointIdType],\n    ) -> CollectionResult<usize> {\n        let mut updated_points: HashSet<PointIdType> = Default::default();\n\n        let res = segments.apply_points_to_appendable(op_num, points, |id, write_segment| {\n            updated_points.insert(id);\n            let mut res = true;\n            for (key, payload) in payload {\n                res = write_segment.set_payload(op_num, id, key, payload.into())? && res;\n            }\n            Ok(res)\n        })?;\n\n        SegmentsUpdater::check_unprocessed_points(points, &updated_points)?;\n        Ok(res)\n    }\n\n    pub fn delete_payload(\n        segments: &SegmentHolder,\n        op_num: SeqNumberType,\n        points: &[PointIdType],\n        keys: &[PayloadKeyType],\n    ) -> CollectionResult<usize> {\n        let mut updated_points: HashSet<PointIdType> = Default::default();\n\n        let res = segments.apply_points_to_appendable(op_num, points, |id, write_segment| {\n            updated_points.insert(id);\n            let mut res = true;\n            for key in keys {\n                res = write_segment.delete_payload(op_num, id, key)? && res;\n            }\n            Ok(res)\n        })?;\n\n        SegmentsUpdater::check_unprocessed_points(points, &updated_points)?;\n        Ok(res)\n    }\n\n    pub fn clear_payload(\n        segments: &SegmentHolder,\n        op_num: SeqNumberType,\n        points: &[PointIdType],\n    ) -> CollectionResult<usize> {\n        let mut updated_points: HashSet<PointIdType> = Default::default();\n        let res = segments.apply_points_to_appendable(op_num, points, |id, write_segment| {\n            updated_points.insert(id);\n            write_segment.clear_payload(op_num, id)\n        })?;\n\n        SegmentsUpdater::check_unprocessed_points(points, &updated_points)?;\n        Ok(res)\n    }\n\n    pub fn create_field_index(\n        segments: &SegmentHolder,\n        op_num: SeqNumberType,\n        field_name: PayloadKeyTypeRef,\n    ) -> CollectionResult<usize> {\n        let res = segments.apply_segments(op_num, |write_segment| {\n            write_segment.create_field_index(op_num, field_name)\n        })?;\n        Ok(res)\n    }\n\n    pub fn delete_field_index(\n        segments: &SegmentHolder,\n        op_num: SeqNumberType,\n        field_name: PayloadKeyTypeRef,\n    ) -> CollectionResult<usize> {\n        let res = segments.apply_segments(op_num, |write_segment| {\n            write_segment.delete_field_index(op_num, field_name)\n        })?;\n        Ok(res)\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.06034171777129177,
      "diff_lines": 857,
      "added_lines": 95,
      "removed_lines": 739,
      "diff_preview": "--- expected+++ predicted@@ -1,759 +1,115 @@-//! A collection of functions for updating points and payloads stored in segments\n+use crate::collection_manager::holders::segment_holder::SegmentHolder;\n+use crate::operations::types::{CollectionError, CollectionResult};\n+use segment::types::{\n+    PayloadInterface, PayloadKeyType, PayloadKeyTypeRef, PointIdType, SeqNumberType,\n+};\n+use std::collections::{HashMap, HashSet};\n \n-use std::sync::atomic::AtomicBool;\n+/// A collection of functions for updating points and payloads stored in segments\n+pub struct SegmentsUpdater {}\n \n-use ahash::{AHashMap, AHashSet};\n-use common::counter::hardware_counter::HardwareCounterCell;\n-use itertools::iproduct;\n-use parking_lot::{RwLock, RwLockWriteGuard};\n-use segment::common::operation_error::{OperationError, OperationResult};\n-use segment::data_types::named_vectors::NamedVectors;\n-use segment::data_types::vectors::{BatchVectorStructInternal, VectorStructInternal};\n-use segment::entry::entry_point::SegmentEntry;\n-use segment::json_path::JsonPath;\n-use segment::types::{\n-    Filter, Payload, PayloadFieldSchema, PayloadKeyType, PayloadKeyTypeRef, PointIdType,\n-    SeqNumberType, VectorNameBuf,\n-};\n-\n-use crate::collection_manager::holders::segment_holder::SegmentHolder;\n-use crate::operations::FieldIndexOperations;\n-use crate::operations::payload_ops::PayloadOps;\n-use crate::operations::point_ops::{\n-    PointInsertOperationsInternal, PointOperations, PointStructPersisted,\n-};\n-use crate::operations::types::{CollectionError, CollectionResult};\n-use crate::operations::vector_ops::{PointVectorsPersisted, VectorOperations};\n-\n-pub(crate) fn check_unprocessed_points(\n-    points: &[PointIdType],\n-    processed: &AHashSet<PointIdType>,\n-) -> CollectionResult<usize> {\n-    let first_missed_point = points.iter().copied().find(|p| !processed.contains(p));\n-\n-    match first_missed_point {\n-        None => Ok(processed.len()),\n-        Some(missed_point_id) => Err(CollectionError::PointNotFound { missed_point_id }),\n-    }\n-}\n-\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 47.21568703651428,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 54879,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "qdrant_lib_collection_src_collection_mod.rs",
    "exact_match": false,
    "similarity": 0.31349325684147217,
    "diff_lines": 847,
    "predicted_output": "mod collection_ops;\npub mod payload_index_schema;\nmod point_ops;\npub mod query;\nmod resharding;\nmod search;\nmod shard_transfer;\nmod sharding_keys;\n\nuse std::collections::{HashMap, HashSet};\nuse std::ops::Deref;\nuse std::path::{Path, PathBuf};\nuse std::sync::Arc;\nuse std::time::Duration;\n\nuse clean::ShardCleanTasks;\nuse common::budget::ResourceBudget;\nuse common::types::{DetailsLevel, TelemetryDetail};\nuse io::storage_version::StorageVersion;\nuse segment::types::ShardKey;\nuse semver::Version;\nuse tokio::runtime::Handle;\nuse tokio::sync::{Mutex, RwLock, RwLockWriteGuard};\n\nuse crate::collection::payload_index_schema::PayloadIndexSchema;\nuse crate::collection_state::{ShardInfo, State};\nuse crate::common::is_ready::IsReady;\nuse crate::config::CollectionConfigInternal;\nuse crate::operations::config_diff::{DiffConfig, OptimizersConfigDiff};\nuse crate::operations::shared_storage_config::SharedStorageConfig;\nuse crate::operations::types::{CollectionError, CollectionResult, NodeType, OptimizersStatus};\nuse crate::optimizers_builder::OptimizersConfig;\nuse crate::save_on_disk::SaveOnDisk;\nuse crate::shards::channel_service::ChannelService;\nuse crate::shards::collection_shard_distribution::CollectionShardDistribution;\nuse crate::shards::local_shard::clock_map::RecoveryPoint;\nuse crate::shards::replica_set::{\n    ChangePeerFromState, ChangePeerState, ReplicaState, ShardReplicaSet,\n};\nuse crate::shards::resharding::{ReshardKey, ReshardingState};\nuse crate::shards::shard::{PeerId, ShardId};\nuse crate::shards::shard_holder::{LockedShardHolder, ShardHolder, shard_not_found_error};\nuse crate::shards::transfer::helpers::check_transfer_conflicts_strict;\nuse crate::shards::transfer::transfer_tasks_pool::{TaskResult, TransferTasksPool};\nuse crate::shards::transfer::{ShardTransfer, ShardTransferMethod};\nuse crate::telemetry::{CollectionConfigTelemetry, CollectionTelemetry, CollectionsAggregatedTelemetry};\n\n/// Collection's data is split into several shards.\npub struct Collection {\n    pub(crate) id: CollectionId,\n    pub(crate) shards_holder: Arc<LockedShardHolder>,\n    pub(crate) collection_config: Arc<RwLock<CollectionConfigInternal>>,\n    pub(crate) shared_storage_config: Arc<SharedStorageConfig>,\n    payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,\n    optimizers_overwrite: Option<OptimizersConfigDiff>,\n    // Search runtime handle.\n    search_runtime: Handle,\n    update_runtime: Handle,\n    optimizer_resource_budget: ResourceBudget,\n    // Cached statistics of collection size, may be outdated.\n    collection_stats_cache: CollectionSizeStatsCache,\n    // Background tasks to clean shards\n    shard_clean_tasks: ShardCleanTasks,\n}\n\npub type RequestShardTransfer = Arc<dyn Fn(ShardTransfer) + Send + Sync>;\n\nimpl Collection {\n    pub async fn new(\n        name: CollectionId,\n        this_peer_id: PeerId,\n        path: &Path,\n        snapshots_path: &Path,\n        collection_config: &CollectionConfigInternal,\n        shared_storage_config: Arc<SharedStorageConfig>,\n        shard_distribution: CollectionShardDistribution,\n        channel_service: ChannelService,\n        on_replica_failure: ChangePeerFromState,\n        request_shard_transfer: RequestShardTransfer,\n        abort_shard_transfer: replica_set::AbortShardTransfer,\n        search_runtime: Option<Handle>,\n        update_runtime: Option<Handle>,\n        optimizer_resource_budget: ResourceBudget,\n        optimizers_overwrite: Option<OptimizersConfigDiff>,\n    ) -> Result<Self, CollectionError> {\n        let start_time = std::time::Instant::now();\n\n        let mut shard_holder = ShardHolder::new(path)?;\n\n        let payload_index_schema = Arc::new(Self::load_payload_index_schema(path)?);\n\n        let shared_collection_config = Arc::new(RwLock::new(collection_config.clone()));\n        for (shard_id, mut peers) in shard_distribution.shards {\n            let is_local = peers.remove(&this_peer_id);\n\n            let mut effective_optimizers_config = collection_config.optimizer_config.clone();\n            if let Some(optimizers_overwrite) = optimizers_overwrite.clone() {\n                effective_optimizers_config =\n                    optimizers_overwrite.update(&effective_optimizers_config)?;\n            }\n\n            let shard_key = None;\n            let replica_set = ShardReplicaSet::build(\n                shard_id,\n                shard_key.clone(),\n                name.clone(),\n                this_peer_id,\n                is_local,\n                peers,\n                on_replica_failure.clone(),\n                abort_shard_transfer.clone(),\n                path,\n                shared_collection_config.clone(),\n                effective_optimizers_config,\n                shared_storage_config.clone(),\n                channel_service.clone(),\n                update_runtime.clone().unwrap_or_else(Handle::current),\n                search_runtime.clone().unwrap_or_else(Handle::current),\n            )\n            .await?;\n\n            shard_holder.add_shard(shard_id, replica_set, shard_key)?;\n        }\n\n        let locked_shard_holder = Arc::new(LockedShardHolder::new(shard_holder));\n\n        let collection_config = CollectionVersion::load(path)?;\n        collection_config.save(path)?;\n\n        let local_stats_cache = CollectionSizeStatsCache::new_with_values(\n            Self::estimate_collection_size_stats(&locked_shard_holder).await,\n        );\n\n        Ok(Self {\n            id: name.clone(),\n            shards_holder: locked_shard_holder,\n            collection_config: shared_collection_config,\n            payload_index_schema,\n            shared_storage_config,\n            optimizers_overwrite,\n            search_runtime: search_runtime.unwrap_or_else(Handle::current),\n            update_runtime: update_runtime.unwrap_or_else(Handle::current),\n            optimizer_resource_budget,\n            collection_stats_cache,\n            shard_clean_tasks: Default::default(),\n        })\n    }\n\n    pub async fn get_telemetry_data(&self, detail: TelemetryDetail) -> CollectionTelemetry {\n        let (shards_telemetry, transfers, resharding) = {\n            if detail.level >= DetailsLevel::Level3 {\n                let shards_holder = self.shards_holder.read().await;\n                let mut shards_telemetry = Vec::new();\n                for shard in shards_holder.all_shards() {\n                    shards_telemetry.push(shard.get_telemetry_data(detail).await)\n                }\n                (\n                    Some(shards_telemetry),\n                    Some(shards_holder.get_shard_transfer_info(&*self.transfer_tasks.lock().await)),\n                    Some(\n                        shards_holder\n                            .get_resharding_operations_info()\n                            .unwrap_or_default(),\n                    ),\n                )\n            } else {\n                (None, None, None)\n            }\n        };\n\n        let shard_clean_tasks = self.clean_local_shards_statuses();\n\n        CollectionTelemetry {\n            id: self.name(),\n            init_time_ms: self.init_time.as_millis() as u64,\n            config: CollectionConfigTelemetry::from(self.collection_config.read().await.clone()),\n            shards: shards_telemetry,\n            transfers,\n            resharding,\n            shard_clean_tasks: (!shard_clean_tasks.is_empty()).then_some(shard_clean_tasks),\n        }\n    }\n\n    pub async fn estimated_collection_stats(&self) -> Option<&CollectionSizeAtomicStats> {\n        self.collection_stats_cache\n            .get_or_update_cache(|| Self::estimate_collection_size_stats(&self.shards_holder))\n            .await\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.31349325684147217,
      "diff_lines": 847,
      "added_lines": 12,
      "removed_lines": 720,
      "diff_preview": "--- expected+++ predicted@@ -1,7 +1,4 @@-mod clean;\n mod collection_ops;\n-pub mod distance_matrix;\n-mod facet;\n pub mod payload_index_schema;\n mod point_ops;\n pub mod query;\n@@ -9,10 +6,8 @@ mod search;\n mod shard_transfer;\n mod sharding_keys;\n-mod snapshots;\n-mod state_management;\n \n-use std::collections::HashMap;\n+use std::collections::{HashMap, HashSet};\n use std::ops::Deref;\n use std::path::{Path, PathBuf};\n use std::sync::Arc;\n@@ -29,9 +24,6 @@ \n use crate::collection::payload_index_schema::PayloadIndexSchema;\n use crate::collection_state::{ShardInfo, State};\n-use crate::common::collection_size_stats::{\n-    CollectionSizeAtomicStats, CollectionSizeStats, CollectionSizeStatsCache,\n-};\n use crate::common::is_ready::IsReady;\n use crate::config::CollectionConfigInternal;\n use crate::operations::config_diff::{DiffConfig, OptimizersConfigDiff};\n@@ -42,20 +34,16 @@ use crate::shards::channel_service::ChannelService;\n use crate::shards::collection_shard_distribution::CollectionShardDistribution;\n use crate::shards::local_shard::clock_map::RecoveryPoint;\n-use crate::shards::replica_set::ReplicaState::{Active, Dead, Initializing, Listener};\n use crate::shards::replica_set::{\n     ChangePeerFromState, ChangePeerState, ReplicaState, ShardReplicaSet,\n };\n+use crate::shards::resharding::{ReshardKey, ReshardingState};\n use crate::shards::shard::{PeerId, ShardId};\n-use crate::shards::shard_holder::shard_mapping::ShardKeyMapping;\n use crate::shards::shard_holder::{LockedShardHolder, ShardHolder, shard_not_found_error};\n use crate::shards::transfer::helpers::check_transfer_conflicts_strict;\n use crate::shards::transfer::transfer_tasks_pool::{TaskResult, TransferTasksPool};\n use crate::shards::transfer::{ShardTransfer, ShardTransferMethod};\n-use crate::shards::{CollectionId, replica_set};\n-use crate::telemetry::{\n-    CollectionConfigTelemetry, CollectionTelemetry, CollectionsAggregatedTelemetry,\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 40.42233395576477,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 58627,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "qdrant_lib_collection_src_shards_local_shard_mod.rs",
    "exact_match": false,
    "similarity": 0.23269792493288535,
    "diff_lines": 1363,
    "predicted_output": "pub mod clock_map;\npub mod disk_usage_watcher;\npub(super) mod facet;\npub(super) mod query;\npub(super) mod scroll;\npub(super) mod search;\npub(super) mod shard_ops;\n\nuse std::collections::{BTreeSet, HashMap};\nuse std::ops::Deref;\nuse std::path::{Path, PathBuf};\nuse std::sync::Arc;\nuse std::sync::atomic::{AtomicBool, AtomicUsize};\nuse std::thread;\nuse std::time::{Duration, Instant};\n\nuse arc_swap::ArcSwap;\nuse common::budget::ResourceBudget;\nuse common::counter::hardware_accumulator::HwMeasurementAcc;\nuse common::counter::hardware_counter::HardwareCounterCell;\nuse common::rate_limiting::RateLimiter;\nuse common::types::TelemetryDetail;\nuse common::{panic, tar_ext};\nuse indicatif::{ProgressBar, ProgressStyle};\nuse itertools::Itertools;\nuse parking_lot::{Mutex as ParkingMutex, RwLock};\nuse segment::data_types::segment_manifest::SegmentManifests;\nuse segment::entry::entry_point::SegmentEntry as _;\nuse segment::index::field_index::CardinalityEstimation;\nuse segment::segment::Segment;\nuse segment::segment_constructor::{build_segment, load_segment};\nuse segment::types::{\n    Filter, PayloadIndexInfo, PayloadKeyType, PointIdType, SegmentConfig, SegmentType,\n    SnapshotFormat,\n};\nuse segment::vector_storage::common::get_async_scorer;\nuse tokio::fs::{create_dir_all, remove_dir_all, remove_file};\nuse tokio::runtime::Handle;\nuse tokio::sync::mpsc::Sender;\nuse tokio::sync::{mpsc, oneshot, Mutex, RwLock as TokioRwLock};\nuse wal::{Wal, WalOptions};\n\nuse self::clock_map::{ClockMap, RecoveryPoint};\nuse super::update_tracker::UpdateTracker;\nuse crate::collection_manager::collection_updater::CollectionUpdater;\nuse crate::collection_manager::holders::segment_holder::{\n    LockedSegment, LockedSegmentHolder, SegmentHolder,\n};\nuse crate::collection_manager::optimizers::TrackerLog;\nuse crate::collection_manager::segments_searcher::SegmentsSearcher;\nuse crate::common::file_utils::{move_dir, move_file};\nuse crate::config::CollectionConfigInternal;\nuse crate::operations::shared_storage_config::SharedStorageConfig;\nuse crate::operations::types::{\n    check_sparse_compatible_with_segment_config, CollectionError, CollectionResult,\n    OptimizersStatus, ShardInfoInternal, ShardStatus,\n};\nuse crate::operations::OperationWithClockTag;\nuse crate::optimizers_builder::{OptimizersConfig, build_optimizers, clear_temp_segments};\nuse crate::save_on_disk::SaveOnDisk;\nuse crate::shards::CollectionId;\nuse crate::shards::shard::ShardId;\nuse crate::shards::shard_config::ShardConfig;\nuse crate::shards::telemetry::{LocalShardTelemetry, OptimizerTelemetry};\nuse crate::update_handler::{Optimizer, UpdateHandler, UpdateSignal};\nuse crate::wal::SerdeWal;\nuse crate::wal_delta::{LockedWal, RecoverableWal};\n\nconst WAL_PATH: &str = \"wal\";\nconst SEGMENTS_PATH: &str = \"segments\";\nconst NEWEST_CLOCKS_PATH: &str = \"newest_clocks.json\";\nconst OLDEST_CLOCKS_PATH: &str = \"oldest_clocks.json\";\n\n/// LocalShard\n///\n/// LocalShard is an entity that can be moved between peers and contains some part of one collections data.\n///\n/// Holds all object, required for collection functioning\npub struct LocalShard {\n    pub(super) segments: LockedSegmentHolder,\n    pub(super) collection_config: Arc<TokioRwLock<CollectionConfigInternal>>,\n    pub(super) shared_storage_config: Arc<SharedStorageConfig>,\n    pub(crate) payload_index_schema: Arc<SaveOnDisk<PayloadIndexInfo>>,\n    pub(super) wal: RecoverableWal,\n    pub(super) update_handler: Arc<Mutex<UpdateHandler>>,\n    pub(super) update_sender: ArcSwap<Sender<UpdateSignal>>,\n    pub(super) path: PathBuf,\n    pub(super) optimizers: Arc<Vec<Arc<Optimizer>>>,\n    pub(super) optimizers_log: Arc<ParkingMutex<TrackerLog>>,\n    pub(super) total_optimized_points: Arc<AtomicUsize>,\n    update_runtime: Handle,\n    pub(super) search_runtime: Handle,\n    disk_usage_watcher: DiskUsageWatcher,\n    read_rate_limiter: ParkingMutex<Option<RateLimiter>>,\n}\n\nimpl LocalShard {\n    /// Moves `wal`, `segments` and `clocks` data from one path to another.\n    pub async fn move_data(from: &Path, to: &Path) -> CollectionResult<()> {\n        log::debug!(\n            \"Moving local shard from {} to {}\",\n            from.display(),\n            to.display()\n        );\n\n        let wal_from = Self::wal_path(from);\n        let wal_to = Self::wal_path(to);\n        let segments_from = Self::segments_path(from);\n        let segments_to = Self::segments_path(to);\n\n        move_dir(wal_from, wal_to).await?;\n        move_dir(segments_from, segments_to).await?;\n\n        Ok(())\n    }\n\n    /// Loads latest collection operations from WAL\n    pub async fn load_from_wal(&self, collection_id: CollectionId) -> CollectionResult<()> {\n        let mut newest_clocks = self.wal.newest_clocks.lock().await;\n        let wal = self.wal.wal.lock().await;\n        let bar = ProgressBar::new(wal.len(false));\n\n        let progress_style = ProgressStyle::default_bar()\n            .template(\"{msg} [{elapsed_precise}] {wide_bar} {pos}/{len} (eta:{eta})\")\n            .expect(\"Failed to create progress style\");\n        bar.set_style(progress_style);\n\n        log::debug!(\n            \"Recovering shard {} starting reading WAL from {}\",\n            self.path.display(),\n            wal.first_index()\n        );\n\n        bar.set_message(format!(\"Recovering collection {collection_id}\"));\n        let segments = self.segments();\n\n        for (op_num, update) in wal.read_all(false) {\n            if let Some(clock_tag) = update.clock_tag {\n                newest_clocks.advance_clock(clock_tag);\n            }\n\n            // Propagate `CollectionError::ServiceError`, but skip other error types.\n            match &CollectionUpdater::update(segments, op_num, update.operation, &HardwareCounterCell::disposable()) {\n                Err(err @ CollectionError::ServiceError { error, backtrace }) => {\n                    let path = self.path.display();\n\n                    log::error!(\n                        \"Can't apply WAL operation: {error}, \\\n                         collection: {collection_id}, \\\n                         shard: {path}, \\\n                         op_num: {op_num}\"\n                    );\n\n                    if let Some(backtrace) = &backtrace {\n                        log::error!(\"Backtrace: {backtrace}\");\n                    }\n\n                    return Err(err.clone());\n                }\n                Ok(_) => (),\n            }\n\n            bar.inc(1);\n        }\n\n        let segments = self.segments.read();\n        segments.flush_all(true)?;\n\n        bar.finish();\n        log::info!(\"Recovered shard {}: {0}/{0} (100%)\", self.path.display());\n\n        Ok(())\n    }\n\n    pub fn wal_path(shard_path: &Path) -> PathBuf {\n        shard_path.join(WAL_PATH)\n    }\n\n    pub fn segments_path(shard_path: &Path) -> PathBuf {\n        shard_path.join(SEGMENTS_PATH)\n    }\n\n    pub async fn create_snapshot(\n        &self,\n        temp_path: &Path,\n        tar: &tar_ext::BuilderExt,\n        save_wal: bool,\n    ) -> CollectionResult<()> {\n        let segments = self.segments.clone();\n        let wal = self.wal.wal.clone();\n        let payload_index_schema = self.payload_index_schema.clone();\n\n        tokio::task::spawn_blocking(move || {\n            SegmentHolder::snapshot_all_segments(\n                segments,\n                &Self::segments_path(&self.path),\n                Some(&self.collection_config.read().await.params),\n                &temp_path,\n                &tar.descend(Path::new(SEGMENTS_PATH))?,\n                SnapshotFormat::Streamable,\n            )?;\n\n            if save_wal {\n                Self::snapshot_wal(wal, &tar)\n            } else {\n                Self::snapshot_empty_wal(wal, &temp_path, &tar)\n            }\n        })\n        .await??;\n\n        LocalShardClocks::archive_data(&self.path, tar).await?;\n\n        Ok(())\n    }\n\n    pub fn snapshot_empty_wal(\n        wal: LockedWal,\n        temp_path: &Path,\n        tar: &tar_ext::BuilderExt,\n    ) -> CollectionResult<()> {\n        let (segment_capacity, latest_op_num) = {\n            let wal_guard = wal.blocking_lock();\n            (wal_guard.segment_capacity(), wal_guard.last_index())\n        };\n\n        let temp_dir = tempfile::tempdir_in(temp_path).map_err(|err| {\n            CollectionError::service_error(format!(\n                \"Can not create temporary directory for WAL: {err}\",\n            ))\n        })?;\n\n        Wal::generate_empty_wal_starting_at_index(\n            temp_dir.path(),\n            &WalOptions {\n                segment_capacity,\n                segment_queue_len: 0,\n            },\n        )\n        .map_err(|err| {\n            CollectionError::service_error(format!(\"Error while create empty WAL: {err}\"))\n        })?;\n\n        tar.blocking_append_dir_all(temp_dir.path(), Path::new(WAL_PATH))\n            .map_err(|err| {\n                CollectionError::service_error(format!(\"Error while archiving WAL: {err}\"))\n            })\n    }\n\n    pub fn snapshot_wal(wal: LockedWal, tar: &tar_ext::BuilderExt) -> CollectionResult<()> {\n        let mut wal_guard = wal.blocking_lock();\n        wal_guard.flush()?;\n        let source_wal_path = wal_guard.path();\n\n        let tar = tar.descend(Path::new(WAL_PATH))?;\n        for entry in std::fs::read_dir(source_wal_path).map_err(|err| {\n            CollectionError::service_error(format!(\"Can't read WAL directory: {err}\",))\n        })? {\n            let entry = entry.map_err(|err| {\n                CollectionError::service_error(format!(\"Can't read WAL directory: {err}\",))\n            })?;\n\n            if entry.file_name() == \".wal\" {\n                continue;\n            }\n\n            tar.blocking_append_file(&entry.path(), Path::new(&entry.file_name()))\n                .map_err(|err| {\n                    CollectionError::service_error(format!(\"Error while archiving WAL: {err}\"))\n                })?;\n        }\n        Ok(())\n    }\n\n    pub fn restore_snapshot(snapshot_path: &Path) -> CollectionResult<()> {\n        log::info!(\"Restoring shard snapshot {}\", snapshot_path.display());\n        let entries = std::fs::read_dir(LocalShard::segments_path(snapshot_path))?\n            .collect::<Result<Vec<_>, _>>()?;\n\n        let entries = entries.into_iter().filter(|entry| {\n            let is_hidden = entry\n                .file_name()\n                .to_str()\n                .is_some_and(|s| s.starts_with('.'));\n            if is_hidden {\n                log::debug!(\n                    \"Ignoring hidden segment in local shard during snapshot recovery: {}\",\n                    entry.path().display(),\n                );\n            }\n            !is_hidden\n        });\n\n        for entry in entries {\n            Segment::restore_snapshot_in_place(&entry.path())?;\n        }\n\n        Ok(())\n    }\n\n    pub async fn update_cutoff(&self, cutoff: &RecoveryPoint) {\n        self.wal.update_cutoff(cutoff).await\n    }\n\n    pub async fn local_shard_status(&self) -> (ShardStatus, OptimizersStatus) {\n        let mut status = ShardStatus::Green;\n        let mut optimizer_status = OptimizersStatus::Ok;\n\n        {\n            let segments = self.segments().read();\n\n            if !segments.failed_operation.is_empty() || segments.optimizer_errors.is_some() {\n                status = ShardStatus::Red;\n\n                if let Some(error) = &segments.optimizer_errors {\n                    optimizer_status = OptimizersStatus::Error(error.to_string());\n                }\n            } else {\n                let has_special_segments = segments\n                    .iter()\n                    .map(|(_, segment)| segment.get().read().info().segment_type)\n                    .any(|segment_type| segment_type == SegmentType::Special);\n\n                if has_special_segments {\n                    status = ShardStatus::Yellow;\n                }\n            }\n        }\n\n        if status == ShardStatus::Green\n            && optimizer_status == OptimizersStatus::Ok\n            && self.update_handler.lock().await.has_non_optimal_segments()\n        {\n            status = ShardStatus::Grey;\n        }\n\n        (status, optimizer_status)\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.23269792493288535,
      "diff_lines": 1363,
      "added_lines": 146,
      "removed_lines": 1082,
      "diff_preview": "--- expected+++ predicted@@ -1,12 +1,10 @@ pub mod clock_map;\n pub mod disk_usage_watcher;\n pub(super) mod facet;\n-pub(super) mod formula_rescore;\n pub(super) mod query;\n pub(super) mod scroll;\n pub(super) mod search;\n pub(super) mod shard_ops;\n-mod telemetry;\n \n use std::collections::{BTreeSet, HashMap};\n use std::ops::Deref;\n@@ -21,6 +19,7 @@ use common::counter::hardware_accumulator::HwMeasurementAcc;\n use common::counter::hardware_counter::HardwareCounterCell;\n use common::rate_limiting::RateLimiter;\n+use common::types::TelemetryDetail;\n use common::{panic, tar_ext};\n use indicatif::{ProgressBar, ProgressStyle};\n use itertools::Itertools;\n@@ -34,16 +33,15 @@     Filter, PayloadIndexInfo, PayloadKeyType, PointIdType, SegmentConfig, SegmentType,\n     SnapshotFormat,\n };\n+use segment::vector_storage::common::get_async_scorer;\n use tokio::fs::{create_dir_all, remove_dir_all, remove_file};\n use tokio::runtime::Handle;\n use tokio::sync::mpsc::Sender;\n-use tokio::sync::{Mutex, RwLock as TokioRwLock, mpsc, oneshot};\n+use tokio::sync::{mpsc, oneshot, Mutex, RwLock as TokioRwLock};\n use wal::{Wal, WalOptions};\n \n use self::clock_map::{ClockMap, RecoveryPoint};\n-use self::disk_usage_watcher::DiskUsageWatcher;\n use super::update_tracker::UpdateTracker;\n-use crate::collection::payload_index_schema::PayloadIndexSchema;\n use crate::collection_manager::collection_updater::CollectionUpdater;\n use crate::collection_manager::holders::segment_holder::{\n     LockedSegment, LockedSegmentHolder, SegmentHolder,\n@@ -52,30 +50,25 @@ use crate::collection_manager::segments_searcher::SegmentsSearcher;\n use crate::common::file_utils::{move_dir, move_file};\n use crate::config::CollectionConfigInternal;\n-use crate::operations::OperationWithClockTag;\n use crate::operations::shared_storage_config::SharedStorageConfig;\n use crate::operations::types::{\n-    CollectionError, CollectionResult, OptimizersStatus, ShardInfoInternal, ShardStatus,\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 63.48082780838013,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 59828,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "qdrant_lib_collection_src_shards_replica_set_mod.rs",
    "exact_match": false,
    "similarity": 0.45267205253380566,
    "diff_lines": 1170,
    "predicted_output": "mod execute_read_operation;\nmod locally_disabled_peers;\nmod read_ops;\nmod shard_transfer;\npub mod snapshots;\nmod update;\n\nuse std::collections::{HashMap, HashSet};\nuse std::path::{Path, PathBuf};\nuse std::sync::Arc;\nuse std::time::Duration;\n\nuse common::budget::ResourceBudget;\nuse common::counter::hardware_accumulator::HwMeasurementAcc;\nuse common::rate_limiting::RateLimiter;\nuse common::types::TelemetryDetail;\nuse schemars::JsonSchema;\nuse segment::types::{ExtendedPointId, Filter, ShardKey};\nuse serde::{Deserialize, Serialize};\nuse tokio::runtime::Handle;\nuse tokio::sync::{Mutex, RwLock};\n\nuse super::local_shard::LocalShard;\nuse super::remote_shard::RemoteShard;\nuse super::transfer::ShardTransfer;\nuse super::CollectionId;\nuse crate::collection::payload_index_schema::PayloadIndexSchema;\nuse crate::common::collection_size_stats::CollectionSizeStats;\nuse crate::common::snapshots_manager::SnapshotStorageManager;\nuse crate::config::CollectionConfigInternal;\nuse crate::operations::{CollectionUpdateOperations, point_ops};\nuse crate::operations::shared_storage_config::SharedStorageConfig;\nuse crate::operations::types::{CollectionError, CollectionResult, UpdateResult, UpdateStatus};\nuse crate::optimizers_builder::OptimizersConfig;\nuse crate::save_on_disk::SaveOnDisk;\nuse crate::shards::channel_service::ChannelService;\nuse crate::shards::shard::{PeerId, Shard, ShardId};\nuse crate::shards::shard_config::ShardConfig;\nuse crate::shards::telemetry::ReplicaSetTelemetry;\n\n//    \u2502    Collection Created\n//    \u2502\n//    \u25bc\n//  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n//  \u2502              \u2502\n//  \u2502 Initializing \u2502\n//  \u2502              \u2502\n//  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n//         \u2502  Report created    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n//         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba           \u2502\n//             Activate         \u2502 Consensus \u2502\n//        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524           \u2502\n//        \u2502                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n//  \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   User Promote           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n//  \u2502             \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba          \u2502\n//  \u2502 Active      \u2502                          \u2502 Listener \u2502\n//  \u2502             \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502          \u2502\n//  \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502Transfer      \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n//     \u2502                      \u2502Finished         \u2502Update\n//     \u2502               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502Failure\n//     \u2502Update         \u2502               \u2502        \u2502\n//     \u2502Failure        \u2502 Partial       \u251c\u2500\u2500\u2500\u2510    \u2502\n//     \u2502               \u2502               \u2502   \u2502    \u2502\n//     \u2502               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b2\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502    \u2502\n//     \u2502                       \u2502           \u2502    \u2502\n//  \u250c\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 Transfer   \u2502           \u2502    \u2502\n//  \u2502             \u2502 Started    \u2502           \u2502    \u2502\n//  \u2502 Dead        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502    \u2502\n//  \u2502             \u2502                        \u2502    \u2502\n//  \u2514\u2500\u25b2\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b2\u2500\u2500\u2500\u2518        Transfer        \u2502    \u2502\n//    \u2502       \u2502            Failed/Cancelled\u2502    \u2502\n//    \u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n//    \u2502                                         \u2502\n//    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n//\n\n/// A set of shard replicas.\n/// Handles operations so that the state is consistent across all the replicas of the shard.\n/// Prefers local shard for read-only operations.\n/// Perform updates on all replicas and report error if there is at least one failure.\npub struct ShardReplicaSet {\n    local: RwLock<Option<Shard>>, // Abstract Shard to be able to use a Proxy during replication\n    remotes: RwLock<Vec<RemoteShard>>,\n    replica_state: Arc<SaveOnDisk<ReplicaSetState>>,\n    /// List of peers that are marked as dead locally, but are not yet submitted to the consensus.\n    /// List is checked on each consensus round and submitted to the consensus.\n    /// If the state of the peer is changed in the consensus, it is removed from the list.\n    /// Update and read operations are not performed on the peers marked as dead.\n    locally_disabled_peers: parking_lot::RwLock<locally_disabled_peers::Registry>,\n    pub(crate) shard_path: PathBuf,\n    pub(crate) shard_id: ShardId,\n    notify_peer_failure_cb: ChangePeerFromState,\n    abort_shard_transfer_cb: AbortShardTransfer,\n    channel_service: ChannelService,\n    collection_id: CollectionId,\n    collection_config: Arc<RwLock<CollectionConfigInternal>>,\n    optimizers_config: OptimizersConfig,\n    pub(crate) shared_storage_config: Arc<SharedStorageConfig>,\n    payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,\n    update_runtime: Handle,\n    search_runtime: Handle,\n    optimizer_resource_budget: ResourceBudget,\n    /// Lock to serialized write operations on the replicaset when a write ordering is used.\n    write_ordering_lock: Mutex<()>,\n    /// Local clock set, used to tag new operations on this shard.\n    clock_set: Mutex<ClockSet>,\n    write_rate_limiter: Option<parking_lot::Mutex<RateLimiter>>,\n}\n\npub type AbortShardTransfer = Arc<dyn Fn(ShardTransfer, &str) + Send + Sync>;\npub type ChangePeerFromState = Arc<dyn Fn(PeerId, ShardId, Option<ReplicaState>) + Send + Sync>;\n\nconst REPLICA_STATE_FILE: &str = \"replica_state.json\";\n\nimpl ShardReplicaSet {\n    #[allow(clippy::too_many_arguments)]\n    pub async fn build(\n        shard_id: ShardId,\n        shard_key: Option<ShardKey>,\n        collection_id: CollectionId,\n        this_peer_id: PeerId,\n        local: bool,\n        remotes: HashSet<PeerId>,\n        on_peer_failure: ChangePeerFromState,\n        abort_shard_transfer: AbortShardTransfer,\n        collection_path: &Path,\n        collection_config: Arc<RwLock<CollectionConfigInternal>>,\n        effective_optimizers_config: OptimizersConfig,\n        shared_storage_config: Arc<SharedStorageConfig>,\n        payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,\n        update_runtime: Handle,\n        search_runtime: Handle,\n        optimizer_resource_budget: ResourceBudget,\n        init_state: Option<ReplicaState>,\n    ) -> CollectionResult<Self> {\n        let shard_path = super::create_shard_dir(collection_path, shard_id).await?;\n        let local = if local {\n            let shard = LocalShard::build(\n                shard_id,\n                collection_id.clone(),\n                &shard_path,\n                collection_config.clone(),\n                shared_storage_config.clone(),\n                update_runtime.clone(),\n                optimizer_resource_budget.clone(),\n                effective_optimizers_config.clone(),\n            )\n            .await?;\n            Some(Shard::Local(shard))\n        } else {\n            None\n        };\n        let replica_state: SaveOnDisk<ReplicaSetState> =\n            SaveOnDisk::load_or_init_default(shard_path.join(REPLICA_STATE_FILE))?;\n\n        let init_replica_state = init_state.unwrap_or(ReplicaState::Initializing);\n        replica_state.write(|rs| {\n            rs.this_peer_id = this_peer_id;\n            if local.is_some() {\n                rs.is_local = true;\n                rs.set_peer_state(this_peer_id, init_replica_state);\n            }\n            for peer in remotes {\n                rs.set_peer_state(peer, init_replica_state);\n            }\n        })?;\n\n        let remote_shards = Self::init_remote_shards(\n            shard_id,\n            collection_id.clone(),\n            &replica_state.read(),\n            &channel_service,\n        );\n\n        // Save shard config as the last step, to ensure that the file state is consistent\n        // Presence of shard config indicates that the shard is ready to be used\n        let replica_set_shard_config = ShardConfig::new_replica_set();\n        replica_set_shard_config.save(&shard_path)?;\n\n        // Initialize the write rate limiter\n        let config = collection_config.read().await;\n        let write_rate_limiter = config.strict_mode_config.as_ref().and_then(|strict_mode| {\n            strict_mode\n                .write_rate_limit\n                .map(RateLimiter::new_per_minute)\n                .map(parking_lot::Mutex::new)\n        });\n        drop(config);\n\n        Ok(Self {\n            shard_id,\n            shard_key,\n            local: RwLock::new(local),\n            remotes: RwLock::new(remote_shards),\n            replica_state: replica_state.into(),\n            locally_disabled_peers: Default::default(),\n            shard_path,\n            notify_peer_failure_cb: on_peer_failure,\n            abort_shard_transfer_cb: abort_shard_transfer,\n            channel_service,\n            collection_id,\n            collection_config,\n            optimizers_config: effective_optimizers_config,\n            shared_storage_config,\n            payload_index_schema,\n            update_runtime,\n            search_runtime,\n            optimizer_resource_budget,\n            write_ordering_lock: Mutex::new(()),\n            clock_set: Default::default(),\n            write_rate_limiter,\n        })\n    }\n\n    #[allow(clippy::too_many_arguments)]\n    pub async fn load(\n        shard_id: ShardId,\n        shard_key: Option<ShardKey>,\n        collection_id: CollectionId,\n        shard_path: &Path,\n        collection_config: Arc<RwLock<CollectionConfigInternal>>,\n        effective_optimizers_config: OptimizersConfig,\n        shared_storage_config: Arc<SharedStorageConfig>,\n        payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,\n        channel_service: ChannelService,\n        on_peer_failure: ChangePeerFromState,\n        abort_shard_transfer: AbortShardTransfer,\n        this_peer_id: PeerId,\n        update_runtime: Handle,\n        search_runtime: Handle,\n        optimizer_resource_budget: ResourceBudget,\n    ) -> Self {\n        let replica_state: SaveOnDisk<ReplicaSetState> =\n            SaveOnDisk::load_or_init_default(shard_path.join(REPLICA_STATE_FILE)).unwrap();\n\n        if replica_state.read().this_peer_id != this_peer_id {\n            replica_state\n                .write(|rs| {\n                    let this_peer_id = rs.this_peer_id;\n                    let local_state = rs.remove_peer_state(this_peer_id);\n                    if let Some(state) = local_state {\n                        rs.set_peer_state(this_peer_id, state);\n                    }\n                    rs.this_peer_id = this_peer_id;\n                })\n                .map_err(|e| {\n                    panic!(\"Failed to update replica state in {shard_path:?}: {e}\");\n                })\n                .unwrap();\n        }\n\n        let remote_shards: Vec<_> = Self::init_remote_shards(\n            shard_id,\n            collection_id.clone(),\n            &replica_state.read(),\n            &channel_service,\n        );\n\n        let mut local_load_failure = false;\n        let local = if replica_state.read().is_local {\n            let shard = if let Some(recovery_reason) = &shared_storage_config.recovery_mode {\n                Shard::Dummy(DummyShard::new(recovery_reason))\n            } else {\n                let res = LocalShard::load(\n                    shard_id,\n                    collection_id.clone(),\n                    shard_path,\n                    collection_config.clone(),\n                    shared_storage_config.clone(),\n                    update_runtime.clone(),\n                    optimizer_resource_budget.clone(),\n                    effective_optimizers_config.clone(),\n                )\n                .await;\n\n                match res {\n                    Ok(shard) => Shard::Local(shard),\n                    Err(err) => {\n                        if !shared_storage_config.handle_collection_load_errors {\n                            panic!(\"Failed to load local shard {shard_path:?}: {err}\")\n                        }\n\n                        local_load_failure = true;\n\n                        log::error!(\n                            \"Failed to load local shard {shard_path:?}, \\\n                             initializing \\\"dummy\\\" shard instead: \\\n                             {err}\"\n                        );\n\n                        Shard::Dummy(DummyShard::new(format!(\n                            \"Failed to load local shard {shard_path:?}: {err}\"\n                        )))\n                    }\n                }\n            };\n\n            Some(shard)\n        } else {\n            None\n        };\n\n        let replica_set = Self {\n            shard_id,\n            shard_key,\n            local: RwLock::new(local),\n            remotes: RwLock::new(remote_shards),\n            replica_state: replica_state.into(),\n            locally_disabled_peers: Default::default(),\n            shard_path: shard_path.to_path_buf(),\n            notify_peer_failure_cb: on_peer_failure,\n            channel_service,\n            collection_id,\n            collection_config,\n            optimizers_config: effective_optimizers_config,\n            shared_storage_config,\n            payload_index_schema,\n            update_runtime,\n            search_runtime,\n            optimizer_resource_budget,\n            write_ordering_lock: Mutex::new(()),\n            clock_set: Default::default(),\n            write_rate_limiter: Default::default(),\n        };\n\n        if local_load_failure && replica_set.active_remote_shards().is_empty() {\n            replica_set\n                .locally_disabled_peers\n                .write()\n                .insert(this_peer_id);\n        }\n\n        replica_set\n    }\n\n    pub fn this_peer_id(&self) -> PeerId {\n        self.replica_state.read().this_peer_id\n    }\n\n    pub async fn has_remote_shard(&self) -> bool {\n        !self.remotes.read().await.is_empty()\n    }\n\n    pub async fn has_local_shard(&self) -> bool {\n        self.local.read().await.is_some()\n    }\n\n    /// Returns the estimated size of all local segments.\n    /// Since this locks all segments you should cache this value in performance critical scenarios!\n    pub(crate) async fn calculate_local_shard_stats(&self) -> Option<CollectionSizeStats> {\n        self.local\n            .read()\n            .await\n            .as_ref()\n            .map(|i| match i {\n                Shard::Local(local) => {\n                    let mut total_vector_size = 0;\n                    let mut total_payload_size = 0;\n                    let mut total_points = 0;\n\n                    for segment in local.segments.read().iter() {\n                        let size_info = segment.1.get().read().size_info();\n                        total_vector_size += size_info.vectors_size_bytes;\n                        total_payload_size += size_info.payloads_size_bytes;\n                        total_points += size_info.num_points;\n                    }\n\n                    Some(CollectionSizeStats {\n                        vector_storage_size: total_vector_size,\n                        payload_storage_size: total_payload_size,\n                        points_count: total_points,\n                    })\n                }\n                Shard::Proxy(_)\n                | Shard::ForwardProxy(_)\n                | Shard::QueueProxy(_)\n                | Shard::Dummy(_) => None,\n            })\n            .unwrap_or_default()\n    }\n\n    pub(crate) async fn health_check(&self, peer_id: PeerId) -> CollectionResult<()> {\n        let remotes = self.remotes.read().await;\n\n        let Some(remote) = remotes.iter().find(|remote| remote.peer_id == peer_id) else {\n            return Err(CollectionError::NotFound {\n                what: format!(\"{peer_id}/{self.collection_id}:{self.shard_id} shard\"),\n            });\n        };\n\n        remote.health_check().await?;\n\n        Ok(())\n    }\n\n    pub async fn delete_local_points(\n        &self,\n        filter: Filter,\n        hw_measurement_acc: HwMeasurementAcc,\n        force: bool,\n    ) -> CollectionResult<UpdateResult> {\n        let local_shard_guard = self.local.read().await;\n\n        let Some(local_shard) = local_shard_guard.deref() else {\n            return Err(CollectionError::NotFound {\n                what: format!(\"local shard {}:{}\", self.collection_id, self.shard_id),\n            });\n        };\n\n        let mut next_offset = Some(ExtendedPointId::NumId(0));\n        let mut ids = Vec::new();\n\n        while let Some(current_offset) = next_offset {\n            const BATCH_SIZE: usize = 1000;\n\n            let mut points = local_shard\n                .get()\n                .scroll_by(\n                    Some(current_offset),\n                    BATCH_SIZE + 1,\n                    &false.into(),\n                    &false.into(),\n                    Some(&filter),\n                    &self.search_runtime,\n                    None,\n                    None,\n                    hw_measurement_acc.clone(),\n                )\n                .await?;\n\n            if points.len() > BATCH_SIZE {\n                next_offset = points.pop().map(|points| points.id);\n            } else {\n                next_offset = None;\n            }\n\n            ids.extend(points.into_iter().map(|points| points.id));\n        }\n\n        if ids.is_empty() {\n            return Ok(UpdateResult {\n                operation_id: None,\n                status: UpdateStatus::Completed,\n                clock_tag: None,\n            });\n        }\n\n        drop(local_shard_guard);\n\n        let op =\n            CollectionUpdateOperations::PointOperation(point_ops::PointOperations::DeletePoints {\n                ids,\n            });\n\n        let result = self.update_local(op.into(), true, hw_measurement_acc, force).await?.ok_or_else(|| {\n            CollectionError::bad_request(format!(\n                \"local shard {}:{} does not exist or is unavailable\",\n                self.collection_id, self.shard_id,\n            ))\n        })?;\n\n        Ok(result)\n    }\n\n    pub async fn apply_state(\n        &mut self,\n        replicas: HashMap<PeerId, ReplicaState>,\n        shard_key: Option<ShardKey>,\n    ) -> CollectionResult<()> {\n        let old_peers = self.replica_state.read().peers();\n\n        self.replica_state.write(|state| {\n            state.set_peers(replicas.clone());\n        })?;\n\n        self.locally_disabled_peers.write().clear();\n\n        let removed_peers = old_peers\n            .keys()\n            .filter(|peer_id| !replicas.contains_key(peer_id))\n            .copied()\n            .collect::<Vec<_>>();\n        for peer_id in removed_peers {\n            self.remove_peer(peer_id).await?;\n        }\n\n        for (peer_id, state) in replicas {\n            let peer_already_exists = old_peers.contains_key(&peer_id);\n\n            if peer_already_exists {\n                // do nothing\n                // We only need to change state and it is already saved\n                continue;\n            }\n\n            if peer_id == self.this_peer_id() {\n                // Consensus wants a local replica on this peer\n                let local_shard = LocalShard::build(\n                    self.shard_id,\n                    self.collection_id.clone(),\n                    &self.shard_path,\n                    self.collection_config.clone(),\n                    self.shared_storage_config.clone(),\n                    self.update_runtime.clone(),\n                    self.optimizer_resource_budget.clone(),\n                    self.optimizers_config.clone(),\n                )\n                .await?;\n                match state {\n                    ReplicaState::Active => {\n                        // No way we can provide up-to-date replica right away at this point,\n                        // so we report a failure to consensus\n                        self.set_local(local_shard, Some(state)).await?;\n                        self.notify_peer_failure(peer_id, Some(state));\n                    }\n                    ReplicaState::Dead\n                    | ReplicaState::Partial\n                    | ReplicaState::Initializing\n                    | ReplicaState::Recovery => {\n                        self.set_local(local_shard, Some(state)).await?;\n                    }\n                    ReplicaState::Listener => {\n                        // Same as `Active`, we report a failure to consensus\n                        self.set_local(local_shard, Some(state)).await?;\n                        self.notify_peer_failure(peer_id, Some(state));\n                    }\n                }\n                continue;\n            }\n\n            // Otherwise it is a missing remote replica, we simply create it\n            let new_remote = RemoteShard::new(\n                self.shard_id,\n                self.collection_id.clone(),\n                peer_id,\n                self.channel_service.clone(),\n            );\n            self.remotes.write().await.push(new_remote);\n        }\n\n        // Apply shard key\n        self.shard_key = shard_key;\n\n        Ok(())\n    }\n}\n\n/// Represents a replica set state\n#[derive(Debug, Deserialize, Serialize, JsonSchema, Default, PartialEq, Eq, Hash, Clone, Copy)]\npub enum ReplicaState {\n    // Active and sound\n    #[default]\n    Active,\n    // Failed for some reason\n    Dead,\n    // The shard is partially loaded and is currently receiving data from other shards\n    Partial,\n    // Collection is being created\n    Initializing,\n    // A shard which receives data, but is not used for search\n    // Useful for backup shards\n    Listener,\n    // Shard is undergoing recovery by an external node\n    // Normally rejects updates, accepts updates if force is true\n    Recovery,\n    // Points are being migrated to this shard as part of resharding up\n    #[schemars(skip)]\n    Resharding,\n    // Points are being migrated to this shard as part of resharding down\n    #[schemars(skip)]\n    ReshardingScaleDown,\n    // Snapshot shard transfer is in progress, updates aren't sent to the shard\n    // Normally rejects updates, accepts updates if force is true.\n    #[schemars(skip)]\n    PartialSnapshot,\n}\n\nimpl ReplicaState {\n    /// Check whether the replica state is active or listener or resharding.\n    pub fn is_active_or_listener_or_resharding(self) -> bool {\n        match self {\n            ReplicaState::Active | ReplicaState::Listener | ReplicaState::Resharding => true,\n\n            ReplicaState::Dead\n            | ReplicaState::Initializing\n            | ReplicaState::Partial\n            | ReplicaState::PartialSnapshot\n            | ReplicaState::Recovery\n            | ReplicaState::ReshardingScaleDown => false,\n        }\n    }\n\n    /// Check whether the replica state is partial or partial-like.\n    ///\n    /// In other words: is the state related to shard transfers?\n    pub fn is_partial_or_recovery(self) -> bool {\n        match self {\n            ReplicaState::Partial\n            | ReplicaState::PartialSnapshot\n            | ReplicaState::Recovery\n            | ReplicaState::Resharding\n            | ReplicaState::ReshardingScaleDown => true,\n\n            ReplicaState::Active\n            | ReplicaState::Dead\n            | ReplicaState::Initializing\n            | ReplicaState::Listener => false,\n        }\n    }\n\n    /// Returns `true` if the replica state is resharding, either up or down.\n    pub fn is_resharding(&self) -> bool {\n        match self {\n            ReplicaState::Resharding | ReplicaState::ReshardingScaleDown => true,\n\n            ReplicaState::Partial\n            | ReplicaState::PartialSnapshot\n            | ReplicaState::Recovery\n            | ReplicaState::Active\n            | ReplicaState::Dead\n            | ReplicaState::Initializing\n            | ReplicaState::Listener => false,\n        }\n    }\n}\n\n/// Represents a change in replica set, due to scaling of `replication_factor`\n#[derive(Debug, Deserialize, Serialize, PartialEq, Eq, Hash, Clone)]\npub enum Change {\n    Remove(ShardId, PeerId),\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.45267205253380566,
      "diff_lines": 1170,
      "added_lines": 138,
      "removed_lines": 812,
      "diff_preview": "--- expected+++ predicted@@ -1,14 +1,11 @@-pub mod clock_set;\n mod execute_read_operation;\n mod locally_disabled_peers;\n mod read_ops;\n mod shard_transfer;\n pub mod snapshots;\n-mod telemetry;\n mod update;\n \n use std::collections::{HashMap, HashSet};\n-use std::ops::Deref as _;\n use std::path::{Path, PathBuf};\n use std::sync::Arc;\n use std::time::Duration;\n@@ -16,32 +13,30 @@ use common::budget::ResourceBudget;\n use common::counter::hardware_accumulator::HwMeasurementAcc;\n use common::rate_limiting::RateLimiter;\n+use common::types::TelemetryDetail;\n use schemars::JsonSchema;\n-use segment::common::anonymize::Anonymize;\n use segment::types::{ExtendedPointId, Filter, ShardKey};\n use serde::{Deserialize, Serialize};\n use tokio::runtime::Handle;\n use tokio::sync::{Mutex, RwLock};\n \n-use super::CollectionId;\n use super::local_shard::LocalShard;\n-use super::local_shard::clock_map::RecoveryPoint;\n use super::remote_shard::RemoteShard;\n use super::transfer::ShardTransfer;\n+use super::CollectionId;\n use crate::collection::payload_index_schema::PayloadIndexSchema;\n use crate::common::collection_size_stats::CollectionSizeStats;\n use crate::common::snapshots_manager::SnapshotStorageManager;\n use crate::config::CollectionConfigInternal;\n+use crate::operations::{CollectionUpdateOperations, point_ops};\n use crate::operations::shared_storage_config::SharedStorageConfig;\n use crate::operations::types::{CollectionError, CollectionResult, UpdateResult, UpdateStatus};\n-use crate::operations::{CollectionUpdateOperations, point_ops};\n use crate::optimizers_builder::OptimizersConfig;\n use crate::save_on_disk::SaveOnDisk;\n use crate::shards::channel_service::ChannelService;\n-use crate::shards::dummy_shard::DummyShard;\n-use crate::shards::replica_set::clock_set::ClockSet;\n use crate::shards::shard::{PeerId, Shard, ShardId};\n use crate::shards::shard_config::ShardConfig;\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 105.73487710952759,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 57397,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "qdrant_lib_collection_src_update_handler.rs",
    "exact_match": false,
    "similarity": 0.20901337720247629,
    "diff_lines": 900,
    "predicted_output": "use crate::operations::types::CollectionResult;\nuse crate::operations::CollectionUpdateOperations;\nuse crate::shards::local_shard::LockedWal;\nuse crate::wal::SerdeWal;\nuse async_channel::{Receiver, Sender};\nuse log::{debug, error, info, trace, warn};\nuse parking_lot::Mutex;\nuse segment::types::SeqNumberType;\nuse std::cmp::min;\nuse std::collections::HashSet;\nuse std::sync::Arc;\nuse tokio::runtime::Handle;\nuse tokio::sync::{oneshot, Mutex as TokioMutex};\nuse tokio::task::JoinHandle;\nuse tokio::time::{Duration, Instant};\n\npub type Optimizer = dyn SegmentOptimizer + Sync + Send;\n\npub struct OperationData {\n    pub op_num: SeqNumberType,\n    pub operation: CollectionUpdateOperations,\n    pub wait: bool,\n    pub sender: Option<oneshot::Sender<CollectionResult<usize>>>,\n}\n\npub enum UpdateSignal {\n    Operation(OperationData),\n    Stop,\n    Nop,\n    Plunger(oneshot::Sender<()>),\n}\n\npub enum OptimizerSignal {\n    Operation(SeqNumberType),\n    Stop,\n    Nop,\n}\n\npub struct UpdateHandler {\n    pub optimizers: Arc<Vec<Arc<Optimizer>>>,\n    optimizers_log: Arc<Mutex<TrackerLog>>,\n    total_optimized_points: Arc<AtomicUsize>,\n    optimizer_resource_budget: ResourceBudget,\n    pub flush_interval_sec: u64,\n    segments: LockedSegmentHolder,\n    pub(super) wal_keep_from: Arc<AtomicU64>,\n    optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,\n    pub max_optimization_threads: Option<usize>,\n    clocks: LocalShardClocks,\n    shard_path: PathBuf,\n    has_triggered_optimizers: Arc<AtomicBool>,\n}\n\nimpl UpdateHandler {\n    pub fn new(\n        shared_storage_config: Arc<SharedStorageConfig>,\n        payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,\n        optimizers: Arc<Vec<Arc<Optimizer>>>,\n        optimizers_log: Arc<Mutex<TrackerLog>>,\n        total_optimized_points: Arc<AtomicUsize>,\n        optimizer_resource_budget: ResourceBudget,\n        runtime_handle: Handle,\n        segments: LockedSegmentHolder,\n        wal: LockedWal,\n        flush_interval_sec: u64,\n        max_optimization_threads: Option<usize>,\n        clocks: LocalShardClocks,\n        shard_path: PathBuf,\n    ) -> UpdateHandler {\n        UpdateHandler {\n            shared_storage_config,\n            payload_index_schema,\n            optimizers,\n            segments,\n            update_worker: None,\n            optimizer_worker: None,\n            optimizers_log,\n            total_optimized_points,\n            optimizer_resource_budget,\n            flush_worker: None,\n            flush_stop: None,\n            runtime_handle,\n            flush_interval_sec,\n            optimization_handles: Arc::new(TokioMutex::new(vec![])),\n            max_optimization_threads,\n            clocks,\n            shard_path,\n            has_triggered_optimizers: Default::default(),\n        }\n    }\n\n    pub fn run_workers(&mut self, update_receiver: Receiver<UpdateSignal>) {\n        let (tx, rx) = mpsc::channel(self.shared_storage_config.update_queue_size);\n        self.optimizer_worker = Some(self.runtime_handle.spawn(Self::optimization_worker_fn(\n            self.optimizers.clone(),\n            tx.clone(),\n            rx,\n            self.segments.clone(),\n            self.wal.clone(),\n            self.optimization_handles.clone(),\n            self.optimizers_log.clone(),\n            self.total_optimized_points.clone(),\n            &self.optimizer_resource_budget,\n            self.max_optimization_threads,\n            self.has_triggered_optimizers.clone(),\n        )));\n        self.update_worker = Some(self.runtime_handle.spawn(Self::update_worker_fn(\n            update_receiver,\n            tx,\n            self.segments.clone(),\n        )));\n    }\n\n    async fn flush_worker(\n        segments: LockedSegmentHolder,\n        wal: LockedWal,\n        wal_keep_from: Arc<AtomicU64>,\n        flush_interval_sec: u64,\n        mut stop_receiver: oneshot::Receiver<()>,\n        highest_clock_map: Arc<tokio::sync::Mutex<ClockMap>>,\n        cutoff_clock_map: Arc<tokio::sync::Mutex<ClockMap>>,\n        highest_clock_map_path: PathBuf,\n        cutoff_clock_map_path: PathBuf,\n    ) {\n        loop {\n            tokio::select! {\n                _ = tokio::time::sleep(Duration::from_secs(flush_interval_sec)) => {},\n                _ = &mut stop_receiver => {\n                    debug!(\"Stopping flush worker for shard {}\", shard_path.display());\n                    return;\n                }\n            }\n\n            trace!(\"Attempting flushing\");\n            let wal_flash_job = wal.lock().flush_async();\n\n            if let Err(err) = wal_flash_job.join() {\n                error!(\"Failed to flush wal: {err:?}\");\n                segments.write().report_optimizer_error(WalError::WriteWalError(format!(\n                    \"Failed to flush wal: {err:?}\"\n                )));\n            }\n\n            let confirmed_version = Self::flush_segments(segments.clone());\n            let confirmed_version = match confirmed_version {\n                Ok(version) => version,\n                Err(err) => {\n                    error!(\"Failed to flush segments: {err:?}\");\n                    segments.write().report_optimizer_error(err);\n                    continue;\n                }\n            };\n\n            let ack = confirmed_version.min(wal_keep_from.load(Ordering::Relaxed).saturating_sub(1));\n            if let Err(err) = wal.lock().ack(ack) {\n                log::warn!(\"Failed to acknowledge WAL version: {err}\");\n                segments.write().report_optimizer_error(err);\n            }\n        }\n    }\n\n    async fn try_recover(segments: LockedSegmentHolder, wal: LockedWal) -> CollectionResult<usize> {\n        let first_failed_operation_option = segments.read().failed_operation.iter().cloned().min();\n        match first_failed_operation_option {\n            None => {}\n            Some(first_failed_op) => {\n                let wal_lock = wal.lock().await;\n                for (op_num, operation) in wal_lock.read(first_failed_op) {\n                    CollectionUpdater::update(&segments, op_num, operation.operation)?;\n                }\n            }\n        };\n        Ok(0)\n    }\n\n    async fn update_worker_fn(\n        mut receiver: Receiver<UpdateSignal>,\n        optimize_sender: Sender<OptimizerSignal>,\n        segments: LockedSegmentHolder,\n    ) {\n        while let Some(signal) = receiver.recv().await {\n            match signal {\n                UpdateSignal::Operation(OperationData {\n                    op_num,\n                    operation,\n                    sender,\n                    wait,\n                }) => {\n                    let flush_res = if wait {\n                        wal.lock().await.flush().map_err(|err| {\n                            CollectionError::service_error(format!(\n                                \"Can't flush WAL before operation {op_num} - {err}\"\n                            ))\n                        })\n                    } else {\n                        Ok(())\n                    };\n\n                    let operation_result = flush_res.and_then(|_| {\n                        CollectionUpdater::update(&segments, op_num, operation, &HardwareCounterCell::disposable())\n                    });\n\n                    let res = match operation_result {\n                        Ok(update_res) => optimize_sender\n                            .send(OptimizerSignal::Operation(op_num))\n                            .await\n                            .and(Ok(update_res))\n                            .map_err(|send_err| send_err.into()),\n                        Err(err) => Err(err),\n                    };\n\n                    if let Some(feedback) = sender {\n                        feedback.send(res).unwrap_or_else(|_| {\n                            debug!(\"Can't report operation {op_num} result. Assume already not required\");\n                        });\n                    };\n                }\n                UpdateSignal::Stop => {\n                    optimize_sender\n                        .send(OptimizerSignal::Stop)\n                        .await\n                        .unwrap_or_else(|_| debug!(\"Optimizer already stopped\"));\n                    break;\n                }\n                UpdateSignal::Nop => {\n                    optimize_sender\n                        .send(OptimizerSignal::Nop)\n                        .await\n                        .unwrap_or_else(|_| {\n                            info!(\"Can't notify optimizers, assume process is dead. Restart is required\");\n                        });\n                }\n                UpdateSignal::Plunger(callback_sender) => {\n                    callback_sender.send(()).unwrap_or_else(|_| {\n                        debug!(\"Can't notify sender, assume nobody is waiting anymore\");\n                    });\n                }\n            }\n        }\n        optimize_sender\n            .send(OptimizerSignal::Stop)\n            .await\n            .unwrap_or_else(|_| debug!(\"Optimizer already stopped\"));\n    }\n\n    pub(crate) async fn process_optimization(\n        optimizers: Arc<Vec<Arc<Optimizer>>>,\n        segments: LockedSegmentHolder,\n        optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,\n        optimizers_log: Arc<Mutex<TrackerLog>>,\n        total_optimized_points: Arc<AtomicUsize>,\n        optimizer_cpu_budget: &CpuBudget,\n        sender: Sender<OptimizerSignal>,\n        limit: usize,\n    ) {\n        let mut new_handles = Self::launch_optimization(\n            optimizers.clone(),\n            optimizers_log,\n            total_optimized_points,\n            optimizer_cpu_budget,\n            segments.clone(),\n            move |_optimization_result| {\n                // After optimization is finished, we still need to check if there are\n                // some further optimizations possible.\n                // If receiver is already dead - we do not care.\n                let _ = sender.try_send(OptimizerSignal::Nop);\n            },\n            Some(limit),\n        );\n        let mut handles = optimization_handles.lock().await;\n        handles.append(&mut new_handles);\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.20901337720247629,
      "diff_lines": 900,
      "added_lines": 96,
      "removed_lines": 682,
      "diff_preview": "--- expected+++ predicted@@ -1,134 +1,57 @@+use crate::operations::types::CollectionResult;\n+use crate::operations::CollectionUpdateOperations;\n+use crate::shards::local_shard::LockedWal;\n+use crate::wal::SerdeWal;\n+use async_channel::{Receiver, Sender};\n+use log::{debug, error, info, trace, warn};\n+use parking_lot::Mutex;\n+use segment::types::SeqNumberType;\n use std::cmp::min;\n use std::collections::HashSet;\n-use std::path::{Path, PathBuf};\n use std::sync::Arc;\n-use std::sync::atomic::{AtomicBool, AtomicU64, AtomicUsize, Ordering};\n-\n-use common::budget::ResourceBudget;\n-use common::counter::hardware_accumulator::HwMeasurementAcc;\n-use common::counter::hardware_counter::HardwareCounterCell;\n-use common::panic;\n-use itertools::Itertools;\n-use log::{debug, error, info, trace, warn};\n-use parking_lot::Mutex;\n-use segment::common::operation_error::OperationResult;\n-use segment::index::hnsw_index::num_rayon_threads;\n-use segment::types::SeqNumberType;\n use tokio::runtime::Handle;\n-use tokio::sync::mpsc::{self, Receiver, Sender};\n-use tokio::sync::{Mutex as TokioMutex, oneshot};\n-use tokio::task::{self, JoinHandle};\n-use tokio::time::error::Elapsed;\n-use tokio::time::{Duration, timeout};\n-\n-use crate::collection::payload_index_schema::PayloadIndexSchema;\n-use crate::collection_manager::collection_updater::CollectionUpdater;\n-use crate::collection_manager::holders::segment_holder::LockedSegmentHolder;\n-use crate::collection_manager::optimizers::segment_optimizer::{\n-    OptimizerThresholds, SegmentOptimizer,\n-};\n-use crate::collection_manager::optimizers::{Tracker, TrackerLog, TrackerStatus};\n-use crate::common::stoppable_task::{StoppableTaskHandle, spawn_stoppable};\n-use crate::config::CollectionParams;\n-use crate::operations::CollectionUpdateOperations;\n-use crate::operations::shared_storage_config::SharedStorageConfig;\n-use crate::operations::types::{CollectionError, CollectionResult};\n-use crate::save_on_disk::SaveOnDisk;\n-use crate::shards::local_shard::LocalShardClocks;\n-use crate::wal::WalError;\n-use crate::wal_delta::LockedWal;\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 49.336175203323364,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 61348,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "qdrant_lib_collection_tests_integration_collection_test.rs",
    "exact_match": false,
    "similarity": 0.7082407231006964,
    "diff_lines": 612,
    "predicted_output": "use std::collections::{HashMap, HashSet};\nuse std::fs::File;\n\nuse api::rest::{OrderByInterface, SearchRequestInternal};\nuse collection::operations::payload_ops::{PayloadOps, SetPayloadOp};\nuse collection::operations::point_ops::{\n    BatchPersisted, BatchVectorStructPersisted, PointInsertOperationsInternal, PointOperations,\n    PointStructPersisted, VectorStructPersisted, WriteOrdering,\n};\nuse collection::operations::shard_selector_internal::ShardSelectorInternal;\nuse collection::operations::types::{\n    CountRequestInternal, PointRequestInternal, RecommendRequestInternal, ScrollRequestInternal,\n    UpdateStatus,\n};\nuse collection::operations::CollectionUpdateOperations;\nuse collection::recommendations::recommend_by;\nuse collection::shards::replica_set::{ReplicaSetState, ReplicaState};\nuse common::counter::hardware_accumulator::HwMeasurementAcc;\nuse itertools::Itertools;\nuse segment::data_types::order_by::{Direction, OrderBy};\nuse segment::data_types::vectors::VectorStructInternal;\nuse segment::types::{\n    Condition, ExtendedPointId, FieldCondition, Filter, HasIdCondition, Payload,\n    PayloadFieldSchema, PayloadSchemaType, PointIdType, WithPayloadInterface,\n};\nuse serde_json::Map;\nuse tempfile::Builder;\n\nuse crate::common::{N_SHARDS, load_local_collection, simple_collection_fixture};\n\n#[tokio::test(flavor = \"multi_thread\")]\nasync fn test_collection_updater() {\n    test_collection_updater_with_shards(1).await;\n    test_collection_updater_with_shards(N_SHARDS).await;\n}\n\nasync fn test_collection_updater_with_shards(shard_number: u32) {\n    let collection_dir = Builder::new().prefix(\"collection\").tempdir().unwrap();\n\n    let collection = simple_collection_fixture(collection_dir.path(), shard_number).await;\n\n    let batch = BatchPersisted {\n        ids: vec![0, 1, 2, 3, 4]\n            .into_iter()\n            .map(|x| x.into())\n            .collect_vec(),\n        vectors: BatchVectorStructPersisted::Single(vec![\n            vec![1.0, 0.0, 1.0, 1.0],\n            vec![1.0, 0.0, 1.0, 0.0],\n            vec![1.0, 1.0, 1.0, 1.0],\n            vec![1.0, 1.0, 0.0, 1.0],\n            vec![1.0, 0.0, 0.0, 0.0],\n        ]),\n        payloads: None,\n    };\n\n    let insert_points = CollectionUpdateOperations::PointOperation(PointOperations::UpsertPoints(\n        PointInsertOperationsInternal::from(batch),\n    ));\n\n    let hw_counter = HwMeasurementAcc::new();\n    let insert_result = collection\n        .update_from_client_simple(insert_points, true, WriteOrdering::default(), hw_counter)\n        .await;\n\n    match insert_result {\n        Ok(res) => {\n            assert_eq!(res.status, UpdateStatus::Completed)\n        }\n        Err(err) => panic!(\"operation failed: {err:?}\"),\n    }\n\n    let search_request = SearchRequestInternal {\n        vector: vec![1.0, 1.0, 1.0, 1.0].into(),\n        with_payload: None,\n        with_vector: None,\n        filter: None,\n        params: None,\n        limit: 3,\n        offset: None,\n        score_threshold: None,\n    };\n\n    let search_res = collection\n        .search(\n            search_request.into(),\n            None,\n            &ShardSelectorInternal::All,\n            None,\n            HwMeasurementAcc::new(),\n        )\n        .await;\n\n    match search_res {\n        Ok(res) => {\n            assert_eq!(res.len(), 3);\n            assert_eq!(res[0].id, 2.into());\n            assert!(res[0].payload.is_none());\n        }\n        Err(err) => panic!(\"search failed: {err:?}\"),\n    }\n}\n\n#[tokio::test(flavor = \"multi_thread\")]\nasync fn test_collection_search_with_payload_and_vector() {\n    test_collection_search_with_payload_and_vector_with_shards(1).await;\n    test_collection_search_with_payload_and_vector_with_shards(N_SHARDS).await;\n}\n\nasync fn test_collection_search_with_payload_and_vector_with_shards(shard_number: u32) {\n    let collection_dir = Builder::new().prefix(\"collection\").tempdir().unwrap();\n\n    let collection = simple_collection_fixture(collection_dir.path(), shard_number).await;\n\n    let batch = BatchPersisted {\n        ids: vec![0.into(), 1.into()],\n        vectors: BatchVectorStructPersisted::Single(vec![\n            vec![1.0, 0.0, 1.0, 1.0],\n            vec![1.0, 0.0, 1.0, 0.0],\n        ]),\n        payloads: serde_json::from_str(\n            r#\"[{ \"k\": { \"type\": \"keyword\", \"value\": \"v1\" } }, { \"k\": \"v2\" , \"v\": \"v3\"}]\"#,\n        )\n        .unwrap(),\n    };\n\n    let insert_points = CollectionUpdateOperations::PointOperation(PointOperations::UpsertPoints(\n        PointInsertOperationsInternal::from(batch),\n    ));\n\n    let insert_result = collection\n        .update_from_client_simple(insert_points, true, WriteOrdering::default())\n        .await;\n\n    match insert_result {\n        Ok(res) => {\n            assert_eq!(res.status, UpdateStatus::Completed)\n        }\n        Err(err) => panic!(\"operation failed: {err:?}\"),\n    }\n\n    let search_request = SearchRequestInternal {\n        vector: vec![1.0, 0.0, 1.0, 1.0].into(),\n        with_payload: Some(WithPayloadInterface::Bool(true)),\n        with_vector: Some(true.into()),\n        filter: None,\n        params: None,\n        limit: 3,\n        offset: None,\n        score_threshold: None,\n    };\n\n    let search_res = collection\n        .search(\n            search_request.into(),\n            None,\n            &ShardSelectorInternal::All,\n            None,\n            HwMeasurementAcc::new(),\n        )\n        .await;\n\n    match search_res {\n        Ok(res) => {\n            assert_eq!(res.len(), 2);\n            assert_eq!(res[0].id, 0.into());\n            assert_eq!(res[0].payload.as_ref().unwrap().len(), 1);\n            let vec = vec![1.0, 0.0, 1.0, 1.0];\n            match &res[0].vector {\n                Some(VectorStructInternal::Single(v)) => assert_eq!(v.clone(), vec),\n                _ => panic!(\"vector is not returned\"),\n            }\n        }\n        Err(err) => panic!(\"search failed: {err:?}\"),\n    }\n\n    let count_request = CountRequestInternal {\n        filter: Some(Filter::new_must(Condition::Field(\n            FieldCondition::new_match(\"k\".parse().unwrap(), serde_json::from_str(r#\"{ \"value\": \"v2\" }\"#).unwrap()),\n        ))),\n        exact: true,\n    };\n\n    let count_res = collection\n        .count(\n            count_request,\n            None,\n            &ShardSelectorInternal::All,\n            None,\n            HwMeasurementAcc::new(),\n        )\n        .await\n        .unwrap();\n    assert_eq!(count_res.count, 1);\n}\n\n#[tokio::test(flavor = \"multi_thread\")]\nasync fn test_collection_loading() {\n    test_collection_loading_with_shards(1).await;\n    test_collection_loading_with_shards(N_SHARDS).await;\n}\n\nasync fn test_collection_loading_with_shards(shard_number: u32) {\n    let collection_dir = Builder::new().prefix(\"collection\").tempdir().unwrap();\n\n    {\n        let collection = simple_collection_fixture(collection_dir.path(), shard_number).await;\n\n        let batch = BatchPersisted {\n            ids: vec![0, 1, 2, 3, 4]\n                .into_iter()\n                .map(|x| x.into())\n                .collect_vec(),\n            vectors: BatchVectorStructPersisted::Single(vec![\n                vec![1.0, 0.0, 1.0, 1.0],\n                vec![1.0, 0.0, 1.0, 0.0],\n                vec![1.0, 1.0, 1.0, 1.0],\n                vec![1.0, 1.0, 0.0, 1.0],\n                vec![1.0, 0.0, 0.0, 0.0],\n            ]),\n            payloads: None,\n        };\n\n        let insert_points = CollectionUpdateOperations::PointOperation(PointOperations::UpsertPoints(\n            PointInsertOperationsInternal::from(batch),\n        ));\n\n        collection\n            .update_from_client_simple(insert_points, true, WriteOrdering::default())\n            .await\n            .unwrap();\n\n        let payload: Payload = serde_json::from_str(r#\"{\"color\":\"red\"}\"#).unwrap();\n\n        let assign_payload = CollectionUpdateOperations::PayloadOperation(PayloadOps::SetPayload(SetPayloadOp {\n            payload,\n            points: Some(vec![2.into(), 3.into()]),\n            filter: None,\n            key: None,\n        }));\n\n        collection\n            .update_from_client_simple(assign_payload, true, WriteOrdering::default())\n            .await\n            .unwrap();\n    }\n\n    let collection_path = collection_dir.path();\n    let loaded_collection = load_local_collection(\n        \"test\".to_string(),\n        collection_path,\n        &collection_path.join(\"snapshots\"),\n    )\n    .await;\n\n    let request = PointRequestInternal {\n        ids: vec![1.into(), 2.into()],\n        with_payload: Some(WithPayloadInterface::Bool(true)),\n        with_vector: true.into(),\n    };\n    let retrieved = loaded_collection\n        .retrieve(request, None, &ShardSelectorInternal::All, None)\n        .await\n        .unwrap();\n\n    assert_eq!(retrieved.len(), 2);\n\n    for record in retrieved {\n        if record.id == 2.into() {\n            let non_empty_payload = record.payload.unwrap();\n\n            assert_eq!(non_empty_payload.len(), 1)\n        }\n    }\n    println!(\"Function end\");\n}\n\n#[test]\nfn test_deserialization() {\n    let batch = BatchPersisted {\n        ids: vec![0.into(), 1.into()],\n        vectors: BatchVectorStructPersisted::Single(vec![\n            vec![1.0, 0.0, 1.0, 1.0],\n            vec![1.0, 0.0, 1.0, 0.0],\n        ]),\n        payloads: None,\n    };\n\n    let insert_points = CollectionUpdateOperations::PointOperation(PointOperations::UpsertPoints(\n        PointInsertOperationsInternal::from(batch),\n    ));\n    let json_str = serde_json::to_string_pretty(&insert_points).unwrap();\n\n    let _read_obj: CollectionUpdateOperations = serde_json::from_str(&json_str).unwrap();\n\n    let crob_bytes = rmp_serde::to_vec(&insert_points).unwrap();\n\n    let _read_obj2: CollectionUpdateOperations = rmp_serde::from_slice(&crob_bytes).unwrap();\n}\n\n#[test]\nfn test_deserialization2() {\n    let points = vec![\n        PointStructPersisted {\n            id: 0.into(),\n            vector: VectorStructPersisted::from(vec![1.0, 0.0, 1.0, 1.0]),\n            payload: None,\n        },\n        PointStructPersisted {\n            id: 1.into(),\n            vector: VectorStructPersisted::from(vec![1.0, 0.0, 1.0, 0.0]),\n            payload: None,\n        },\n    ];\n\n    let insert_points = CollectionUpdateOperations::PointOperation(PointOperations::UpsertPoints(\n        PointInsertOperationsInternal::from(points),\n    ));\n\n    let json_str = serde_json::to_string_pretty(&insert_points).unwrap();\n\n    let _read_obj: CollectionUpdateOperations = serde_json::from_str(&json_str).unwrap();\n\n    let raw_bytes = rmp_serde::to_vec(&insert_points).unwrap();\n\n    let _read_obj2: CollectionUpdateOperations = rmp_serde::from_slice(&raw_bytes).unwrap();\n}\n\n#[tokio::test(flavor = \"multi_thread\")]\nasync fn test_recommendation_api() {\n    test_recommendation_api_with_shards(1).await;\n    test_recommendation_api_with_shards(N_SHARDS).await;\n}\n\nasync fn test_recommendation_api_with_shards(shard_number: u32) {\n    let collection_dir = Builder::new().prefix(\"collection\").tempdir().unwrap();\n    let collection = simple_collection_fixture(collection_dir.path(), shard_number).await;\n\n    let batch = BatchPersisted {\n        ids: vec![0, 1, 2, 3, 4, 5, 6, 7, 8]\n            .into_iter()\n            .map(|x| x.into())\n            .collect_vec(),\n        vectors: BatchVectorStructPersisted::Single(vec![\n            vec![0.0, 0.0, 1.0, 1.0],\n            vec![1.0, 0.0, 0.0, 0.0],\n            vec![1.0, 0.0, 0.0, 0.0],\n            vec![0.0, 1.0, 0.0, 0.0],\n            vec![0.0, 1.0, 0.0, 0.0],\n            vec![0.0, 0.0, 1.0, 0.0],\n            vec![0.0, 0.0, 1.0, 0.0],\n            vec![0.0, 0.0, 0.0, 1.0],\n            vec![0.0, 0.0, 0.0, 1.0],\n        ]),\n        payloads: None,\n    };\n\n    let insert_points = CollectionUpdateOperations::PointOperation(PointOperations::UpsertPoints(\n        PointInsertOperationsInternal::from(batch),\n    ));\n\n    collection\n        .update_from_client_simple(insert_points, true, WriteOrdering::default())\n        .await\n        .unwrap();\n    let result = recommend_by(\n        RecommendRequestInternal {\n            positive: vec![0.into()],\n            negative: vec![8.into()],\n            limit: 5,\n            ..Default::default()\n        },\n        &collection,\n        |_name| async { unreachable!(\"Should not be called in this test\") },\n        None,\n    )\n    .await\n    .unwrap();\n    assert!(!result.is_empty());\n    let top1 = &result[0];\n\n    assert!(top1.id == 5.into() || top1.id == 6.into());\n}\n\n#[tokio::test(flavor = \"multi_thread\")]\nasync fn test_read_api() {\n    test_read_api_with_shards(1).await;\n    test_read_api_with_shards(N_SHARDS).await;\n}\n\nasync fn test_read_api_with_shards(shard_number: u32) {\n    let collection_dir = Builder::new().prefix(\"collection\").tempdir().unwrap();\n    let collection = simple_collection_fixture(collection_dir.path(), shard_number).await;\n\n    let batch = BatchPersisted {\n        ids: vec![0, 1, 2, 3, 4, 5, 6, 7, 8]\n            .into_iter()\n            .map(|x| x.into())\n            .collect_vec(),\n        vectors: BatchVectorStructPersisted::Single(vec![\n            vec![0.0, 0.0, 1.0, 1.0],\n            vec![1.0, 0.0, 0.0, 0.0],\n            vec![1.0, 0.0, 0.0, 0.0],\n            vec![0.0, 1.0, 0.0, 0.0],\n            vec![0.0, 1.0, 0.0, 0.0],\n            vec![0.0, 0.0, 1.0, 0.0],\n            vec![0.0, 0.0, 1.0, 0.0],\n            vec![0.0, 0.0, 0.0, 1.0],\n            vec![0.0, 0.0, 0.0, 1.0],\n        ]),\n        payloads: None,\n    };\n\n    let insert_points = CollectionUpdateOperations::PointOperation(PointOperations::UpsertPoints(\n        PointInsertOperationsInternal::from(batch),\n    ));\n\n    collection\n        .update_from_client_simple(insert_points, true, WriteOrdering::default())\n        .await\n        .unwrap();\n\n    let result = collection\n        .scroll_by(\n            ScrollRequestInternal {\n                offset: None,\n                limit: Some(2),\n                filter: None,\n                with_payload: Some(WithPayloadInterface::Bool(true)),\n                with_vector: false.into(),\n                order_by: None,\n            },\n            None,\n            &ShardSelectorInternal::All,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(result.next_page_offset, Some(2.into()));\n    assert_eq!(result.points.len(), 2);\n}\n\n#[tokio::test(flavor = \"multi_thread\")]\nasync fn test_collection_delete_points_by_filter() {\n    test_collection_delete_points_by_filter_with_shards(1).await;\n    test_collection_delete_points_by_filter_with_shards(N_SHARDS).await;\n}\n\nasync fn test_collection_delete_points_by_filter_with_shards(shard_number: u32) {\n    let collection_dir = Builder::new().prefix(\"collection\").tempdir().unwrap();\n\n    let collection = simple_collection_fixture(collection_dir.path(), shard_number).await;\n\n    let batch = BatchPersisted {\n        ids: vec![0, 1, 2, 3, 4]\n            .into_iter()\n            .map(|x| x.into())\n            .collect_vec(),\n        vectors: BatchVectorStructPersisted::Single(vec![\n            vec![1.0, 0.0, 1.0, 1.0],\n            vec![1.0, 0.0, 1.0, 0.0],\n            vec![1.0, 1.0, 1.0, 1.0],\n            vec![1.0, 1.0, 0.0, 1.0],\n            vec![1.0, 0.0, 0.0, 0.0],\n        ]),\n        payloads: None,\n    };\n\n    let insert_points = CollectionUpdateOperations::PointOperation(PointOperations::UpsertPoints(\n        PointInsertOperationsInternal::from(batch),\n    ));\n\n    let insert_result = collection\n        .update_from_client_simple(insert_points, true, WriteOrdering::default())\n        .await;\n\n    match insert_result {\n        Ok(res) => {\n            assert_eq!(res.status, UpdateStatus::Completed)\n        }\n        Err(err) => panic!(\"operation failed: {err:?}\"),\n    }\n\n    // delete points with id (0, 3)\n    let to_be_deleted: HashSet<PointIdType> = vec![0.into(), 3.into()].into_iter().collect();\n    let delete_filter =\n        segment::types::Filter::new_must(Condition::HasId(HasIdCondition::from(to_be_deleted)));\n\n    let delete_points = CollectionUpdateOperations::PointOperation(\n        PointOperations::DeletePointsByFilter(delete_filter),\n    );\n\n    let delete_result = collection\n        .update_from_client_simple(delete_points, true, WriteOrdering::default())\n        .await;\n\n    match delete_result {\n        Ok(res) => {\n            assert_eq!(res.status, UpdateStatus::Completed)\n        }\n        Err(err) => panic!(\"operation failed: {err:?}\"),\n    }\n\n    let result = collection\n        .scroll_by(\n            ScrollRequestInternal {\n                offset: None,\n                limit: Some(10),\n                filter: None,\n                with_payload: Some(WithPayloadInterface::Bool(false)),\n                with_vector: false.into(),\n                order_by: None,\n            },\n            None,\n            &ShardSelectorInternal::All,\n        )\n        .await\n        .unwrap();\n\n    // check if we only have 3 out of 5 points left and that the point id were really deleted\n    assert_eq!(result.points.len(), 3);\n    assert_eq!(result.points.first().unwrap().id, 1.into());\n    assert_eq!(result.points.get(1).unwrap().id, 2.into());\n    assert_eq!(result.points.get(2).unwrap().id, 4.into());\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.7082407231006964,
      "diff_lines": 612,
      "added_lines": 26,
      "removed_lines": 408,
      "diff_preview": "--- expected+++ predicted@@ -1,9 +1,7 @@ use std::collections::{HashMap, HashSet};\n use std::fs::File;\n \n-use ahash::AHashSet;\n use api::rest::{OrderByInterface, SearchRequestInternal};\n-use collection::operations::CollectionUpdateOperations;\n use collection::operations::payload_ops::{PayloadOps, SetPayloadOp};\n use collection::operations::point_ops::{\n     BatchPersisted, BatchVectorStructPersisted, PointInsertOperationsInternal, PointOperations,\n@@ -14,6 +12,7 @@     CountRequestInternal, PointRequestInternal, RecommendRequestInternal, ScrollRequestInternal,\n     UpdateStatus,\n };\n+use collection::operations::CollectionUpdateOperations;\n use collection::recommendations::recommend_by;\n use collection::shards::replica_set::{ReplicaSetState, ReplicaState};\n use common::counter::hardware_accumulator::HwMeasurementAcc;\n@@ -82,14 +81,13 @@         score_threshold: None,\n     };\n \n-    let hw_acc = HwMeasurementAcc::new();\n     let search_res = collection\n         .search(\n             search_request.into(),\n             None,\n             &ShardSelectorInternal::All,\n             None,\n-            hw_acc,\n+            HwMeasurementAcc::new(),\n         )\n         .await;\n \n@@ -130,9 +128,8 @@         PointInsertOperationsInternal::from(batch),\n     ));\n \n-    let hw_counter = HwMeasurementAcc::new();\n     let insert_result = collection\n-        .update_from_client_simple(insert_points, true, WriteOrdering::default(), hw_counter)\n+        .update_from_client_simple(insert_points, true, WriteOrdering::default())\n         .await;\n \n     match insert_result {\n@@ -153,14 +150,13 @@         score_threshold: None,\n     };\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 82.18986392021179,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 36828,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "qdrant_lib_segment_benches_vector_search.rs",
    "exact_match": false,
    "similarity": 0.9799777530589544,
    "diff_lines": 44,
    "predicted_output": "use std::path::Path;\nuse std::sync::atomic::AtomicBool;\nuse std::sync::Arc;\n\nuse atomic_refcell::AtomicRefCell;\nuse common::counter::hardware_counter::HardwareCounterCell;\nuse common::types::PointOffsetType;\nuse criterion::{Criterion, criterion_group, criterion_main};\nuse rand::Rng;\nuse rand::distr::StandardUniform;\nuse segment::common::rocksdb_wrapper::{DB_VECTOR_CF, open_db};\nuse segment::data_types::vectors::{DenseVector, VectorInternal, VectorRef};\nuse segment::fixtures::payload_context_fixture::FixtureIdTracker;\nuse segment::id_tracker::IdTrackerSS;\nuse segment::types::Distance;\nuse segment::vector_storage::dense::simple_dense_vector_storage::open_simple_dense_vector_storage;\nuse segment::vector_storage::{VectorStorage, VectorStorageEnum, new_raw_scorer_for_test};\nuse tempfile::Builder;\n\nconst NUM_VECTORS: usize = 100000;\nconst DIM: usize = 1024; // Larger dimensionality - greater the SIMD advantage\n\nfn random_vector(size: usize) -> DenseVector {\n    let rng = rand::rng();\n    rng.sample_iter(StandardUniform).take(size).collect()\n}\n\nfn init_vector_storage(\n    path: &Path,\n    dim: usize,\n    num: usize,\n    dist: Distance,\n) -> (VectorStorageEnum, Arc<AtomicRefCell<IdTrackerSS>>) {\n    let db = open_db(path, &[DB_VECTOR_CF]).unwrap();\n    let id_tracker = Arc::new(AtomicRefCell::new(FixtureIdTracker::new(num)));\n    let mut storage =\n        open_simple_dense_vector_storage(db, DB_VECTOR_CF, dim, dist, &AtomicBool::new(false))\n            .unwrap();\n\n    {\n        for i in 0..num {\n            let vector: VectorInternal = random_vector(dim).into();\n            storage\n                .insert_vector(i as PointOffsetType, VectorRef::from(&vector))\n                .unwrap();\n        }\n    }\n\n    (storage, id_tracker)\n}\n\nfn benchmark_naive(c: &mut Criterion) {\n    let dir = Builder::new().prefix(\"storage_dir\").tempdir().unwrap();\n\n    let dist = Distance::Dot;\n    let (storage, id_tracker) = init_vector_storage(dir.path(), DIM, NUM_VECTORS, dist);\n    let borrowed_id_tracker = id_tracker.borrow();\n\n    let mut group = c.benchmark_group(\"storage-score-all\");\n\n    group.bench_function(\"storage vector search\", |b| {\n        b.iter(|| {\n            let vector = random_vector(DIM);\n            let vector = vector.as_slice().into();\n            new_raw_scorer_for_test(\n                vector,\n                &storage,\n                borrowed_id_tracker.deleted_point_bitslice(),\n            )\n            .unwrap()\n            .peek_top_all(10, &DEFAULT_STOPPED)\n            .unwrap();\n        })\n    });\n}\n\nfn random_access_benchmark(c: &mut Criterion) {\n    let dir = Builder::new().prefix(\"storage_dir\").tempdir().unwrap();\n\n    let dist = Distance::Dot;\n    let (storage, id_tracker) = init_vector_storage(dir.path(), DIM, NUM_VECTORS, dist);\n    let borrowed_id_tracker = id_tracker.borrow();\n\n    let mut group = c.benchmark_group(\"storage-score-random\");\n\n    let vector = random_vector(DIM);\n    let vector = vector.as_slice().into();\n\n    let scorer = new_raw_scorer_for_test(\n        vector,\n        &storage,\n        borrowed_id_tracker.deleted_point_bitslice(),\n    )\n    .unwrap();\n\n    let mut total_score = 0.;\n    group.bench_function(\"storage vector search\", |b| {\n        b.iter(|| {\n            let random_id = rand::rng().random_range(0..NUM_VECTORS) as PointOffsetType;\n            total_score += scorer.score_point(random_id);\n        })\n    });\n    eprintln!(\"total_score = {total_score:?}\");\n}\n\ncriterion_group!(benches, benchmark_naive, random_access_benchmark);\ncriterion_main!(benches);",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9799777530589544,
      "diff_lines": 44,
      "added_lines": 3,
      "removed_lines": 8,
      "diff_preview": "--- expected+++ predicted@@ -1,6 +1,6 @@ use std::path::Path;\n+use std::sync::atomic::AtomicBool;\n use std::sync::Arc;\n-use std::sync::atomic::AtomicBool;\n \n use atomic_refcell::AtomicRefCell;\n use common::counter::hardware_counter::HardwareCounterCell;\n@@ -14,9 +14,7 @@ use segment::id_tracker::IdTrackerSS;\n use segment::types::Distance;\n use segment::vector_storage::dense::simple_dense_vector_storage::open_simple_dense_vector_storage;\n-use segment::vector_storage::{\n-    DEFAULT_STOPPED, VectorStorage, VectorStorageEnum, new_raw_scorer_for_test,\n-};\n+use segment::vector_storage::{VectorStorage, VectorStorageEnum, new_raw_scorer_for_test};\n use tempfile::Builder;\n \n const NUM_VECTORS: usize = 100000;\n@@ -24,7 +22,6 @@ \n fn random_vector(size: usize) -> DenseVector {\n     let rng = rand::rng();\n-\n     rng.sample_iter(StandardUniform).take(size).collect()\n }\n \n@@ -40,13 +37,11 @@         open_simple_dense_vector_storage(db, DB_VECTOR_CF, dim, dist, &AtomicBool::new(false))\n             .unwrap();\n \n-    let hw_counter = HardwareCounterCell::new();\n-\n     {\n         for i in 0..num {\n             let vector: VectorInternal = random_vector(dim).into();\n             storage\n-                .insert_vector(i as PointOffsetType, VectorRef::from(&vector), &hw_counter)\n+                .insert_vector(i as PointOffsetType, VectorRef::from(&vector))\n                 .unwrap();\n         }\n     }\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 18.145464181900024,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 20990,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "qdrant_lib_segment_src_entry_entry_point.rs",
    "exact_match": false,
    "similarity": 0.05388041176900051,
    "diff_lines": 378,
    "predicted_output": "use thiserror::Error;\nuse std::path::Path;\nuse crate::types::{SeqNumberType, VectorElementType, Filter, PointIdType, ScoreType, PayloadKeyType, PayloadType, SearchParams, ScoredPoint, TheMap, SegmentStats};\nuse std::result;\nuse crate::payload_storage::payload_storage::TheMap;\n\npub trait VersionedPersistable {\n    fn persist(&self, directory: &Path) -> SeqNumberType;\n    fn load(directory: &Path) -> Self;\n\n    fn ack_persistance(&mut self, version: SeqNumberType);\n}\n\n#[derive(Error, Debug)]\n#[error(\"{0}\")]\npub enum OperationError {\n    #[error(\"Vector inserting error: expected dim: {expected_dim}, got {received_dim}\")]\n    WrongVector { expected_dim: usize, received_dim: usize },\n    #[error(\"Wrong operation ordering: segment state:{current_state}, operation: {operation_num}\")]\n    SeqError { current_state: SeqNumberType, operation_num: SeqNumberType },\n    #[error(\"No point with id {missed_point_id} found\")]\n    PointIdError { missed_point_id: PointIdType },\n    #[error(\"Payload `{key}` type mismatch for point {point_id}: expected: {required_type}, got {received_type}\")]\n    PayloadError {\n        point_id: PointIdType,\n        key: PayloadKeyType,\n        required_type: String,\n        received_type: String\n    },\n}\n\npub type Result<T> = result::Result<T, OperationError>;\n\npub trait SegmentEntry {\n    fn version(&self) -> SeqNumberType;\n\n    fn search(&self,\n              vector: &Vec<VectorElementType>,\n              filter: Option<&Filter>,\n              top: usize,\n              params: Option<&SearchParams>,\n    ) -> Vec<ScoredPoint>;\n\n    fn upsert_point(&mut self, op_num: SeqNumberType, point_id: PointIdType, vector: &Vec<VectorElementType>) -> Result<bool>;\n\n    fn delete_point(&mut self, op_num: SeqNumberType, point_id: PointIdType) -> Result<bool>;\n\n    fn set_payload(&mut self, op_num: SeqNumberType, point_id: PointIdType, key: PayloadKeyType, payload: PayloadType) -> Result<bool>;\n\n    fn delete_payload(&mut self, op_num: SeqNumberType, point_id: PointIdType, key: PayloadKeyType) -> Result<bool>;\n\n    fn clear_payload(&mut self, op_num: SeqNumberType, point_id: PointIdType) -> Result<bool>;\n\n    fn wipe_payload(&mut self, op_num: SeqNumberType) -> Result<bool>;\n\n    fn vector(&self, point_id: PointIdType) -> Result<Vec<VectorElementType>>;\n\n    fn payload(&self, point_id: PointIdType) -> Result<TheMap<PayloadKeyType, PayloadType>>;\n\n    fn iter_points(&self) -> Box<dyn Iterator<Item=PointIdType> + '_>;\n\n    fn has_point(&self, point_id: PointIdType) -> bool;\n\n    fn vectors_count(&self) -> usize;\n\n    fn info(&self) -> SegmentStats;\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.05388041176900051,
      "diff_lines": 378,
      "added_lines": 48,
      "removed_lines": 308,
      "diff_preview": "--- expected+++ predicted@@ -1,327 +1,67 @@-use std::collections::{BTreeSet, HashMap, HashSet};\n-use std::path::PathBuf;\n-use std::sync::Arc;\n-use std::sync::atomic::AtomicBool;\n+use thiserror::Error;\n+use std::path::Path;\n+use crate::types::{SeqNumberType, VectorElementType, Filter, PointIdType, ScoreType, PayloadKeyType, PayloadType, SearchParams, ScoredPoint, TheMap, SegmentStats};\n+use std::result;\n+use crate::payload_storage::payload_storage::TheMap;\n \n-use common::counter::hardware_counter::HardwareCounterCell;\n-use common::types::TelemetryDetail;\n+pub trait VersionedPersistable {\n+    fn persist(&self, directory: &Path) -> SeqNumberType;\n+    fn load(directory: &Path) -> Self;\n \n-use crate::common::operation_error::{OperationResult, SegmentFailedState};\n-use crate::data_types::facets::{FacetParams, FacetValue};\n-use crate::data_types::named_vectors::NamedVectors;\n-use crate::data_types::order_by::{OrderBy, OrderValue};\n-use crate::data_types::query_context::{FormulaContext, QueryContext, SegmentQueryContext};\n-use crate::data_types::vectors::{QueryVector, VectorInternal};\n-use crate::entry::snapshot_entry::SnapshotEntry;\n-use crate::index::field_index::{CardinalityEstimation, FieldIndex};\n-use crate::json_path::JsonPath;\n-use crate::telemetry::SegmentTelemetry;\n-use crate::types::{\n-    Filter, Payload, PayloadFieldSchema, PayloadKeyType, PayloadKeyTypeRef, PointIdType,\n-    ScoredPoint, SearchParams, SegmentConfig, SegmentInfo, SegmentType, SeqNumberType, VectorName,\n-    VectorNameBuf, WithPayload, WithVector,\n-};\n+    fn ack_persistance(&mut self, version: SeqNumberType);\n+}\n \n-/// Define all operations which can be performed with Segment or Segment-like entity.\n-///\n-/// Assume all operations are idempotent - which means that no matter how many times an operation\n-/// is executed - the storage state will be the same.\n-pub trait SegmentEntry: SnapshotEntry {\n-    /// Get current update version of the segment\n+#[derive(Error, Debug)]\n+#[error(\"{0}\")]\n+pub enum OperationError {\n+    #[error(\"Vector inserting error: expected dim: {expected_dim}, got {received_dim}\")]\n+    WrongVector { expected_dim: usize, received_dim: usize },\n+    #[error(\"Wrong operation ordering: segment state:{current_state}, operation: {operation_num}\")]\n+    SeqError { current_state: SeqNumberType, operation_num: SeqNumberType },\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 18.24735999107361,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 59831,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "qdrant_lib_segment_src_id_tracker_immutable_id_tracker.rs",
    "exact_match": false,
    "similarity": 0.23846598339466563,
    "diff_lines": 1162,
    "predicted_output": "// TODO: Remove when we release the next version and integrate the immutable id tracker\n#![allow(dead_code)]\nuse std::collections::BTreeMap;\nuse std::fs::File;\nuse std::io::{BufReader, BufWriter, Read, Write};\nuse std::mem::size_of_val;\nuse std::path::{Path, PathBuf};\n\nuse bitvec::prelude::BitSlice;\nuse bitvec::vec::BitVec;\nuse byteorder::{LittleEndian, ReadBytesExt, WriteBytesExt};\nuse common::types::PointOffsetType;\nuse memory::mmap_ops::{create_and_ensure_length, open_write_mmap};\nuse uuid::Uuid;\n\nuse crate::common::mmap_bitslice_buffered_update_wrapper::MmapBitSliceBufferedUpdateWrapper;\nuse crate::common::mmap_slice_buffered_update_wrapper::MmapSliceBufferedUpdateWrapper;\nuse crate::common::mmap_type::{MmapBitSlice, MmapSlice};\nuse crate::common::operation_error::{OperationError, OperationResult};\nuse crate::common::Flusher;\nuse crate::id_tracker::IdTracker;\nuse crate::types::{ExtendedPointId, PointIdType, SeqNumberType};\n\npub const DELETED_FILE_NAME: &str = \"id_tracker.deleted\";\npub const MAPPINGS_FILE_NAME: &str = \"id_tracker.mappings\";\npub const VERSION_MAPPING_FILE_NAME: &str = \"id_tracker.versions\";\n\n#[derive(Debug)]\npub struct ImmutableIdTracker {\n    path: PathBuf,\n\n    deleted: BitVec,\n    deleted_wrapper: MmapBitSliceBufferedUpdateWrapper,\n\n    internal_to_version: Vec<SeqNumberType>,\n    internal_to_version_wrapper: MmapSliceBufferedUpdateWrapper<SeqNumberType>,\n\n    mappings: PointMappings,\n}\n\n#[derive(Clone, PartialEq, Debug)]\npub struct PointMappings {\n    pub(crate) internal_to_external: Vec<PointIdType>,\n\n    // Having two separate maps allows us iterating only over one type at a time without having to filter.\n    pub(crate) external_to_internal_num: BTreeMap<u64, PointOffsetType>,\n    pub(crate) external_to_internal_uuid: BTreeMap<Uuid, PointOffsetType>,\n}\n\n/// Used endianness for storing PointMapping-files.\ntype FileEndianess = LittleEndian;\n\nimpl PointMappings {\n    const EXTERNAL_ID_NUMBER_BYTE: u8 = 0;\n    const EXTERNAL_ID_UUID_BYTE: u8 = 1;\n\n    /// Loads a `PointMappings` from the given reader. Applies an optional filter of deleted items\n    /// to prevent allocating unneeded data.\n    pub fn load<R: Read>(mut reader: R, filter: Option<&BitSlice>) -> OperationResult<Self> {\n        // Deserialize the header\n        let len = reader.read_u64::<FileEndianess>()? as usize;\n\n        let mut internal_to_external = Vec::with_capacity(len);\n        let mut external_to_internal_num: BTreeMap<u64, PointOffsetType> = BTreeMap::new();\n        let mut external_to_internal_uuid: BTreeMap<Uuid, PointOffsetType> = BTreeMap::new();\n\n        // Deserialize the list entries\n        for i in 0..len {\n            let (internal_id, external_id) = Self::read_entry(&mut reader)?;\n\n            // Need to push this regardless of point deletion as the vecs index represents the internal id\n            // which would become wrong if we leave out entries.\n            internal_to_external.push(external_id);\n\n            let deleted = filter\n                .as_ref()\n                .and_then(|deleted| deleted.get(i).as_deref().copied())\n                .unwrap_or_default();\n\n            if deleted {\n                continue;\n            }\n\n            match external_id {\n                ExtendedPointId::NumId(num) => {\n                    external_to_internal_num.insert(num, internal_id);\n                }\n                ExtendedPointId::Uuid(uuid) => {\n                    external_to_internal_uuid.insert(uuid, internal_id);\n                }\n            }\n        }\n\n        // Check that the file has ben fully read.\n        #[cfg(debug_assertions)] // Only for dev builds\n        {\n            let mut buf = vec![];\n            let read_bytes = reader.read_to_end(&mut buf).unwrap();\n            assert_eq!(buf.len(), 0);\n            assert_eq!(read_bytes, 0);\n        }\n\n        Ok(PointMappings {\n            internal_to_external,\n            external_to_internal_num,\n            external_to_internal_uuid,\n        })\n    }\n\n    /// Loads a single entry from a reader. Expects the reader to be aligned so, that the next read\n    /// byte is the first byte of a new entry.\n    /// This function reads exact one entry which means after calling this function, the reader\n    /// will be at the start of the next entry.\n    fn read_entry<R: Read>(mut reader: R) -> OperationResult<(PointOffsetType, ExtendedPointId)> {\n        let point_id_type = reader.read_u8()?;\n\n        let external_id = if point_id_type == Self::EXTERNAL_ID_NUMBER_BYTE {\n            let num = reader.read_u64::<FileEndianess>()?;\n            PointIdType::NumId(num)\n        } else if point_id_type == Self::EXTERNAL_ID_UUID_BYTE {\n            let uuid_u128 = reader.read_u128::<FileEndianess>()?;\n            PointIdType::Uuid(Uuid::from_u128_le(uuid_u128))\n        } else {\n            return Err(OperationError::InconsistentStorage {\n                description: \"Invalid byte read when deserializing Immutable id tracker\"\n                    .to_string(),\n            });\n        };\n\n        let internal_id = reader.read_u32::<FileEndianess>()? as PointOffsetType;\n        Ok((internal_id, external_id))\n    }\n\n    /// Serializes the `PointMappings` into the given writer using the file format specified below.\n    ///\n    /// ## File format\n    /// In general the format looks like this:\n    /// +---------------------------+-----------------+\n    /// | Header (list length: u64) | List of entries |\n    /// +---------------------------+-----------------+\n    ///\n    /// A single list entry:\n    /// +-----------------+-----------------------+------------------+\n    /// | PointIdType: u8 | Number/UUID: u64/u128 | Internal ID: u32 |\n    /// +-----------------+-----------------------+------------------+\n    /// A single entry is thus either 1+8+4=13 or 1+16+4=21 bytes in size depending\n    /// on the PointIdType.\n\n    pub fn store<W: Write>(&self, mut writer: W) -> OperationResult<()> {\n        // Serialize the header (=length).\n        writer.write_u64::<FileEndianess>(self.internal_to_external.len() as u64)?;\n\n        // Serialize all entries\n        for external_id in self.internal_to_external.iter() {\n            self.write_entry(&mut writer, external_id)?;\n        }\n\n        writer.flush()?;\n        Ok(())\n    }\n\n    fn write_entry<W: Write>(\n        &self,\n        mut writer: W,\n        external_id: &PointIdType,\n    ) -> OperationResult<()> {\n        // Serializing External ID\n        match external_id {\n            PointIdType::NumId(num) => {\n                // Byte to distinguish between Number and UUID\n                writer.write_u8(Self::EXTERNAL_ID_NUMBER_BYTE)?;\n\n                // The PointID's number\n                writer.write_u64::<FileEndianess>(*num)?;\n            }\n            PointIdType::Uuid(uuid) => {\n                // Byte to distinguish between Number and UUID\n                writer.write_u8(Self::EXTERNAL_ID_UUID_BYTE)?;\n\n                // The PointID's UUID\n                writer.write_u128::<FileEndianess>(uuid.to_u128_le())?;\n            }\n        }\n\n        let internal_id = match external_id {\n            PointIdType::NumId(n) => self.external_to_internal_num.get(n),\n            PointIdType::Uuid(u) => self.external_to_internal_uuid.get(u),\n        }\n        .ok_or(OperationError::PointIdError {\n            missed_point_id: *external_id,\n        })?;\n\n        // Serializing Internal ID\n        writer.write_u32::<FileEndianess>(*internal_id)?;\n\n        Ok(())\n    }\n}\n\nimpl ImmutableIdTracker {\n    pub fn open(segment_path: &Path) -> OperationResult<Self> {\n        let deleted_raw = open_write_mmap(&Self::deleted_file_path(segment_path))?;\n        let deleted_mmap = MmapBitSlice::try_from(deleted_raw, 0)?;\n        let deleted_bitvec = deleted_mmap.to_bitvec();\n        let deleted_wrapper = MmapBitSliceBufferedUpdateWrapper::new(deleted_mmap);\n\n        let internal_to_version_map =\n            open_write_mmap(&Self::version_mapping_file_path(segment_path))?;\n        let internal_to_version_mapslice: MmapSlice<SeqNumberType> =\n            unsafe { MmapSlice::try_from(internal_to_version_map)? };\n        let internal_to_version = internal_to_version_mapslice.to_vec();\n        let internal_to_version_wrapper =\n            MmapSliceBufferedUpdateWrapper::new(internal_to_version_mapslice);\n\n        let reader = BufReader::new(File::open(Self::mappings_file_path(segment_path))?);\n        let mappings = PointMappings::load(reader, Some(&deleted_bitvec))?;\n\n        Ok(Self {\n            path: segment_path.to_path_buf(),\n            deleted: deleted_bitvec,\n            deleted_wrapper,\n            internal_to_version_wrapper,\n            internal_to_version,\n            mappings,\n        })\n    }\n\n    pub(super) fn new(\n        path: &Path,\n        deleted: &BitSlice,\n        internal_to_version: &[SeqNumberType],\n        mappings: PointMappings,\n    ) -> OperationResult<Self> {\n        // Create mmap file for deleted bitvec\n        let deleted_filepath = Self::deleted_file_path(path);\n        {\n            let deleted_size = bitmap_mmap_size(deleted);\n            create_and_ensure_length(&deleted_filepath, deleted_size)?;\n        }\n\n        let deleted_mmap = open_write_mmap(&deleted_filepath)?;\n        let mut deleted_new = MmapBitSlice::try_from(deleted_mmap, 0)?;\n        deleted_new[..deleted.len()].copy_from_bitslice(deleted);\n        let deleted_wrapper = MmapBitSliceBufferedUpdateWrapper::new(deleted_new);\n\n        // Create mmap file for internal-to-version list\n        let version_filepath = Self::version_mapping_file_path(path);\n        {\n            let version_size = size_of_val(internal_to_version);\n            create_and_ensure_length(&version_filepath, version_size)?;\n        }\n        let mut internal_to_version_wrapper =\n            unsafe { MmapSlice::try_from(open_write_mmap(&version_filepath)?)? };\n        internal_to_version_wrapper.copy_from_slice(internal_to_version);\n        let internal_to_version = internal_to_version_wrapper.to_vec();\n        let internal_to_version_wrapper =\n            MmapSliceBufferedUpdateWrapper::new(internal_to_version_wrapper);\n\n        // Write mappings to disk.\n        let writer = BufWriter::new(File::create(Self::mappings_file_path(path))?);\n        mappings.store(writer)?;\n\n        Ok(Self {\n            path: path.to_path_buf(),\n            deleted: deleted.to_bitvec(),\n            deleted_wrapper,\n            internal_to_version_wrapper,\n            internal_to_version,\n            mappings,\n        })\n    }\n\n    fn deleted_file_path(base: &Path) -> PathBuf {\n        base.join(DELETED_FILE_NAME)\n    }\n\n    fn version_mapping_file_path(base: &Path) -> PathBuf {\n        base.join(VERSION_MAPPING_FILE_NAME)\n    }\n\n    pub(crate) fn mappings_file_path(base: &Path) -> PathBuf {\n        base.join(MAPPINGS_FILE_NAME)\n    }\n}\n\n/// Returns the required mmap filesize for a `BitSlice`.\nfn bitmap_mmap_size(deleted: &BitSlice) -> usize {\n    let usize_bytes = std::mem::size_of::<usize>();\n    let num_bytes = deleted.len().div_ceil(8); // used bytes\n    num_bytes.div_ceil(usize_bytes) * usize_bytes // Make it a multiple of usize-width.\n}\n\nimpl IdTracker for ImmutableIdTracker {\n    fn internal_version(&self, internal_id: PointOffsetType) -> Option<SeqNumberType> {\n        self.internal_to_version.get(internal_id as usize).copied()\n    }\n\n    fn set_internal_version(\n        &mut self,\n        internal_id: PointOffsetType,\n        version: SeqNumberType,\n    ) -> OperationResult<()> {\n        if self.external_id(internal_id).is_some() {\n            if let Some(old_version) = self.internal_to_version.get_mut(internal_id as usize) {\n                *old_version = version;\n                self.internal_to_version_wrapper\n                    .set(internal_id as usize, version);\n            }\n        }\n\n        Ok(())\n    }\n\n    fn internal_id(&self, external_id: PointIdType) -> Option<PointOffsetType> {\n        match external_id {\n            PointIdType::NumId(num) => self.mappings.external_to_internal_num.get(&num).copied(),\n            PointIdType::Uuid(uuid) => self.mappings.external_to_internal_uuid.get(&uuid).copied(),\n        }\n    }\n\n    fn external_id(&self, internal_id: PointOffsetType) -> Option<PointIdType> {\n        if *self.deleted.get(internal_id as usize)? {\n            return None;\n        }\n\n        self.mappings\n            .internal_to_external\n            .get(internal_id as usize)\n            .map(|i| i.into())\n    }\n\n    fn set_link(\n        &mut self,\n        _external_id: PointIdType,\n        _internal_id: PointOffsetType,\n    ) -> OperationResult<()> {\n        panic!(\"Trying to call a mutating function (`set_link`) of an immutable id tracker\");\n    }\n\n    fn drop(&mut self, external_id: PointIdType) -> OperationResult<()> {\n        let internal_id = match external_id {\n            // We \"temporarily\" remove existing points from the BTreeMaps without writing them to disk\n            // because we remove deleted points of a previous load directly when loading.\n            PointIdType::NumId(num) => self.mappings.external_to_internal_num.remove(&num),\n            PointIdType::Uuid(uuid) => self.mappings.external_to_internal_uuid.remove(&uuid),\n        };\n\n        if let Some(internal_id) = internal_id {\n            self.deleted.set(internal_id as usize, true);\n            self.deleted_wrapper.set(internal_id as usize, true);\n        }\n\n        Ok(())\n    }\n\n    fn iter_external(&self) -> Box<dyn Iterator<Item = PointIdType> + '_> {\n        let iter_num = self\n            .mappings\n            .external_to_internal_num\n            .keys()\n            .map(|i| PointIdType::NumId(*i));\n\n        let iter_uuid = self\n            .mappings\n            .external_to_internal_uuid\n            .keys()\n            .map(|i| PointIdType::Uuid(*i));\n        // order is important here, we want to iterate over the u64 ids first\n        Box::new(iter_num.chain(iter_uuid))\n    }\n\n    fn iter_internal(&self) -> Box<dyn Iterator<Item = PointOffsetType> + '_> {\n        Box::new(\n            (0..self.mappings.internal_to_external.len() as PointOffsetType)\n                .filter(move |i| !self.deleted[*i as usize]),\n        )\n    }\n\n    fn iter_from(\n        &self,\n        external_id: Option<PointIdType>,\n    ) -> Box<dyn Iterator<Item = (PointIdType, PointOffsetType)> + '_> {\n        let full_num_iter = || {\n            self.mappings\n                .external_to_internal_num\n                .iter()\n                .map(|(k, v)| (PointIdType::NumId(*k), *v))\n        };\n        let offset_num_iter = |offset: u64| {\n            self.mappings\n                .external_to_internal_num\n                .range(offset..)\n                .map(|(k, v)| (PointIdType::NumId(*k), *v))\n        };\n        let full_uuid_iter = || {\n            self.mappings\n                .external_to_internal_uuid\n                .iter()\n                .map(|(k, v)| (PointIdType::Uuid(*k), *v))\n        };\n        let offset_uuid_iter = |offset: Uuid| {\n            self.mappings\n                .external_to_internal_uuid\n                .range(offset..)\n                .map(|(k, v)| (PointIdType::Uuid(*k), *v))\n        };\n\n        match external_id {\n            None => {\n                let iter_num = full_num_iter();\n                let iter_uuid = full_uuid_iter();\n                // order is important here, we want to iterate over the u64 ids first\n                Box::new(iter_num.chain(iter_uuid))\n            }\n            Some(offset) => match offset {\n                PointIdType::NumId(idx) => {\n                    // Because u64 keys are less that uuid key, we can just use the full iterator for uuid\n                    let iter_num = offset_num_iter(idx);\n                    let iter_uuid = full_uuid_iter();\n                    // order is important here, we want to iterate over the u64 ids first\n                    Box::new(iter_num.chain(iter_uuid))\n                }\n                PointIdType::Uuid(uuid) => {\n                    // if offset is a uuid, we can only iterate over uuids\n                    Box::new(offset_uuid_iter(uuid))\n                }\n            },\n        }\n    }\n\n    fn iter_ids(&self) -> Box<dyn Iterator<Item = PointOffsetType> + '_> {\n        self.iter_internal()\n    }\n\n    /// Creates a flusher function, that writes the deleted points bitvec to disk.\n    fn mapping_flusher(&self) -> Flusher {\n        // Only flush deletions because mappings are immutable\n        self.deleted_wrapper.flusher()\n    }\n\n    /// Creates a flusher function, that writes the points versions to disk.\n    fn versions_flusher(&self) -> Flusher {\n        self.internal_to_version_wrapper.flusher()\n    }\n\n    fn total_point_count(&self) -> usize {\n        self.mappings.internal_to_external.len()\n    }\n\n    fn available_point_count(&self) -> usize {\n        self.mappings.external_to_internal_num.len() + self.mappings.external_to_internal_uuid.len()\n    }\n\n    fn deleted_point_count(&self) -> usize {\n        self.total_point_count() - self.available_point_count()\n    }\n\n    fn deleted_point_bitslice(&self) -> &BitSlice {\n        &self.deleted\n    }\n\n    fn is_deleted_point(&self, key: PointOffsetType) -> bool {\n        let key = key as usize;\n        if key >= self.deleted.len() {\n            return true;\n        }\n        self.deleted[key]\n    }\n\n    fn name(&self) -> &'static str {\n        \"immutable id tracker\"\n    }\n\n    fn cleanup_versions(&mut self) -> OperationResult<()> {\n        let mut to_remove = Vec::new();\n        for internal_id in self.iter_internal() {\n            if self.internal_version(internal_id).is_none() {\n                if let Some(external_id) = self.external_id(internal_id) {\n                    to_remove.push(external_id);\n                } else {\n                    debug_assert!(false, \"internal id {} has no external id\", internal_id);\n                }\n            }\n        }\n        for external_id in to_remove {\n            self.drop(external_id)?;\n            #[cfg(debug_assertions)] // Only for dev builds\n            {\n                log::debug!(\"dropped version for point {} without version\", external_id);\n            }\n        }\n        Ok(())\n    }\n\n    fn files(&self) -> Vec<PathBuf> {\n        vec![\n            Self::deleted_file_path(&self.path),\n            Self::mappings_file_path(&self.path),\n            Self::version_mapping_file_path(&self.path),\n        ]\n    }\n}\n\n#[cfg(test)]\nmod test {\n    use std::collections::{HashMap, HashSet};\n\n    use itertools::Itertools;\n    use rand::prelude::*;\n    use rand::Rng;\n    use tempfile::Builder;\n\n    use super::*;\n    use crate::common::rocksdb_wrapper::{open_db, DB_VECTOR_CF};\n    use crate::id_tracker::simple_id_tracker::SimpleIdTracker;\n    use crate::id_tracker::IdTrackerEnum;\n\n    const RAND_SEED: u64 = 42;\n\n    #[test]\n    fn test_iterator() {\n        let dir = Builder::new().prefix(\"storage_dir\").tempdir().unwrap();\n        let db = open_db(dir.path(), &[DB_VECTOR_CF]).unwrap();\n\n        let mut id_tracker = SimpleIdTracker::open(db).unwrap();\n\n        id_tracker.set_link(200.into(), 0).unwrap();\n        id_tracker.set_link(100.into(), 1).unwrap();\n        id_tracker.set_link(150.into(), 2).unwrap();\n        id_tracker.set_link(120.into(), 3).unwrap();\n        id_tracker.set_link(180.into(), 4).unwrap();\n        id_tracker.set_link(110.into(), 5).unwrap();\n        id_tracker.set_link(115.into(), 6).unwrap();\n        id_tracker.set_link(190.into(), 7).unwrap();\n        id_tracker.set_link(177.into(), 8).unwrap();\n        id_tracker.set_link(118.into(), 9).unwrap();\n\n        let id_tracker = id_tracker.make_immutable(dir.path()).unwrap();\n\n        let first_four = id_tracker.iter_from(None).take(4).collect_vec();\n\n        assert_eq!(first_four.len(), 4);\n        assert_eq!(first_four[0].0, 100.into());\n\n        let last = id_tracker.iter_from(Some(first_four[3].0)).collect_vec();\n        assert_eq!(last.len(), 7);\n    }\n\n    const TEST_POINTS: &[PointIdType] = &[\n        PointIdType::NumId(100),\n        PointIdType::Uuid(Uuid::from_u128(123_u128)),\n        PointIdType::Uuid(Uuid::from_u128(156_u128)),\n        PointIdType::NumId(150),\n        PointIdType::NumId(120),\n        PointIdType::Uuid(Uuid::from_u128(12_u128)),\n        PointIdType::NumId(180),\n        PointIdType::NumId(110),\n        PointIdType::NumId(115),\n        PointIdType::Uuid(Uuid::from_u128(673_u128)),\n        PointIdType::NumId(190),\n        PointIdType::NumId(177),\n        PointIdType::Uuid(Uuid::from_u128(971_u128)),\n    ];\n\n    fn make_immutable_tracker(path: &Path) -> ImmutableIdTracker {\n        let db = open_db(path, &[DB_VECTOR_CF]).unwrap();\n\n        let mut id_tracker = SimpleIdTracker::open(db).unwrap();\n\n        for (id, value) in TEST_POINTS.iter().enumerate() {\n            id_tracker.set_link(*value, id as PointOffsetType).unwrap();\n        }\n\n        match id_tracker.make_immutable(path).unwrap() {\n            IdTrackerEnum::MutableIdTracker(_) => {\n                unreachable!()\n            }\n            IdTrackerEnum::ImmutableIdTracker(m) => {\n                m.mapping_flusher()().unwrap();\n                m.versions_flusher()().unwrap();\n                m\n            }\n        }\n    }\n\n    #[test]\n    fn test_mixed_types_iterator() {\n        let dir = Builder::new().prefix(\"storage_dir\").tempdir().unwrap();\n        let id_tracker = make_immutable_tracker(dir.path());\n\n        let sorted_from_tracker = id_tracker.iter_from(None).map(|(k, _)| k).collect_vec();\n\n        let mut values = TEST_POINTS.to_vec();\n        values.sort();\n\n        assert_eq!(sorted_from_tracker, values);\n    }\n\n    #[test]\n    fn test_load_store() {\n        let dir = Builder::new().prefix(\"storage_dir\").tempdir().unwrap();\n        let (old_deleted, old_mappings, old_versions) = {\n            let id_tracker = make_immutable_tracker(dir.path());\n            (\n                id_tracker.deleted.to_bitvec(),\n                id_tracker.mappings,\n                id_tracker.internal_to_version,\n            )\n        };\n\n        let mut loaded_id_tracker = ImmutableIdTracker::open(dir.path()).unwrap();\n\n        // We may extend the length of deleted bitvec as memory maps need to be aligned to\n        // a multiple of `usize-width`.\n        assert_eq!(old_deleted, loaded_id_tracker.deleted[..old_deleted.len()]);\n\n        assert_eq!(old_versions, loaded_id_tracker.internal_to_version);\n\n        assert_eq!(old_mappings, loaded_id_tracker.mappings);\n\n        loaded_id_tracker.drop(PointIdType::NumId(180)).unwrap();\n    }\n\n    /// Mutates an ID tracker and stores it to disk. Tests whether loading results in the exact same\n    /// ID tracker.\n    #[test]\n    fn test_store_load_mutated() {\n        let mut rng = StdRng::seed_from_u64(RAND_SEED);\n\n        let dir = Builder::new().prefix(\"storage_dir\").tempdir().unwrap();\n        let (dropped_points, custom_version) = {\n            let mut id_tracker = make_immutable_tracker(dir.path());\n\n            let mut dropped_points = HashSet::new();\n            let mut custom_version = HashMap::new();\n\n            for (index, point) in TEST_POINTS.iter().enumerate() {\n                if index % 2 == 0 {\n                    continue;\n                }\n\n                if index % 3 == 0 {\n                    id_tracker.drop(*point).unwrap();\n                    dropped_points.insert(*point);\n                    continue;\n                }\n\n                if index % 5 == 0 {\n                    let new_version = rng.next_u64();\n                    id_tracker\n                        .set_internal_version(index as PointOffsetType, new_version)\n                        .unwrap();\n                    custom_version.insert(index as PointOffsetType, new_version);\n                }\n            }\n\n            id_tracker.mapping_flusher()().unwrap();\n            id_tracker.versions_flusher()().unwrap();\n\n            (dropped_points, custom_version)\n        };\n\n        let id_tracker = ImmutableIdTracker::open(dir.path()).unwrap();\n        for (index, point) in TEST_POINTS.iter().enumerate() {\n            let internal_id = index as PointOffsetType;\n\n            if dropped_points.contains(point) {\n                assert!(id_tracker.is_deleted_point(internal_id));\n                assert_eq!(id_tracker.external_id(internal_id), None);\n                match point {\n                    PointIdType::NumId(num) => {\n                        assert!(!id_tracker\n                            .mappings\n                            .external_to_internal_num\n                            .contains_key(num));\n                    }\n                    PointIdType::Uuid(uuid) => {\n                        assert!(!id_tracker\n                            .mappings\n                            .external_to_internal_uuid\n                            .contains_key(uuid));\n                    }\n                }\n\n                continue;\n            }\n\n            // Check version\n            let expect_version = custom_version.get(&internal_id).unwrap_or(&0);\n            assert_eq!(\n                id_tracker.internal_to_version.get(internal_id as usize),\n                Some(expect_version)\n            );\n\n            // Check that unmodified points still haven't changed.\n            assert_eq!(\n                id_tracker.external_id(index as PointOffsetType),\n                Some(*point)\n            );\n        }\n    }\n\n    #[test]\n    fn test_all_points_have_version() {\n        let dir = Builder::new().prefix(\"storage_dir\").tempdir().unwrap();\n        let id_tracker = make_immutable_tracker(dir.path());\n        for i in id_tracker.iter_ids() {\n            assert!(id_tracker.internal_version(i).is_some());\n        }\n    }\n\n    #[test]\n    fn test_point_deletion_correctness() {\n        let dir = Builder::new().prefix(\"storage_dir\").tempdir().unwrap();\n        let id_tracker = make_immutable_tracker(dir.path());\n        assert_point_deletion_correctness(IdTrackerEnum::ImmutableIdTracker(id_tracker));\n    }\n\n    fn assert_point_deletion_correctness(mut id_tracker: IdTrackerEnum) {\n        // No deletions yet\n        assert_eq!(\n            id_tracker.total_point_count(),\n            id_tracker.available_point_count()\n        );\n\n        let point_to_delete = PointIdType::NumId(100);\n\n        assert!(id_tracker.iter_external().contains(&point_to_delete));\n\n        assert_eq!(id_tracker.internal_id(point_to_delete), Some(0));\n\n        id_tracker.drop(point_to_delete).unwrap();\n\n        assert!(!point_exists(&id_tracker, point_to_delete));\n\n        assert_eq!(\n            id_tracker.available_point_count(),\n            id_tracker.total_point_count() - 1\n        );\n    }\n\n    fn point_exists(id_tracker: &IdTrackerEnum, point: PointIdType) -> bool {\n        id_tracker.internal_id(point).is_some()\n            && id_tracker.iter_external().contains(&point)\n            && id_tracker.iter_from(None).any(|i| i.0 == point)\n    }\n\n    #[test]\n    fn test_point_deletion_persists_reload() {\n        let dir = Builder::new().prefix(\"storage_dir\").tempdir().unwrap();\n\n        let point_to_delete = PointIdType::NumId(100);\n\n        let old_mappings = {\n            let mut id_tracker = make_immutable_tracker(dir.path());\n            let intetrnal_id = id_tracker\n                .internal_id(point_to_delete)\n                .expect(\"Point to delete exists.\");\n            assert!(!id_tracker.is_deleted_point(intetrnal_id));\n            id_tracker.drop(point_to_delete).unwrap();\n            id_tracker.versions_flusher()().unwrap();\n            id_tracker.mapping_flusher()().unwrap();\n            id_tracker.mappings\n        };\n\n        // Point should still be gone\n        let id_tracker = ImmutableIdTracker::open(dir.path()).unwrap();\n        assert_eq!(id_tracker.internal_id(point_to_delete), None);\n\n        // Old mappings should be the same as newly loaded one.\n        assert_eq!(\n            old_mappings.external_to_internal_num,\n            id_tracker.mappings.external_to_internal_num\n        );\n    }\n\n    fn gen_random_point_mappings(size: usize, rand: &mut StdRng) -> PointMappings {\n        const UUID_LIKELYNESS: f64 = 0.5;\n\n        let mut external_to_internal_num = BTreeMap::new();\n        let mut external_to_internal_uuid = BTreeMap::new();\n\n        let internal_to_external = (0..size)\n            .map(|_| {\n                if rand.gen_bool(UUID_LIKELYNESS) {\n                    PointIdType::Uuid(Uuid::new_v4())\n                } else {\n                    PointIdType::NumId(rand.next_u64())\n                }\n            })\n            .enumerate()\n            .inspect(|(pos, point_type)| match point_type {\n                ExtendedPointId::NumId(num) => {\n                    external_to_internal_num.insert(*num, *pos as u32);\n                }\n                ExtendedPointId::Uuid(uuid) => {\n                    external_to_internal_uuid.insert(*uuid, *pos as u32);\n                }\n            })\n            .map(|i| i.1)\n            .collect();\n\n        PointMappings {\n            internal_to_external,\n            external_to_internal_num,\n            external_to_internal_uuid,\n        }\n    }\n\n    /// Tests de/serializing of whole `PointMappings`.\n    #[test]\n    fn test_point_mappings_de_serialization() {\n        let mut rng = StdRng::seed_from_u64(RAND_SEED);\n\n        let mut buf = vec![];\n\n        // Test different sized PointMappings, growing exponentially to also test large ones.\n        // This way we test up to 2^22=4_194_304 points.\n        for size_exp in (0..23u32).step_by(3) {\n            buf.clear();\n\n            let size = 2usize.pow(size_exp);\n\n            let mappings = gen_random_point_mappings(size, &mut rng);\n\n            mappings.store(&mut buf).unwrap();\n\n            // 16 is the min byte size of an entry. The exact number is not that important\n            // we just want to ensure that the written bytes correlate to the amount of entries.\n            assert!(buf.len() >= size * 16);\n\n            let new_mappings = PointMappings::load(&*buf, None).unwrap();\n\n            assert_eq!(new_mappings.internal_to_external.len(), size);\n            assert_eq!(mappings, new_mappings);\n        }\n    }\n\n    /// Verifies that de/serializing works properly for empty `PointMappings`.\n    #[test]\n    fn test_point_mappings_de_serialization_empty() {\n        let mut rng = StdRng::seed_from_u64(RAND_SEED);\n        let mappings = gen_random_point_mappings(0, &mut rng);\n\n        let mut buf = vec![];\n\n        mappings.store(&mut buf).unwrap();\n\n        // We still have a header!\n        assert!(!buf.is_empty());\n\n        let new_mappings = PointMappings::load(&*buf, None).unwrap();\n\n        assert!(new_mappings.internal_to_external.is_empty());\n        assert_eq!(mappings, new_mappings);\n    }\n\n    /// Tests de/serializing of only single ID mappings.\n    #[test]\n    fn test_point_mappings_de_serialization_single() {\n        let mut rng = StdRng::seed_from_u64(RAND_SEED);\n\n        const SIZE: usize = 400_000;\n\n        let mappings = gen_random_point_mappings(SIZE, &mut rng);\n\n        for i in 0..SIZE {\n            let mut buf = vec![];\n\n            let expected_external = mappings.internal_to_external[i];\n\n            mappings.write_entry(&mut buf, &expected_external).unwrap();\n\n            let (got_internal, got_external) = PointMappings::read_entry(&*buf).unwrap();\n\n            assert_eq!(i as PointOffsetType, got_internal);\n            assert_eq!(expected_external, got_external);\n        }\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.23846598339466563,
      "diff_lines": 1162,
      "added_lines": 347,
      "removed_lines": 436,
      "diff_preview": "--- expected+++ predicted@@ -1,99 +1,68 @@+// TODO: Remove when we release the next version and integrate the immutable id tracker\n+#![allow(dead_code)]\n+use std::collections::BTreeMap;\n use std::fs::File;\n use std::io::{BufReader, BufWriter, Read, Write};\n-use std::mem::{size_of, size_of_val};\n+use std::mem::size_of_val;\n use std::path::{Path, PathBuf};\n \n use bitvec::prelude::BitSlice;\n use bitvec::vec::BitVec;\n-use byteorder::{ReadBytesExt, WriteBytesExt};\n-use common::ext::BitSliceExt as _;\n+use byteorder::{LittleEndian, ReadBytesExt, WriteBytesExt};\n use common::types::PointOffsetType;\n-use memory::madvise::AdviceSetting;\n use memory::mmap_ops::{create_and_ensure_length, open_write_mmap};\n-use memory::mmap_type::{MmapBitSlice, MmapSlice};\n use uuid::Uuid;\n \n-use crate::common::Flusher;\n use crate::common::mmap_bitslice_buffered_update_wrapper::MmapBitSliceBufferedUpdateWrapper;\n use crate::common::mmap_slice_buffered_update_wrapper::MmapSliceBufferedUpdateWrapper;\n+use crate::common::mmap_type::{MmapBitSlice, MmapSlice};\n use crate::common::operation_error::{OperationError, OperationResult};\n+use crate::common::Flusher;\n use crate::id_tracker::IdTracker;\n-use crate::id_tracker::compressed::compressed_point_mappings::CompressedPointMappings;\n-use crate::id_tracker::compressed::external_to_internal::CompressedExternalToInternal;\n-use crate::id_tracker::compressed::internal_to_external::CompressedInternalToExternal;\n-use crate::id_tracker::compressed::versions_store::CompressedVersions;\n-use crate::id_tracker::in_memory_id_tracker::InMemoryIdTracker;\n-use crate::id_tracker::point_mappings::FileEndianess;\n use crate::types::{ExtendedPointId, PointIdType, SeqNumberType};\n \n pub const DELETED_FILE_NAME: &str = \"id_tracker.deleted\";\n pub const MAPPINGS_FILE_NAME: &str = \"id_tracker.mappings\";\n pub const VERSION_MAPPING_FILE_NAME: &str = \"id_tracker.versions\";\n \n-#[derive(Copy, Clone)]\n-#[repr(u8)]\n-enum ExternalIdType {\n-    Number = 0,\n-    Uuid = 1,\n-}\n-\n-impl ExternalIdType {\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 131.5093069076538,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 29271,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "qdrant_lib_segment_src_index_field_index_field_index_base.rs",
    "exact_match": false,
    "similarity": 0.09094311377245509,
    "diff_lines": 690,
    "predicted_output": "use crate::index::field_index::map_index::PersistedMapIndex;\nuse crate::index::field_index::numeric_index::PersistedNumericIndex;\nuse crate::index::field_index::{CardinalityEstimation, PayloadBlockCondition};\nuse crate::types::{\n    FieldCondition, FloatPayloadType, IntPayloadType, PayloadKeyType, PayloadType, PointOffsetType,\n};\nuse serde::{Deserialize, Serialize};\n\npub trait PayloadFieldIndex {\n    /// Get iterator over points fitting given `condition`\n    fn filter(\n        &self,\n        condition: &FieldCondition,\n    ) -> Option<Box<dyn Iterator<Item = PointOffsetType> + '_>>;\n\n    /// Return estimation of points amount which satisfy given condition\n    fn estimate_cardinality(&self, condition: &FieldCondition) -> Option<CardinalityEstimation>;\n\n    /// Iterate conditions for payload blocks with minimum size of `threshold`\n    /// Required for building HNSW index\n    fn payload_blocks(\n        &self,\n        threshold: usize,\n        key: PayloadKeyType,\n    ) -> Box<dyn Iterator<Item = PayloadBlockCondition> + '_>;\n}\n\npub trait PayloadFieldIndexBuilder {\n    fn add(&mut self, id: PointOffsetType, value: &PayloadType);\n\n    fn build(&mut self) -> FieldIndex;\n}\n\n#[derive(Serialize, Deserialize)]\npub enum FieldIndex {\n    IntIndex(PersistedNumericIndex<IntPayloadType>),\n    IntMapIndex(PersistedMapIndex<IntPayloadType>),\n    KeywordIndex(PersistedMapIndex<String>),\n    FloatIndex(PersistedNumericIndex<FloatPayloadType>),\n}\n\nimpl FieldIndex {\n    pub fn get_payload_field_index(&self) -> &dyn PayloadFieldIndex {\n        match self {\n            FieldIndex::IntIndex(payload_field_index) => payload_field_index,\n            FieldIndex::IntMapIndex(payload_field_index) => payload_field_index,\n            FieldIndex::KeywordIndex(payload_field_index) => payload_field_index,\n            FieldIndex::FloatIndex(payload_field_index) => payload_field_index,\n        }\n    }\n}\n\nimpl PayloadFieldIndex for FieldIndex {\n    fn filter(\n        &self,\n        condition: &FieldCondition,\n    ) -> Option<Box<dyn Iterator<Item = PointOffsetType> + '_>> {\n        self.get_payload_field_index().filter(condition)\n    }\n\n    fn estimate_cardinality(&self, condition: &FieldCondition) -> Option<CardinalityEstimation> {\n        self.get_payload_field_index()\n            .estimate_cardinality(condition)\n    }\n\n    fn payload_blocks(\n        &self,\n        threshold: usize,\n        key: PayloadKeyType,\n    ) -> Box<dyn Iterator<Item = PayloadBlockCondition> + '_> {\n        self.get_payload_field_index()\n            .payload_blocks(threshold, key)\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.09094311377245509,
      "diff_lines": 690,
      "added_lines": 33,
      "removed_lines": 616,
      "diff_preview": "--- expected+++ predicted@@ -1,64 +1,20 @@-use std::fmt::Formatter;\n-use std::path::PathBuf;\n-\n-use common::counter::hardware_counter::HardwareCounterCell;\n-use common::types::PointOffsetType;\n-use serde_json::Value;\n-\n-use super::bool_index::BoolIndex;\n-use super::bool_index::mmap_bool_index::MmapBoolIndexBuilder;\n-use super::bool_index::simple_bool_index::BoolIndexBuilder;\n-use super::facet_index::FacetIndexEnum;\n-use super::full_text_index::mmap_text_index::FullTextMmapIndexBuilder;\n-use super::full_text_index::text_index::{FullTextIndex, FullTextIndexBuilder};\n-use super::geo_index::{GeoMapIndexBuilder, GeoMapIndexMmapBuilder};\n-use super::map_index::{MapIndex, MapIndexBuilder, MapIndexMmapBuilder};\n-use super::numeric_index::{\n-    NumericIndex, NumericIndexBuilder, NumericIndexMmapBuilder, StreamRange,\n+use crate::index::field_index::map_index::PersistedMapIndex;\n+use crate::index::field_index::numeric_index::PersistedNumericIndex;\n+use crate::index::field_index::{CardinalityEstimation, PayloadBlockCondition};\n+use crate::types::{\n+    FieldCondition, FloatPayloadType, IntPayloadType, PayloadKeyType, PayloadType, PointOffsetType,\n };\n-use crate::common::Flusher;\n-use crate::common::operation_error::OperationResult;\n-use crate::data_types::order_by::OrderValue;\n-use crate::index::field_index::geo_index::GeoMapIndex;\n-use crate::index::field_index::null_index::mmap_null_index::{MmapNullIndex, MmapNullIndexBuilder};\n-use crate::index::field_index::numeric_index::NumericIndexInner;\n-use crate::index::field_index::{CardinalityEstimation, PayloadBlockCondition};\n-use crate::telemetry::PayloadIndexTelemetry;\n-use crate::types::{\n-    DateTimePayloadType, FieldCondition, FloatPayloadType, IntPayloadType, Match, MatchText,\n-    PayloadKeyType, RangeInterface, UuidIntType, UuidPayloadType,\n-};\n+use serde::{Deserialize, Serialize};\n \n pub trait PayloadFieldIndex {\n-    /// Return number of points with at least one value indexed in here\n-    fn count_indexed_points(&self) -> usize;\n-\n-    /// Load index from disk.\n-    fn load(&mut self) -> OperationResult<bool>;\n-\n-    /// Remove db content or files of the current payload index\n-    fn cleanup(self) -> OperationResult<()>;\n-\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 14.649167776107788,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 54941,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "qdrant_lib_segment_src_index_field_index_full_text_index_inverted_index.rs",
    "exact_match": false,
    "similarity": 0.5790246619739707,
    "diff_lines": 332,
    "predicted_output": "use std::collections::{BTreeSet, HashMap};\n\nuse common::types::PointOffsetType;\nuse serde::{Deserialize, Serialize};\n\nuse crate::common::operation_error::{OperationError, OperationResult};\nuse crate::index::field_index::{CardinalityEstimation, PayloadBlockCondition, PrimaryCondition};\nuse crate::types::{FieldCondition, Match, PayloadKeyType};\n\npub type TokenId = u32;\n\n#[derive(Default, Serialize, Deserialize, Clone)]\npub struct Document {\n    tokens: Vec<TokenId>,\n}\n\nimpl Document {\n    pub fn new(mut tokens: Vec<TokenId>) -> Self {\n        tokens.sort_unstable();\n        Self { tokens }\n    }\n\n    pub fn len(&self) -> usize {\n        self.tokens.len()\n    }\n\n    pub fn is_empty(&self) -> bool {\n        self.tokens.is_empty()\n    }\n\n    pub fn tokens(&self) -> &[TokenId] {\n        &self.tokens\n    }\n\n    pub fn check(&self, token: TokenId) -> bool {\n        self.tokens.binary_search(&token).is_ok()\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct ParsedQuery {\n    pub tokens: Vec<Option<TokenId>>,\n}\n\nimpl ParsedQuery {\n    pub fn check_match(&self, document: &Document) -> bool {\n        if self.tokens.contains(&None) {\n            return false;\n        }\n\n        // Check that all tokens are in document\n        self.tokens\n            .iter()\n            .all(|query_token| document.check(query_token.unwrap()))\n    }\n}\n\npub trait InvertedIndex {\n    fn get_vocab_mut(&mut self) -> &mut HashMap<String, TokenId>;\n\n    fn document_from_tokens(&mut self, tokens: &BTreeSet<String>) -> Document {\n        let vocab = self.get_vocab_mut();\n        let mut document_tokens = vec![];\n        for token in tokens {\n            // check if in vocab\n            let vocab_idx = match vocab.get(token) {\n                Some(&idx) => idx,\n                None => {\n                    let next_token_id = vocab.len() as TokenId;\n                    vocab.insert(token.to_string(), next_token_id);\n                    next_token_id\n                }\n            };\n            document_tokens.push(vocab_idx);\n        }\n        Document::new(document_tokens)\n    }\n\n    fn index_document(\n        &mut self,\n        idx: PointOffsetType,\n        document: Document,\n        hw_counter: &common::counter::hardware_counter::HardwareCounterCell,\n    ) -> OperationResult<()>;\n\n    fn remove_document(&mut self, idx: PointOffsetType) -> bool;\n\n    fn filter<'a>(\n        &'a self,\n        query: ParsedQuery,\n        hw_counter: &'a common::counter::hardware_counter::HardwareCounterCell,\n    ) -> Box<dyn Iterator<Item = PointOffsetType> + 'a>;\n\n    fn get_posting_len(&self, token_id: TokenId, hw_counter: &common::counter::hardware_counter::HardwareCounterCell) -> Option<usize>;\n\n    fn estimate_cardinality(\n        &self,\n        query: &ParsedQuery,\n        condition: &FieldCondition,\n        hw_counter: &common::counter::hardware_counter::HardwareCounterCell,\n    ) -> CardinalityEstimation {\n        let points_count = self.points_count();\n\n        let posting_lengths: Option<Vec<usize>> = query\n            .tokens\n            .iter()\n            .map(|&vocab_idx| match vocab_idx {\n                None => None,\n                Some(idx) => self.get_posting_len(idx, hw_counter),\n            })\n            .collect();\n        if posting_lengths.is_none() || points_count == 0 {\n            // There are unseen tokens -> no matches\n            return CardinalityEstimation {\n                primary_clauses: vec![PrimaryCondition::Condition(Box::new(condition.clone()))],\n                min: 0,\n                exp: 0,\n                max: 0,\n            };\n        }\n        let postings = posting_lengths.unwrap();\n        if postings.is_empty() {\n            // Empty request -> no matches\n            return CardinalityEstimation {\n                primary_clauses: vec![PrimaryCondition::Condition(Box::new(condition.clone()))],\n                min: 0,\n                exp: 0,\n                max: 0,\n            };\n        }\n        // Smallest posting is the largest possible cardinality\n        let smallest_posting = postings.iter().min().copied().unwrap();\n\n        if postings.len() == 1 {\n            CardinalityEstimation {\n                primary_clauses: vec![PrimaryCondition::Condition(Box::new(condition.clone()))],\n                min: smallest_posting,\n                exp: smallest_posting,\n                max: smallest_posting,\n            }\n        } else {\n            let expected_frac: f64 = postings.iter().map(|posting| *posting as f64 / points_count as f64).product();\n            let exp = (expected_frac * points_count as f64) as usize;\n            CardinalityEstimation {\n                primary_clauses: vec![PrimaryCondition::Condition(Box::new(condition.clone()))],\n                min: 0, // ToDo: make better estimation\n                exp,\n                max: smallest_posting,\n            }\n        }\n    }\n\n    fn vocab_with_postings_len_iter(&self) -> impl Iterator<Item = (&str, usize)> + '_;\n\n    fn payload_blocks(\n        &self,\n        threshold: usize,\n        key: PayloadKeyType,\n    ) -> impl Iterator<Item = PayloadBlockCondition> + '_ {\n        let map_filter_condition = move |(token, postings_len): (&str, usize)| {\n            if postings_len >= threshold {\n                Some(PayloadBlockCondition {\n                    condition: FieldCondition::new_match(key.clone(), Match::new_text(token)),\n                    cardinality: postings_len,\n                })\n            } else {\n                None\n            }\n        };\n\n        self.vocab_with_postings_len_iter()\n            .filter_map(map_filter_condition)\n    }\n\n    fn check_match(\n        &self,\n        parsed_query: &ParsedQuery,\n        point_id: PointOffsetType,\n    ) -> bool;\n\n    fn values_is_empty(&self, point_id: PointOffsetType) -> bool;\n\n    fn points_count(&self) -> usize;\n\n    fn get_token_id(&self, token: &str) -> Option<TokenId>;\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.5790246619739707,
      "diff_lines": 332,
      "added_lines": 8,
      "removed_lines": 248,
      "diff_preview": "--- expected+++ predicted@@ -1,16 +1,15 @@ use std::collections::{BTreeSet, HashMap};\n \n-use common::counter::hardware_counter::HardwareCounterCell;\n use common::types::PointOffsetType;\n use serde::{Deserialize, Serialize};\n \n-use crate::common::operation_error::OperationResult;\n+use crate::common::operation_error::{OperationError, OperationResult};\n use crate::index::field_index::{CardinalityEstimation, PayloadBlockCondition, PrimaryCondition};\n use crate::types::{FieldCondition, Match, PayloadKeyType};\n \n pub type TokenId = u32;\n \n-#[derive(Default, Serialize, Deserialize, Debug, Clone)]\n+#[derive(Default, Serialize, Deserialize, Clone)]\n pub struct Document {\n     tokens: Vec<TokenId>,\n }\n@@ -52,7 +51,6 @@         // Check that all tokens are in document\n         self.tokens\n             .iter()\n-            // unwrap crash safety: all tokens exist in the vocabulary if it passes the above check\n             .all(|query_token| document.check(query_token.unwrap()))\n     }\n }\n@@ -75,7 +73,6 @@             };\n             document_tokens.push(vocab_idx);\n         }\n-\n         Document::new(document_tokens)\n     }\n \n@@ -83,7 +80,7 @@         &mut self,\n         idx: PointOffsetType,\n         document: Document,\n-        hw_counter: &HardwareCounterCell,\n+        hw_counter: &common::counter::hardware_counter::HardwareCounterCell,\n     ) -> OperationResult<()>;\n \n     fn remove_document(&mut self, idx: PointOffsetType) -> bool;\n@@ -91,17 +88,16 @@     fn filter<'a>(\n         &'a self,\n         query: ParsedQuery,\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 35.66463303565979,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 33649,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "qdrant_lib_segment_src_index_field_index_full_text_index_text_index.rs",
    "exact_match": false,
    "similarity": 0.7202433670915322,
    "diff_lines": 272,
    "predicted_output": "use std::collections::{BTreeSet, HashSet};\nuse std::path::PathBuf;\nuse std::sync::Arc;\n\nuse common::counter::hardware_counter::HardwareCounterCell;\nuse common::types::PointOffsetType;\nuse parking_lot::RwLock;\nuse rocksdb::DB;\nuse serde::{Deserialize, Serialize};\nuse serde_json::Value;\n\nuse super::immutable_text_index::ImmutableFullTextIndex;\nuse super::inverted_index::{Document, InvertedIndex, ParsedQuery, TokenId};\nuse super::mmap_text_index::{FullTextMmapIndexBuilder, MmapFullTextIndex};\nuse super::mutable_text_index::MutableFullTextIndex;\nuse super::tokenizers::Tokenizer;\nuse crate::common::operation_error::{OperationError, OperationResult};\nuse crate::common::rocksdb_buffered_delete_wrapper::DatabaseColumnScheduledDeleteWrapper;\nuse crate::common::rocksdb_wrapper::DatabaseColumnWrapper;\nuse crate::common::Flusher;\nuse crate::data_types::index::TextIndexParams;\nuse crate::index::field_index::{\n    CardinalityEstimation, FieldIndexBuilderTrait, PayloadBlockCondition, PayloadFieldIndex,\n    ValueIndexer,\n};\nuse crate::telemetry::PayloadIndexTelemetry;\nuse crate::types::{FieldCondition, Match, PayloadKeyType};\n\npub enum FullTextIndex {\n    Mutable(MutableFullTextIndex),\n    Immutable(ImmutableFullTextIndex),\n    Mmap(Box<MmapFullTextIndex>),\n}\n\nimpl FullTextIndex {\n    pub fn new(\n        db: Arc<RwLock<DB>>,\n        config: TextIndexParams,\n        field: &str,\n        is_appendable: bool,\n    ) -> Self {\n        let store_cf_name = Self::storage_cf_name(field);\n        let db_wrapper = DatabaseColumnScheduledDeleteWrapper::new(DatabaseColumnWrapper::new(\n            db,\n            &store_cf_name,\n        ));\n        if is_appendable {\n            Self::Mutable(MutableFullTextIndex::new(db_wrapper, config))\n        } else {\n            Self::Immutable(ImmutableFullTextIndex::new(db_wrapper, config))\n        }\n    }\n\n    pub fn new_mmap(\n        path: PathBuf,\n        config: TextIndexParams,\n        is_on_disk: bool,\n    ) -> OperationResult<Self> {\n        Ok(Self::Mmap(Box::new(MmapFullTextIndex::open(\n            path, config, is_on_disk,\n        )?)))\n    }\n\n    pub fn init(&mut self) -> OperationResult<()> {\n        match self {\n            Self::Mutable(index) => index.init(),\n            Self::Immutable(index) => index.init(),\n            Self::Mmap(_) => unreachable!(\"not applicable for mmap immutable index\"),\n        }\n    }\n\n    pub fn builder(\n        db: Arc<RwLock<DB>>,\n        config: TextIndexParams,\n        field: &str,\n    ) -> FullTextIndexBuilder {\n        FullTextIndexBuilder(Self::new(db, config, field, true))\n    }\n\n    pub fn builder_mmap(\n        path: PathBuf,\n        config: TextIndexParams,\n        is_on_disk: bool,\n    ) -> FullTextMmapIndexBuilder {\n        FullTextMmapIndexBuilder::new(path, config, is_on_disk)\n    }\n\n    fn storage_cf_name(field: &str) -> String {\n        format!(\"{field}_fts\")\n    }\n\n    fn points_count(&self) -> usize {\n        match self {\n            Self::Mutable(index) => index.inverted_index.points_count(),\n            Self::Immutable(index) => index.inverted_index.points_count(),\n            Self::Mmap(index) => index.inverted_index.points_count(),\n        }\n    }\n\n    fn get_token(&self, token: &str, hw_counter: &HardwareCounterCell) -> Option<TokenId> {\n        match self {\n            Self::Mutable(index) => index.inverted_index.get_token_id(token, hw_counter),\n            Self::Immutable(index) => index.inverted_index.get_token_id(token, hw_counter),\n            Self::Mmap(index) => index.inverted_index.get_token_id(token, hw_counter),\n        }\n    }\n\n    fn filter(\n        &self,\n        query: &ParsedQuery,\n        hw_counter: &HardwareCounterCell,\n    ) -> Box<dyn Iterator<Item = PointOffsetType> + '_> {\n        match self {\n            Self::Mutable(index) => index.inverted_index.filter(query, hw_counter),\n            Self::Immutable(index) => index.inverted_index.filter(query, hw_counter),\n            Self::Mmap(index) => index.inverted_index.filter(query, hw_counter),\n        }\n    }\n\n    fn estimate_cardinality(\n        &self,\n        query: &ParsedQuery,\n        condition: &FieldCondition,\n        hw_counter: &HardwareCounterCell,\n    ) -> CardinalityEstimation {\n        match self {\n            Self::Mutable(index) => index.inverted_index.estimate_cardinality(query, condition, hw_counter),\n            Self::Immutable(index) => index.inverted_index.estimate_cardinality(query, condition, hw_counter),\n            Self::Mmap(index) => index.inverted_index.estimate_cardinality(query, condition, hw_counter),\n        }\n    }\n\n    pub fn check_match(\n        &self,\n        query: &ParsedQuery,\n        point_id: PointOffsetType,\n        hw_counter: &HardwareCounterCell,\n    ) -> bool {\n        match self {\n            Self::Mutable(index) => index.inverted_index.check_match(query, point_id, hw_counter),\n            Self::Immutable(index) => index.inverted_index.check_match(query, point_id, hw_counter),\n            Self::Mmap(index) => index.inverted_index.check_match(query, point_id, hw_counter),\n        }\n    }\n\n    pub fn values_count(&self, point_id: PointOffsetType) -> usize {\n        match self {\n            Self::Mutable(index) => index.inverted_index.values_count(point_id),\n            Self::Immutable(index) => index.inverted_index.values_count(point_id),\n            Self::Mmap(index) => index.inverted_index.values_count(point_id),\n        }\n    }\n\n    pub fn values_is_empty(&self, point_id: PointOffsetType) -> bool {\n        match self {\n            Self::Mutable(index) => index.inverted_index.values_is_empty(point_id),\n            Self::Immutable(index) => index.inverted_index.values_is_empty(point_id),\n            Self::Mmap(index) => index.inverted_index.values_is_empty(point_id),\n        }\n    }\n\n    pub fn get_telemetry_data(&self) -> PayloadIndexTelemetry {\n        PayloadIndexTelemetry {\n            field_name: None,\n            index_type: match self {\n                FullTextIndex::Mutable(_) => \"mutable_full_text\",\n                FullTextIndex::Immutable(_) => \"immutable_full_text\",\n                FullTextIndex::Mmap(_) => \"mmap_full_text\",\n            },\n            points_values_count: self.points_count(),\n            points_count: self.points_count(),\n            histogram_bucket_size: None,\n        }\n    }\n\n    pub fn parse_query(&self, text: &str, hw_counter: &HardwareCounterCell) -> ParsedQuery {\n        let mut tokens = HashSet::new();\n        Tokenizer::tokenize_query(text, self.config(), |token| {\n            tokens.insert(self.get_token(token, hw_counter));\n        });\n        ParsedQuery {\n            tokens: tokens.into_iter().collect(),\n        }\n    }\n\n    pub fn parse_document(&self, text: &str, hw_counter: &HardwareCounterCell) -> Document {\n        let mut document_tokens = vec![];\n        Tokenizer::tokenize_doc(text, self.config(), |token| {\n            if let Some(token_id) = self.get_token(token, hw_counter) {\n                document_tokens.push(token_id);\n            }\n        });\n        Document::new(document_tokens)\n    }\n\n    #[cfg(test)]\n    pub fn query(\n        &self,\n        query: &str,\n        hw_counter: &HardwareCounterCell,\n    ) -> Box<dyn Iterator<Item = PointOffsetType> + '_> {\n        let parsed_query = self.parse_query(query, hw_counter);\n        self.filter(parsed_query, hw_counter)\n    }\n\n    pub fn populate(&self) -> OperationResult<()> {\n        match self {\n            FullTextIndex::Mutable(_) => {}   // Not a mmap\n            FullTextIndex::Immutable(_) => {} // Not a mmap\n            FullTextIndex::Mmap(index) => index.populate()?,\n        }\n        Ok(())\n    }\n\n    pub fn clear_cache(&self) -> OperationResult<()> {\n        match self {\n            FullTextIndex::Mutable(_) => {}   // Not a mmap\n            FullTextIndex::Immutable(_) => {} // Not a mmap\n            FullTextIndex::Mmap(index) => index.clear_cache()?,\n        }\n        Ok(())\n    }\n\n    pub fn is_on_disk(&self) -> bool {\n        match self {\n            FullTextIndex::Mutable(_) => false,\n            FullTextIndex::Immutable(_) => false,\n            FullTextIndex::Mmap(index) => index.is_on_disk(),\n        }\n    }\n}\n\npub struct FullTextIndexBuilder(FullTextIndex);\n\nimpl FieldIndexBuilderTrait for FullTextIndexBuilder {\n    type FieldIndexType = FullTextIndex;\n\n    fn init(&mut self) -> OperationResult<()> {\n        self.0.init()\n    }\n\n    fn add_point(\n        &mut self,\n        id: PointOffsetType,\n        payload: &[&Value],\n        hw_counter: &HardwareCounterCell,\n    ) -> OperationResult<()> {\n        self.0.add_point(id, payload, hw_counter)\n    }\n\n    fn finalize(self) -> OperationResult<Self::FieldIndexType> {\n        Ok(self.0)\n    }\n}\n\nimpl ValueIndexer for FullTextIndex {\n    type ValueType = String;\n\n    fn add_many(\n        &mut self,\n        idx: PointOffsetType,\n        values: Vec<String>,\n        hw_counter: &HardwareCounterCell,\n    ) -> OperationResult<()> {\n        match self {\n            Self::Mutable(index) => index.add_many(idx, values, hw_counter),\n            Self::Immutable(_) => Err(OperationError::service_error(\n                \"Cannot add values to immutable text index\",\n            )),\n            Self::Mmap(_) => Err(OperationError::service_error(\n                \"Cannot add values to mmap text index\",\n            )),\n        }\n    }\n\n    fn get_value(value: &Value) -> Option<String> {\n        if let Value::String(keyword) = value {\n            return Some(keyword.to_owned());\n        }\n        None\n    }\n\n    fn remove_point(&mut self, id: PointOffsetType) -> OperationResult<()> {\n        match self {\n            FullTextIndex::Mutable(index) => index.remove_point(id),\n            FullTextIndex::Immutable(index) => index.remove_point(id),\n            FullTextIndex::Mmap(index) => index.remove_point(id),\n        }\n    }\n}\n\nimpl PayloadFieldIndex for FullTextIndex {\n    fn count_indexed_points(&self) -> usize {\n        self.points_count()\n    }\n\n    fn load(&mut self) -> OperationResult<bool> {\n        match self {\n            Self::Mutable(index) => index.load_from_db(),\n            Self::Immutable(index) => index.load_from_db(),\n            Self::Mmap(_) => Ok(true), // mmap index is always loaded\n        }\n    }\n\n    fn clear(self) -> OperationResult<()> {\n        match self {\n            Self::Mutable(index) => index.clear(),\n            Self::Immutable(index) => index.clear(),\n            Self::Mmap(index) => index.clear(),\n        }\n    }\n\n    fn flusher(&self) -> Flusher {\n        match self {\n            Self::Mutable(index) => index.db_wrapper.flusher(),\n            Self::Immutable(index) => index.db_wrapper.flusher(),\n            Self::Mmap(index) => index.flusher(),\n        }\n    }\n\n    fn files(&self) -> Vec<PathBuf> {\n        match self {\n            Self::Mutable(_) => vec![],\n            Self::Immutable(_) => vec![],\n            Self::Mmap(index) => index.files(),\n        }\n    }\n\n    fn filter(\n        &self,\n        condition: &FieldCondition,\n        hw_acc: HwMeasurementAcc,\n    ) -> Option<Box<dyn Iterator<Item = PointOffsetType> + '_>> {\n        let hw_counter = hw_acc.get_counter_cell();\n        if let Some(Match::Text(text_match)) = &condition.r#match {\n            let parsed_query = self.parse_query(&text_match.text, hw_counter);\n            return Some(self.filter(parsed_query, hw_counter));\n        }\n        None\n    }\n\n    fn estimate_cardinality(\n        &self,\n        condition: &FieldCondition,\n        hw_counter: &HardwareCounterCell,\n    ) -> Option<CardinalityEstimation> {\n        if let Some(Match::Text(text_match)) = &condition.r#match {\n            let parsed_query = self.parse_query(&text_match.text, hw_counter);\n            return Some(self.estimate_cardinality(&parsed_query, condition, hw_counter));\n        }\n        None\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.7202433670915322,
      "diff_lines": 272,
      "added_lines": 48,
      "removed_lines": 121,
      "diff_preview": "--- expected+++ predicted@@ -14,10 +14,10 @@ use super::mmap_text_index::{FullTextMmapIndexBuilder, MmapFullTextIndex};\n use super::mutable_text_index::MutableFullTextIndex;\n use super::tokenizers::Tokenizer;\n-use crate::common::Flusher;\n use crate::common::operation_error::{OperationError, OperationResult};\n use crate::common::rocksdb_buffered_delete_wrapper::DatabaseColumnScheduledDeleteWrapper;\n use crate::common::rocksdb_wrapper::DatabaseColumnWrapper;\n+use crate::common::Flusher;\n use crate::data_types::index::TextIndexParams;\n use crate::index::field_index::{\n     CardinalityEstimation, FieldIndexBuilderTrait, PayloadBlockCondition, PayloadFieldIndex,\n@@ -33,7 +33,7 @@ }\n \n impl FullTextIndex {\n-    pub fn new_memory(\n+    pub fn new(\n         db: Arc<RwLock<DB>>,\n         config: TextIndexParams,\n         field: &str,\n@@ -74,7 +74,7 @@         config: TextIndexParams,\n         field: &str,\n     ) -> FullTextIndexBuilder {\n-        FullTextIndexBuilder(Self::new_memory(db, config, field, true))\n+        FullTextIndexBuilder(Self::new(db, config, field, true))\n     }\n \n     pub fn builder_mmap(\n@@ -89,14 +89,6 @@         format!(\"{field}_fts\")\n     }\n \n-    fn config(&self) -> &TextIndexParams {\n-        match self {\n-            Self::Mutable(index) => &index.config,\n-            Self::Immutable(index) => &index.config,\n-            Self::Mmap(index) => &index.config,\n-        }\n-    }\n-\n     fn points_count(&self) -> usize {\n         match self {\n             Self::Mutable(index) => index.inverted_index.points_count(),\n@@ -113,11 +105,11 @@         }\n     }\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 56.972580909729004,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 34279,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "qdrant_lib_segment_src_index_field_index_map_index_mod.rs",
    "exact_match": false,
    "similarity": 0.5834863368937373,
    "diff_lines": 1255,
    "predicted_output": "pub mod immutable_map_index;\npub mod mutable_map_index;\n\nuse std::borrow::Borrow;\nuse std::fmt::{Debug, Display};\nuse std::hash::{BuildHasher, Hash};\nuse std::iter;\nuse std::str::FromStr;\nuse std::sync::Arc;\n\nuse ahash::HashMap;\nuse common::mmap_hashmap::Key;\nuse common::types::PointOffsetType;\nuse indexmap::IndexSet;\nuse itertools::Itertools;\nuse parking_lot::RwLock;\nuse rocksdb::DB;\nuse serde_json::Value;\nuse smol_str::SmolStr;\n\nuse self::immutable_map_index::ImmutableMapIndex;\nuse self::mutable_map_index::MutableMapIndex;\nuse super::FieldIndexBuilderTrait;\nuse crate::common::operation_error::{OperationError, OperationResult};\nuse crate::common::rocksdb_buffered_delete_wrapper::DatabaseColumnScheduledDeleteWrapper;\nuse crate::common::Flusher;\nuse crate::index::field_index::stat_tools::number_of_selected_points;\nuse crate::index::field_index::{\n    CardinalityEstimation, PayloadBlockCondition, PayloadFieldIndex, PrimaryCondition, ValueIndexer,\n};\nuse crate::types::{\n    AnyVariants, FieldCondition, IntPayloadType, Match, MatchAny, MatchExcept, MatchValue,\n    PayloadKeyType, ValueVariants,\n};\n\npub enum MapIndex<N: MapIndexKey + ?Sized> {\n    Mutable(MutableMapIndex<N>),\n    Immutable(ImmutableMapIndex<N>),\n}\n\nimpl<N: MapIndexKey + ?Sized> MapIndex<N> {\n    pub fn new(db: Arc<RwLock<DB>>, field_name: &str, is_appendable: bool) -> Self {\n        if is_appendable {\n            MapIndex::Mutable(MutableMapIndex::new(db, field_name))\n        } else {\n            MapIndex::Immutable(ImmutableMapIndex::new(db, field_name))\n        }\n    }\n\n    pub fn new_mmap(path: &Path, is_on_disk: bool) -> OperationResult<Self> {\n        Ok(MapIndex::Mmap(Box::new(MmapMapIndex::load(\n            path, is_on_disk,\n        )?)))\n    }\n\n    pub fn builder(db: Arc<RwLock<DB>>, field_name: &str) -> MapIndexBuilder<N> {\n        MapIndexBuilder(MapIndex::Mutable(MutableMapIndex::new(db, field_name)))\n    }\n\n    pub fn mmap_builder(path: &Path, is_on_disk: bool) -> MapIndexMmapBuilder<N> {\n        MapIndexMmapBuilder {\n            path: path.to_owned(),\n            point_to_values: Default::default(),\n            values_to_points: Default::default(),\n            is_on_disk,\n        }\n    }\n\n    fn get_db_wrapper(&self) -> &DatabaseColumnScheduledDeleteWrapper {\n        match self {\n            MapIndex::Mutable(index) => index.get_db_wrapper(),\n            MapIndex::Immutable(index) => index.get_db_wrapper(),\n        }\n    }\n\n    fn load_from_db(&mut self) -> OperationResult<bool> {\n        match self {\n            MapIndex::Mutable(index) => index.load_from_db(),\n            MapIndex::Immutable(index) => index.load_from_db(),\n            MapIndex::Mmap(_) => Ok(true),\n        }\n    }\n\n    pub fn get_values(&self, idx: PointOffsetType) -> Option<Box<dyn Iterator<Item = &N> + '_>> {\n        match self {\n            MapIndex::Mutable(index) => Some(Box::new(index.get_values(idx)?)),\n            MapIndex::Immutable(index) => Some(Box::new(index.get_values(idx)?)),\n            MapIndex::Mmap(index) => Some(Box::new(index.get_values(idx)?)),\n        }\n    }\n\n    pub fn values_count(&self, idx: PointOffsetType) -> usize {\n        match self {\n            MapIndex::Mutable(index) => index.values_count(idx).unwrap_or_default(),\n            MapIndex::Immutable(index) => index.values_count(idx).unwrap_or_default(),\n            MapIndex::Mmap(index) => index.values_count(idx).unwrap_or_default(),\n        }\n    }\n\n    pub fn get_indexed_points(&self) -> usize {\n        match self {\n            MapIndex::Mutable(index) => index.get_indexed_points(),\n            MapIndex::Immutable(index) => index.get_indexed_points(),\n            MapIndex::Mmap(index) => index.get_indexed_points(),\n        }\n    }\n\n    pub fn get_values_count(&self) -> usize {\n        match self {\n            MapIndex::Mutable(index) => index.get_values_count(),\n            MapIndex::Immutable(index) => index.get_values_count(),\n            MapIndex::Mmap(index) => index.get_values_count(),\n        }\n    }\n\n    pub fn get_unique_values_count(&self) -> usize {\n        match self {\n            MapIndex::Mutable(index) => index.get_unique_values_count(),\n            MapIndex::Immutable(index) => index.get_unique_values_count(),\n            MapIndex::Mmap(index) => index.get_unique_values_count(),\n        }\n    }\n\n    pub fn get_count_for_value(&self, value: &N) -> Option<usize> {\n        match self {\n            MapIndex::Mutable(index) => index.get_count_for_value(value),\n            MapIndex::Immutable(index) => index.get_count_for_value(value),\n            MapIndex::Mmap(index) => index.get_count_for_value(value),\n        }\n    }\n\n    pub fn get_iterator(&self, value: &N) -> Box<dyn Iterator<Item = PointOffsetType> + '_> {\n        match self {\n            MapIndex::Mutable(index) => index.get_iterator(value),\n            MapIndex::Immutable(index) => index.get_iterator(value),\n            MapIndex::Mmap(index) => index.get_iterator(value),\n        }\n    }\n\n    pub fn iter_values(&self) -> Box<dyn Iterator<Item = &N> + '_> {\n        match self {\n            MapIndex::Mutable(index) => index.iter_values(),\n            MapIndex::Immutable(index) => index.iter_values(),\n            MapIndex::Mmap(index) => index.iter_values(),\n        }\n    }\n\n    pub fn iter_values_map(&self) -> Box<dyn Iterator<Item = (&N, IdIter<'_>)> + '_> {\n        match self {\n            MapIndex::Mutable(index) => Box::new(index.iter_values_map()),\n            MapIndex::Immutable(index) => Box::new(index.iter_values_map()),\n            MapIndex::Mmap(index) => Box::new(index.iter_values_map()),\n        }\n    }\n\n    pub fn iter_counts_per_value(&self) -> Box<dyn Iterator<Item = (&N, usize)> + '_> {\n        match self {\n            MapIndex::Mutable(index) => Box::new(index.iter_counts_per_value()),\n            MapIndex::Immutable(index) => Box::new(index.iter_counts_per_value()),\n            MapIndex::Mmap(index) => Box::new(index.iter_counts_per_value()),\n        }\n    }\n\n    pub fn is_on_disk(&self) -> bool {\n        match self {\n            MapIndex::Mutable(_) => false,\n            MapIndex::Immutable(_) => false,\n            MapIndex::Mmap(index) => index.is_on_disk(),\n        }\n    }\n\n    pub fn populate(&self) -> OperationResult<()> {\n        match self {\n            MapIndex::Mutable(_) => {}   // Not a mmap\n            MapIndex::Immutable(_) => {} // Not a mmap\n            MapIndex::Mmap(index) => index.populate()?,\n        }\n        Ok(())\n    }\n\n    pub fn clear_cache(&self) -> OperationResult<()> {\n        match self {\n            MapIndex::Mutable(_) => {}   // Not a mmap\n            MapIndex::Immutable(_) => {} // Not a mmap\n            MapIndex::Mmap(index) => index.clear_cache()?,\n        }\n        Ok(())\n    }\n}\n\npub struct MapIndexBuilder<N: MapIndexKey + ?Sized>(MapIndex<N>);\n\nimpl<N: MapIndexKey + ?Sized> FieldIndexBuilderTrait for MapIndexBuilder<N>\nwhere\n    MapIndex<N>: PayloadFieldIndex + ValueIndexer,\n{\n    type FieldIndexType = MapIndex<N>;\n\n    fn init(&mut self) -> OperationResult<()> {\n        match &mut self.0 {\n            MapIndex::Mutable(index) => index.get_db_wrapper().recreate_column_family(),\n            MapIndex::Immutable(index) => index.get_db_wrapper().recreate_column_family(),\n            MapIndex::Mmap(_) => unreachable!(),\n        }\n    }\n\n    fn add_point(&mut self, id: PointOffsetType, values: &[&Value]) -> OperationResult<()> {\n        self.0.add_point(id, values)\n    }\n\n    fn finalize(self) -> OperationResult<Self::FieldIndexType> {\n        Ok(self.0)\n    }\n}\n\npub struct MapIndexMmapBuilder<N: MapIndexKey + ?Sized> {\n    path: PathBuf,\n    point_to_values: Vec<Vec<N::Owned>>,\n    values_to_points: HashMap<N::Owned, Vec<PointOffsetType>>,\n    is_on_disk: bool,\n}\n\nimpl<N: MapIndexKey + ?Sized> FieldIndexBuilderTrait for MapIndexMmapBuilder<N>\nwhere\n    MapIndex<N>: PayloadFieldIndex + ValueIndexer,\n    <MapIndex<N> as ValueIndexer>::ValueType: Into<N::Owned>,\n{\n    type FieldIndexType = MapIndex<N>;\n\n    fn init(&mut self) -> OperationResult<()> {\n        Ok(())\n    }\n\n    fn add_point(&mut self, id: PointOffsetType, payload: &[&Value]) -> OperationResult<()> {\n        let mut flatten_values: Vec<_> = vec![];\n        for value in payload.iter() {\n            let payload_values = <MapIndex<N> as ValueIndexer>::get_values(value);\n            flatten_values.extend(payload_values);\n        }\n        let flatten_values: Vec<N::Owned> = flatten_values.into_iter().map(Into::into).collect();\n\n        if self.point_to_values.len() <= id as usize {\n            self.point_to_values.resize_with(id as usize + 1, Vec::new);\n        }\n\n        self.point_to_values[id as usize].extend(flatten_values.clone());\n\n        let mut hw_counter_val = 0;\n\n        for value in flatten_values {\n            let entry = self.values_to_points.entry(value);\n\n            if let Entry::Vacant(e) = &entry {\n                let size = N::mmapped_size(N::as_referenced(e.key().borrow()));\n                hw_counter_val += size;\n            }\n\n            hw_counter_val += size_of_val(&id);\n            entry.or_default().push(id);\n        }\n\n        Ok(())\n    }\n\n    fn finalize(self) -> OperationResult<Self::FieldIndexType> {\n        Ok(MapIndex::Mmap(Box::new(MmapMapIndex::build(\n            &self.path,\n            self.point_to_values,\n            self.values_to_points,\n            self.is_on_disk,\n        )?)))\n    }\n}\n\nimpl PayloadFieldIndex for MapIndex<str> {\n    fn count_indexed_points(&self) -> usize {\n        self.get_indexed_points()\n    }\n\n    fn load(&mut self) -> OperationResult<bool> {\n        self.load_from_db()\n    }\n\n    fn clear(self) -> OperationResult<()> {\n        self.clear()\n    }\n\n    fn flusher(&self) -> Flusher {\n        MapIndex::flusher(self)\n    }\n\n    fn files(&self) -> Vec<PathBuf> {\n        self.files()\n    }\n\n    fn filter<'a>(\n        &'a self,\n        condition: &'a FieldCondition,\n        hw_counter: &HardwareCounterCell,\n    ) -> Option<Box<dyn Iterator<Item = PointOffsetType> + 'a>> {\n        match &condition.r#match {\n            Some(Match::Value(MatchValue { value })) => match value {\n                ValueVariants::String(keyword) => Some(Box::new(\n                    self.get_iterator(keyword.as_str(), hw_counter).copied(),\n                )),\n                ValueVariants::Integer(_) => None,\n                ValueVariants::Bool(_) => None,\n            },\n            Some(Match::Any(MatchAny { any: any_variant })) => match any_variant {\n                AnyVariants::Strings(keywords) => Some(Box::new(\n                    keywords\n                        .iter()\n                        .flat_map(move |keyword| {\n                            self.get_iterator(keyword.as_str(), hw_counter).copied()\n                        })\n                        .unique(),\n                )),\n                AnyVariants::Integers(integers) => {\n                    if integers.is_empty() {\n                        Some(Box::new(iter::empty()))\n                    } else {\n                        None\n                    }\n                }\n            },\n            Some(Match::Except(MatchExcept { except })) => match except {\n                AnyVariants::Strings(keywords) => Some(self.except_set(keywords, hw_counter)),\n                AnyVariants::Integers(other) => {\n                    if other.is_empty() {\n                        Some(Box::new(iter::empty()))\n                    } else {\n                        None\n                    }\n                }\n            },\n            _ => None,\n        }\n    }\n\n    fn estimate_cardinality(\n        &self,\n        condition: &FieldCondition,\n        hw_counter: &HardwareCounterCell,\n    ) -> Option<CardinalityEstimation> {\n        match &condition.r#match {\n            Some(Match::Value(MatchValue { value })) => match value {\n                ValueVariants::String(keyword) => {\n                    let mut estimation = self.match_cardinality(keyword.as_str(), hw_counter);\n                    estimation\n                        .primary_clauses\n                        .push(PrimaryCondition::Condition(Box::new(condition.clone())));\n                    Some(estimation)\n                }\n                ValueVariants::Integer(_) => None,\n                ValueVariants::Bool(_) => None,\n            },\n            Some(Match::Any(MatchAny { any: any_variant })) => match any_variant {\n                AnyVariants::Strings(keywords) => {\n                    let estimations = keywords\n                        .iter()\n                        .map(|keyword| self.match_cardinality(keyword.as_str(), hw_counter))\n                        .collect::<Vec<_>>();\n                    let estimation = if estimations.is_empty() {\n                        CardinalityEstimation::exact(0)\n                    } else {\n                        combine_should_estimations(&estimations, self.get_indexed_points())\n                    };\n                    Some(estimation.with_primary_clause(PrimaryCondition::Condition(Box::new(\n                        condition.clone(),\n                    ))))\n                }\n                AnyVariants::Integers(integers) => {\n                    if integers.is_empty() {\n                        Some(CardinalityEstimation::exact(0).with_primary_clause(\n                            PrimaryCondition::Condition(Box::new(condition.clone())),\n                        ))\n                    } else {\n                        None\n                    }\n                }\n            },\n            Some(Match::Except(MatchExcept { except })) => {\n                match except {\n                    AnyVariants::Strings(keywords) => {\n                        Some(self.except_cardinality(keywords.iter().map(|k| k.as_str()), hw_counter))\n                    }\n                    AnyVariants::Integers(others) => {\n                        if others.is_empty() {\n                            Some(CardinalityEstimation::exact(0).with_primary_clause(\n                                PrimaryCondition::Condition(Box::new(condition.clone())),\n                            ))\n                        } else {\n                            None\n                        }\n                    }\n                }\n            }\n            _ => None,\n        }\n    }\n}\n\nimpl PayloadFieldIndex for MapIndex<UuidIntType> {\n    fn count_indexed_points(&self) -> usize {\n        self.get_indexed_points()\n    }\n\n    fn load(&mut self) -> OperationResult<bool> {\n        self.load_from_db()\n    }\n\n    fn clear(self) -> OperationResult<()> {\n        self.clear()\n    }\n\n    fn flusher(&self) -> Flusher {\n        MapIndex::flusher(self)\n    }\n\n    fn files(&self) -> Vec<PathBuf> {\n        self.files()\n    }\n\n    fn filter<'a>(\n        &'a self,\n        condition: &'a FieldCondition,\n        hw_counter: &HardwareCounterCell,\n    ) -> Option<Box<dyn Iterator<Item = PointOffsetType> + 'a>> {\n        match &condition.r#match {\n            Some(Match::Value(MatchValue { value })) => match value {\n                ValueVariants::String(uuid_string) => {\n                    let uuid = Uuid::from_str(uuid_string).ok()?;\n                    Some(Box::new(\n                        self.get_iterator(&uuid.as_u128(), hw_counter).copied(),\n                    ))\n                }\n                ValueVariants::Integer(_) => None,\n                ValueVariants::Bool(_) => None,\n            },\n            Some(Match::Any(MatchAny { any: any_variant })) => match any_variant {\n                AnyVariants::Strings(uuids_string) => {\n                    let uuids: Result<IndexSet<u128>, _> = uuids_string\n                        .iter()\n                        .map(|uuid_string| Uuid::from_str(uuid_string).map(|x| x.as_u128()))\n                        .collect();\n\n                    let excluded_uuids = uuids.ok()?;\n                    let exclude_iter = self\n                        .iter_values()\n                        .filter(move |key| !excluded_uuids.contains(*key))\n                        .flat_map(move |key| self.get_iterator(key, hw_counter).copied())\n                        .unique();\n                    Some(Box::new(exclude_iter))\n                }\n                AnyVariants::Integers(integers) => {\n                    if integers.is_empty() {\n                        Some(Box::new(iter::empty()))\n                    } else {\n                        None\n                    }\n                }\n            },\n            Some(Match::Except(MatchExcept { except })) => match except {\n                AnyVariants::Strings(uuids_string) => {\n                    let uuids: Result<IndexSet<u128>, _> = uuids_string\n                        .iter()\n                        .map(|uuid_string| Uuid::from_str(uuid_string).map(|x| x.as_u128()))\n                        .collect();\n\n                    let excluded_uuids = uuids.ok()?;\n                    Some(self.except_cardinality(excluded_uuids.iter(), hw_counter))\n                }\n                AnyVariants::Integers(other) => {\n                    if other.is_empty() {\n                        Some(CardinalityEstimation::exact(0).with_primary_clause(\n                            PrimaryCondition::Condition(Box::new(condition.clone())),\n                        ))\n                    } else {\n                        None\n                    }\n                }\n            },\n            _ => None,\n        }\n    }\n\n    fn estimate_cardinality(\n        &self,\n        condition: &FieldCondition,\n        hw_counter: &HardwareCounterCell,\n    ) -> Option<CardinalityEstimation> {\n        match &condition.r#match {\n            Some(Match::Value(MatchValue { value })) => match value {\n                ValueVariants::String(uuid_string) => {\n                    let uuid = Uuid::from_str(uuid_string).ok()?;\n                    let mut estimation = self.match_cardinality(&uuid.as_u128(), hw_counter);\n                    estimation\n                        .primary_clauses\n                        .push(PrimaryCondition::Condition(Box::new(condition.clone())));\n                    Some(estimation)\n                }\n                ValueVariants::Integer(_) => None,\n                ValueVariants::Bool(_) => None,\n            },\n            Some(Match::Any(MatchAny { any: any_variant })) => match any_variant {\n                AnyVariants::Strings(uuids_string) => {\n                    let uuids: Result<IndexSet<u128>, _> = uuids_string\n                        .iter()\n                        .map(|uuid_string| Uuid::from_str(uuid_string).map(|x| x.as_u128()))\n                        .collect();\n\n                    let uuids = uuids.ok()?;\n                    let estimations = uuids\n                        .into_iter()\n                        .map(|uuid| self.match_cardinality(&uuid, hw_counter))\n                        .collect::<Vec<_>>();\n                    let estimation = if estimations.is_empty() {\n                        CardinalityEstimation::exact(0)\n                    } else {\n                        combine_should_estimations(&estimations, self.get_indexed_points())\n                    };\n                    Some(estimation.with_primary_clause(PrimaryCondition::Condition(Box::new(\n                        condition.clone(),\n                    ))))\n                }\n                AnyVariants::Integers(integers) => {\n                    if integers.is_empty() {\n                        Some(CardinalityEstimation::exact(0).with_primary_clause(\n                            PrimaryCondition::Condition(Box::new(condition.clone())),\n                        ))\n                    } else {\n                        None\n                    }\n                }\n            },\n            Some(Match::Except(MatchExcept { except })) => {\n                match except {\n                    AnyVariants::Strings(uuids_string) => {\n                        let uuids: Result<IndexSet<u128>, _> = uuids_string\n                            .iter()\n                            .map(|uuid_string| Uuid::from_str(uuid_string).map(|x| x.as_u128()))\n                            .collect();\n\n                        let excluded_uuids = uuids.ok()?;\n                        Some(self.except_cardinality(excluded_uuids.iter(), hw_counter))\n                    }\n                    AnyVariants::Integers(other) => {\n                        if other.is_empty() {\n                            Some(CardinalityEstimation::exact(0).with_primary_clause(\n                                PrimaryCondition::Condition(Box::new(condition.clone())),\n                            ))\n                        } else {\n                            None\n                        }\n                    }\n                }\n            }\n            _ => None,\n        }\n    }\n\n    fn payload_blocks(\n        &self,\n        threshold: usize,\n        key: PayloadKeyType,\n    ) -> Box<dyn Iterator<Item = PayloadBlockCondition> + '_> {\n        Box::new(\n            self.iter_values()\n                .map(|value| (value, self.get_count_for_value(value, &HardwareCounterCell::disposable()).unwrap_or(0)))\n                .filter(move |(_value, count)| *count >= threshold)\n                .map(move |(value, count)| PayloadBlockCondition {\n                    condition: FieldCondition::new_match(key.clone(), (*value).into()),\n                    cardinality: count,\n                }),\n        )\n    }\n}\n\nimpl PayloadFieldIndex for MapIndex<IntPayloadType> {\n    fn count_indexed_points(&self) -> usize {\n        self.get_indexed_points()\n    }\n\n    fn load(&mut self) -> OperationResult<bool> {\n        self.load_from_db()\n    }\n\n    fn clear(self) -> OperationResult<()> {\n        self.clear()\n    }\n\n    fn flusher(&self) -> Flusher {\n        MapIndex::flusher(self)\n    }\n\n    fn files(&self) -> Vec<PathBuf> {\n        self.files()\n    }\n\n    fn filter<'a>(\n        &'a self,\n        condition: &'a FieldCondition,\n        hw_counter: &HardwareCounterCell,\n    ) -> Option<Box<dyn Iterator<Item = PointOffsetType> + 'a>> {\n        match &condition.r#match {\n            Some(Match::Value(MatchValue { value })) => match value {\n                ValueVariants::String(_) => None,\n                ValueVariants::Integer(integer) => {\n                    Some(Box::new(self.get_iterator(integer, hw_counter).copied()))\n                }\n                ValueVariants::Bool(_) => None,\n            },\n            Some(Match::Any(MatchAny { any: any_variant })) => match any_variant {\n                AnyVariants::Strings(_) => None,\n                AnyVariants::Integers(integers) => Some(self.except_set(integers, hw_counter)),\n            },\n            Some(Match::Except(MatchExcept { except })) => match except {\n                AnyVariants::Strings(_) => None,\n                AnyVariants::Integers(integers) => Some(self.except_set(integers, hw_counter)),\n            },\n            _ => None,\n        }\n    }\n\n    fn estimate_cardinality(\n        &self,\n        condition: &FieldCondition,\n        hw_counter: &HardwareCounterCell,\n    ) -> Option<CardinalityEstimation> {\n        match &condition.r#match {\n            Some(Match::Value(MatchValue { value })) => match value {\n                ValueVariants::String(_) => None,\n                ValueVariants::Integer(integer) => {\n                    let mut estimation = self.match_cardinality(integer, hw_counter);\n                    estimation\n                        .primary_clauses\n                        .push(PrimaryCondition::Condition(Box::new(condition.clone())));\n                    Some(estimation)\n                }\n                ValueVariants::Bool(_) => None,\n            },\n            Some(Match::Any(MatchAny { any: any_variants })) => match any_variants {\n                AnyVariants::Strings(_) => None,\n                AnyVariants::Integers(integers) => {\n                    let estimations = integers\n                        .iter()\n                        .map(|integer| self.match_cardinality(integer, hw_counter))\n                        .collect::<Vec<_>>();\n                    let estimation = if estimations.is_empty() {\n                        CardinalityEstimation::exact(0)\n                    } else {\n                        combine_should_estimations(&estimations, self.get_indexed_points())\n                    };\n                    Some(estimation.with_primary_clause(PrimaryCondition::Condition(Box::new(\n                        condition.clone(),\n                    ))))\n                }\n            },\n            Some(Match::Except(MatchExcept { except })) => {\n                match except {\n                    AnyVariants::Strings(_) => None,\n                    AnyVariants::Integers(integers) => {\n                        Some(self.except_cardinality(integers.iter(), hw_counter))\n                    }\n                }\n            }\n            _ => None,\n        }\n    }\n\n    fn payload_blocks(\n        &self,\n        threshold: usize,\n        key: PayloadKeyType,\n    ) -> Box<dyn Iterator<Item = PayloadBlockCondition> + '_> {\n        Box::new(\n            self.iter_values()\n                .map(|value| (value, self.get_count_for_value(value, &HardwareCounterCell::disposable()).unwrap_or(0)))\n                .filter(move |(_value, count)| *count >= threshold)\n                .map(move |(value, count)| PayloadBlockCondition {\n                    condition: FieldCondition::new_match(key.clone(), (*value).into()),\n                    cardinality: count,\n                }),\n        )\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.5834863368937373,
      "diff_lines": 1255,
      "added_lines": 132,
      "removed_lines": 844,
      "diff_preview": "--- expected+++ predicted@@ -1,89 +1,45 @@+pub mod immutable_map_index;\n+pub mod mutable_map_index;\n+\n use std::borrow::Borrow;\n-use std::collections::hash_map::Entry;\n use std::fmt::{Debug, Display};\n use std::hash::{BuildHasher, Hash};\n use std::iter;\n-use std::path::{Path, PathBuf};\n use std::str::FromStr;\n use std::sync::Arc;\n \n use ahash::HashMap;\n-use common::counter::hardware_counter::HardwareCounterCell;\n use common::mmap_hashmap::Key;\n use common::types::PointOffsetType;\n use indexmap::IndexSet;\n use itertools::Itertools;\n-use mmap_map_index::MmapMapIndex;\n use parking_lot::RwLock;\n use rocksdb::DB;\n use serde_json::Value;\n use smol_str::SmolStr;\n-use uuid::Uuid;\n \n use self::immutable_map_index::ImmutableMapIndex;\n use self::mutable_map_index::MutableMapIndex;\n use super::FieldIndexBuilderTrait;\n-use super::facet_index::FacetIndex;\n-use super::mmap_point_to_values::MmapValue;\n+use crate::common::operation_error::{OperationError, OperationResult};\n+use crate::common::rocksdb_buffered_delete_wrapper::DatabaseColumnScheduledDeleteWrapper;\n use crate::common::Flusher;\n-use crate::common::operation_error::{OperationError, OperationResult};\n-use crate::data_types::facets::{FacetHit, FacetValueRef};\n use crate::index::field_index::stat_tools::number_of_selected_points;\n use crate::index::field_index::{\n     CardinalityEstimation, PayloadBlockCondition, PayloadFieldIndex, PrimaryCondition, ValueIndexer,\n };\n-use crate::index::query_estimator::combine_should_estimations;\n-use crate::telemetry::PayloadIndexTelemetry;\n use crate::types::{\n     AnyVariants, FieldCondition, IntPayloadType, Match, MatchAny, MatchExcept, MatchValue,\n-    PayloadKeyType, UuidIntType, ValueVariants,\n+    PayloadKeyType, ValueVariants,\n };\n-\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 106.41569590568542,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 56553,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "qdrant_lib_segment_src_index_field_index_numeric_index_mmap_numeric_index.rs",
    "exact_match": false,
    "similarity": 0.9414685770107457,
    "diff_lines": 122,
    "predicted_output": "use std::fs::{create_dir_all, remove_dir};\nuse std::ops::Bound;\nuse std::path::{Path, PathBuf};\n\nuse common::counter::hardware_counter::HardwareCounterCell;\nuse common::counter::iterator_hw_measurement::HwMeasurementIteratorExt;\nuse common::types::PointOffsetType;\nuse io::file_operations::{atomic_save_json, read_json};\nuse memmap2::MmapMut;\nuse memory::fadvise::clear_disk_cache;\nuse memory::madvise::AdviceSetting;\nuse memory::mmap_ops::{self, create_and_ensure_length};\nuse memory::mmap_type::{MmapBitSlice, MmapSlice};\nuse serde::{Deserialize, Serialize};\n\nuse super::mutable_numeric_index::InMemoryNumericIndex;\nuse super::Encodable;\nuse crate::common::mmap_bitslice_buffered_update_wrapper::MmapBitSliceBufferedUpdateWrapper;\nuse crate::common::operation_error::OperationResult;\nuse crate::common::Flusher;\nuse crate::index::field_index::histogram::{Histogram, Numericable, Point};\nuse crate::index::field_index::mmap_point_to_values::{MmapPointToValues, MmapValue};\n\nconst PAIRS_PATH: &str = \"data.bin\";\nconst DELETED_PATH: &str = \"deleted.bin\";\nconst CONFIG_PATH: &str = \"mmap_field_index_config.json\";\n\npub struct MmapNumericIndex<T: Encodable + Numericable + Default + MmapValue + 'static> {\n    path: PathBuf,\n    deleted: MmapBitSliceBufferedUpdateWrapper,\n    pairs: MmapSlice<Point<T>>,\n    histogram: Histogram<T>,\n    deleted_count: usize,\n    max_values_per_point: usize,\n    point_to_values: MmapPointToValues<T>,\n    is_on_disk: bool,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\nstruct MmapNumericIndexConfig {\n    max_values_per_point: usize,\n}\n\npub(super) struct NumericIndexPairsIterator<'a, T: Encodable + Numericable> {\n    pairs: &'a [Point<T>],\n    deleted: &'a MmapBitSliceBufferedUpdateWrapper,\n    start_index: usize,\n    end_index: usize,\n}\n\nimpl<'a, T: Encodable + Numericable> Iterator for NumericIndexPairsIterator<'a, T> {\n    type Item = Point<T>;\n\n    fn next(&mut self) -> Option<Self::Item> {\n        while self.start_index < self.end_index {\n            let key = self.pairs[self.start_index].clone();\n            let deleted = self.deleted.get(key.idx as usize).unwrap_or(true);\n            self.start_index += 1;\n            if deleted {\n                continue;\n            }\n            return Some(key);\n        }\n        None\n    }\n}\n\nimpl<T: Encodable + Numericable + Default + MmapValue> MmapNumericIndex<T> {\n    pub fn build(\n        in_memory_index: InMemoryNumericIndex<T>,\n        path: &Path,\n        is_on_disk: bool,\n    ) -> OperationResult<Self> {\n        create_dir_all(path)?;\n\n        let pairs_path = path.join(PAIRS_PATH);\n        let deleted_path = path.join(DELETED_PATH);\n        let config_path = path.join(CONFIG_PATH);\n\n        atomic_save_json(\n            &config_path,\n            &MmapNumericIndexConfig {\n                max_values_per_point: in_memory_index.max_values_per_point,\n            },\n        )?;\n\n        in_memory_index.histogram.save(path)?;\n\n        MmapPointToValues::<T>::from_iter(\n            path,\n            in_memory_index\n                .point_to_values\n                .iter()\n                .enumerate()\n                .map(|(idx, values)| {\n                    (\n                        idx as PointOffsetType,\n                        values.iter().map(|v| T::as_referenced(v)),\n                    )\n                }),\n        )?;\n\n        {\n            let pairs_file = create_and_ensure_length(\n                &pairs_path,\n                in_memory_index.map.len() * std::mem::size_of::<Point<T>>(),\n            )?;\n            let pairs_mmap = unsafe { MmapMut::map_mut(&pairs_file)? };\n            let mut pairs = unsafe { MmapSlice::<Point<T>>::try_from(pairs_mmap)? };\n            for (src, dst) in in_memory_index.map.iter().zip(pairs.iter_mut()) {\n                *dst = src.clone();\n            }\n        }\n\n        {\n            const BITS_IN_BYTE: usize = 8;\n            let deleted_flags_count = in_memory_index.point_to_values.len();\n            let deleted_file = create_and_ensure_length(\n                &deleted_path,\n                BITS_IN_BYTE\n                    * BITS_IN_BYTE\n                    * deleted_flags_count.div_ceil(BITS_IN_BYTE * BITS_IN_BYTE),\n            )?;\n            let mut deleted_mmap = unsafe { MmapMut::map_mut(&deleted_file)? };\n            deleted_mmap.fill(0);\n            let mut deleted_bitflags = MmapBitSlice::from(deleted_mmap, 0);\n            for (idx, values) in in_memory_index.point_to_values.iter().enumerate() {\n                if values.is_empty() {\n                    deleted_bitflags.set(idx, true);\n                }\n            }\n        }\n\n        Self::load(path, is_on_disk)\n    }\n\n    pub fn load(path: &Path, is_on_disk: bool) -> OperationResult<Self> {\n        let pairs_path = path.join(PAIRS_PATH);\n        let deleted_path = path.join(DELETED_PATH);\n        let config_path = path.join(CONFIG_PATH);\n\n        let histogram = Histogram::<T>::load(path)?;\n        let config: MmapNumericIndexConfig = read_json(&config_path)?;\n        let deleted = mmap_ops::open_write_mmap(&deleted_path, AdviceSetting::Global, false)?;\n        let deleted = MmapBitSlice::from(deleted, 0);\n        let deleted_count = deleted.count_ones();\n        let do_populate = !is_on_disk;\n        let map = unsafe {\n            MmapSlice::try_from(mmap_ops::open_write_mmap(\n                &pairs_path,\n                AdviceSetting::Global,\n                do_populate,\n            )?)?\n        };\n        let point_to_values = MmapPointToValues::open(path, do_populate)?;\n\n        Ok(Self {\n            pairs: map,\n            deleted: MmapBitSliceBufferedUpdateWrapper::new(deleted),\n            path: path.to_path_buf(),\n            histogram,\n            deleted_count,\n            max_values_per_point: config.max_values_per_point,\n            point_to_values,\n            is_on_disk,\n        })\n    }\n\n    pub fn clear(self) -> OperationResult<()> {\n        let files = self.files();\n        let Self { path, .. } = self;\n        for file in files {\n            std::fs::remove_file(file)?;\n        }\n        let _ = remove_dir(path);\n        Ok(())\n    }\n\n    pub fn files(&self) -> Vec<PathBuf> {\n        let mut files = vec![\n            self.path.join(PAIRS_PATH),\n            self.path.join(DELETED_PATH),\n            self.path.join(CONFIG_PATH),\n        ];\n        files.extend(self.point_to_values.files());\n        files.extend(Histogram::<T>::files(&self.path));\n        files\n    }\n\n    pub fn flusher(&self) -> Flusher {\n        self.deleted.flusher()\n    }\n\n    pub fn check_values_any(\n        &self,\n        idx: PointOffsetType,\n        check_fn: impl Fn(&T) -> bool,\n        hw_counter: &HardwareCounterCell,\n    ) -> bool {\n        let hw_counter = self.make_conditioned_counter(hw_counter);\n\n        if self.deleted.get(idx as usize) == Some(false) {\n            self.point_to_values.check_values_any(\n                idx,\n                |v| check_fn(T::from_referenced(&v)),\n                &hw_counter,\n            )\n        } else {\n            false\n        }\n    }\n\n    pub fn get_values(&self, idx: PointOffsetType) -> Option<Box<dyn Iterator<Item = T> + '_>> {\n        if self.deleted.get(idx as usize) == Some(false) {\n            Some(Box::new(\n                self.point_to_values\n                    .get_values(idx)?\n                    .map(|v| *T::from_referenced(&v)),\n            ))\n        } else {\n            None\n        }\n    }\n\n    pub fn values_count(&self, idx: PointOffsetType) -> Option<usize> {\n        if self.deleted.get(idx as usize) == Some(false) {\n            self.point_to_values.get_values_count(idx)\n        } else {\n            None\n        }\n    }\n\n    pub(super) fn total_unique_values_count(&self) -> usize {\n        self.pairs.len()\n    }\n\n    pub(super) fn values_range<'a>(\n        &'a self,\n        start_bound: Bound<Point<T>>,\n        end_bound: Bound<Point<T>>,\n        hw_counter: &'a HardwareCounterCell,\n    ) -> impl Iterator<Item = PointOffsetType> + 'a {\n        let hw_counter = self.make_conditioned_counter(hw_counter);\n\n        self.values_range_iterator(start_bound, end_bound)\n            .map(|Point { idx, .. }| idx)\n            .measure_hw_with_condition_cell(hw_counter, std::mem::size_of::<Point<T>>(), |i| {\n                i.payload_index_io_read_counter()\n            })\n    }\n\n    pub(super) fn orderable_values_range(\n        &self,\n        start_bound: Bound<Point<T>>,\n        end_bound: Bound<Point<T>>,\n        hw_counter: &'a HardwareCounterCell,\n    ) -> impl DoubleEndedIterator<Item = (T, PointOffsetType)> + 'a {\n        let hw_counter = self.make_conditioned_counter(hw_counter);\n\n        self.values_range_iterator(start_bound, end_bound)\n            .map(|Point { val, idx }| (val, idx))\n            .measure_hw_with_condition_cell(hw_counter, std::mem::size_of::<Point<T>>(), |i| {\n                i.payload_index_io_read_counter()\n            })\n    }\n\n    pub fn remove_point(&mut self, idx: PointOffsetType) {\n        let idx = idx as usize;\n        if idx < self.deleted.len() && !self.deleted.get(idx).unwrap_or(true) {\n            self.deleted.set(idx, true);\n            self.deleted_count += 1;\n        }\n    }\n\n    pub(super) fn get_histogram(&self) -> &Histogram<T> {\n        &self.histogram\n    }\n\n    pub(super) fn get_points_count(&self) -> usize {\n        self.point_to_values.len() - self.deleted_count\n    }\n\n    pub(super) fn get_max_values_per_point(&self) -> usize {\n        self.max_values_per_point\n    }\n\n    pub(super) fn values_range_size(\n        &self,\n        start_bound: Bound<Point<T>>,\n        end_bound: Bound<Point<T>>,\n    ) -> usize {\n        let iterator = self.values_range_iterator(start_bound, end_bound);\n        iterator.end_index - iterator.start_index\n    }\n\n    fn values_range_iterator(\n        &self,\n        start_bound: Bound<Point<T>>,\n        end_bound: Bound<Point<T>>,\n    ) -> NumericIndexPairsIterator<'_, T> {\n        let start_index = match start_bound {\n            Bound::Included(bound) => self.pairs.binary_search(&bound).unwrap_or_else(|idx| idx),\n            Bound::Excluded(bound) => match self.pairs.binary_search(&bound) {\n                Ok(idx) => idx + 1,\n                Err(idx) => idx,\n            },\n            Bound::Unbounded => 0,\n        };\n\n        if start_index >= self.pairs.len() {\n            return NumericIndexPairsIterator {\n                pairs: &self.pairs,\n                deleted: &self.deleted,\n                start_index: self.pairs.len(),\n                end_index: self.pairs.len(),\n            };\n        }\n\n        let end_index = match end_bound {\n            Bound::Included(bound) => match self.pairs[start_index..].binary_search(&bound) {\n                Ok(idx) => idx + 1 + start_index,\n                Err(idx) => idx + start_index,\n            },\n            Bound::Excluded(bound) => {\n                let end_bound = self.pairs[start_index..].binary_search(&bound);\n                end_bound.unwrap_or_else(|idx| idx) + start_index\n            }\n            Bound::Unbounded => self.pairs.len(),\n        };\n\n        NumericIndexPairsIterator {\n            pairs: &self.pairs,\n            deleted: &self.deleted,\n            start_index,\n            end_index,\n        }\n    }\n\n    fn make_conditioned_counter<'a>(\n        &self,\n        hw_counter: &'a HardwareCounterCell,\n    ) -> ConditionedCounter<'a> {\n        ConditionedCounter::new(self.is_on_disk, hw_counter)\n    }\n\n    pub fn is_on_disk(&self) -> bool {\n        self.is_on_disk\n    }\n\n    pub fn populate(&self) -> OperationResult<()> {\n        self.pairs.populate()?;\n        self.point_to_values.populate();\n        Ok(())\n    }\n\n    pub fn clear_cache(&self) -> OperationResult<()> {\n        let pairs_path = self.path.join(PAIRS_PATH);\n        let deleted_path = self.path.join(DELETED_PATH);\n\n        clear_disk_cache(&pairs_path)?;\n        clear_disk_cache(&deleted_path)?;\n\n        self.point_to_values.clear_cache()?;\n\n        Ok(())\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9414685770107457,
      "diff_lines": 122,
      "added_lines": 11,
      "removed_lines": 28,
      "diff_preview": "--- expected+++ predicted@@ -2,7 +2,6 @@ use std::ops::Bound;\n use std::path::{Path, PathBuf};\n \n-use common::counter::conditioned_counter::ConditionedCounter;\n use common::counter::hardware_counter::HardwareCounterCell;\n use common::counter::iterator_hw_measurement::HwMeasurementIteratorExt;\n use common::types::PointOffsetType;\n@@ -14,11 +13,11 @@ use memory::mmap_type::{MmapBitSlice, MmapSlice};\n use serde::{Deserialize, Serialize};\n \n+use super::mutable_numeric_index::InMemoryNumericIndex;\n use super::Encodable;\n-use super::mutable_numeric_index::InMemoryNumericIndex;\n-use crate::common::Flusher;\n use crate::common::mmap_bitslice_buffered_update_wrapper::MmapBitSliceBufferedUpdateWrapper;\n use crate::common::operation_error::OperationResult;\n+use crate::common::Flusher;\n use crate::index::field_index::histogram::{Histogram, Numericable, Point};\n use crate::index::field_index::mmap_point_to_values::{MmapPointToValues, MmapValue};\n \n@@ -29,7 +28,6 @@ pub struct MmapNumericIndex<T: Encodable + Numericable + Default + MmapValue + 'static> {\n     path: PathBuf,\n     deleted: MmapBitSliceBufferedUpdateWrapper,\n-    // sorted pairs (id + value), sorted by value (by id if values are equal)\n     pairs: MmapSlice<Point<T>>,\n     histogram: Histogram<T>,\n     deleted_count: usize,\n@@ -50,7 +48,7 @@     end_index: usize,\n }\n \n-impl<T: Encodable + Numericable> Iterator for NumericIndexPairsIterator<'_, T> {\n+impl<'a, T: Encodable + Numericable> Iterator for NumericIndexPairsIterator<'a, T> {\n     type Item = Point<T>;\n \n     fn next(&mut self) -> Option<Self::Item> {\n@@ -58,21 +56,6 @@             let key = self.pairs[self.start_index].clone();\n             let deleted = self.deleted.get(key.idx as usize).unwrap_or(true);\n             self.start_index += 1;\n-            if deleted {\n-                continue;\n-            }\n-            return Some(key);\n-        }\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 53.332170248031616,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 11917,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "qdrant_lib_segment_src_index_field_index_numeric_index_mutable_numeric_index.rs",
    "exact_match": false,
    "similarity": 0.4583031881541504,
    "diff_lines": 359,
    "predicted_output": "use std::collections::BTreeSet;\nuse std::ops::Bound;\nuse std::ops::Bound::{Excluded, Unbounded};\nuse std::sync::Arc;\n\nuse common::types::PointOffsetType;\nuse parking_lot::RwLock;\nuse rocksdb::DB;\n\nuse super::{Encodable, HISTOGRAM_MAX_BUCKET_SIZE, HISTOGRAM_PRECISION, numeric_index_storage_cf_name};\nuse crate::common::operation_error::{OperationError, OperationResult};\nuse crate::common::rocksdb_buffered_delete_wrapper::DatabaseColumnScheduledDeleteWrapper;\nuse crate::index::field_index::histogram::{Histogram, Numericable, Point};\n\npub struct MutableNumericIndex<T: Encodable + Numericable> {\n    db_wrapper: DatabaseColumnScheduledDeleteWrapper,\n    in_memory_index: InMemoryNumericIndex<T>,\n}\n\n// Numeric Index with insertions and deletions without persistence\npub struct InMemoryNumericIndex<T: Encodable + Numericable> {\n    pub map: BTreeSet<Point<T>>,\n    pub histogram: Histogram<T>,\n    pub points_count: usize,\n    pub max_values_per_point: usize,\n    pub point_to_values: Vec<Vec<T>>,\n}\n\nimpl<T: Encodable + Numericable> Default for InMemoryNumericIndex<T> {\n    fn default() -> Self {\n        Self {\n            map: BTreeSet::new(),\n            histogram: Histogram::new(HISTOGRAM_MAX_BUCKET_SIZE, HISTOGRAM_PRECISION),\n            points_count: 0,\n            max_values_per_point: 0,\n            point_to_values: Default::default(),\n        }\n    }\n}\n\nimpl<T: Encodable + Numericable + Default> FromIterator<(PointOffsetType, T)> for InMemoryNumericIndex<T> {\n    fn from_iter<I: IntoIterator<Item = (PointOffsetType, T)>>(iter: I) -> Self {\n        let mut index = InMemoryNumericIndex::default();\n        for (idx, value) in iter {\n            if index.point_to_values.len() <= idx as usize {\n                index.point_to_values.resize_with(idx as usize + 1, Vec::new);\n            }\n            index.point_to_values[idx as usize].push(value);\n            let key = Point::new(value, idx);\n            InMemoryNumericIndex::add_to_map(&mut index.map, &mut index.histogram, key);\n        }\n        index\n    }\n}\n\nimpl<T: Encodable + Numericable + Default> MutableNumericIndex<T> {\n    pub fn new_from_db_wrapper(db_wrapper: DatabaseColumnScheduledDeleteWrapper) -> Self {\n        Self {\n            db_wrapper,\n            in_memory_index: InMemoryNumericIndex::default(),\n        }\n    }\n\n    pub fn into_in_memory_index(self) -> InMemoryNumericIndex<T> {\n        self.in_memory_index\n    }\n\n    pub fn get_db_wrapper(&self) -> &DatabaseColumnScheduledDeleteWrapper {\n        &self.db_wrapper\n    }\n\n    pub fn new(db: Arc<RwLock<DB>>, field: &str) -> Self {\n        let store_cf_name = numeric_index_storage_cf_name(field);\n        let db_wrapper = DatabaseColumnScheduledDeleteWrapper::new(DatabaseColumnWrapper::new(\n            db,\n            &store_cf_name,\n        ));\n        Self {\n            db_wrapper,\n            in_memory_index: InMemoryNumericIndex::default(),\n        }\n    }\n\n    pub fn load(&mut self) -> OperationResult<bool> {\n        if !self.db_wrapper.has_column_family()? {\n            return Ok(false);\n        };\n\n        self.in_memory_index = self\n            .db_wrapper\n            .lock_db()\n            .iter()?\n            .map(|(key, value)| {\n                let value_idx =\n                    u32::from_be_bytes(value.as_ref().try_into().map_err(|_| {\n                        OperationError::service_error(\"incorrect numeric index value\")\n                    })?);\n                let (idx, value) = T::decode_key(&key);\n                if idx != value_idx {\n                    return Err(OperationError::service_error(\n                        \"incorrect numeric index key-value pair\",\n                    ));\n                }\n                Ok((idx, value))\n            })\n            .collect::<Result<InMemoryNumericIndex<_>, OperationError>>()?;\n\n        Ok(true)\n    }\n\n    pub fn add_many_to_list(\n        &mut self,\n        idx: PointOffsetType,\n        values: Vec<T>,\n        hw_counter: &HardwareCounterCell,\n    ) -> OperationResult<()> {\n        for value in &values {\n            let key = value.encode_key(idx);\n            self.db_wrapper.put(&key, idx.to_be_bytes())?;\n        }\n        self.in_memory_index.add_many_to_list(idx, values);\n        Ok(())\n    }\n\n    pub fn remove_point(&mut self, idx: PointOffsetType) -> OperationResult<()> {\n        self.in_memory_index\n            .get_values(idx)\n            .map(|mut values| {\n                values.try_for_each(|value| {\n                    let key = value.encode_key(idx);\n                    self.db_wrapper.remove(key)\n                })\n            })\n            .transpose()?;\n        self.in_memory_index.remove_point(idx);\n        Ok(())\n    }\n\n    pub fn total_unique_values_count(&self) -> usize {\n        self.in_memory_index.total_unique_values_count()\n    }\n\n    pub fn check_values_any(&self, idx: PointOffsetType, check_fn: impl Fn(&T) -> bool) -> bool {\n        self.in_memory_index.check_values_any(idx, check_fn)\n    }\n\n    pub fn get_points_count(&self) -> usize {\n        self.in_memory_index.get_points_count()\n    }\n\n    pub fn get_values(&self, idx: PointOffsetType) -> Option<Box<dyn Iterator<Item = T> + '_>> {\n        self.in_memory_index.get_values(idx)\n    }\n\n    pub fn values_count(&self, idx: PointOffsetType) -> Option<usize> {\n        self.in_memory_index.values_count(idx)\n    }\n\n    pub fn values_range(\n        &self,\n        start_bound: Bound<Point<T>>,\n        end_bound: Bound<Point<T>>,\n    ) -> impl Iterator<Item = PointOffsetType> {\n        self.in_memory_index.values_range(start_bound, end_bound)\n    }\n\n    pub fn orderable_values_range(\n        &self,\n        start_bound: Bound<Point<T>>,\n        end_bound: Bound<Point<T>>,\n    ) -> impl DoubleEndedIterator<Item = (T, PointOffsetType)> {\n        self.in_memory_index.orderable_values_range(start_bound, end_bound)\n    }\n\n    pub fn get_histogram(&self) -> &Histogram<T> {\n        self.in_memory_index.get_histogram()\n    }\n\n    pub fn get_max_values_per_point(&self) -> usize {\n        self.in_memory_index.get_max_values_per_point()\n    }\n}\n\nimpl<T: Encodable + Numericable> InMemoryNumericIndex<T> {\n    pub fn add_many_to_list(&mut self, idx: PointOffsetType, values: Vec<T>) {\n        if self.point_to_values.len() <= idx as usize {\n            self.point_to_values.resize_with(idx as usize + 1, Vec::new);\n        }\n        for value in &values {\n            let key = Point::new(*value, idx);\n            Self::add_to_map(&mut self.map, &mut self.histogram, key);\n        }\n        if !values.is_empty() {\n            self.points_count += 1;\n            self.max_values_per_point = self.max_values_per_point.max(values.len());\n        }\n        self.point_to_values[idx as usize] = values;\n    }\n\n    pub fn remove_point(&mut self, idx: PointOffsetType) {\n        if let Some(values) = self.point_to_values.get_mut(idx as usize) {\n            if !values.is_empty() {\n                self.points_count = self.points_count.checked_sub(1).unwrap_or_default();\n            }\n            for value in values.iter() {\n                let key = Point::new(*value, idx);\n                Self::remove_from_map(&mut self.map, &mut self.histogram, key);\n            }\n            *values = Default::default();\n        }\n    }\n\n    pub fn check_values_any(&self, idx: PointOffsetType, check_fn: impl Fn(&T) -> bool) -> bool {\n        self.point_to_values\n            .get(idx as usize)\n            .map(|values| values.iter().any(check_fn))\n            .unwrap_or(false)\n    }\n\n    pub fn values_count(&self, idx: PointOffsetType) -> Option<usize> {\n        self.point_to_values.get(idx as usize).map(Vec::len)\n    }\n\n    pub fn total_unique_values_count(&self) -> usize {\n        self.map.len()\n    }\n\n    pub fn get_values(&self, idx: PointOffsetType) -> Option<Box<dyn Iterator<Item = T> + '_>> {\n        Some(Box::new(\n            self.point_to_values\n                .get(idx as usize)\n                .map(|v| v.iter().cloned())?,\n        ))\n    }\n\n    pub fn values_range(\n        &self,\n        start_bound: Bound<Point<T>>,\n        end_bound: Bound<Point<T>>,\n    ) -> impl Iterator<Item = PointOffsetType> {\n        self.map.range((start_bound, end_bound)).map(|point| point.idx)\n    }\n\n    pub fn orderable_values_range(\n        &self,\n        start_bound: Bound<Point<T>>,\n        end_bound: Bound<Point<T>>,\n    ) -> impl DoubleEndedIterator<Item = (T, PointOffsetType)> {\n        self.map\n            .range((start_bound, end_bound))\n            .map(|point| (point.val, point.idx))\n    }\n\n    fn add_to_map(map: &mut BTreeSet<Point<T>>, histogram: &mut Histogram<T>, key: Point<T>) {\n        let was_added = map.insert(key.clone());\n        if was_added {\n            histogram.insert(\n                key,\n                |x| Self::get_histogram_left_neighbor(map, x.clone()),\n                |x| Self::get_histogram_right_neighbor(map, x.clone()),\n            );\n        }\n    }\n\n    fn remove_from_map(map: &mut BTreeSet<Point<T>>, histogram: &mut Histogram<T>, key: Point<T>) {\n        let was_removed = map.remove(&key);\n        if was_removed {\n            histogram.remove(\n                &key,\n                |x| Self::get_histogram_left_neighbor(map, x.clone()),\n                |x| Self::get_histogram_right_neighbor(map, x.clone()),\n            );\n        }\n    }\n\n    fn get_histogram_left_neighbor(map: &BTreeSet<Point<T>>, key: Point<T>) -> Option<Point<T>> {\n        map.range((Unbounded, Excluded(key))).next_back().cloned()\n    }\n\n    fn get_histogram_right_neighbor(map: &BTreeSet<Point<T>>, key: Point<T>) -> Option<Point<T>> {\n        map.range((Excluded(key), Unbounded)).next().cloned()\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.4583031881541504,
      "diff_lines": 359,
      "added_lines": 117,
      "removed_lines": 164,
      "diff_preview": "--- expected+++ predicted@@ -3,17 +3,13 @@ use std::ops::Bound::{Excluded, Unbounded};\n use std::sync::Arc;\n \n-use common::counter::hardware_counter::HardwareCounterCell;\n use common::types::PointOffsetType;\n use parking_lot::RwLock;\n use rocksdb::DB;\n \n-use super::{\n-    Encodable, HISTOGRAM_MAX_BUCKET_SIZE, HISTOGRAM_PRECISION, numeric_index_storage_cf_name,\n-};\n+use super::{Encodable, HISTOGRAM_MAX_BUCKET_SIZE, HISTOGRAM_PRECISION, numeric_index_storage_cf_name};\n use crate::common::operation_error::{OperationError, OperationResult};\n use crate::common::rocksdb_buffered_delete_wrapper::DatabaseColumnScheduledDeleteWrapper;\n-use crate::common::rocksdb_wrapper::DatabaseColumnWrapper;\n use crate::index::field_index::histogram::{Histogram, Numericable, Point};\n \n pub struct MutableNumericIndex<T: Encodable + Numericable> {\n@@ -42,150 +38,18 @@     }\n }\n \n-impl<T: Encodable + Numericable + Default> FromIterator<(PointOffsetType, T)>\n-    for InMemoryNumericIndex<T>\n-{\n+impl<T: Encodable + Numericable + Default> FromIterator<(PointOffsetType, T)> for InMemoryNumericIndex<T> {\n     fn from_iter<I: IntoIterator<Item = (PointOffsetType, T)>>(iter: I) -> Self {\n         let mut index = InMemoryNumericIndex::default();\n-        for pair in iter {\n-            let (idx, value) = pair;\n-\n+        for (idx, value) in iter {\n             if index.point_to_values.len() <= idx as usize {\n-                index\n-                    .point_to_values\n-                    .resize_with(idx as usize + 1, Vec::new)\n+                index.point_to_values.resize_with(idx as usize + 1, Vec::new);\n             }\n-\n             index.point_to_values[idx as usize].push(value);\n-\n             let key = Point::new(value, idx);\n             InMemoryNumericIndex::add_to_map(&mut index.map, &mut index.histogram, key);\n         }\n-        for values in &index.point_to_values {\n-            if !values.is_empty() {\n-                index.points_count += 1;\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 55.1921648979187,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 20896,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "qdrant_lib_segment_src_index_hnsw_index_graph_layers.rs",
    "exact_match": false,
    "similarity": 0.08768140192886914,
    "diff_lines": 875,
    "predicted_output": "use serde::{Deserialize, Serialize};\nuse crate::types::{PointOffsetType, ScoreType};\nuse crate::spaces::tools::FixedLengthPriorityQueue;\nuse std::cmp::{max, min};\nuse std::path::{Path, PathBuf};\nuse crate::entry::entry_point::OperationResult;\nuse crate::common::file_operations::{read_bin, atomic_save_bin};\nuse crate::index::hnsw_index::point_scorer::FilteredScorer;\nuse crate::index::hnsw_index::entry_points::EntryPoints;\nuse crate::vector_storage::vector_storage::ScoredPointOffset;\nuse crate::index::visited_pool::{VisitedList, VisitedPool};\nuse crate::index::hnsw_index::search_context::SearchContext;\nuse crate::common::utils::rev_range;\nuse rand::distributions::Uniform;\nuse rand::prelude::ThreadRng;\nuse rand::Rng;\nuse std::collections::BinaryHeap;\nuse itertools::Itertools;\n\npub type LinkContainer = Vec<PointOffsetType>;\npub type LayersContainer = Vec<LinkContainer>;\n\npub const HNSW_GRAPH_FILE: &str = \"graph.bin\";\n\n#[derive(Deserialize, Serialize, Debug)]\npub struct GraphLayers {\n    max_level: usize,\n    m: usize,\n    m0: usize,\n    ef_construct: usize,\n    level_factor: f64,\n    use_heuristic: bool,\n    links_layers: Vec<LayersContainer>,\n    entry_points: EntryPoints,\n\n    #[serde(skip)]\n    visited_pool: VisitedPool,\n}\n\nimpl GraphLayers {\n    pub fn new_with_params(\n        num_vectors: usize,\n        m: usize,\n        m0: usize,\n        ef_construct: usize,\n        entry_points_num: usize,\n        use_heuristic: bool,\n        reserve: bool,\n    ) -> Self {\n        let mut links_layers: Vec<LayersContainer> = vec![];\n\n        for _i in 0..num_vectors {\n            let mut links: LinkContainer = Vec::new();\n            if reserve {\n                links.reserve(m0);\n            }\n            links_layers.push(vec![links]);\n        }\n\n        GraphLayers {\n            max_level: 0,\n            m,\n            m0,\n            ef_construct,\n            level_factor: 1.0 / (m as f64).ln(),\n            use_heuristic,\n            links_layers,\n            entry_points: EntryPoints::new(entry_points_num),\n            visited_pool: VisitedPool::new(),\n        }\n    }\n\n    pub fn new(\n        num_vectors: usize,\n        m: usize,\n        m0: usize,\n        ef_construct: usize,\n        entry_points_num: usize,\n        use_heuristic: bool,\n    ) -> Self {\n        Self::new_with_params(num_vectors, m, m0, ef_construct, entry_points_num, use_heuristic, true)\n    }\n\n    fn num_points(&self) -> usize {\n        self.links_layers.len()\n    }\n\n    pub fn point_level(&self, point_id: PointOffsetType) -> usize {\n        self.links_layers[point_id as usize].len() - 1\n    }\n\n    fn links(&self, point_id: PointOffsetType, level: usize) -> &LinkContainer {\n        &self.links_layers[point_id as usize][level]\n    }\n\n    fn get_m(&self, level: usize) -> usize {\n        if level == 0 {\n            self.m0\n        } else {\n            self.m\n        }\n    }\n\n    pub fn get_random_layer(&self, thread_rng: &mut ThreadRng) -> usize {\n        let distribution = Uniform::new(0.0, 1.0);\n        let sample: f64 = thread_rng.sample(distribution);\n        let picked_level = -sample.ln() * self.level_factor;\n        picked_level.round() as usize\n    }\n\n    fn set_levels(&mut self, point_id: PointOffsetType, level: usize) {\n        if self.links_layers.len() <= point_id as usize {\n            self.links_layers.resize(point_id as usize, vec![]);\n        }\n        let point_layers = &mut self.links_layers[point_id as usize];\n        while point_layers.len() <= level {\n            let mut links = vec![];\n            links.reserve(self.m);\n            point_layers.push(links);\n        }\n        self.max_level = max(level, self.max_level);\n    }\n\n    fn _search_on_level(&self, searcher: &mut SearchContext, level: usize, visited_list: &mut VisitedList, points_scorer: &FilteredScorer) {\n        while let Some(candidate) = searcher.candidates.pop() {\n            if candidate.score < searcher.lower_bound() {\n                break;\n            }\n            let mut links_iter = self.links(candidate.idx, level)\n                .iter()\n                .cloned()\n                .filter(|point_id| !visited_list.check_and_update_visited(*point_id));\n\n            points_scorer.score_iterable_points(\n                &mut links_iter,\n                self.get_m(level),\n                |score_point| searcher.process_candidate(score_point),\n            );\n        }\n    }\n\n    fn search_on_level(&self, level_entry: ScoredPointOffset, level: usize, ef: usize, points_scorer: &FilteredScorer) -> FixedLengthPriorityQueue<ScoredPointOffset> {\n        let mut visited_list = self.visited_pool.get(self.num_points());\n        visited_list.check_and_update_visited(level_entry.idx);\n        let mut search_context = SearchContext::new(level_entry, ef);\n\n        self._search_on_level(&mut search_context, level, &mut visited_list, points_scorer);\n\n        self.visited_pool.return_back(visited_list);\n        search_context.nearest\n    }\n\n    fn search_entry(&self, entry_point: PointOffsetType, top_level: usize, target_level: usize, points_scorer: &FilteredScorer) -> ScoredPointOffset {\n        let mut current_point = ScoredPointOffset {\n            idx: entry_point,\n            score: points_scorer.score_point(entry_point),\n        };\n        for level in (target_level..=top_level).rev() {\n            let mut changed = true;\n            while changed {\n                changed = false;\n                let mut links = self.links(current_point.idx, level).iter().cloned();\n                points_scorer.score_iterable_points(&mut links, self.get_m(level), |score_point| {\n                    if score_point.score > current_point.score {\n                        changed = true;\n                        current_point = score_point;\n                    }\n                });\n            }\n        }\n        current_point\n    }\n\n    fn connect_new_point<F>(\n        links: &mut LinkContainer,\n        new_point_id: PointOffsetType,\n        target_point_id: PointOffsetType,\n        level_m: usize,\n        mut score_internal: F,\n    ) where\n        F: FnMut(PointOffsetType, PointOffsetType) -> ScoreType,\n    {\n        let new_to_target = score_internal(target_point_id, new_point_id);\n\n        let mut id_to_insert = links.len();\n        for i in 0..links.len() {\n            let target_to_link = score_internal(target_point_id, links[i]);\n            if target_to_link < new_to_target {\n                id_to_insert = i;\n                break;\n            }\n        }\n\n        if links.len() < level_m {\n            links.insert(id_to_insert, new_point_id);\n        } else {\n            if id_to_insert != links.len() {\n                links.pop();\n                links.insert(id_to_insert, new_point_id);\n            }\n        }\n    }\n\n    fn select_candidate_with_heuristic_from_sorted<F>(\n        candidates: impl Iterator<Item = ScoredPointOffset>,\n        m: usize,\n        mut score_internal: F,\n    ) -> Vec<PointOffsetType>\n    where\n        F: FnMut(PointOffsetType, PointOffsetType) -> ScoreType,\n    {\n        let mut result_list = vec![];\n        result_list.reserve(m);\n        for current_closest in candidates {\n            if result_list.len() >= m {\n                break;\n            }\n            let mut is_good = true;\n            for selected_point in result_list.iter().cloned() {\n                let dist_to_already_selected = score_internal(current_closest.idx, selected_point);\n                if dist_to_already_selected > current_closest.score {\n                    is_good = false;\n                    break;\n                }\n            }\n            if is_good {\n                result_list.push(current_closest.idx);\n            }\n        }\n\n        result_list\n    }\n\n    fn select_candidates_with_heuristic<F>(\n        candidates: FixedLengthPriorityQueue<ScoredPointOffset>,\n        m: usize,\n        score_internal: F,\n    ) -> Vec<PointOffsetType>\n    where\n        F: FnMut(PointOffsetType, PointOffsetType) -> ScoreType,\n    {\n        let closest_iter = candidates.into_iter();\n        return Self::select_candidate_with_heuristic_from_sorted(closest_iter, m, score_internal);\n    }\n\n    pub fn link_new_point(&mut self, point_id: PointOffsetType, level: usize, points_scorer: &FilteredScorer) {\n        self.set_levels(point_id, level);\n\n        let entry_point_opt = self.entry_points.new_point(point_id, level, |point_id| points_scorer.check_point(point_id));\n        match entry_point_opt {\n            None => {}\n            Some(entry_point) => {\n                let mut level_entry = if entry_point.level > level {\n                    self.search_entry(entry_point.point_id, entry_point.level, level, points_scorer)\n                } else {\n                    ScoredPointOffset {\n                        idx: entry_point.point_id,\n                        score: points_scorer.score_internal(point_id, entry_point.point_id),\n                    }\n                };\n                let linking_level = min(level, entry_point.level);\n\n                for curr_level in (0..=linking_level).rev() {\n                    let level_m = self.get_m(curr_level);\n                    let existing_links = &self.links_layers[point_id as usize][curr_level];\n\n                    let nearest_points = self.search_on_level(level_entry, curr_level, self.ef_construct, points_scorer);\n\n                    if self.use_heuristic {\n                        let selected_nearest = Self::select_candidates_with_heuristic(nearest_points, level_m, |a, b| points_scorer.score_internal(a, b));\n                        self.links_layers[point_id as usize][curr_level].clone_from(&selected_nearest);\n\n                        for other_point in selected_nearest.iter().cloned() {\n                            let other_point_links = &mut self.links_layers[other_point as usize][curr_level];\n                            if other_point_links.len() < level_m {\n                                other_point_links.push(point_id);\n                            } else {\n                                let mut candidates = BinaryHeap::with_capacity(level_m + 1);\n                                candidates.push(ScoredPointOffset {\n                                    idx: point_id,\n                                    score: points_scorer.score_internal(point_id, other_point),\n                                });\n                                for other_point_link in other_point_links.iter().take(level_m).cloned() {\n                                    candidates.push(ScoredPointOffset {\n                                        idx: other_point_link,\n                                        score: points_scorer.score_internal(other_point_link, other_point),\n                                    });\n                                }\n                                let selected_candidates = Self::select_candidate_with_heuristic_from_sorted(candidates.into_sorted_vec().into_iter().rev(), level_m, |a, b| points_scorer.score_internal(a, b));\n                                for (idx, selected) in selected_candidates.iter().cloned().enumerate() {\n                                    other_point_links[idx] = selected;\n                                }\n                            }\n                        }\n                    } else {\n                        for nearest_point in nearest_points.iter() {\n                            Self::connect_new_point(&mut self.links_layers[point_id as usize][curr_level], nearest_point.idx, point_id, level_m, |a, b| points_scorer.score_internal(a, b));\n                            Self::connect_new_point(&mut self.links_layers[nearest_point.idx as usize][curr_level], point_id, nearest_point.idx, level_m, |a, b| points_scorer.score_internal(a, b));\n                            if nearest_point.score > level_entry.score {\n                                level_entry = nearest_point.clone();\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    pub fn merge_from_other(&mut self, other: GraphLayers) {\n        let mut visited_list = self.visited_pool.get(self.num_points());\n        if other.links_layers.len() > self.links_layers.len() {\n            self.links_layers.resize(other.links_layers.len(), vec![]);\n        }\n        for (point_id, layers) in other.links_layers.into_iter().enumerate() {\n            let current_layers = &mut self.links_layers[point_id];\n            for (level, other_links) in layers.into_iter().enumerate() {\n                if current_layers.len() <= level {\n                    current_layers.push(other_links);\n                } else {\n                    visited_list.next_iteration();\n                    let current_links = &mut current_layers[level];\n                    current_links.iter().cloned().for_each(|x| { visited_list.check_and_update_visited(x); });\n                    for other_link in other_links.into_iter().filter(|x| !visited_list.check_and_update_visited(*x)) {\n                        current_links.push(other_link);\n                    }\n                }\n            }\n        }\n        self.entry_points.merge_from_other(other.entry_points);\n        self.visited_pool.return_back(visited_list);\n    }\n\n    pub fn search(&self, top: usize, ef: usize, mut points_scorer: FilteredScorer, custom_entry_points: Option<&[PointOffsetType]>) -> Vec<ScoredPointOffset> {\n        let Some(entry_point) = self.get_entry_point(&points_scorer, custom_entry_points) else {\n            return vec![];\n        };\n\n        let zero_level_entry = self.search_entry(entry_point.point_id, entry_point.level, 0, &mut points_scorer);\n        let nearest = self.search_on_level(zero_level_entry, 0, max(top, ef), &mut points_scorer);\n        nearest.into_iter_sorted().take(top).collect_vec()\n    }\n\n    pub fn get_path(path: &Path) -> PathBuf {\n        path.join(HNSW_GRAPH_FILE)\n    }\n\n    pub fn get_links_path(path: &Path) -> PathBuf {\n        path.join(HNSW_LINKS_FILE)\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.08768140192886914,
      "diff_lines": 875,
      "added_lines": 309,
      "removed_lines": 522,
      "diff_preview": "--- expected+++ predicted@@ -1,151 +1,166 @@-use std::borrow::Cow;\n-use std::cmp::max;\n+use serde::{Deserialize, Serialize};\n+use crate::types::{PointOffsetType, ScoreType};\n+use crate::spaces::tools::FixedLengthPriorityQueue;\n+use std::cmp::{max, min};\n use std::path::{Path, PathBuf};\n-use std::sync::atomic::AtomicBool;\n-\n-use common::fixed_length_priority_queue::FixedLengthPriorityQueue;\n-use common::types::{PointOffsetType, ScoredPointOffset};\n-use io::file_operations::read_bin;\n+use crate::entry::entry_point::OperationResult;\n+use crate::common::file_operations::{read_bin, atomic_save_bin};\n+use crate::index::hnsw_index::point_scorer::FilteredScorer;\n+use crate::index::hnsw_index::entry_points::EntryPoints;\n+use crate::vector_storage::vector_storage::ScoredPointOffset;\n+use crate::index::visited_pool::{VisitedList, VisitedPool};\n+use crate::index::hnsw_index::search_context::SearchContext;\n+use crate::common::utils::rev_range;\n+use rand::distributions::Uniform;\n+use rand::prelude::ThreadRng;\n+use rand::Rng;\n+use std::collections::BinaryHeap;\n use itertools::Itertools;\n-use serde::{Deserialize, Serialize};\n-\n-use super::entry_points::EntryPoint;\n-use super::graph_links::{GraphLinks, GraphLinksFormat};\n-use crate::common::operation_error::{\n-    CancellableResult, OperationError, OperationResult, check_process_stopped,\n-};\n-use crate::common::utils::rev_range;\n-use crate::index::hnsw_index::entry_points::EntryPoints;\n-use crate::index::hnsw_index::graph_links::GraphLinksSerializer;\n-use crate::index::hnsw_index::point_scorer::FilteredScorer;\n-use crate::index::hnsw_index::search_context::SearchContext;\n-use crate::index::visited_pool::{VisitedListHandle, VisitedPool};\n \n pub type LinkContainer = Vec<PointOffsetType>;\n pub type LayersContainer = Vec<LinkContainer>;\n \n pub const HNSW_GRAPH_FILE: &str = \"graph.bin\";\n-pub const HNSW_LINKS_FILE: &str = \"links.bin\";\n-pub const COMPRESSED_HNSW_LINKS_FILE: &str = \"links_compressed.bin\";\n-\n-/// Contents of the `graph.bin` file.\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 105.00935816764832,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 69134,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "qdrant_lib_segment_src_index_hnsw_index_graph_links.rs",
    "exact_match": false,
    "similarity": 0.4142309605313204,
    "diff_lines": 601,
    "predicted_output": "use std::cmp::Reverse;\nuse std::fs::{File, OpenOptions};\nuse std::io::{Read as _, Write};\nuse std::mem::take;\nuse std::path::Path;\nuse std::sync::Arc;\n\nuse common::bitpacking::packed_bits;\nuse common::bitpacking_links::{for_each_packed_link, pack_links, MIN_BITS_PER_VALUE};\nuse common::bitpacking_ordered;\nuse common::types::PointOffsetType;\nuse common::zeros::WriteZerosExt as _;\nuse itertools::{Either, Itertools as _};\nuse memmap2::Mmap;\nuse memory::madvise::{Advice, AdviceSetting};\nuse memory::mmap_ops;\nuse memory::mmap_ops::open_read_mmap;\nuse zerocopy::little_endian::U64 as LittleU64;\nuse zerocopy::native_endian::U64 as NativeU64;\nuse zerocopy::{FromBytes, Immutable, IntoBytes, KnownLayout};\n\nuse crate::common::operation_error::{OperationError, OperationResult};\n\npub const MMAP_PANIC_MESSAGE: &str = \"Mmap links are not loaded\";\n\nconst HEADER_SIZE: usize = 64;\n\n#[derive(Clone, Debug)]\nstruct GraphLinksFileInfo {\n    point_count: usize,\n    reindex_start: usize,\n    links_start: usize,\n    offsets_start: usize,\n    offsets_end: usize,\n    compression: Option<CompressionInfo>,\n}\n\n#[derive(Clone, Debug)]\nstruct GraphLinksView<'a> {\n    reindex: &'a [PointOffsetType],\n    compression: CompressionInfo<'a>,\n    level_offsets: Vec<u64>,\n}\n\n#[derive(Clone, Debug)]\nenum CompressionInfo<'a> {\n    Uncompressed {\n        links: &'a [u32],\n        offsets: &'a [NativeU64],\n    },\n    Compressed {\n        compressed_links: &'a [u8],\n        offsets: bitpacking_ordered::Reader<'a>,\n        m: usize,\n        m0: usize,\n        bits_per_unsorted: u8,\n    },\n}\n\n/// File header for the plain format.\n#[derive(FromBytes, Immutable, IntoBytes, KnownLayout)]\n#[repr(C)]\nstruct HeaderPlain {\n    point_count: u64,\n    levels_count: u64,\n    total_links_count: u64,\n    total_offset_count: u64,\n    offsets_padding_bytes: u64,\n    zero_padding: [u8; 24],\n}\n\n/// File header for the compressed format.\n#[derive(FromBytes, Immutable, IntoBytes, KnownLayout)]\n#[repr(C, align(8))]\nstruct HeaderCompressed {\n    point_count: LittleU64,\n    version: LittleU64,\n    levels_count: LittleU64,\n    total_links_bytes: LittleU64,\n    offsets_parameters: bitpacking_ordered::Parameters,\n    m: LittleU64,\n    m0: LittleU64,\n    zero_padding: [u8; 5],\n}\n\nconst HEADER_VERSION_COMPRESSED: u64 = 0xFFFF_FFFF_FFFF_FF01;\n\nimpl GraphLinksView<'_> {\n    fn load(data: &[u8], format: GraphLinksFormat) -> OperationResult<GraphLinksView> {\n        match format {\n            GraphLinksFormat::Compressed => Self::load_compressed(data),\n            GraphLinksFormat::Plain => Self::load_plain(data),\n        }\n    }\n\n    fn load_plain(data: &[u8]) -> OperationResult<GraphLinksView> {\n        let (header, data) =\n            HeaderPlain::ref_from_prefix(data).map_err(|_| Self::error_unsufficent_size())?;\n        let (level_offsets, data) =\n            Self::read_level_offsets(data, header.levels_count, header.total_offset_count)?;\n        let (reindex, data) = Self::get_slice::<PointOffsetType>(data, header.point_count)?;\n        let (links, data) = Self::get_slice::<u32>(data, header.total_links_count)?;\n        let (_, data) = Self::get_slice::<u8>(data, header.offsets_padding_bytes)?;\n        let (offsets, _bytes) = Self::get_slice::<NativeU64>(data, header.total_offset_count)?;\n        Ok(GraphLinksView {\n            reindex,\n            compression: CompressionInfo::Uncompressed { links, offsets },\n            level_offsets,\n        })\n    }\n\n    fn load_compressed(data: &[u8]) -> OperationResult<GraphLinksView> {\n        let (header, data) =\n            HeaderCompressed::ref_from_prefix(data).map_err(|_| Self::error_unsufficent_size())?;\n        debug_assert_eq!(header.version.get(), HEADER_VERSION_COMPRESSED);\n        let (level_offsets, data) = Self::read_level_offsets(\n            data,\n            header.levels_count.get(),\n            header.total_offset_count.get(),\n        )?;\n        let (reindex, data) = Self::get_slice::<PointOffsetType>(data, header.point_count.get())?;\n        let (compressed_links, data) = Self::get_slice::<u8>(data, header.total_links_bytes.get())?;\n        let (offsets, _bytes) = bitpacking_ordered::Reader::new(header.offsets_parameters, data)\n            .map_err(|e| {\n                OperationError::service_error(format!(\"Can't create decompressor: {e}\"))\n            })?;\n        Ok(GraphLinksView {\n            reindex,\n            compression: CompressionInfo::Compressed {\n                compressed_links,\n                offsets,\n                m: header.m.get() as usize,\n                m0: header.m0.get() as usize,\n                bits_per_unsorted: MIN_BITS_PER_VALUE.max(packed_bits(\n                    u32::try_from(header.point_count.get().saturating_sub(1)).map_err(|_| {\n                        OperationError::service_error(\"Too many points in GraphLinks file\")\n                    })?,\n                )),\n            },\n            level_offsets,\n        })\n    }\n\n    fn read_level_offsets(\n        bytes: &[u8],\n        levels_count: u64,\n        total_offset_count: u64,\n    ) -> OperationResult<(Vec<u64>, &[u8])> {\n        let (level_offsets, bytes) = Self::get_slice::<u64>(bytes, levels_count)?;\n        let mut result = Vec::with_capacity(level_offsets.len() + 1);\n        result.extend_from_slice(level_offsets);\n        result.push(total_offset_count.checked_sub(1).ok_or_else(|| {\n            OperationError::service_error(\n                \"Total offset count should be at least 1 in GraphLinks file\",\n            )\n        })?);\n        Ok((result, bytes))\n    }\n\n    fn get_slice<T: FromBytes + Immutable>(\n        data: &[u8],\n        length: u64,\n    ) -> OperationResult<(&[T], &[u8])> {\n        <[T]>::ref_from_prefix_with_elems(data, length as usize)\n            .map_err(|_| Self::error_unsufficent_size())\n    }\n\n    fn error_unsufficent_size() -> OperationError {\n        OperationError::service_error(\"Unsufficent file size for GraphLinks file\")\n    }\n\n    fn for_each_link(\n        &self,\n        point_id: PointOffsetType,\n        level: usize,\n        f: impl FnMut(PointOffsetType),\n    ) {\n        let idx = if level == 0 {\n            point_id as usize\n        } else {\n            self.level_offsets[level] as usize + self.reindex[point_id as usize] as usize\n        };\n        let links_range = self.offsets[idx] as usize..self.offsets[idx + 1] as usize;\n\n        match self.compression {\n            CompressionInfo::Uncompressed { links, offsets } => {\n                let links_range = offsets[idx].get() as usize..offsets[idx + 1].get() as usize;\n                links[links_range].iter().copied().for_each(f)\n            }\n            CompressionInfo::Compressed {\n                compressed_links,\n                ref offsets,\n                m,\n                m0,\n                bits_per_unsorted,\n            } => {\n                let links_range =\n                    offsets.get(idx).unwrap() as usize..offsets.get(idx + 1).unwrap() as usize;\n                for_each_packed_link(\n                    &compressed_links[links_range],\n                    bits_per_unsorted,\n                    if level == 0 { m0 } else { m },\n                    f,\n                );\n            }\n        }\n    }\n\n    fn point_level(&self, point_id: PointOffsetType) -> usize {\n        let reindexed_point_id = u64::from(self.reindex[point_id as usize]);\n        for (level, (&a, &b)) in self\n            .level_offsets\n            .iter()\n            .skip(1)\n            .tuple_windows()\n            .enumerate()\n        {\n            if reindexed_point_id >= b - a {\n                return level;\n            }\n        }\n        self.level_offsets.len() - 2\n    }\n}\n\npub struct GraphLinksConverter {\n    m: usize,\n    m0: usize,\n    links: Vec<u8>,\n    kind: GraphLinksConverterKind,\n    reindex: Vec<PointOffsetType>,\n    level_offsets: Vec<u64>,\n}\n\nenum GraphLinksConverterKind {\n    Uncompressed {\n        offsets_padding: usize,\n        offsets: Vec<u64>,\n    },\n    Compressed {\n        compressed_offsets: Vec<u8>,\n        offsets_parameters: bitpacking_ordered::Parameters,\n    },\n}\n\nimpl GraphLinksConverter {\n    pub fn new(\n        mut edges: Vec<Vec<Vec<PointOffsetType>>>,\n        format: GraphLinksFormat,\n        m: usize,\n        m0: usize,\n    ) -> Self {\n        let mut back_index: Vec<usize> = (0..edges.len()).collect();\n        back_index.sort_unstable_by_key(|&i| Reverse(edges[i].len()));\n\n        let mut reindex = vec![0; back_index.len()];\n        for i in 0..back_index.len() {\n            reindex[back_index[i]] = i as PointOffsetType;\n        }\n\n        let levels_count = back_index\n            .first()\n            .map_or(0, |&point_id| edges[point_id].len());\n        let mut point_count_by_level = vec![0; levels_count];\n        for point in &edges {\n            point_count_by_level[point.len() - 1] += 1;\n        }\n\n        let mut total_offsets_len = 0;\n        let mut level_offsets = Vec::with_capacity(levels_count);\n        let mut suffix_sum = point_count_by_level.iter().sum::<u64>();\n        for &value in point_count_by_level.iter() {\n            level_offsets.push(total_offsets_len);\n            total_offsets_len += suffix_sum;\n            suffix_sum -= value;\n        }\n        total_offsets_len += 1;\n\n        let mut links = Vec::new();\n        let mut offsets = Vec::with_capacity(total_offsets_len as usize);\n        offsets.push(0);\n        let bits_per_unsorted = packed_bits(u32::try_from(edges.len().saturating_sub(1)).unwrap())\n            .max(MIN_BITS_PER_VALUE);\n\n        for level in 0..levels_count {\n            let count = point_count_by_level.iter().skip(level).sum::<u64>() as usize;\n            let (sorted_count, iter) = match level {\n                0 => (m0, Either::Left(0..count)),\n                _ => (m, Either::Right(back_index[..count].iter().copied())),\n            };\n            iter.for_each(|id| {\n                let raw_links = take(&mut edges[id][level]);\n                match format {\n                    GraphLinksFormat::Compressed => {\n                        pack_links(&mut links, raw_links, bits_per_unsorted, sorted_count);\n                        offsets.push(links.len() as u64);\n                    }\n                    GraphLinksFormat::Plain => {\n                        links.extend_from_slice(raw_links.as_bytes());\n                        offsets.push((links.len() as u64) / size_of::<PointOffsetType>() as u64);\n                    }\n                }\n            });\n        }\n\n        let kind = match format {\n            GraphLinksFormat::Compressed => {\n                let (compressed_offsets, offsets_parameters) =\n                    bitpacking_ordered::compress(&offsets);\n                GraphLinksConverterKind::Compressed {\n                    compressed_offsets,\n                    offsets_parameters,\n                }\n            }\n            GraphLinksFormat::Plain => {\n                let len = links.len() + reindex.as_bytes().len();\n                GraphLinksConverterKind::Uncompressed {\n                    offsets_padding: len.next_multiple_of(size_of::<u64>()) - len,\n                    offsets,\n                }\n            }\n        };\n\n        Self {\n            m,\n            m0,\n            links,\n            kind,\n            reindex,\n            level_offsets,\n        }\n    }\n\n    pub fn to_graph_links_ram(&self) -> GraphLinks {\n        let size = self.level_offsets.as_bytes().len()\n            + self.reindex.as_bytes().len()\n            + self.links.len()\n            + (match &self.kind {\n                GraphLinksConverterKind::Uncompressed {\n                    offsets_padding: padding,\n                    offsets,\n                } => size_of::<HeaderPlain>() + padding + offsets.as_bytes().len(),\n                GraphLinksConverterKind::Compressed {\n                    compressed_offsets,\n                    offsets_parameters: _,\n                } => size_of::<HeaderCompressed>() + compressed_offsets.len(),\n            });\n\n        let mut data = Vec::with_capacity(size);\n        self.serialize_to_writer(&mut data).unwrap();\n        debug_assert_eq!(data.len(), size);\n        GraphLinks::try_new(GraphLinksEnum::Ram(data), |x| x.load_view()).unwrap()\n    }\n\n    fn serialize_to_writer(&self, writer: &mut impl Write) -> std::io::Result<()> {\n        match &self.kind {\n            GraphLinksConverterKind::Uncompressed {\n                offsets_padding,\n                offsets,\n            } => {\n                let header = HeaderPlain {\n                    point_count: self.reindex.len() as u64,\n                    levels_count: self.level_offsets.len() as u64,\n                    total_links_count: self.links.len() as u64\n                        / size_of::<PointOffsetType>() as u64,\n                    total_offset_count: offsets.len() as u64,\n                    offsets_padding_bytes: *offsets_padding as u64,\n                    zero_padding: [0; 24],\n                };\n                writer.write_all(header.as_bytes())?;\n            }\n            GraphLinksConverterKind::Compressed {\n                compressed_offsets,\n                offsets_parameters,\n            } => {\n                let header = HeaderCompressed {\n                    version: HEADER_VERSION_COMPRESSED.into(),\n                    point_count: LittleU64::new(self.reindex.len() as u64),\n                    total_links_bytes: LittleU64::new(self.links.len() as u64),\n                    offsets_parameters: *offsets_parameters,\n                    levels_count: LittleU64::new(self.level_offsets.len() as u64),\n                    m: LittleU64::new(self.m as u64),\n                    m0: LittleU64::new(self.m0 as u64),\n                    zero_padding: [0; 5],\n                };\n                writer.write_all(header.as_bytes())?;\n            }\n        }\n\n        writer.write_all(self.level_offsets.as_bytes())?;\n        writer.write_all(self.reindex.as_bytes())?;\n        writer.write_all(&self.links)?;\n        match &self.kind {\n            GraphLinksConverterKind::Uncompressed {\n                offsets_padding: padding,\n                offsets,\n            } => {\n                writer.write_zeros(*padding)?;\n                writer.write_all(offsets.as_bytes())?;\n            }\n            GraphLinksConverterKind::Compressed {\n                compressed_offsets,\n                offsets_parameters: _,\n            } => {\n                writer.write_all(compressed_offsets)?;\n            }\n        }\n\n        Ok(())\n    }\n\n    pub fn save_as(&self, path: &Path) -> OperationResult<()> {\n        let temp_path = path.with_extension(\"tmp\");\n        let file = File::create(temp_path.as_path())?;\n        let mut buf = std::io::BufWriter::new(&file);\n        self.serialize_to_writer(&mut buf)?;\n        file.sync_all()?;\n        std::fs::rename(temp_path, path)?;\n        Ok(())\n    }\n}\n\nself_cell::self_cell! {\n    pub struct GraphLinks {\n        owner: GraphLinksEnum,\n        #[covariant]\n        dependent: GraphLinksView,\n    }\n}\n\nimpl GraphLinksEnum {\n    fn load_view(&self) -> OperationResult<GraphLinksView> {\n        let data = match self {\n            GraphLinksEnum::Ram(data) => data.as_slice(),\n            GraphLinksEnum::Mmap(mmap) => &mmap[..],\n        };\n        GraphLinksView::load(data, GraphLinksFormat::Plain)\n    }\n}\n\nimpl GraphLinks {\n    pub fn load_from_file(path: &Path, on_disk: bool, format: GraphLinksFormat) -> OperationResult<Self> {\n        let file = OpenOptions::new()\n            .read(true)\n            .write(false)\n            .create(false)\n            .open(path)?;\n        if on_disk {\n            let len = file.metadata()?.len();\n            let mut data = Vec::new();\n            data.try_set_capacity_exact(len as usize)?;\n            file.take(len).read_to_end(&mut data)?;\n            Self::try_new(GraphLinksEnum::Ram(data), |x| x.load_view(format))\n        } else {\n            let mmap = unsafe { Mmap::map(&file)? };\n            madvise::madvise(&mmap, madvise::get_global())?;\n            Self::try_new(GraphLinksEnum::Mmap(Arc::new(mmap)), |x| x.load_view(format))\n        }\n    }\n\n    pub fn view(&self) -> &GraphLinksView {\n        self.borrow_dependent()\n    }\n\n    pub fn format(&self) -> GraphLinksFormat {\n        match self.view().compression {\n            CompressionInfo::Uncompressed { .. } => GraphLinksFormat::Plain,\n            CompressionInfo::Compressed { .. } => GraphLinksFormat::Compressed,\n        }\n    }\n\n    pub fn on_disk(&self) -> bool {\n        matches!(self.borrow_owner(), GraphLinksEnum::Ram(_))\n    }\n\n    pub fn num_points(&self) -> usize {\n        self.view().reindex.len()\n    }\n\n    pub fn for_each_link(\n        &self,\n        point_id: PointOffsetType,\n        level: usize,\n        f: impl FnMut(PointOffsetType),\n    ) {\n        self.links(point_id, level).for_each(f);\n    }\n\n    pub fn links(&self, point_id: PointOffsetType, level: usize) -> LinksIterator {\n        self.view().links(point_id, level)\n    }\n\n    pub fn point_level(&self, point_id: PointOffsetType) -> usize {\n        self.view().point_level(point_id)\n    }\n\n    /// Convert the graph links to a vector of edges, suitable for passing into\n    /// [`GraphLinksSerializer::new`] or using in tests.\n    pub fn into_edges(self) -> Vec<Vec<Vec<PointOffsetType>>> {\n        let mut edges = Vec::with_capacity(self.num_points());\n        for point_id in 0..self.num_points() {\n            let num_levels = self.point_level(point_id as PointOffsetType) + 1;\n            let mut levels = Vec::with_capacity(num_levels);\n            for level in 0..num_levels {\n                levels.push(self.links(point_id as PointOffsetType, level).collect());\n            }\n            edges.push(levels);\n        }\n        edges\n    }\n\n    /// Populate the disk cache with data, if applicable.\n    /// This is a blocking operation.\n    pub fn populate(&self) -> OperationResult<()> {\n        match self.borrow_owner() {\n            GraphLinksEnum::Mmap(mmap) => mmap.populate(),\n            GraphLinksEnum::Ram(_) => {}\n        };\n        Ok(())\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use rand::Rng;\n    use tempfile::Builder;\n\n    use super::*;\n\n    fn compare_links(\n        mut left: Vec<Vec<Vec<PointOffsetType>>>,\n        mut right: Vec<Vec<Vec<PointOffsetType>>>,\n        format: GraphLinksFormat,\n        m: usize,\n        m0: usize,\n    ) {\n        for links in [&mut left, &mut right].iter_mut() {\n            links.iter_mut().for_each(|levels| {\n                levels\n                    .iter_mut()\n                    .enumerate()\n                    .for_each(|(level_idx, links)| {\n                        *links = normalize_links(\n                            match format {\n                                GraphLinksFormat::Compressed => {\n                                    if level_idx == 0 {\n                                        m0\n                                    } else {\n                                        m\n                                    }\n                                }\n                                GraphLinksFormat::Plain => 0,\n                            },\n                            std::mem::take(links),\n                        );\n                    })\n            });\n        }\n        assert_eq!(left, right);\n    }\n\n    /// Test that random links can be saved by [`GraphLinksSerializer`] and\n    /// loaded correctly by a [`GraphLinks`] impl.\n    fn test_save_load(\n        points_count: usize,\n        max_levels_count: usize,\n        on_disk: bool,\n        format: GraphLinksFormat,\n        m: usize,\n        m0: usize,\n    ) {\n        let path = Builder::new().prefix(\"graph_dir\").tempdir().unwrap();\n        let links_file = path.path().join(\"links.bin\");\n        let links = random_links(points_count, max_levels_count, m, m0);\n        GraphLinksSerializer::new(links.clone(), format, m, m0)\n            .save_as(&links_file)\n            .unwrap();\n        let cmp_links = GraphLinks::load_from_file(&links_file, on_disk, format)\n            .unwrap()\n            .into_edges();\n        compare_links(links, cmp_links, format, m, m0);\n    }\n\n    #[rstest]\n    #[case::uncompressed(GraphLinksFormat::Plain)]\n    #[case::compressed(GraphLinksFormat::Compressed)]\n    fn test_graph_links_construction(#[case] format: GraphLinksFormat) {\n        let m = 2;\n        let m0 = m * 2;\n\n        let make_cmp_links = |links: Vec<Vec<Vec<PointOffsetType>>>,\n                              m: usize,\n                              m0: usize|\n         -> Vec<Vec<Vec<PointOffsetType>>> {\n            GraphLinksSerializer::new(links, format, m, m0)\n                .to_graph_links_ram()\n                .into_edges()\n        };\n\n        // no points\n        let links: Vec<Vec<Vec<PointOffsetType>>> = vec![];\n        let cmp_links = make_cmp_links(links.clone(), m, m0);\n        compare_links(links, cmp_links, format, m, m0);\n\n        // 2 points without any links\n        let links: Vec<Vec<Vec<PointOffsetType>>> = vec![vec![vec![]], vec![vec![]]];\n        let cmp_links = make_cmp_links(links.clone(), m, m0);\n        compare_links(links, cmp_links, format, m, m0);\n\n        // one link at level 0\n        let links: Vec<Vec<Vec<PointOffsetType>>> = vec![vec![vec![1]], vec![vec![0]]];\n        let cmp_links = make_cmp_links(links.clone(), m, m0);\n        compare_links(links, cmp_links, format, m, m0);\n\n        // 3 levels with no links at second level\n        let links: Vec<Vec<Vec<PointOffsetType>>> = vec![\n            vec![vec![1, 2]],\n            vec![vec![0, 2], vec![], vec![2]],\n            vec![vec![0, 1], vec![], vec![1]],\n        ];\n        let cmp_links = make_cmp_links(links.clone(), m, m0);\n        compare_links(links, cmp_links, format, m, m0);\n\n        // 3 levels with no links at last level\n        let links: Vec<Vec<Vec<PointOffsetType>>> = vec![\n            vec![vec![1, 2], vec![2], vec![]],\n            vec![vec![0, 2], vec![1], vec![]],\n            vec![vec![0, 1]],\n        ];\n        let cmp_links = make_cmp_links(links.clone(), m, m0);\n        compare_links(links, cmp_links, format, m, m0);\n\n        // 4 levels with random nonexistent links\n        let links: Vec<Vec<Vec<PointOffsetType>>> = vec![\n            vec![vec![1, 2, 5, 6]],\n            vec![vec![0, 2, 7, 8], vec![], vec![34, 45, 10]],\n            vec![vec![0, 1, 1, 2], vec![3, 5, 9], vec![9, 8], vec![9], vec![]],\n            vec![vec![0, 1, 5, 6], vec![1, 5, 0]],\n            vec![vec![0, 1, 9, 18], vec![1, 5, 6], vec![5], vec![9]],\n        ];\n        let cmp_links = make_cmp_links(links.clone(), m, m0);\n        compare_links(links, cmp_links, format, m, m0);\n\n        // fully random links\n        let links = random_links(100, 10, m, m0);\n        let cmp_links = make_cmp_links(links.clone(), m, m0);\n        compare_links(links, cmp_links, format, m, m0);\n    }\n\n    #[test]\n    fn test_graph_links_mmap_ram_compatibility() {\n        let m = 8;\n        let m0 = m * 2;\n        test_save_load(1000, 10, true, GraphLinksFormat::Compressed, m, m0);\n        test_save_load(1000, 10, false, GraphLinksFormat::Compressed, m, m0);\n        test_save_load(1000, 10, true, GraphLinksFormat::Plain, m, m0);\n        test_save_load(1000, 10, false, GraphLinksFormat::Plain, m, m0);\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.4142309605313204,
      "diff_lines": 601,
      "added_lines": 436,
      "removed_lines": 108,
      "diff_preview": "--- expected+++ predicted@@ -1,57 +1,423 @@+use std::cmp::Reverse;\n+use std::fs::{File, OpenOptions};\n+use std::io::{Read as _, Write};\n+use std::mem::take;\n use std::path::Path;\n use std::sync::Arc;\n \n+use common::bitpacking::packed_bits;\n+use common::bitpacking_links::{for_each_packed_link, pack_links, MIN_BITS_PER_VALUE};\n+use common::bitpacking_ordered;\n use common::types::PointOffsetType;\n+use common::zeros::WriteZerosExt as _;\n+use itertools::{Either, Itertools as _};\n use memmap2::Mmap;\n-use memory::madvise::{Advice, AdviceSetting, Madviseable};\n+use memory::madvise::{Advice, AdviceSetting};\n+use memory::mmap_ops;\n use memory::mmap_ops::open_read_mmap;\n-\n-use crate::common::operation_error::OperationResult;\n-\n-mod header;\n-mod serializer;\n-mod view;\n-\n-pub use serializer::GraphLinksSerializer;\n-pub use view::LinksIterator;\n-use view::{CompressionInfo, GraphLinksView};\n-\n-/*\n-Links data for whole graph layers.\n-\n-                                    sorted\n-                     points:        points:\n-points to lvl        012345         142350\n-     0 -> 0\n-     1 -> 4    lvl4:  7       lvl4: 7\n-     2 -> 2    lvl3:  Z  Y    lvl3: ZY\n-     3 -> 2    lvl2:  abcd    lvl2: adbc\n-     4 -> 3    lvl1:  ABCDE   lvl1: ADBCE\n-     5 -> 1    lvl0: 123456   lvl0: 123456  <- lvl 0 is not sorted\n-\n-\n-lvl offset:        6       11     15     17\n-                   \u2502       \u2502      \u2502      \u2502\n-                   \u2502       \u2502      \u2502      \u2502\n-                   \u25bc       \u25bc      \u25bc      \u25bc\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 99.14304375648499,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 65935,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  },
  {
    "success": true,
    "id": "qdrant_lib_segment_src_index_struct_payload_index.rs",
    "exact_match": false,
    "similarity": 0.09222017926913353,
    "diff_lines": 928,
    "predicted_output": "use crate::index::index::{PayloadIndex};\nuse crate::types::{Filter, PayloadKeyType, PayloadSchemaType, PayloadFieldSchema};\nuse std::sync::Arc;\nuse atomic_refcell::AtomicRefCell;\nuse crate::payload_storage::payload_storage::{ConditionChecker, PayloadStorage};\nuse crate::index::field_index::{Estimation, FieldIndex};\nuse std::path::{Path, PathBuf};\nuse std::collections::HashMap;\nuse std::fs::{File, create_dir_all};\nuse std::io::Error;\nuse crate::entry::entry_point::{OperationResult, OperationError};\nuse crate::index::field_index::index_builder::{IndexBuilderTypes, IndexBuilder};\nuse crate::index::field_index::numeric_index::PersistedNumericIndex;\nuse uuid::Builder;\nuse crate::index::field_index::field_index::PayloadFieldIndexBuilder;\nuse crate::index::field_index::index_selector::index_selector;\nuse crate::index::payload_config::PayloadConfig;\n\npub const PAYLOAD_FIELD_INDEX_PATH: &str = \"fields\";\n\ntype IndexesMap = HashMap<PayloadKeyType, Vec<FieldIndex>>;\n\n#[derive(Debug)]\npub struct StructPayloadIndex {\n    condition_checker: Arc<dyn ConditionChecker>,\n    payload: Arc<AtomicRefCell<dyn PayloadStorage>>,\n    field_indexes: IndexesMap,\n    config: PayloadConfig,\n    path: PathBuf,\n}\n\nimpl StructPayloadIndex {\n    pub fn open(condition_checker: Arc<dyn ConditionChecker>,\n                payload: Arc<AtomicRefCell<dyn PayloadStorage>>,\n                path: &Path,\n    ) -> OperationResult<Self> {\n        let config_path = PayloadConfig::get_config_path(path);\n        let config = PayloadConfig::load(&config_path)?;\n\n        let file = File::open(path);\n        let field_indexes: IndexesMap = match file {\n            Ok(file_reader) => serde_cbor::from_reader(file_reader).unwrap(),\n            Err(_) => Default::default()\n        };\n\n        let index = StructPayloadIndex {\n            condition_checker,\n            payload,\n            field_indexes,\n            config,\n            path: path.to_owned(),\n        };\n\n        Ok(index)\n    }\n\n    pub fn build_field_index(\n        &self,\n        field: PayloadKeyType,\n        payload_schema: PayloadFieldSchema,\n    ) -> OperationResult<Vec<FieldIndex>> {\n        let payload_storage = self.payload.borrow();\n        let mut builders = index_selector(&payload_schema);\n\n        payload_storage.iter(|point_id, point_payload| {\n            let field_value = &point_payload.get_value(field);\n            for builder in builders.iter_mut() {\n                builder.add(point_id, field_value)?;\n            }\n            Ok(true)\n        })?;\n\n        let field_indexes = builders\n            .iter_mut()\n            .map(|builder| builder.build())\n            .collect();\n\n        Ok(field_indexes)\n    }\n\n    fn save_field_index(&self, field: PayloadKeyType) -> OperationResult<()> {\n        let field_index_dir = Self::get_field_index_dir(&self.path);\n        let field_index_path = Self::get_field_index_path(&self.path, field);\n        create_dir_all(field_index_dir)?;\n\n        match self.field_indexes.get(&field) {\n            None => {}\n            Some(indexes) => {\n                let file = File::create(&field_index_path)?;\n                serde_cbor::to_writer(file, indexes).map_err(|err| {\n                    OperationError::ServiceError {\n                        description: format!(\"Unable to save index: {:?}\", err),\n                    }\n                })?;\n            }\n        }\n        Ok(())\n    }\n\n    fn load_all_fields(&mut self) -> OperationResult<()> {\n        let mut field_indexes: IndexesMap = Default::default();\n\n        for (field, payload_schema) in &self.config.indexed_fields {\n            let field_index = self.load_from_db(field, payload_schema.to_owned())?;\n            field_indexes.insert(field.clone(), field_index);\n        }\n        self.field_indexes = field_indexes;\n        Ok(())\n    }\n\n    fn load_from_db(\n        &self,\n        field: PayloadKeyTypeRef,\n        payload_schema: PayloadFieldSchema,\n    ) -> OperationResult<Vec<FieldIndex>> {\n        let mut indexes = index_selector(field, &payload_schema, self.db.clone());\n\n        let mut is_loaded = true;\n        for ref mut index in indexes.iter_mut() {\n            if !index.load()? {\n                is_loaded = false;\n                break;\n            }\n        }\n        if !is_loaded {\n            debug!(\"Index for `{field}` was not loaded. Building...\");\n            indexes = self.build_field_indexes(field, payload_schema)?;\n        }\n\n        Ok(indexes)\n    }\n\n    pub fn build_field_indexes(\n        &self,\n        field: PayloadKeyTypeRef,\n        payload_schema: PayloadFieldSchema,\n    ) -> OperationResult<Vec<FieldIndex>> {\n        let payload_storage = self.payload.borrow();\n        let mut builders = index_selector(&payload_schema);\n\n        payload_storage.iter(|point_id, point_payload| {\n            let field_value = &point_payload.get_value(field);\n            for builder in builders.iter_mut() {\n                builder.add(point_id, field_value)?;\n            }\n            Ok(true)\n        })?;\n\n        let field_indexes = builders\n            .iter_mut()\n            .map(|builder| builder.build())\n            .collect();\n\n        Ok(field_indexes)\n    }\n\n    pub fn config(&self) -> &PayloadConfig {\n        &self.config\n    }\n\n    pub fn clear_cache(&self) -> OperationResult<()> {\n        for (_, field_indexes) in self.field_indexes.iter() {\n            for index in field_indexes {\n                index.clear_cache()?;\n            }\n        }\n        Ok(())\n    }\n\n    pub fn files(&self) -> Vec<PathBuf> {\n        let mut files = self\n            .field_indexes\n            .values()\n            .flat_map(|indexes| indexes.iter().flat_map(|index| index.files().into_iter()))\n            .collect::<Vec<PathBuf>>();\n        files.push(self.config_path());\n        files\n    }\n}\n\nimpl PayloadIndex for StructPayloadIndex {\n    fn indexed_fields(&self) -> HashMap<PayloadKeyType, PayloadFieldSchema> {\n        self.config.indexed_fields.clone()\n    }\n\n    fn set_indexed(\n        &mut self,\n        field: PayloadKeyTypeRef,\n        payload_schema: PayloadFieldSchema,\n    ) -> OperationResult<()> {\n        if let Some(prev_schema) = self\n            .config\n            .indexed_fields\n            .insert(field.to_owned(), payload_schema.clone())\n        {\n            if prev_schema == payload_schema {\n                return Ok(());\n            }\n        }\n\n        let indexes = self.build_field_indexes(field, payload_schema)?;\n        self.field_indexes.insert(field.clone(), indexes);\n        self.save_config()?;\n\n        Ok(())\n    }\n\n    fn drop_index(&mut self, field: PayloadKeyTypeRef) -> OperationResult<()> {\n        self.config.indexed_fields.remove(field);\n        self.save_config()?;\n        self.field_indexes.remove(field);\n\n        let field_index_path = Self::get_field_index_path(&self.path, field);\n        if field_index_path.exists() {\n            remove_file(&field_index_path)?;\n        }\n\n        Ok(())\n    }\n\n    fn estimate_cardinality(&self, query: &Filter) -> CardinalityEstimation {\n        let available_points = self.available_point_count();\n        let estimator = |condition: &Condition| self.condition_cardinality(condition, None);\n        estimate_filter(&estimator, query, available_points)\n    }\n\n    fn query_points(&self, query: &Filter) -> Vec<PointOffsetType> {\n        let query_cardinality = self.estimate_cardinality(query);\n        let id_tracker = self.id_tracker.borrow();\n        self.iter_filtered_points(query, &*id_tracker, &query_cardinality)\n            .collect()\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.09222017926913353,
      "diff_lines": 928,
      "added_lines": 165,
      "removed_lines": 702,
      "diff_preview": "--- expected+++ predicted@@ -1,127 +1,107 @@+use crate::index::index::{PayloadIndex};\n+use crate::types::{Filter, PayloadKeyType, PayloadSchemaType, PayloadFieldSchema};\n+use std::sync::Arc;\n+use atomic_refcell::AtomicRefCell;\n+use crate::payload_storage::payload_storage::{ConditionChecker, PayloadStorage};\n+use crate::index::field_index::{Estimation, FieldIndex};\n+use std::path::{Path, PathBuf};\n use std::collections::HashMap;\n-use std::fs::create_dir_all;\n-use std::path::{Path, PathBuf};\n-use std::sync::Arc;\n-\n-use ahash::AHashSet;\n-use atomic_refcell::AtomicRefCell;\n-use common::counter::hardware_counter::HardwareCounterCell;\n-use common::counter::iterator_hw_measurement::HwMeasurementIteratorExt;\n-use common::flags::feature_flags;\n-use common::types::PointOffsetType;\n-use itertools::Either;\n-use log::debug;\n-use parking_lot::RwLock;\n-use rocksdb::DB;\n-use schemars::_serde_json::Value;\n-\n-use super::field_index::FieldIndexBuilderTrait as _;\n-use super::field_index::facet_index::FacetIndexEnum;\n-use super::field_index::index_selector::{IndexSelector, IndexSelectorMmap, IndexSelectorRocksDb};\n-use crate::common::Flusher;\n-use crate::common::operation_error::{OperationError, OperationResult};\n-use crate::common::rocksdb_wrapper::open_db_with_existing_cf;\n-use crate::common::utils::IndexesMap;\n-use crate::id_tracker::IdTrackerSS;\n-use crate::index::PayloadIndex;\n-use crate::index::field_index::{\n-    CardinalityEstimation, FieldIndex, PayloadBlockCondition, PrimaryCondition,\n-};\n+use std::fs::{File, create_dir_all};\n+use std::io::Error;\n+use crate::entry::entry_point::{OperationResult, OperationError};\n+use crate::index::field_index::index_builder::{IndexBuilderTypes, IndexBuilder};\n+use crate::index::field_index::numeric_index::PersistedNumericIndex;\n+use uuid::Builder;\n+use crate::index::field_index::field_index::PayloadFieldIndexBuilder;\n+use crate::index::field_index::index_selector::index_selector;\n use crate::index::payload_config::PayloadConfig;\n-use crate::index::query_estimator::estimate_filter;\n-use crate::index::query_optimization::payload_provider::PayloadProvider;\n"
    },
    "llm_calls": 2,
    "iterations": 2,
    "depth": 0,
    "elapsed_time": 38.71487379074097,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 82321,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "rlm"
  }
]