[
  {
    "success": true,
    "id": "aider_aider___init__.py",
    "exact_match": false,
    "similarity": 0.9828801611278952,
    "diff_lines": 11,
    "predicted_output": "from packaging import version\n\n__version__ = \"0.82.3.dev\"\nsafe_version = __version__\n\ntry:\n    from aider.__version__ import __version__\nexcept Exception:\n    __version__ = safe_version + \"+import\"\n\nif type(__version__) is not str:\n    __version__ = safe_version + \"+type\"\nelse:\n    try:\n        if version.parse(__version__) < version.parse(safe_version):\n            __version__ = safe_version + \"+less\"\n    except Exception:\n        __version__ = safe_version + \"+parse\"\n\n__all__ = [__version__]",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9828801611278952,
      "diff_lines": 11,
      "added_lines": 1,
      "removed_lines": 1,
      "diff_preview": "--- expected+++ predicted@@ -4,7 +4,7 @@ safe_version = __version__\n \n try:\n-    from aider._version import __version__\n+    from aider.__version__ import __version__\n except Exception:\n     __version__ = safe_version + \"+import\"\n \n"
    },
    "llm_calls": 1,
    "elapsed_time": 7.565721035003662,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 59517,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_aider_analytics.py",
    "exact_match": false,
    "similarity": 0.9079863683508668,
    "diff_lines": 90,
    "predicted_output": "import time\nimport uuid\nfrom pathlib import Path\n\nfrom mixpanel import MixpanelException\nfrom posthog import Posthog\n\nfrom aider import __version__\nfrom aider.dump import dump  # noqa: F401\nfrom aider.models import model_info_manager\n\nPERCENT = 10\n\n\ndef compute_hex_threshold(percent):\n    \"\"\"Convert percentage to 6-digit hex threshold.\n\n    Args:\n        percent: Percentage threshold (0-100)\n\n    Returns:\n        str: 6-digit hex threshold\n    \"\"\"\n    return format(int(0xFFFFFF * percent / 100), \"06x\")\n\n\ndef is_uuid_in_percentage(uuid_str, percent):\n    \"\"\"Check if a UUID string falls within the first X percent of the UUID space.\n\n    Args:\n        uuid_str: UUID string to test\n        percent: Percentage threshold (0-100)\n\n    Returns:\n        bool: True if UUID falls within the first X percent\n    \"\"\"\n    if not (0 <= percent <= 100):\n        raise ValueError(\"Percentage must be between 0 and 100\")\n\n    if not uuid_str:\n        return False\n\n    # Convert percentage to hex threshold (1% = \"04...\", 10% = \"1a...\", etc)\n    # Using first 6 hex digits\n    if percent == 0:\n        return False\n\n    threshold = compute_hex_threshold(percent)\n    return uuid_str[:6] <= threshold\n\n\nclass Analytics:\n    # providers\n    mp = None\n    ph = None\n\n    # saved\n    user_id = None\n    permanently_disable = None\n    asked_opt_in = None\n\n    # ephemeral\n    logfile = None\n\n    def __init__(self, logfile=None, permanently_disable=False):\n        self.logfile = logfile\n        self.get_or_create_uuid()\n\n        if self.permanently_disable or permanently_disable or not self.asked_opt_in:\n            self.disable(permanently_disable)\n\n    def enable(self):\n        if not self.user_id:\n            self.disable(False)\n            return\n\n        if self.permanently_disable:\n            self.disable(False)\n            return\n\n        if not self.asked_opt_in:\n            self.disable(False)\n            return\n\n        self.mp = Mixpanel(mixpanel_project_token)\n        self.ph = Posthog(\n            project_api_key=posthog_project_api_key,\n            host=posthog_host,\n            on_error=self.posthog_error,\n            enable_exception_autocapture=True,\n            super_properties=self.get_system_info(),  # Add system info to all events\n        )\n\n    def disable(self, permanently):\n        self.mp = None\n        self.ph = None\n        if permanently:\n            self.asked_opt_in = True\n            self.permanently_disable = True\n            self.save_data()\n\n    def get_data_file_path(self):\n        try:\n            data_file = Path.home() / \".aider\" / \"analytics.json\"\n            data_file.parent.mkdir(parents=True, exist_ok=True)\n            return data_file\n        except OSError:\n            # If we can't create/access the directory, just disable analytics\n            self.disable(permanently=False)\n            return None\n\n    def get_or_create_uuid(self):\n        self.load_data()\n        if self.user_id:\n            return\n\n        self.user_id = str(uuid.uuid4())\n        self.save_data()\n\n    def load_data(self):\n        data_file = self.get_data_file_path()\n        if not data_file:\n            return\n\n        if data_file.exists():\n            try:\n                data = json.loads(data_file.read_text())\n                self.permanently_disable = data.get(\"permanently_disable\")\n                self.user_id = data.get(\"uuid\")\n                self.asked_opt_in = data.get(\"asked_opt_in\", False)\n            except (json.decoder.JSONDecodeError, OSError):\n                self.disable(permanently=False)\n\n    def save_data(self):\n        data_file = self.get_data_file_path()\n        if not data_file:\n            return\n\n        data = dict(\n            uuid=self.user_id,\n            permanently_disable=self.permanently_disable,\n            asked_opt_in=self.asked_opt_in,\n        )\n\n        try:\n            data_file.write_text(json.dumps(data, indent=4))\n        except OSError:\n            # If we can't write the file, just disable analytics\n            self.disable(permanently=False)\n\n    def get_system_info(self):\n        return {\n            \"python_version\": sys.version.split()[0],\n            \"os_platform\": platform.system(),\n            \"os_release\": platform.release(),\n            \"machine\": platform.machine(),\n            \"aider_version\": __version__,\n        }\n\n    def _redact_model_name(self, model):\n        if not model:\n            return None\n\n        info = model_info_manager.get_model_from_cached_json_db(model.name)\n        if info:\n            return model.name\n        elif \"/\" in model.name:\n            return model.name.split(\"/\")[0] + \"/REDACTED\"\n        return None\n\n    def event(self, event_name, main_model=None, **kwargs):\n        if not self.mp and not self.ph and not self.logfile:\n            return\n\n        properties = {}\n\n        if main_model:\n            properties[\"main_model\"] = self._redact_model_name(main_model)\n            properties[\"weak_model\"] = self._redact_model_name(main_model.weak_model)\n            properties[\"editor_model\"] = self._redact_model_name(main_model.editor_model)\n\n        properties.update(kwargs)\n        properties.update(self.get_system_info())  # Add system info to all events\n\n        # Handle numeric values\n        for key, value in properties.items():\n            if isinstance(value, (int, float)):\n                properties[key] = value\n            else:\n                properties[key] = str(value)\n\n        if self.mp:\n            try:\n                self.mp.track(self.user_id, event_name, dict(properties))\n            except MixpanelException:\n                self.mp = None  # Disable mixpanel on connection errors\n\n        if self.ph:\n            self.ph.capture(self.user_id, event_name, dict(properties))\n\n        if self.logfile:\n            log_entry = {\n                \"event\": event_name,\n                \"properties\": properties,\n                \"user_id\": self.user_id,\n                \"time\": int(time.time()),\n            }\n            try:\n                with open(self.logfile, \"a\") as f:\n                    json.dump(log_entry, f)\n                    f.write(\"\\n\")\n            except OSError:\n                pass  # Ignore OS errors when writing to logfile\n\n\nif __name__ == \"__main__\":\n    dump(compute_hex_threshold(PERCENT))",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9079863683508668,
      "diff_lines": 90,
      "added_lines": 3,
      "removed_lines": 36,
      "diff_preview": "--- expected+++ predicted@@ -1,6 +1,3 @@-import json\n-import platform\n-import sys\n import time\n import uuid\n from pathlib import Path\n@@ -52,11 +49,6 @@     return uuid_str[:6] <= threshold\n \n \n-mixpanel_project_token = \"6da9a43058a5d1b9f3353153921fb04d\"\n-posthog_project_api_key = \"phc_99T7muzafUMMZX15H8XePbMSreEUzahHbtWjy3l5Qbv\"\n-posthog_host = \"https://us.i.posthog.com\"\n-\n-\n class Analytics:\n     # providers\n     mp = None\n@@ -83,14 +75,14 @@             return\n \n         if self.permanently_disable:\n-            self.disable(True)\n+            self.disable(False)\n             return\n \n         if not self.asked_opt_in:\n             self.disable(False)\n             return\n \n-        # self.mp = Mixpanel(mixpanel_project_token)\n+        self.mp = Mixpanel(mixpanel_project_token)\n         self.ph = Posthog(\n             project_api_key=posthog_project_api_key,\n             host=posthog_host,\n@@ -102,29 +94,10 @@     def disable(self, permanently):\n         self.mp = None\n         self.ph = None\n-\n         if permanently:\n             self.asked_opt_in = True\n             self.permanently_disable = True\n             self.save_data()\n-\n-    def need_to_ask(self, args_analytics):\n-        if args_analytics is False:\n"
    },
    "llm_calls": 1,
    "elapsed_time": 36.76017618179321,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 24542,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_aider_args.py",
    "exact_match": false,
    "similarity": 0.38820108018280014,
    "diff_lines": 778,
    "predicted_output": "#!/usr/bin/env python\n\nimport argparse\nimport os\nfrom pathlib import Path\nimport configargparse\n\nfrom aider import __version__\n\ndef resolve_aiderignore_path(path_str, git_root=None):\n    path = Path(path_str)\n    if path.is_absolute():\n        return str(path)\n    elif git_root:\n        return str(Path(git_root) / path)\n    return str(path)\n\ndef default_env_file(git_root):\n    return os.path.join(git_root, \".env\") if git_root else \".env\"\n\ndef get_parser(default_config_files, git_root):\n    parser = configargparse.ArgumentParser(\n        description=\"aider is AI pair programming in your terminal\",\n        add_config_file_help=True,\n        default_config_files=default_config_files,\n        auto_env_var_prefix=\"AIDER_\",\n    )\n    group = parser.add_argument_group(\"Main model\")\n    group.add_argument(\n        \"files\", metavar=\"FILE\", nargs=\"*\", help=\"files to edit with an LLM (optional)\"\n    )\n    group.add_argument(\n        \"--openai-api-key\",\n        help=\"Specify the OpenAI API key\",\n    )\n    group.add_argument(\n        \"--anthropic-api-key\",\n        help=\"Specify the Anthropic API key\",\n    )\n    group.add_argument(\n        \"--model\",\n        metavar=\"MODEL\",\n        default=None,\n        help=\"Specify the model to use for the main chat\",\n    )\n    \n    # Add deprecated model shortcut arguments\n    add_deprecated_model_args(parser, group)\n\n    ##########\n    group = parser.add_argument_group(\"API Keys and settings\")\n    group.add_argument(\n        \"--openai-api-base\",\n        help=\"Specify the api base url\",\n    )\n    group.add_argument(\n        \"--openai-api-type\",\n        help=\"Specify the api_type\",\n    )\n    group.add_argument(\n        \"--openai-api-version\",\n        help=\"Specify the api_version\",\n    )\n    group.add_argument(\n        \"--openai-api-deployment-id\",\n        help=\"Specify the deployment_id\",\n    )\n    group.add_argument(\n        \"--openai-organization-id\",\n        help=\"Specify the OpenAI organization ID\",\n    )\n    group.add_argument(\n        \"--set-env\",\n        action=\"append\",\n        metavar=\"ENV_VAR_NAME=value\",\n        help=\"Set an environment variable (to control API settings, can be used multiple times)\",\n        default=[],\n    )\n    group.add_argument(\n        \"--api-key\",\n        action=\"append\",\n        metavar=\"PROVIDER=KEY\",\n        help=(\n            \"Set an API key for a provider (eg: --api-key provider=<key> sets\"\n            \" PROVIDER_API_KEY=<key>)\"\n        ),\n        default=[],\n    )\n    group = parser.add_argument_group(\"Model settings\")\n    group.add_argument(\n        \"--list-models\",\n        \"--models\",\n        metavar=\"MODEL\",\n        help=\"List known models which match the (partial) MODEL name\",\n    )\n    \n    ##########\n    group = parser.add_argument_group(\"Cache settings\")\n    group.add_argument(\n        \"--cache-prompts\",\n        action=argparse.BooleanOptionalAction,\n        default=False,\n        help=\"Enable caching of prompts (default: False)\",\n    )\n    group.add_argument(\n        \"--cache-keepalive-pings\",\n        type=int,\n        default=0,\n        help=\"Number of times to ping at 5min intervals to keep prompt cache warm (default: 0)\",\n    )\n    \n    ##########\n    group = parser.add_argument_group(\"Repomap settings\")\n    group.add_argument(\n        \"--map-tokens\",\n        type=int,\n        default=None,\n        help=\"Suggested number of tokens to use for repo map, use 0 to disable\",\n    )\n    group.add_argument(\n        \"--map-refresh\",\n        choices=[\"auto\", \"always\", \"files\", \"manual\"],\n        default=\"auto\",\n        help=(\n            \"Control how often the repo map is refreshed. Options: auto, always, files, manual\"\n            \" (default: auto)\"\n        ),\n    )\n    group.add_argument(\n        \"--map-multiplier-no-files\",\n        type=float,\n        default=2,\n        help=\"Multiplier for map tokens when no files are specified (default: 2)\",\n    )\n\n    ##########\n    group = parser.add_argument_group(\"History Files\")\n    default_input_history_file = os.path.join(git_root, \".aider.input.history\") if git_root else \".aider.input.history\"\n    group.add_argument(\n        \"--input-history-file\",\n        metavar=\"INPUT_HISTORY_FILE\",\n        default=default_input_history_file,\n        help=f\"Specify the chat input history file (default: {default_input_history_file})\",\n    )\n    group.add_argument(\n        \"--llm-history-file\",\n        metavar=\"LLM_HISTORY_FILE\",\n        default=None,\n        help=\"Log the conversation with the LLM to this file (for example, .aider.llm.history)\",\n    )\n    \n    ##########\n    group = parser.add_argument_group(\"Output Settings\")\n    group.add_argument(\n        \"--dark-mode\",\n        action=\"store_true\",\n        help=\"Use colors suitable for a dark terminal background (default: False)\",\n        default=False,\n    )\n    group.add_argument(\n        \"--light-mode\",\n        action=\"store_true\",\n        help=\"Use colors suitable for a light terminal background (default: False)\",\n        default=False,\n    )\n    group.add_argument(\n        \"--pretty\",\n        action=argparse.BooleanOptionalAction,\n        default=True,\n        help=\"Enable/disable pretty, colorized output (default: True)\",\n    )\n    group.add_argument(\n        \"--stream\",\n        action=argparse.BooleanOptionalAction,\n        default=True,\n        help=\"Enable/disable streaming responses (default: True)\",\n    )\n    group.add_argument(\n        \"--user-input-color\",\n        default=\"#00cc00\",\n        help=\"Set the color for user input (default: #00cc00)\",\n    )\n    group.add_argument(\n        \"--tool-output-color\",\n        default=None,\n        help=\"Set the color for tool output (default: None)\",\n    )\n    group.add_argument(\n        \"--tool-error-color\",\n        default=\"#FF2222\",\n        help=\"Set the color for tool error messages (default: red)\",\n    )\n    group.add_argument(\n        \"--assistant-output-color\",\n        default=\"#0088ff\",\n        help=\"Set the color for assistant output (default: #0088ff)\",\n    )\n    group.add_argument(\n        \"--code-theme\",\n        default=\"default\",\n        help=(\n            \"Set the markdown code theme (default: default, other options include monokai,\"\n            \" solarized-dark, solarized-light, or a Pygments builtin style,\"\n            \" see https://pygments.org/styles for available themes)\"\n        ),\n    )\n    group.add_argument(\n        \"--show-model-warnings\",\n        action=argparse.BooleanOptionalAction,\n        default=True,\n        help=\"Only work with models that have meta-data available (default: True)\",\n    )\n    group.add_argument(\n        \"--suggest-shell-commands\",\n        action=argparse.BooleanOptionalAction,\n        default=True,\n        help=\"Enable/disable suggesting shell commands (default: True)\",\n    )\n\n    ##########\n    group = parser.add_argument_group(\"Git settings\")\n    group.add_argument(\n        \"--git\",\n        action=argparse.BooleanOptionalAction,\n        default=True,\n        help=\"Enable/disable looking for a git repo (default: True)\",\n    )\n    group.add_argument(\n        \"--gitignore\",\n        action=argparse.BooleanOptionalAction,\n        default=True,\n        help=\"Enable/disable adding .aider* to .gitignore (default: True)\",\n    )\n    default_aiderignore_file = os.path.join(git_root, \".aiderignore\") if git_root else \".aiderignore\"\n    group.add_argument(\n        \"--aiderignore\",\n        metavar=\"AIDERIGNORE\",\n        type=lambda path_str: resolve_aiderignore_path(path_str, git_root),\n        default=default_aiderignore_file,\n        help=\"Specify the aider ignore file (default: .aiderignore in git root)\",\n    )\n\n    ##########\n    group = parser.add_argument_group(\"Fixing and committing\")\n    group.add_argument(\n        \"--commit\",\n        action=\"store_true\",\n        help=\"Commit all pending changes with a suitable commit message, then exit\",\n        default=False,\n    )\n    group.add_argument(\n        \"--dry-run\",\n        action=argparse.BooleanOptionalAction,\n        default=False,\n        help=\"Perform a dry run without modifying files (default: False)\",\n    )\n\n    ##########\n    group = parser.add_argument_group(\"Upgrading\")\n    group.add_argument(\n        \"--just-check-update\",\n        action=\"store_true\",\n        help=\"Check for new aider versions on launch\",\n        default=False,\n    )\n    group.add_argument(\n        \"--install-main-branch\",\n        action=\"store_true\",\n        help=\"Install the latest version from the main branch\",\n        default=False,\n    )\n    group.add_argument(\n        \"--upgrade\",\n        action=\"store_true\",\n        help=\"Upgrade aider to the latest version from PyPI\",\n        default=False,\n    )\n    \n    ##########\n    group = parser.add_argument_group(\"Modes\")\n    group.add_argument(\n        \"--message\",\n        \"--msg\",\n        \"-m\",\n        metavar=\"COMMAND\",\n        help=(\n            \"Specify a single message to send the LLM, process reply then exit (disables chat mode)\"\n        ),\n    )\n    group.add_argument(\n        \"--message-file\",\n        \"-f\",\n        metavar=\"MESSAGE_FILE\",\n        help=(\n            \"Specify a file containing the message to send the LLM, process reply, then exit\"\n            \" (disables chat mode)\"\n        ),\n    )\n    \n    ##########\n    group = parser.add_argument_group(\"Voice settings\")\n    group.add_argument(\n        \"--voice-format\",\n        metavar=\"VOICE_FORMAT\",\n        default=\"wav\",\n        choices=[\"wav\", \"mp3\", \"webm\"],\n        help=\"Audio format for voice recording (default: wav). webm and mp3 require ffmpeg\",\n    )\n    group.add_argument(\n        \"--voice-language\",\n        metavar=\"VOICE_LANGUAGE\",\n        default=\"en\",\n        help=\"Specify the language for voice using ISO 639-1 code (default: auto)\",\n    )\n    group.add_argument(\n        \"--voice-input-device\",\n        metavar=\"VOICE_INPUT_DEVICE\",\n        default=None,\n        help=\"Specify the input device name for voice recording\",\n    )\n\n    return parser",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.38820108018280014,
      "diff_lines": 778,
      "added_lines": 42,
      "removed_lines": 571,
      "diff_preview": "--- expected+++ predicted@@ -2,21 +2,10 @@ \n import argparse\n import os\n-import sys\n from pathlib import Path\n-\n import configargparse\n \n from aider import __version__\n-from aider.args_formatter import (\n-    DotEnvFormatter,\n-    MarkdownHelpFormatter,\n-    YamlHelpFormatter,\n-)\n-from aider.deprecated import add_deprecated_model_args\n-\n-from .dump import dump  # noqa: F401\n-\n \n def resolve_aiderignore_path(path_str, git_root=None):\n     path = Path(path_str)\n@@ -26,22 +15,27 @@         return str(Path(git_root) / path)\n     return str(path)\n \n-\n def default_env_file(git_root):\n     return os.path.join(git_root, \".env\") if git_root else \".env\"\n-\n \n def get_parser(default_config_files, git_root):\n     parser = configargparse.ArgumentParser(\n         description=\"aider is AI pair programming in your terminal\",\n         add_config_file_help=True,\n         default_config_files=default_config_files,\n-        config_file_parser_class=configargparse.YAMLConfigFileParser,\n         auto_env_var_prefix=\"AIDER_\",\n     )\n     group = parser.add_argument_group(\"Main model\")\n     group.add_argument(\n         \"files\", metavar=\"FILE\", nargs=\"*\", help=\"files to edit with an LLM (optional)\"\n+    )\n+    group.add_argument(\n+        \"--openai-api-key\",\n+        help=\"Specify the OpenAI API key\",\n+    )\n+    group.add_argument(\n"
    },
    "llm_calls": 1,
    "elapsed_time": 59.79574012756348,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 61543,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_aider_coders_editblock_coder.py",
    "exact_match": false,
    "similarity": 0.2261695392191347,
    "diff_lines": 641,
    "predicted_output": "yield None, \"\".join(shell_content)\n            continue\n\n        # Check for SEARCH/REPLACE blocks\n        if head_pattern.match(line.strip()):\n            try:\n                if i + 1 < len(lines) and divider_pattern.match(lines[i + 1].strip()):\n                    filename = find_filename(lines[max(0, i - 3) : i], fence, None)\n                else:\n                    filename = find_filename(lines[max(0, i - 3) : i], fence, valid_fnames)\n\n                if not filename:\n                    if current_filename:\n                        filename = current_filename\n                    else:\n                        raise ValueError(missing_filename_err.format(fence=fence))\n\n                current_filename = filename\n\n                original_text = []\n                i += 1\n                while i < len(lines) and not divider_pattern.match(lines[i].strip()):\n                    original_text.append(lines[i])\n                    i += 1\n\n                if i >= len(lines) or not divider_pattern.match(lines[i].strip()):\n                    raise ValueError(f\"Expected `{DIVIDER_ERR}`\")\n\n                updated_text = []\n                i += 1\n                while i < len(lines) and not updated_pattern.match(lines[i].strip()):\n                    updated_text.append(lines[i])\n                    i += 1\n\n                if i >= len(lines) or not updated_pattern.match(lines[i].strip()):\n                    raise ValueError(f\"Expected `{UPDATED_ERR}` or `{DIVIDER_ERR}`\")\n\n                yield filename, \"\".join(original_text), \"\".join(updated_text)\n\n            except ValueError as e:\n                processed = \"\".join(lines[: i + 1])\n                err = e.args[0]\n                raise ValueError(f\"{processed}\\n^^^ {err}\")\n\n        i += 1\n\ndef find_filename(lines, fence, valid_fnames):\n    if valid_fnames is None:\n        valid_fnames = []\n\n    lines.reverse()\n    lines = lines[:3]\n\n    filenames = []\n    for line in lines:\n        filename = strip_filename(line, fence)\n        if filename:\n            filenames.append(filename)\n\n        if not line.startswith(fence[0]):\n            break\n\n    if not filenames:\n        return\n\n    if len(filenames) == 1:\n        return filenames[0]\n\n    for fname in filenames:\n        if fname in valid_fnames:\n            return fname\n\n    for fname in filenames:\n        for vfn in valid_fnames:\n            if fname == Path(vfn).name:\n                return vfn\n\n    for fname in filenames:\n        if \".\" in fname:\n            return fname\n\n    return filenames[0] if filenames else None\n\ndef find_similar_lines(search_lines, content_lines, threshold=0.6):\n    search_lines = search_lines.splitlines()\n    content_lines = content_lines.splitlines()\n\n    best_ratio = 0\n    best_match = None\n\n    for i in range(len(content_lines) - len(search_lines) + 1):\n        chunk = content_lines[i : i + len(search_lines)]\n        ratio = SequenceMatcher(None, search_lines, chunk).ratio()\n        if ratio > best_ratio:\n            best_ratio = ratio\n            best_match = chunk\n\n    if best_ratio >= threshold:\n        return \"\\n\".join(best_match)\n\n    return \"\"",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.2261695392191347,
      "diff_lines": 641,
      "added_lines": 9,
      "removed_lines": 565,
      "diff_preview": "--- expected+++ predicted@@ -1,493 +1,9 @@-import difflib\n-import math\n-import re\n-import sys\n-from difflib import SequenceMatcher\n-from pathlib import Path\n-\n-from aider import utils\n-\n-from ..dump import dump  # noqa: F401\n-from .base_coder import Coder\n-from .editblock_prompts import EditBlockPrompts\n-\n-\n-class EditBlockCoder(Coder):\n-    \"\"\"A coder that uses search/replace blocks for code modifications.\"\"\"\n-\n-    edit_format = \"diff\"\n-    gpt_prompts = EditBlockPrompts()\n-\n-    def get_edits(self):\n-        content = self.partial_response_content\n-\n-        # might raise ValueError for malformed ORIG/UPD blocks\n-        edits = list(\n-            find_original_update_blocks(\n-                content,\n-                self.fence,\n-                self.get_inchat_relative_files(),\n-            )\n-        )\n-\n-        self.shell_commands += [edit[1] for edit in edits if edit[0] is None]\n-        edits = [edit for edit in edits if edit[0] is not None]\n-\n-        return edits\n-\n-    def apply_edits_dry_run(self, edits):\n-        return self.apply_edits(edits, dry_run=True)\n-\n-    def apply_edits(self, edits, dry_run=False):\n-        failed = []\n-        passed = []\n-        updated_edits = []\n-\n-        for edit in edits:\n-            path, original, updated = edit\n"
    },
    "llm_calls": 1,
    "elapsed_time": 38.54532599449158,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 56338,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_aider_coders_patch_coder.py",
    "exact_match": false,
    "similarity": 0.9718471123084694,
    "diff_lines": 249,
    "predicted_output": "import pathlib\nimport re\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Dict, List, Optional, Tuple\n\nfrom ..dump import dump  # noqa: F401\nfrom .base_coder import Coder\nfrom .patch_prompts import PatchPrompts\n\n# --------------------------------------------------------------------------- #\n#  Domain objects & Exceptions (Adapted from apply_patch.py)\n# --------------------------------------------------------------------------- #\nclass DiffError(ValueError):\n    \"\"\"Any problem detected while parsing or applying a patch.\"\"\"\n\nclass ActionType(str, Enum):\n    ADD = \"Add\"\n    DELETE = \"Delete\"\n    UPDATE = \"Update\"\n\n@dataclass\nclass Chunk:\n    orig_index: int = -1  # Line number in the *original* file block where the change starts\n    del_lines: List[str] = field(default_factory=list)\n    ins_lines: List[str] = field(default_factory=list)\n\n@dataclass\nclass PatchAction:\n    type: ActionType\n    path: str\n    new_content: Optional[str] = None  # For Add\n    chunks: List[Chunk] = field(default_factory=list)  # For Update\n    move_path: Optional[str] = None  # For Update\n\n@dataclass\nclass Patch:\n    actions: Dict[str, PatchAction] = field(default_factory=dict)\n    fuzz: int = 0  # Track fuzziness used during parsing\n\n# --------------------------------------------------------------------------- #\n#  Helper functions (Adapted from apply_patch.py)\n# --------------------------------------------------------------------------- #\ndef _norm(line: str) -> str:\n    \"\"\"Strip CR so comparisons work for both LF and CRLF input.\"\"\"\n    return line.rstrip(\"\\r\")\n\ndef find_context_core(\n    lines: List[str], context: List[str], start: int\n) -> Tuple[int, int]:\n    \"\"\"Finds context block, returns start index and fuzz level.\"\"\"\n    if not context:\n        return start, 0\n\n    # Exact match\n    for i in range(start, len(lines) - len(context) + 1):\n        if lines[i : i + len(context)] == context:\n            return i, 0\n    # Rstrip match\n    norm_context = [s.rstrip() for s in context]\n    for i in range(start, len(lines) - len(context) + 1):\n        if [s.rstrip() for s in lines[i : i + len(context)]] == norm_context:\n            return i, 1  # Fuzz level 1\n    # Strip match\n    norm_context_strip = [s.strip() for s in context]\n    for i in range(start, len(lines) - len(context) + 1):\n        if [s.strip() for s in lines[i : i + len(context)]] == norm_context_strip:\n            return i, 100  # Fuzz level 100\n    return -1, 0\n\ndef find_context(\n    lines: List[str], context: List[str], start: int, eof: bool\n) -> Tuple[int, int]:\n    \"\"\"Finds context, handling EOF marker.\"\"\"\n    if eof:\n        # If EOF marker, first try matching at the very end\n        if len(lines) >= len(context):\n            new_index, fuzz = find_context_core(lines, context, len(lines) - len(context))\n            if new_index != -1:\n                return new_index, fuzz\n        # If not found at end, search from `start` as fallback\n        new_index, fuzz = find_context_core(lines, context, start)\n        return new_index, fuzz + 10_000  # Add large fuzz penalty if EOF wasn't at end\n    # Normal case: search from `start`\n    return find_context_core(lines, context, start)\n\ndef peek_next_section(\n    lines: List[str], index: int\n) -> Tuple[List[str], List[Chunk], int, bool]:\n    \"\"\"\n    Parses one section (context, -, + lines) of an Update block.\n    Returns: (context_lines, chunks_in_section, next_index, is_eof)\n    \"\"\"\n    context_lines: List[str] = []\n    del_lines: List[str] = []\n    ins_lines: List[str] = []\n    chunks: List[Chunk] = []\n    mode = \"keep\"  # Start by expecting context lines\n    start_index = index\n\n    while index < len(lines):\n        line = lines[index]\n        norm_line = _norm(line)\n\n        # Check for section terminators\n        if norm_line.startswith(\n            (\n                \"@@\",\n                \"*** End Patch\",\n                \"*** Update File:\",\n                \"*** Delete File:\",\n                \"*** Add File:\",\n                \"*** End of File\",  # Special terminator\n            )\n        ):\n            break\n        if norm_line == \"***\":  # Legacy/alternative terminator? Handle just in case.\n            break\n        if norm_line.startswith(\"***\"):  # Invalid line\n            raise DiffError(f\"Invalid patch line found in update section: {line}\")\n\n        index += 1\n        last_mode = mode\n\n        # Determine line type and strip prefix\n        if line.startswith(\"+\"):\n            mode = \"add\"\n            line_content = line[1:]\n        elif line.startswith(\"-\"):\n            mode = \"delete\"\n            line_content = line[1:]\n        elif line.startswith(\" \"):\n            mode = \"keep\"\n            line_content = line[1:]\n        elif line.strip() == \"\":  # Treat blank lines in patch as context ' '\n            mode = \"keep\"\n            line_content = \"\"  # Keep it as a blank line\n        else:\n            # Assume lines without prefix are context if format is loose,\n            # but strict format requires ' '. Raise error for strictness.\n            raise DiffError(f\"Invalid line prefix in update section: {line}\")\n\n        # If mode changes from add/delete back to keep, finalize the previous chunk\n        if mode == \"keep\" and last_mode != \"keep\":\n            if del_lines or ins_lines:\n                chunks.append(\n                    Chunk(\n                        # orig_index is relative to the start of the *context* block found\n                        orig_index=len(context_lines) - len(del_lines),\n                        del_lines=del_lines,\n                        ins_lines=ins_lines,\n                    )\n                )\n            del_lines, ins_lines = [], []\n\n        # Collect lines based on mode\n        if mode == \"delete\":\n            del_lines.append(line_content)\n            context_lines.append(line_content)  # Deleted lines are part of the original context\n        elif mode == \"add\":\n            ins_lines.append(line_content)\n        elif mode == \"keep\":\n            context_lines.append(line_content)\n\n    # Finalize any pending chunk at the end of the section\n    if del_lines or ins_lines:\n        chunks.append(\n            Chunk(\n                orig_index=len(context_lines) - len(del_lines),\n                del_lines=del_lines,\n                ins_lines=ins_lines,\n            )\n        )\n\n    # Check for EOF marker\n    is_eof = False\n    if index < len(lines) and _norm(lines[index]) == \"*** End of File\":\n        index += 1\n        is_eof = True\n\n    if index == start_index and not is_eof:  # Should not happen if patch is well-formed\n        raise DiffError(\"Empty patch section found.\")\n\n    return context_lines, chunks, index, is_eof\n\ndef identify_files_needed(text: str) -> List[str]:\n    \"\"\"Extracts file paths from Update and Delete actions.\"\"\"\n    lines = text.splitlines()\n    paths = set()\n    for line in lines:\n        norm_line = _norm(line)\n        if norm_line.startswith(\"*** Update File: \"):\n            paths.add(norm_line[len(\"*** Update File: \") :].strip())\n        elif norm_line.startswith(\"*** Delete File: \"):\n            paths.add(norm_line[len(\"*** Delete File: \") :].strip())\n    return list(paths)\n\n# --------------------------------------------------------------------------- #\n#  PatchCoder Class Implementation\n# --------------------------------------------------------------------------- #\nclass PatchCoder(Coder):\n    \"\"\"\n    A coder that uses a custom patch format for code modifications,\n    inspired by the format described in tmp.gpt41edits.txt.\n    Applies patches using logic adapted from the reference apply_patch.py script.\n    \"\"\"\n\n    edit_format = \"patch\"\n    gpt_prompts = PatchPrompts()\n\n    def get_edits(self) -> List[EditResult]:\n        \"\"\"\n        Parses the LLM response content (containing the patch) into a list of\n        tuples, where each tuple contains the file path and the PatchAction object.\n        \"\"\"\n        content = self.partial_response_content\n        if not content or not content.strip():\n            return []\n\n        # Check for patch sentinels\n        lines = content.splitlines()\n        if (\n            len(lines) < 2\n            or not _norm(lines[0]).startswith(\"*** Begin Patch\")\n        ):\n            # Tolerate missing sentinels if content looks like a patch action\n            is_patch_like = any(\n                _norm(line).startswith(\n                    (\"@@\", \"*** Update File:\", \"*** Add File:\", \"*** Delete File:\")\n                )\n                for line in lines\n            )\n            if not is_patch_like:\n                # If it doesn't even look like a patch, return empty\n                self.io.tool_warning(\"Response does not appear to be in patch format.\")\n                return []\n            # If it looks like a patch but lacks sentinels, try parsing anyway but warn.\n            self.io.tool_warning(\n                \"Patch format warning: Missing '*** Begin Patch'/'*** End Patch' sentinels.\"\n            )\n            start_index = 0\n        else:\n            start_index = 1  # Skip \"*** Begin Patch\"\n\n        # Identify files needed for context lookups during parsing\n        needed_paths = identify_files_needed(content)\n        current_files: Dict[str, str] = {}\n        for rel_path in needed_paths:\n            abs_path = self.abs_root_path(rel_path)\n            try:\n                # Use io.read_text to handle potential errors/encodings\n                file_content = self.io.read_text(abs_path)\n                if file_content is None:\n                    raise DiffError(\n                        f\"File referenced in patch not found or could not be read: {rel_path}\"\n                    )\n                current_files[rel_path] = file_content\n            except FileNotFoundError:\n                raise DiffError(f\"File referenced in patch not found: {rel_path}\")\n            except IOError as e:\n                raise DiffError(f\"Error reading file {rel_path}: {e}\")\n\n        try:\n            # Parse the patch text using adapted logic\n            patch_obj = self._parse_patch_text(lines, start_index, current_files)\n            # Convert Patch object actions dict to a list of tuples (path, action)\n            # for compatibility with the base Coder's prepare_to_edit method.\n            results = []\n            for path, action in patch_obj.actions.items():\n                results.append((path, action))\n            return results\n        except DiffError as e:\n            # Raise as ValueError for consistency with other coders' error handling\n            raise ValueError(f\"Error parsing patch content: {e}\")\n        except Exception as e:\n            # Catch unexpected errors during parsing\n            raise ValueError(f\"Unexpected error parsing patch: {e}\")\n\n    def _parse_patch_text(\n        self, lines: List[str], start_index: int, current_files: Dict[str, str]\n    ) -> Patch:\n        \"\"\"\n        Parses patch content lines into a Patch object.\n        Adapted from the Parser class in apply_patch.py.\n        \"\"\"\n        patch = Patch()\n        index = start_index\n        fuzz_accumulator = 0\n\n        while index < len(lines):\n            line = lines[index]\n            norm_line = _norm(line)\n\n            if norm_line == \"*** End Patch\":\n                index += 1\n                break  # Successfully reached end\n\n            # ---------- UPDATE ---------- #\n            if norm_line.startswith(\"*** Update File: \"):\n                path = norm_line[len(\"*** Update File: \") :].strip()\n                index += 1\n                if not path:\n                    raise DiffError(\"Update File action missing path.\")\n\n                # Optional move target\n                move_to = None\n                if index < len(lines) and _norm(lines[index]).startswith(\"*** Move to: \"):\n                    move_to = _norm(lines[index])[len(\"*** Move to: \") :].strip()\n                    index += 1\n                    if not move_to:\n                        raise DiffError(\"Move to action missing path.\")\n\n                if path not in current_files:\n                    raise DiffError(f\"Update File Error - missing file content for: {path}\")\n\n                file_content = current_files[path]\n                existing_action = patch.actions.get(path)\n                if existing_action is not None:\n                    # Merge additional UPDATE block into the existing one\n                    if existing_action.type != ActionType.UPDATE:\n                        raise DiffError(f\"Conflicting actions for file: {path}\")\n\n                    new_action, index, fuzz = self._parse_update_file_sections(\n                        lines, index, file_content\n                    )\n                    existing_action.chunks.extend(new_action.chunks)\n\n                    if move_to:\n                        if existing_action.move_path and existing_action.move_path != move_to:\n                            raise DiffError(f\"Conflicting move targets for file: {path}\")\n                        existing_action.move_path = move_to\n                    fuzz_accumulator += fuzz\n                else:\n                    # First UPDATE block for this file\n                    action, index, fuzz = self._parse_update_file_sections(\n                        lines, index, file_content\n                    )\n                    action.path = path\n                    action.move_path = move_to\n                    patch.actions[path] = action\n                    fuzz_accumulator += fuzz\n                continue\n\n            # ---------- DELETE ---------- #\n            elif norm_line.startswith(\"*** Delete File: \"):\n                path = norm_line[len(\"*** Delete File: \") :].strip()\n                index += 1\n                if not path:\n                    raise DiffError(\"Delete File action missing path.\")\n                existing_action = patch.actions.get(path)\n                if existing_action:\n                    if existing_action.type == ActionType.DELETE:\n                        # Duplicate delete \u2013 ignore the extra block\n                        self.io.tool_warning(f\"Duplicate delete action for file: {path} ignored.\")\n                        continue\n                    else:\n                        raise DiffError(f\"Conflicting actions for file: {path}\")\n                if path not in current_files:\n                    raise DiffError(\n                        f\"Delete File Error - file not found: {path}\"\n                    )  # Check against known files\n\n                patch.actions[path] = PatchAction(type=ActionType.DELETE, path=path)\n                continue\n\n            # ---------- ADD ---------- #\n            elif norm_line.startswith(\"*** Add File: \"):\n                path = norm_line[len(\"*** Add File: \") :].strip()\n                index += 1\n                if not path:\n                    raise DiffError(\"Add File action missing path.\")\n                if path in patch.actions:\n                    raise DiffError(f\"Duplicate action for file: {path}\")\n                # Check if file exists in the context provided (should not for Add).\n                # Note: We only have needed files, a full check requires FS access.\n                # if path in current_files:\n                #     raise DiffError(f\"Add File Error - file already exists: {path}\")\n\n                action, index = self._parse_add_file_content(lines, index)\n                action.path = path  # Ensure path is set\n                patch.actions[path] = action\n                continue\n\n            # If we are here, the line is unexpected\n            # Allow blank lines between actions\n            if not norm_line.strip():\n                index += 1\n                continue\n\n            raise DiffError(f\"Unknown or misplaced line while parsing patch: {line}\")\n\n        patch.fuzz = fuzz_accumulator\n        return patch\n\n    def _parse_update_file_sections(\n        self, lines: List[str], index: int, file_content: str\n    ) -> Tuple[PatchAction, int, int]:\n        \"\"\"Parses all sections (@@, context, -, +) for a single Update File action.\"\"\"\n        action = PatchAction(type=ActionType.UPDATE, path=\"\")  # Path set by caller\n        orig_lines = file_content.splitlines()  # Use splitlines for consistency\n        current_file_index = 0  # Track position in original file content\n        total_fuzz = 0\n\n        while index < len(lines):\n            norm_line = _norm(lines[index])\n            # Check for terminators for *this* file update\n            if norm_line.startswith(\n                (\n                    \"*** End Patch\",\n                    \"*** Update File:\",\n                    \"*** Delete File:\",\n                    \"*** Add File:\",\n                )\n            ):\n                break  # End of this file's update section\n\n            # Handle @@ scope lines (optional)\n            scope_lines = []\n            while index < len(lines) and _norm(lines[index]).startswith(\"@@\"):\n                scope_line_content = lines[index][len(\"@@\") :].strip()\n                if scope_line_content:  # Ignore empty @@ lines?\n                    scope_lines.append(scope_line_content)\n                index += 1\n\n            # Find the scope in the original file if specified\n            if scope_lines:\n                # Simple scope finding: search from current position\n                # A more robust finder could handle nested scopes like the reference @@ @@\n                found_scope = False\n                temp_index = current_file_index\n                while temp_index < len(orig_lines):\n                    # Check if all scope lines match sequentially from temp_index\n                    match = True\n                    for i, scope in enumerate(scope_lines):\n                        if (\n                            temp_index + i >= len(orig_lines)\n                            or _norm(orig_lines[temp_index + i]).strip() != scope\n                        ):\n                            match = False\n                            break\n                    if match:\n                        current_file_index = temp_index + len(scope_lines)\n                        found_scope = True\n                        break\n                    temp_index += 1\n\n                if not found_scope:\n                    # Try fuzzy scope matching (strip whitespace)\n                    temp_index = current_file_index\n                    while temp_index < len(orig_lines):\n                        match = True\n                        for i, scope in enumerate(scope_lines):\n                            if (\n                                temp_index + i >= len(orig_lines)\n                                or _norm(orig_lines[temp_index + i]).strip() != scope.strip()\n                            ):\n                                match = False\n                                break\n                        if match:\n                            current_file_index = temp_index + len(scope_lines)\n                            found_scope = True\n                            total_fuzz += 1  # Add fuzz for scope match difference\n                            break\n                        temp_index += 1\n\n                if not found_scope:\n                    scope_txt = \"\\n\".join(scope_lines)\n                    raise DiffError(f\"Could not find scope context:\\n{scope_txt}\")\n\n            # Peek and parse the next context/change section\n            context_block, chunks_in_section, next_index, is_eof = peek_next_section(lines, index)\n\n            # Find where this context block appears in the original file\n            found_index, fuzz = find_context(orig_lines, context_block, current_file_index, is_eof)\n            total_fuzz += fuzz\n\n            if found_index == -1:\n                ctx_txt = \"\\n\".join(context_block)\n                marker = \"*** End of File\" if is_eof else \"\"\n                raise DiffError(\n                    f\"Could not find patch context {marker} starting near line\"\n                    f\" {current_file_index}:\\n{ctx_txt}\"\n                )\n\n            # Adjust chunk original indices to be absolute within the file\n            for chunk in chunks_in_section:\n                # chunk.orig_index from peek is relative to context_block start\n                # We need it relative to the file start\n                chunk.orig_index += found_index\n                action.chunks.append(chunk)\n\n            # Advance file index past the matched context block\n            current_file_index = found_index + len(context_block)\n            # Advance line index past the processed section in the patch\n            index = next_index\n\n        return action, index, total_fuzz\n\n    def _parse_add_file_content(\n        self, lines: List[str], index: int\n    ) -> Tuple[PatchAction, int]:\n        \"\"\"Parses the content (+) lines for an Add File action.\"\"\"\n        added_lines: List[str] = []\n        while index < len(lines):\n            line = lines[index]\n            norm_line = _norm(line)\n            # Stop if we hit another action or end marker\n            if norm_line.startswith(\n                (\n                    \"*** End Patch\",\n                    \"*** Update File:\",\n                    \"*** Delete File:\",\n                    \"*** Add File:\",\n                )\n            ):\n                break\n\n            # Expect lines to start with '+'\n            if not line.startswith(\"+\"):\n                # Tolerate blank lines? Or require '+'? Reference implies '+' required.\n                if norm_line.strip() == \"\":\n                    # Treat blank line as adding a blank line\n                    added_lines.append(\"\")\n                else:\n                    raise DiffError(f\"Invalid Add File line (missing '+'): {line}\")\n            else:\n                added_lines.append(line[1:])  # Strip leading '+'\n\n            index += 1\n\n        action = PatchAction(type=ActionType.ADD, path=\"\", new_content=\"\\n\".join(added_lines))\n        return action, index\n\n    def apply_edits(self, edits: List[Tuple[str, PatchAction]]):\n        \"\"\"\n        Applies the parsed PatchActions to the corresponding files.\n        \"\"\"\n        if not edits:\n            return\n\n        for _path_tuple_element, action in edits:\n            # action is the PatchAction object\n            # action.path is the canonical path within the action logic\n            full_path = self.abs_root_path(action.path)\n            path_obj = pathlib.Path(full_path)\n\n            try:\n                if action.type == ActionType.ADD:\n                    # Check existence *before* writing\n                    if path_obj.exists():\n                        raise DiffError(f\"ADD Error: File already exists: {action.path}\")\n                    if action.new_content is None:\n                        raise DiffError(f\"ADD change for {action.path} has no content\")\n\n                    self.io.tool_output(f\"Adding {action.path}\")\n                    path_obj.parent.mkdir(parents=True, exist_ok=True)\n                    # Ensure single trailing newline, matching reference behavior\n                    content_to_write = action.new_content\n                    if not content_to_write.endswith(\"\\n\"):\n                        content_to_write += \"\\n\"\n                    self.io.write_text(full_path, content_to_write)\n\n                elif action.type == ActionType.DELETE:\n                    self.io.tool_output(f\"Deleting {action.path}\")\n                    if not path_obj.exists():\n                        self.io.tool_warning(f\"DELETE Warning: File not found, skipping: {action.path}\")\n                    else:\n                        path_obj.unlink()\n\n                elif action.type == ActionType.UPDATE:\n                    if not path_obj.exists():\n                        raise DiffError(f\"UPDATE Error: File does not exist: {action.path}\")\n\n                    current_content = self.io.read_text(full_path)\n                    if current_content is None:\n                        raise DiffError(f\"Could not read file for UPDATE: {action.path}\")\n\n                    # Apply the update logic using the parsed chunks\n                    new_content = self._apply_update(current_content, action, action.path)\n\n                    target_full_path = self.abs_root_path(action.move_path) if action.move_path else full_path\n                    target_path_obj = pathlib.Path(target_full_path)\n\n                    if action.move_path:\n                        self.io.tool_output(f\"Updating and moving {action.path} to {action.move_path}\")\n                        # Check if target exists before overwriting/moving\n                        if target_path_obj.exists() and full_path != target_full_path:\n                            self.io.tool_warning(f\"UPDATE Warning: Target file for move already exists, overwriting: {action.move_path}\")\n                    else:\n                        self.io.tool_output(f\"Updating {action.path}\")\n\n                    # Ensure parent directory exists for target\n                    target_path_obj.parent.mkdir(parents=True, exist_ok=True)\n                    self.io.write_text(target_full_path, new_content)\n\n                    # Remove original file *after* successful write to new location if moved\n                    if action.move_path and full_path != target_full_path:\n                        path_obj.unlink()\n\n                else:\n                    # Should not happen\n                    raise DiffError(f\"Unknown action type encountered: {action.type}\")\n\n            except (DiffError, FileNotFoundError, IOError, OSError) as e:\n                # Raise a ValueError to signal failure, consistent with other coders.\n                raise ValueError(f\"Error applying action '{action.type}' to {action.path}: {e}\")\n            except Exception as e:\n                # Catch unexpected errors during application\n                raise ValueError(f\"Unexpected error applying action '{action.type}' to {action.path}: {e}\")\n\n    def _apply_update(self, text: str, action: PatchAction, path: str) -> str:\n        \"\"\"\n        Applies UPDATE chunks to the given text content.\n        Adapted from _get_updated_file in apply_patch.py.\n        \"\"\"\n        if action.type is not ActionType.UPDATE:\n            # Should not be called otherwise, but check for safety\n            raise DiffError(\"_apply_update called with non-update action\")\n\n        orig_lines = text.splitlines()  # Use splitlines to handle endings consistently\n        dest_lines: List[str] = []\n        current_orig_line_idx = 0  # Tracks index in orig_lines processed so far\n\n        # Sort chunks by their original index to apply them sequentially\n        sorted_chunks = sorted(action.chunks, key=lambda c: c.orig_index)\n\n        for chunk in sorted_chunks:\n            chunk_start_index = chunk.orig_index\n\n            if chunk_start_index < current_orig_line_idx:\n                # This indicates overlapping chunks or incorrect indices from parsing\n                raise DiffError(\n                    f\"{path}: Overlapping or out-of-order chunk detected.\"\n                    f\" Current index {current_orig_line_idx}, chunk starts at {chunk_start_index}.\"\n                )\n\n            # Add lines from original file between the last chunk and this one\n            dest_lines.extend(orig_lines[current_orig_line_idx:chunk_start_index])\n\n            # Verify that the lines to be deleted actually match the original file content\n            # (The parser should have used find_context, but double-check here)\n            num_del = len(chunk.del_lines)\n            actual_deleted_lines = orig_lines[chunk_start_index:chunk_start_index + num_del]\n\n            # Use the same normalization as find_context_core for comparison robustness\n            norm_chunk_del = [_norm(s).strip() for s in chunk.del_lines]\n            norm_actual_del = [_norm(s).strip() for s in actual_deleted_lines]\n\n            if norm_chunk_del != norm_actual_del:\n                # This indicates the context matching failed or the file changed since parsing\n                # Provide detailed error message\n                expected_str = \"\\n\".join(f\"- {s}\" for s in chunk.del_lines)\n                actual_str = \"\\n\".join(f\"  {s}\" for s in actual_deleted_lines)\n                raise DiffError(\n                    f\"{path}: Mismatch applying patch near line {chunk_start_index + 1}.\\n\"\n                    f\"Expected lines to remove:\\n{expected_str}\\n\"\n                    f\"Found lines in file:\\n{actual_str}\"\n                )\n\n            # Add the inserted lines from the chunk\n            dest_lines.extend(chunk.ins_lines)\n\n            # Advance the original line index past the lines processed (deleted lines)\n            current_orig_line_idx = chunk_start_index + num_del\n\n        # Add any remaining lines from the original file after the last chunk\n        dest_lines.extend(orig_lines[current_orig_line_idx:])\n\n        # Join lines and ensure a single trailing newline\n        result = \"\\n\".join(dest_lines)\n        if result or orig_lines:  # Add newline unless result is empty and original was empty\n            result += \"\\n\"\n        return result",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9718471123084694,
      "diff_lines": 249,
      "added_lines": 24,
      "removed_lines": 57,
      "diff_preview": "--- expected+++ predicted@@ -1,11 +1,12 @@ import pathlib\n+import re\n from dataclasses import dataclass, field\n from enum import Enum\n from typing import Dict, List, Optional, Tuple\n \n+from ..dump import dump  # noqa: F401\n from .base_coder import Coder\n from .patch_prompts import PatchPrompts\n-\n \n # --------------------------------------------------------------------------- #\n #  Domain objects & Exceptions (Adapted from apply_patch.py)\n@@ -13,12 +14,10 @@ class DiffError(ValueError):\n     \"\"\"Any problem detected while parsing or applying a patch.\"\"\"\n \n-\n class ActionType(str, Enum):\n     ADD = \"Add\"\n     DELETE = \"Delete\"\n     UPDATE = \"Update\"\n-\n \n @dataclass\n class Chunk:\n@@ -26,27 +25,18 @@     del_lines: List[str] = field(default_factory=list)\n     ins_lines: List[str] = field(default_factory=list)\n \n-\n @dataclass\n class PatchAction:\n     type: ActionType\n     path: str\n-    # For ADD:\n-    new_content: Optional[str] = None\n-    # For UPDATE:\n-    chunks: List[Chunk] = field(default_factory=list)\n-    move_path: Optional[str] = None\n-\n-\n-# Type alias for the return type of get_edits\n-EditResult = Tuple[str, PatchAction]\n-\n+    new_content: Optional[str] = None  # For Add\n+    chunks: List[Chunk] = field(default_factory=list)  # For Update\n"
    },
    "llm_calls": 1,
    "elapsed_time": 141.71269488334656,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 22441,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_aider_coders_wholefile_coder.py",
    "exact_match": false,
    "similarity": 0.6213887480993411,
    "diff_lines": 116,
    "predicted_output": "from pathlib import Path\n\nfrom aider import diffs\nfrom ..dump import dump  # noqa: F401\nfrom .base_coder import Coder\nfrom .wholefile_prompts import WholeFilePrompts\n\n\nclass WholeFileCoder(Coder):\n    \"\"\"A coder that operates on entire files for code modifications.\"\"\"\n\n    edit_format = \"whole\"\n    gpt_prompts = WholeFilePrompts()\n\n    def update_cur_messages(self, edited):\n        if edited:\n            self.cur_messages += [\n                dict(role=\"assistant\", content=self.gpt_prompts.redacted_edit_message)\n            ]\n        else:\n            self.cur_messages += [dict(role=\"assistant\", content=self.partial_response_content)]\n\n    def render_incremental_response(self, final):\n        try:\n            return self.get_edits(mode=\"diff\")\n        except ValueError:\n            return self.get_multi_response_content_in_progress()\n\n    def get_edits(self, mode=\"update\"):\n        content = self.get_multi_response_content_in_progress()\n\n        chat_files = self.get_inchat_relative_files()\n\n        output = []\n        lines = content.splitlines(keepends=True)\n\n        edits = []\n        saw_fname = None\n        fname = None\n        fname_source = None\n        new_lines = []\n        for i, line in enumerate(lines):\n            if line.startswith(self.fence[0]) or line.startswith(self.fence[1]):\n                if fname is not None:\n                    # ending an existing block\n                    saw_fname = None\n\n                    full_path = self.abs_root_path(fname)\n\n                    if mode == \"diff\":\n                        output += self.do_live_diff(full_path, new_lines, True)\n                    else:\n                        edits.append((fname, fname_source, new_lines))\n\n                    fname = None\n                    fname_source = None\n                    new_lines = []\n                    continue\n\n                # fname==None ... starting a new block\n                if i > 0:\n                    fname_source = \"block\"\n                    fname = lines[i - 1].strip()\n                    fname = fname.strip(\"*\")  # handle **filename.py**\n                    fname = fname.rstrip(\":\")\n                    fname = fname.strip(\"`\")\n                    fname = fname.lstrip(\"#\")\n                    fname = fname.strip()\n\n                    # Issue #1232\n                    if len(fname) > 250:\n                        fname = \"\"\n                    # Did gpt prepend a bogus dir? It especially likes to\n                    # include the path/to prefix from the one-shot example in\n                    # the prompt.\n                    if fname and fname not in chat_files and Path(fname).name in chat_files:\n                        fname = Path(fname).name\n                if not fname:  # blank line? or",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.6213887480993411,
      "diff_lines": 116,
      "added_lines": 9,
      "removed_lines": 75,
      "diff_preview": "--- expected+++ predicted@@ -1,7 +1,6 @@ from pathlib import Path\n \n from aider import diffs\n-\n from ..dump import dump  # noqa: F401\n from .base_coder import Coder\n from .wholefile_prompts import WholeFilePrompts\n@@ -12,6 +11,14 @@ \n     edit_format = \"whole\"\n     gpt_prompts = WholeFilePrompts()\n+\n+    def update_cur_messages(self, edited):\n+        if edited:\n+            self.cur_messages += [\n+                dict(role=\"assistant\", content=self.gpt_prompts.redacted_edit_message)\n+            ]\n+        else:\n+            self.cur_messages += [dict(role=\"assistant\", content=self.partial_response_content)]\n \n     def render_incremental_response(self, final):\n         try:\n@@ -28,7 +35,6 @@         lines = content.splitlines(keepends=True)\n \n         edits = []\n-\n         saw_fname = None\n         fname = None\n         fname_source = None\n@@ -64,81 +70,9 @@                     # Issue #1232\n                     if len(fname) > 250:\n                         fname = \"\"\n-\n                     # Did gpt prepend a bogus dir? It especially likes to\n                     # include the path/to prefix from the one-shot example in\n                     # the prompt.\n                     if fname and fname not in chat_files and Path(fname).name in chat_files:\n                         fname = Path(fname).name\n-                if not fname:  # blank line? or ``` was on first line i==0\n-                    if saw_fname:\n-                        fname = saw_fname\n-                        fname_source = \"saw\"\n-                    elif len(chat_files) == 1:\n-                        fname = chat_files[0]\n-                        fname_source = \"chat\"\n"
    },
    "llm_calls": 1,
    "elapsed_time": 22.621943712234497,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 20299,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_aider_exceptions.py",
    "exact_match": false,
    "similarity": 0.9533189422558014,
    "diff_lines": 64,
    "predicted_output": "from dataclasses import dataclass\n\n@dataclass\nclass ExInfo:\n    name: str\n    retry: bool\n    description: str\n\nEXCEPTIONS = [\n    ExInfo(\"APIConnectionError\", True, None),\n    ExInfo(\"APIError\", True, None),\n    ExInfo(\"APIResponseValidationError\", True, None),\n    ExInfo(\n        \"AuthenticationError\",\n        False,\n        \"The API provider is not able to authenticate you. Check your API key.\",\n    ),\n    ExInfo(\"AzureOpenAIError\", True, None),\n    ExInfo(\"BadRequestError\", False, None),\n    ExInfo(\"BudgetExceededError\", True, None),\n    ExInfo(\n        \"ContentPolicyViolationError\",\n        True,\n        \"The API provider has refused the request due to a safety policy about the content.\",\n    ),\n    ExInfo(\"ContextWindowExceededError\", False, None),  # special case handled in base_coder\n    ExInfo(\"InternalServerError\", True, \"The API provider's servers are down or overloaded.\"),\n    ExInfo(\"InvalidRequestError\", True, None),\n    ExInfo(\"JSONSchemaValidationError\", True, None),\n    ExInfo(\"NotFoundError\", False, None),\n    ExInfo(\"OpenAIError\", True, None),\n    ExInfo(\n        \"RateLimitError\",\n        True,\n        \"The API provider has rate limited you. Try again later or check your quotas.\",\n    ),\n    ExInfo(\"RouterRateLimitError\", True, None),\n    ExInfo(\"ServiceUnavailableError\", True, \"The API provider's servers are down or overloaded.\"),\n    ExInfo(\"UnprocessableEntityError\", True, None),\n    ExInfo(\"UnsupportedParamsError\", True, None),\n    ExInfo(\n        \"Timeout\",\n        True,\n        \"The API provider timed out without returning a response. They may be down or overloaded.\",\n    ),\n]\n\nclass LiteLLMExceptions:\n    exceptions = dict()\n    exception_info = {exi.name: exi for exi in EXCEPTIONS}\n\n    def __init__(self):\n        self._load()\n\n    def _load(self, strict=False):\n        import litellm\n\n        for var in dir(litellm):\n            if var.endswith(\"Error\"):\n                if var not in self.exception_info:\n                    raise ValueError(f\"{var} is in litellm but not in aider's exceptions list\")\n                \n                ex = getattr(litellm, var)\n                self.exceptions[ex] = self.exception_info[var]\n\n    def exceptions_tuple(self):\n        return tuple(self.exceptions)\n\n    def get_ex_info(self, ex):\n        \"\"\"Return the ExInfo for a given exception instance\"\"\"\n        import litellm\n\n        if ex.__class__ is litellm.APIConnectionError:\n            if \"google.auth\" in str(ex):\n                return ExInfo(\n                    \"APIConnectionError\", False, \"You need to: pip install google-generativeai\"\n                )\n            if \"boto3\" in str(ex):\n                return ExInfo(\"APIConnectionError\", False, \"You need to: pip install boto3\")\n            if \"OpenrouterException\" in str(ex) and \"'choices'\" in str(ex):\n                return ExInfo(\n                    \"APIConnectionError\", True, \"The OpenRouter API provider is down or overloaded.\"\n                )\n        # Check for specific non-retryable APIError cases like insufficient credits\n        if ex.__class__ is litellm.APIError:\n            err_str = str(ex).lower()\n            if \"insufficient credits\" in err_str and '\"code\":402' in err_str:\n                return ExInfo(\n                    \"APIError\",\n                    False,\n                    \"Insufficient credits with the API provider. Please add credits.\",\n                )\n            # Fall through to default APIError handling if not the specific credits error\n        \n        return self.exceptions.get(ex.__class__, ExInfo(None, None, None))\n\nlitellm_ex = LiteLLMExceptions()",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9533189422558014,
      "diff_lines": 64,
      "added_lines": 7,
      "removed_lines": 17,
      "diff_preview": "--- expected+++ predicted@@ -1,14 +1,10 @@ from dataclasses import dataclass\n-\n-from aider.dump import dump  # noqa: F401\n-\n \n @dataclass\n class ExInfo:\n     name: str\n     retry: bool\n     description: str\n-\n \n EXCEPTIONS = [\n     ExInfo(\"APIConnectionError\", True, None),\n@@ -49,7 +45,6 @@     ),\n ]\n \n-\n class LiteLLMExceptions:\n     exceptions = dict()\n     exception_info = {exi.name: exi for exi in EXCEPTIONS}\n@@ -64,10 +59,9 @@             if var.endswith(\"Error\"):\n                 if var not in self.exception_info:\n                     raise ValueError(f\"{var} is in litellm but not in aider's exceptions list\")\n-\n-        for var in self.exception_info:\n-            ex = getattr(litellm, var)\n-            self.exceptions[ex] = self.exception_info[var]\n+                \n+                ex = getattr(litellm, var)\n+                self.exceptions[ex] = self.exception_info[var]\n \n     def exceptions_tuple(self):\n         return tuple(self.exceptions)\n@@ -85,14 +79,8 @@                 return ExInfo(\"APIConnectionError\", False, \"You need to: pip install boto3\")\n             if \"OpenrouterException\" in str(ex) and \"'choices'\" in str(ex):\n                 return ExInfo(\n-                    \"APIConnectionError\",\n-                    True,\n-                    (\n-                        \"OpenRouter or the upstream API provider is down, overloaded or rate\"\n-                        \" limiting your requests.\"\n-                    ),\n+                    \"APIConnectionError\", True, \"The OpenRouter API provider is down or overloaded.\"\n"
    },
    "llm_calls": 1,
    "elapsed_time": 24.47903609275818,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 7265,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_aider_history.py",
    "exact_match": false,
    "similarity": 0.8555392874572962,
    "diff_lines": 106,
    "predicted_output": "import argparse\n\nfrom aider import models, prompts\nfrom aider.dump import dump  # noqa: F401\n\nclass ChatSummary:\n    def __init__(self, models=None, max_tokens=1024):\n        if not models:\n            raise ValueError(\"At least one model must be provided\")\n        self.models = models if isinstance(models, list) else [models]\n        self.max_tokens = max_tokens\n        self.token_count = self.models[0].token_count\n\n    def too_big(self, messages):\n        sized = self.tokenize(messages)\n        total = sum(tokens for tokens, _msg in sized)\n        return total > self.max_tokens\n\n    def tokenize(self, messages):\n        sized = []\n        for msg in messages:\n            tokens = len(self.token_count(msg))\n            sized.append((tokens, msg))\n        return sized\n\n    def summarize(self, messages, depth=0):\n        messages = self.summarize_real(messages)\n        if messages and messages[-1][\"role\"] != \"assistant\":\n            messages.append(dict(role=\"assistant\", content=\"Ok.\"))\n        return messages\n\n    def summarize_real(self, messages, depth=0):\n        if not self.models:\n            raise ValueError(\"No models available for summarization\")\n\n        sized = self.tokenize(messages)\n        total = sum(tokens for tokens, _msg in sized)\n        if total <= self.max_tokens and depth == 0:\n            return messages\n\n        tail_tokens = 0\n        split_index = len(messages)\n\n        # These sometimes come set with value = None\n        model_max_input_tokens = self.models[0].info.get(\"max_input_tokens\") or 4096\n        model_max_input_tokens -= 512\n\n        for i in range(split_index):\n            tail_tokens += sized[i][0]\n            if tail_tokens > model_max_input_tokens:\n                split_index = i\n                break\n\n        head = messages[:split_index]\n        tail = messages[split_index:]\n\n        summary = self.summarize_all(head)\n        tail_tokens = sum(tokens for tokens, msg in sized[split_index:])\n        summary_tokens = self.token_count(summary)\n\n        result = summary + tail\n        if summary_tokens + tail_tokens < self.max_tokens:\n            return result\n\n        return self.summarize_real(result, depth + 1)\n\n    def summarize_all(self, messages):\n        content = \"\"\n        for msg in messages:\n            role = msg[\"role\"].upper()\n            if role not in (\"USER\", \"ASSISTANT\"):\n                continue\n            content += f\"# {role}\\n\"\n            content += msg[\"content\"]\n            if not content.endswith(\"\\n\"):\n                content += \"\\n\"\n\n        messages = [\n            dict(role=\"system\", content=prompts.summarize),\n            dict(role=\"user\", content=content),\n        ]\n\n        for model in self.models:\n            try:\n                summary = model.simple_send_with_retries(messages)\n                if summary is not None:\n                    summary = prompts.summary_prefix + summary\n                    return [dict(role=\"user\", content=summary)]\n            except Exception as e:\n                print(f\"Summarization failed for model {model.name}: {str(e)}\")\n\n        raise ValueError(\"summarizer unexpectedly failed for all models\")\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"filename\", help=\"Markdown file to parse\")\n    args = parser.parse_args()\n\n    model_names = [\"gpt-3.5-turbo\", \"gpt-4\"]  # Add more model names as needed\n    model_list = [models.Model(name) for name in model_names]\n    summarizer = ChatSummary(model_list)\n\n    with open(args.filename, \"r\") as f:\n        text = f.read()\n\n    summary = summarizer.summarize_chat_history_markdown(text)\n    dump(summary)",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.8555392874572962,
      "diff_lines": 106,
      "added_lines": 10,
      "removed_lines": 45,
      "diff_preview": "--- expected+++ predicted@@ -2,7 +2,6 @@ \n from aider import models, prompts\n from aider.dump import dump  # noqa: F401\n-\n \n class ChatSummary:\n     def __init__(self, models=None, max_tokens=1024):\n@@ -20,7 +19,7 @@     def tokenize(self, messages):\n         sized = []\n         for msg in messages:\n-            tokens = self.token_count(msg)\n+            tokens = len(self.token_count(msg))\n             sized.append((tokens, msg))\n         return sized\n \n@@ -39,53 +38,23 @@         if total <= self.max_tokens and depth == 0:\n             return messages\n \n-        min_split = 4\n-        if len(messages) <= min_split or depth > 3:\n-            return self.summarize_all(messages)\n-\n         tail_tokens = 0\n         split_index = len(messages)\n-        half_max_tokens = self.max_tokens // 2\n-\n-        # Iterate over the messages in reverse order\n-        for i in range(len(sized) - 1, -1, -1):\n-            tokens, _msg = sized[i]\n-            if tail_tokens + tokens < half_max_tokens:\n-                tail_tokens += tokens\n-                split_index = i\n-            else:\n-                break\n-\n-        # Ensure the head ends with an assistant message\n-        while messages[split_index - 1][\"role\"] != \"assistant\" and split_index > 1:\n-            split_index -= 1\n-\n-        if split_index <= min_split:\n-            return self.summarize_all(messages)\n-\n-        head = messages[:split_index]\n-        tail = messages[split_index:]\n-\n"
    },
    "llm_calls": 1,
    "elapsed_time": 21.994559049606323,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 18915,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_aider_linter.py",
    "exact_match": false,
    "similarity": 0.7270507439908432,
    "diff_lines": 233,
    "predicted_output": "import subprocess\nimport sys\nimport traceback\nimport warnings\nimport shlex\nfrom dataclasses import dataclass\nfrom pathlib import Path\n\nfrom grep_ast import TreeContext, filename_to_lang\nfrom grep_ast.tsl import get_parser  # noqa: E402\n\nfrom aider.dump import dump  # noqa: F401\nfrom aider.run_cmd import run_cmd_subprocess  # noqa: F401\n\n# tree_sitter is throwing a FutureWarning\nwarnings.simplefilter(\"ignore\", category=FutureWarning)\n\nclass Linter:\n    def __init__(self, encoding=\"utf-8\", root=None):\n        self.encoding = encoding\n        self.root = root\n\n        self.languages = dict(\n            python=self.py_lint,\n        )\n        self.all_lint_cmd = None\n\n    def set_linter(self, lang, cmd):\n        if lang:\n            self.languages[lang] = cmd\n            return\n\n        self.all_lint_cmd = cmd\n\n    def get_rel_fname(self, fname):\n        if self.root:\n            try:\n                return os.path.relpath(fname, self.root)\n            except ValueError:\n                return fname\n        else:\n            return fname\n\n    def run_cmd(self, cmd, rel_fname, code):\n        cmd += \" \" + shlex.quote(rel_fname)\n\n        returncode = 0\n        stdout = \"\"\n        try:\n            process = subprocess.Popen(\n                cmd,\n                cwd=self.root,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.STDOUT,\n                encoding=self.encoding,\n                errors=\"replace\",\n            )\n        except OSError as err:\n            print(f\"Unable to execute lint command: {err}\")\n            return\n        stdout, _ = process.communicate()\n        errors = stdout\n        if process.returncode == 0:\n            return  # zero exit status\n\n        res = f\"## Running: {cmd}\\n\\n\"\n        res += errors\n\n        return self.errors_to_lint_result(rel_fname, res)\n\n    def errors_to_lint_result(self, rel_fname, errors):\n        if not errors:\n            return\n\n        linenums = []\n        filenames_linenums = find_filenames_and_linenums(errors, [rel_fname])\n        if filenames_linenums:\n            filename, linenums = next(iter(filenames_linenums.items()))\n            linenums = [num - 1 for num in linenums]\n\n        return LintResult(text=errors, lines=linenums)\n\n    def lint(self, fname, cmd=None):\n        rel_fname = self.get_rel_fname(fname)\n        code = Path(fname).read_text(encoding=self.encoding, errors=\"replace\")\n\n        if cmd:\n            cmd = cmd.strip()\n        if not cmd:\n            lang = filename_to_lang(fname)\n            if not lang:\n                return\n            cmd = self.languages.get(lang)\n\n        if callable(cmd):\n            lintres = cmd(fname, rel_fname, code)\n        elif cmd:\n            lintres = self.run_cmd(cmd, rel_fname, code)\n        else:\n            lintres = basic_lint(rel_fname, code)\n\n        if not lintres:\n            return\n\n        res = \"# Fix any errors below, if possible.\\n\\n\"\n        res += lintres.text\n        res += \"\\n\"\n        res += tree_context(rel_fname, code, lintres.lines)\n\n        return res\n\n    def py_lint(self, fname, rel_fname, code):\n        basic_res = basic_lint(rel_fname, code)\n        compile_res = lint_python_compile(fname, code)\n\n        text = \"\"\n        lines = set()\n        for res in [basic_res, compile_res]:\n            if res:\n                if text:\n                    text += \"\\n\"\n                text += res.text\n                lines.update(res.lines)\n\n        if text or lines:\n            return LintResult(text, lines)\n\n@dataclass\nclass LintResult:\n    text: str\n    lines: list\n\ndef basic_lint(fname, code):\n    \"\"\"\n    Use tree-sitter to look for syntax errors, display them with tree context.\n    \"\"\"\n    lang = filename_to_lang(fname)\n    if not lang:\n        return\n\n    # Tree-sitter linter is not capable of working with typescript #1132\n    if lang == \"typescript\":\n        return\n\n    try:\n        parser = get_parser(lang)\n    except Exception as err:\n        print(f\"Unable to load parser: {err}\")\n        return\n\n    tree = parser.parse(bytes(code, \"utf-8\"))\n\n    try:\n        errors = traverse_tree(tree.root_node)\n    except RecursionError:\n        print(f\"Unable to lint {fname} due to RecursionError\")\n        return\n\n    if not errors:\n        return\n\n    return LintResult(text=\"\", lines=errors)\n\ndef traverse_tree(node):\n    errors = []\n    if node.type == \"ERROR\" or node.is_missing:\n        line_no = node.start_point[0]\n        errors.append(line_no)\n\n    for child in node.children:\n        errors += traverse_tree(child)\n\n    return errors\n\ndef find_filenames_and_linenums(text, fnames):\n    \"\"\"\n    Search text for all occurrences of <filename>:\\d+ and make a list of them\n    where <filename> is one of the filenames in the list `fnames`.\n    \"\"\"\n    pattern = re.compile(r\"(\\b(?:\" + \"|\".join(re.escape(fname) for fname in fnames) + r\"):\\d+\\b)\")\n    matches = pattern.findall(text)\n    result = {}\n    for match in matches:\n        fname, linenum = match.rsplit(\":\", 1)\n        if fname not in result:\n            result[fname] = set()\n        result[fname].add(int(linenum))\n    return result",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.7270507439908432,
      "diff_lines": 233,
      "added_lines": 16,
      "removed_lines": 132,
      "diff_preview": "--- expected+++ predicted@@ -1,5 +1,3 @@-import os\n-import re\n import subprocess\n import sys\n import traceback\n@@ -16,7 +14,6 @@ \n # tree_sitter is throwing a FutureWarning\n warnings.simplefilter(\"ignore\", category=FutureWarning)\n-\n \n class Linter:\n     def __init__(self, encoding=\"utf-8\", root=None):\n@@ -50,16 +47,20 @@         returncode = 0\n         stdout = \"\"\n         try:\n-            returncode, stdout = run_cmd_subprocess(\n+            process = subprocess.Popen(\n                 cmd,\n                 cwd=self.root,\n+                stdout=subprocess.PIPE,\n+                stderr=subprocess.STDOUT,\n                 encoding=self.encoding,\n+                errors=\"replace\",\n             )\n         except OSError as err:\n             print(f\"Unable to execute lint command: {err}\")\n             return\n+        stdout, _ = process.communicate()\n         errors = stdout\n-        if returncode == 0:\n+        if process.returncode == 0:\n             return  # zero exit status\n \n         res = f\"## Running: {cmd}\\n\\n\"\n@@ -81,11 +82,7 @@ \n     def lint(self, fname, cmd=None):\n         rel_fname = self.get_rel_fname(fname)\n-        try:\n-            code = Path(fname).read_text(encoding=self.encoding, errors=\"replace\")\n-        except OSError as err:\n-            print(f\"Unable to read {fname}: {err}\")\n-            return\n+        code = Path(fname).read_text(encoding=self.encoding, errors=\"replace\")\n \n"
    },
    "llm_calls": 1,
    "elapsed_time": 33.40225577354431,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 35338,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_aider_mdstream.py",
    "exact_match": false,
    "similarity": 0.22655591669676176,
    "diff_lines": 239,
    "predicted_output": "#!/usr/bin/env python\nimport io\nimport time\n\nfrom rich import box\nfrom rich.console import Console\nfrom rich.live import Live\nfrom rich.markdown import CodeBlock, Heading, Markdown\nfrom rich.panel import Panel\nfrom rich.syntax import Syntax\nfrom rich.text import Text\n\nfrom aider.dump import dump  # noqa: F401\n\n_text_prefix = \"\"\"\n# Header\n\nLorem Ipsum is simply dummy text of the printing and typesetting industry. \nLorem Ipsum has been the industry's standard dummy text ever since the 1500s, \nwhen an unknown printer took a galley of type and scrambled it to make a type \nspecimen book. It has survived not only five centuries, but also the leap into \nelectronic typesetting, remaining essentially unchanged. It was popularised in \nthe 1960s with the release of Letraset sheets containing Lorem Ipsum passages, \nand more recently with desktop publishing software like Aldus PageMaker \nincluding versions of Lorem Ipsum.\n\n\n\"\"\"\n\n_text_suffix = \"\"\"",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.22655591669676176,
      "diff_lines": 239,
      "added_lines": 8,
      "removed_lines": 215,
      "diff_preview": "--- expected+++ predicted@@ -1,5 +1,4 @@ #!/usr/bin/env python\n-\n import io\n import time\n \n@@ -16,222 +15,16 @@ _text_prefix = \"\"\"\n # Header\n \n-Lorem Ipsum is simply dummy text of the printing and typesetting industry.\n-Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\n-when an unknown printer took a galley of type and scrambled it to make a type\n-specimen book. It has survived not only five centuries, but also the leap into\n-electronic typesetting, remaining essentially unchanged. It was popularised in\n-the 1960s with the release of Letraset sheets containing Lorem Ipsum passages,\n-and more recently with desktop publishing software like Aldus PageMaker\n+Lorem Ipsum is simply dummy text of the printing and typesetting industry. \n+Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \n+when an unknown printer took a galley of type and scrambled it to make a type \n+specimen book. It has survived not only five centuries, but also the leap into \n+electronic typesetting, remaining essentially unchanged. It was popularised in \n+the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, \n+and more recently with desktop publishing software like Aldus PageMaker \n including versions of Lorem Ipsum.\n \n \n-\n-## Sub header\n-\n-- List 1\n-- List 2\n-- List me\n-- List you\n-\n-\n-\n-```python\n \"\"\"\n \n-_text_suffix = \"\"\"\n-```\n-\n-## Sub header too\n-\n-The end.\n-\n-\"\"\"  # noqa: E501\n"
    },
    "llm_calls": 1,
    "elapsed_time": 31.288558959960938,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 14962,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_aider_onboarding.py",
    "exact_match": false,
    "similarity": 0.768932457179791,
    "diff_lines": 316,
    "predicted_output": "import http.server\nimport os\nimport secrets\nimport socketserver\nimport threading\nimport time\nimport webbrowser\nfrom urllib.parse import parse_qs, urlparse\n\nimport requests\n\nfrom aider import urls\nfrom aider.io import InputOutput\n\n\ndef check_openrouter_tier(api_key):\n    \"\"\"\n    Checks if the user is on a free tier for OpenRouter.\n\n    Args:\n        api_key: The OpenRouter API key to check.\n\n    Returns:\n        A boolean indicating if the user is on a free tier (True) or paid tier (False).\n        Returns True if the check fails.\n    \"\"\"\n    try:\n        response = requests.get(\n            \"https://openrouter.ai/api/v1/auth/key\",\n            headers={\"Authorization\": f\"Bearer {api_key}\"},\n            timeout=5  # Add a reasonable timeout\n        )\n        response.raise_for_status()\n        data = response.json()\n        # According to the documentation, 'is_free_tier' will be true if the user has never paid\n        return data.get(\"data\", {}).get(\"is_free_tier\", True)  # Default to True if not found\n    except Exception:\n        # If there's any error, we'll default to assuming free tier\n        return True\n\n\ndef try_to_select_default_model():\n    \"\"\"\n    Attempts to select a default model based on available API keys.\n    Checks OpenRouter tier status to select appropriate model.\n\n    Returns:\n        The name of the selected model, or None if no suitable default is found.\n    \"\"\"\n    # Special handling for OpenRouter\n    openrouter_key = os.environ.get(\"OPENROUTER_API_KEY\")\n    if openrouter_key:\n        # Check if the user is on a free tier\n        is_free_tier = check_openrouter_tier(openrouter_key)\n        if is_free_tier:\n            return \"openrouter/google/gemini-2.5-pro-exp-03-25:free\"\n        else:\n            return \"openrouter/anthropic/claude-3.7-sonnet\"\n    \n    # Select model based on other available API keys\n    model_key_pairs = [\n        (\"ANTHROPIC_API_KEY\", \"sonnet\"),\n        (\"DEEPSEEK_API_KEY\", \"deepseek\"),\n        (\"OPENAI_API_KEY\", \"gpt-4o\"),\n        (\"GEMINI_API_KEY\", \"gemini/gemini-2.5-pro-exp-03-25\"),\n        (\"VERTEXAI_PROJECT\", \"vertex_ai/gemini-2.5-pro-exp-03-25\"),\n    ]\n\n    for env_key, model_name in model_key_pairs:\n        api_key_value = os.environ.get(env_key)\n        if api_key_value:\n            return model_name\n\n    return None\n\n\ndef offer_openrouter_oauth(io, analytics):\n    \"\"\"\n    Offers OpenRouter OAuth flow to the user if no API keys are found.\n    \n    Args:\n        io: The InputOutput object for user interaction.\n        analytics: The Analytics object for tracking events.\n        \n    Returns:\n        True if authentication was successful, False otherwise.\n    \"\"\"\n    # No API keys found - Offer OpenRouter OAuth\n    no_model_msg = \"No model was specified and no API keys were provided.\"\n    io.tool_warning(no_model_msg)\n    io.tool_output(\"OpenRouter provides free and paid access to many LLMs.\")\n    # Use confirm_ask which handles non-interactive cases\n    if io.confirm_ask(\n        \"Login to OpenRouter or create a free account?\",\n        default=\"y\",\n    ):\n        analytics.event(\"oauth_flow_initiated\", provider=\"openrouter\")\n        openrouter_key = start_openrouter_oauth_flow(io, analytics)\n        if openrouter_key:\n            # Successfully got key via OAuth, use the default OpenRouter model\n            os.environ[\"OPENROUTER_API_KEY\"] = openrouter_key\n            analytics.event(\"oauth_flow_success\", provider=\"openrouter\")\n            return True\n        else:\n            io.tool_error(\"OpenRouter authentication did not complete successfully.\")\n            # Fall through to the final error message\n\n    return False\n\n\ndef select_default_model(args, io, analytics):\n    \"\"\"\n    Selects a default model based on available API keys if no model is specified.\n    Offers OAuth flow for OpenRouter if no keys are found.\n\n    Args:\n        args: The command line arguments object.\n        io: The InputOutput object for user interaction.\n        analytics: The Analytics object for tracking events.\n\n    Returns:\n        The name of the selected model, or None if no suitable default is found.\n    \"\"\"\n    if args.model:\n        return args.model  # Model already specified\n\n    # Try OAuth if no model was detected\n    offer_openrouter_oauth(io, analytics)\n    \n    # Check again after potential OAuth success\n    model = try_to_select_default_model()\n    if model:\n        return model\n\n    io.offer_url(urls.models_and_keys, \"Open documentation URL for more info?\")\n    return None\n\n\ndef exchange_code_for_key(code, code_verifier, io):\n    try:\n        response = requests.post(\n            \"https://openrouter.ai/api/v1/auth/keys\",\n            headers={\"Content-Type\": \"application/json\"},\n            json={\n                \"code\": code,\n                \"code_verifier\": code_verifier,\n                \"code_challenge_method\": \"S256\",\n            },\n            timeout=30,  # Add a timeout\n        )\n        response.raise_for_status()  # Raise exception for bad status codes (4xx or 5xx)\n        data = response.json()\n        api_key = data.get(\"key\")\n        if not api_key:\n            io.tool_error(\"Error: 'key' not found in OpenRouter response.\")\n            io.tool_error(f\"Response: {response.text}\")\n            return None\n        return api_key\n    except requests.exceptions.Timeout:\n        io.tool_error(\"Error: Request to OpenRouter timed out during code exchange.\")\n        return None\n    except requests.exceptions.HTTPError as e:\n        io.tool_error(\n            \"Error exchanging code for OpenRouter key:\"\n            f\" {e.response.status_code} {e.response.reason}\"\n        )\n        io.tool_error(f\"Response: {e.response.text}\")\n        return None\n    except requests.exceptions.RequestException as e:\n        io.tool_error(f\"Error exchanging code for OpenRouter key: {e}\")\n        return None\n    except Exception as e:\n        io.tool_error(f\"Unexpected error during code exchange: {e}\")\n        return None\n\n\ndef start_openrouter_oauth_flow(io, analytics):\n    \"\"\"Initiates the OpenRouter OAuth PKCE flow using a local server.\"\"\"\n\n    port = find_available_port()\n    if not port:\n        io.tool_error(\"Could not find an available port between 8484 and 8584.\")\n        return None\n\n    callback_url = f\"http://localhost:{port}/callback/aider\"\n    auth_code = None\n    server_error = None\n    server_started = threading.Event()\n    shutdown_server = threading.Event()\n\n    class OAuthCallbackHandler(http.server.SimpleHTTPRequestHandler):\n        def do_GET(self):\n            nonlocal auth_code, server_error\n            parsed_path = urlparse(self.path)\n            if parsed_path.path == \"/callback/aider\":\n                query_params = parse_qs(parsed_path.query)\n                if \"code\" in query_params:\n                    auth_code = query_params[\"code\"][0]\n                    self.send_response(200)\n                    self.send_header(\"Content-type\", \"text/html\")\n                    self.end_headers()\n                    self.wfile.write(\n                        b\"<html><body><h1>Success!</h1>\"\n                        b\"<p>Aider has received the authentication code. \"\n                        b\"You can close this browser tab.</p></body></html>\"\n                    )\n                    # Signal the main thread to shut down the server\n                    shutdown_server.set()\n                else:\n                    self.send_response(302)  # Redirect to the main website\n                    self.send_header(\"Location\", urls.website)\n                    self.end_headers()\n                    shutdown_server.set()\n            else:\n                self.send_response(302)\n                self.send_header(\"Location\", urls.website)\n                self.end_headers()\n\n    def run_server():\n        nonlocal server_error\n        try:\n            with socketserver.TCPServer((\"localhost\", port), OAuthCallbackHandler) as httpd:\n                io.tool_output(f\"Temporary server listening on {callback_url}\", log_only=True)\n                server_started.set()  # Signal that the server is ready\n                while not shutdown_server.is_set():\n                    httpd.handle_request()\n                    time.sleep(0.1)\n                io.tool_output(\"Shutting down temporary server.\", log_only=True)\n        except Exception as e:\n            server_error = f\"Failed to start or run temporary server: {e}\"\n            server_started.set()  # Signal even if failed\n            shutdown_server.set()  # Ensure shutdown logic proceeds\n\n    server_thread = threading.Thread(target=run_server, daemon=True)\n    server_thread.start()\n\n    # Wait briefly for the server to start, or for an error\n    if not server_started.wait(timeout=5):\n        io.tool_error(\"Temporary authentication server failed to start in time.\")\n        shutdown_server.set()  # Ensure thread exits if it eventually starts\n        server_thread.join(timeout=1)\n        return None\n\n    if server_error:\n        io.tool_error(server_error)\n        shutdown_server.set()  # Ensure thread exits\n        server_thread.join(timeout=1)\n        return None\n\n    # Generate codes and URL\n    code_verifier = secrets.token_urlsafe(64)\n    hasher = hashlib.sha256()\n    hasher.update(code_verifier.encode(\"utf-8\"))\n    code_challenge = base64.urlsafe_b64encode(hasher.digest()).rstrip(b\"=\").decode(\"utf-8\")\n    \n    auth_url = f\"https://openrouter.ai/auth?callback_url={callback_url}&code_challenge={code_challenge}&code_challenge_method=S256\"\n\n    io.tool_output(\"\\nPlease open this URL in your browser to connect Aider with OpenRouter:\")\n    io.tool_output()\n    print(auth_url)\n    \n    MINUTES = 5\n    io.tool_output(f\"\\nWaiting up to {MINUTES} minutes for you to finish in the browser...\")\n    io.tool_output(\"Use Control-C to interrupt.\")\n\n    try:\n        webbrowser.open(auth_url)\n    except Exception:\n        pass\n\n    # Wait for the callback to set the auth_code or for timeout/error\n    interrupted = False\n    try:\n        shutdown_server.wait(timeout=MINUTES * 60)  # Convert minutes to seconds\n    except KeyboardInterrupt:\n        io.tool_warning(\"\\nOAuth flow interrupted.\")\n        analytics.event(\"oauth_flow_failed\", provider=\"openrouter\", reason=\"user_interrupt\")\n        interrupted = True\n        shutdown_server.set()\n\n    server_thread.join(timeout=1)\n\n    if not auth_code:\n        io.tool_error(\"Authentication with OpenRouter failed.\")\n        analytics.event(\"oauth_flow_failed\", provider=\"openrouter\")\n        return None\n\n    io.tool_output(\"Completing authentication...\")\n    analytics.event(\"oauth_flow_code_received\", provider=\"openrouter\")\n\n    # Exchange code for key\n    api_key = exchange_code_for_key(auth_code, code_verifier, io)\n\n    if api_key:\n        # Append the key to the oauth-keys.env file\n        try:\n            config_dir = os.path.expanduser(\"~/.aider\")\n            os.makedirs(config_dir, exist_ok=True)\n            key_file = os.path.join(config_dir, \"oauth-keys.env\")\n            with open(key_file, \"a\", encoding=\"utf-8\") as f:\n                f.write(f'OPENROUTER_API_KEY=\"{api_key}\"\\n')\n            io.tool_warning(\"Aider will load the OpenRouter key automatically in future sessions.\")\n            io.tool_output()\n            analytics.event(\"oauth_flow_success\", provider=\"openrouter\")\n            return api_key\n        except Exception as e:\n            io.tool_error(f\"Authentication with OpenRouter failed.\")\n            analytics.event(\"oauth_flow_failed\", provider=\"openrouter\", reason=\"code_exchange_failed\")\n            return None\n    else:\n        io.tool_error(\"Authentication with OpenRouter failed.\")\n        analytics.event(\"oauth_flow_failed\", provider=\"openrouter\", reason=\"code_exchange_failed\")\n        return None",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.768932457179791,
      "diff_lines": 316,
      "added_lines": 27,
      "removed_lines": 142,
      "diff_preview": "--- expected+++ predicted@@ -1,5 +1,3 @@-import base64\n-import hashlib\n import http.server\n import os\n import secrets\n@@ -30,7 +28,7 @@         response = requests.get(\n             \"https://openrouter.ai/api/v1/auth/key\",\n             headers={\"Authorization\": f\"Bearer {api_key}\"},\n-            timeout=5,  # Add a reasonable timeout\n+            timeout=5  # Add a reasonable timeout\n         )\n         response.raise_for_status()\n         data = response.json()\n@@ -58,7 +56,7 @@             return \"openrouter/google/gemini-2.5-pro-exp-03-25:free\"\n         else:\n             return \"openrouter/anthropic/claude-3.7-sonnet\"\n-\n+    \n     # Select model based on other available API keys\n     model_key_pairs = [\n         (\"ANTHROPIC_API_KEY\", \"sonnet\"),\n@@ -79,15 +77,17 @@ def offer_openrouter_oauth(io, analytics):\n     \"\"\"\n     Offers OpenRouter OAuth flow to the user if no API keys are found.\n-\n+    \n     Args:\n         io: The InputOutput object for user interaction.\n         analytics: The Analytics object for tracking events.\n-\n+        \n     Returns:\n         True if authentication was successful, False otherwise.\n     \"\"\"\n     # No API keys found - Offer OpenRouter OAuth\n+    no_model_msg = \"No model was specified and no API keys were provided.\"\n+    io.tool_warning(no_model_msg)\n     io.tool_output(\"OpenRouter provides free and paid access to many LLMs.\")\n     # Use confirm_ask which handles non-interactive cases\n     if io.confirm_ask(\n@@ -98,17 +98,12 @@         openrouter_key = start_openrouter_oauth_flow(io, analytics)\n         if openrouter_key:\n             # Successfully got key via OAuth, use the default OpenRouter model\n"
    },
    "llm_calls": 1,
    "elapsed_time": 70.419362783432,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 17935,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_aider_repo.py",
    "exact_match": false,
    "similarity": 0.40677101922194464,
    "diff_lines": 446,
    "predicted_output": "import os\nimport time\nfrom pathlib import Path, PurePosixPath\n\ntry:\n    import git\n    ANY_GIT_ERROR = [\n        git.exc.ODBError,\n        git.exc.GitError,\n        git.exc.InvalidGitRepositoryError,\n        git.exc.GitCommandNotFound,\n    ]\nexcept ImportError:\n    git = None\n    ANY_GIT_ERROR = []\n\nimport pathspec\n\nfrom aider import prompts, utils\nfrom aider.sendchat import simple_send_with_retries\n\nfrom .dump import dump  # noqa: F401\n\n\nclass GitRepo:\n    repo = None\n    aider_ignore_file = None\n    aider_ignore_spec = None\n    aider_ignore_ts = 0\n    subtree_only = False\n    ignore_file_cache = {}\n\n    def __init__(\n        self,\n        io,\n        fnames,\n        git_dname,\n        aider_ignore_file=None,\n        models=None,\n        attribute_author=True,\n        attribute_committer=True,\n        attribute_commit_message_author=False,\n        attribute_commit_message_committer=False,\n        commit_prompt=None,\n        subtree_only=False,\n        git_commit_verify=True,\n    ):\n        self.io = io\n        self.models = models\n\n        self.attribute_author = attribute_author\n        self.attribute_committer = attribute_committer\n        self.attribute_commit_message_author = attribute_commit_message_author\n        self.attribute_commit_message_committer = attribute_commit_message_committer\n        self.commit_prompt = commit_prompt\n        self.subtree_only = subtree_only\n        self.git_commit_verify = git_commit_verify\n        self.ignore_file_cache = {}\n\n        if git_dname:\n            check_fnames = [git_dname]\n        elif fnames:\n            check_fnames = fnames\n        else:\n            check_fnames = [\".\"]\n            \n        repo_paths = []\n        for fname in check_fnames:\n            fname = Path(fname)\n            try:\n                repo_path = git.Repo(fname, search_parent_directories=True).working_dir\n                repo_path = utils.safe_abs_path(repo_path)\n                repo_paths.append(repo_path)\n            except (git.exc.ODBError, git.exc.GitError):\n                pass\n\n        num_repos = len(set(repo_paths))\n\n        if num_repos == 0:\n            raise Exception(\"No valid git repository found.\")\n        \n        self.repo = git.Repo(repo_paths.pop(), odbt=git.GitDB)\n        self.root = utils.safe_abs_path(self.repo.working_tree_dir)\n\n    def commit(self, fnames=None, context=None, message=None, aider_edits=False):\n        if not fnames and not self.repo.is_dirty():\n            return\n\n        if message:\n            commit_message = message\n        else:\n            diffs = self.get_diffs(fnames)\n            if aider_edits and self.attribute_commit_message_author:\n                commit_message = \"aider: \" + commit_message\n            elif self.attribute_commit_message_committer:\n                commit_message = \"aider: \" + commit_message\n\n        if not commit_message:\n            commit_message = \"(no commit message provided)\"\n\n        full_commit_message = commit_message\n        \n        cmd = [\"-m\", full_commit_message]\n        if not self.git_commit_verify:\n            cmd.append(\"--no-verify\")\n        if fnames:\n            fnames = [str(self.abs_root_path(fn)) for fn in fnames]\n            for fname in fnames:\n                try:\n                    self.repo.git.add(fname)\n                except ANY_GIT_ERROR as err:\n                    self.io.tool_error(f\"Unable to add {fname}: {err}\")\n            cmd += [\"--\"] + fnames\n        else:\n            cmd += [\"-a\"]\n\n        try:\n            self.repo.git.commit(cmd)\n            commit_hash = self.get_head_commit_sha(short=True)\n            self.io.tool_output(f\"Commit {commit_hash} {commit_message}\", bold=True)\n            return commit_hash, commit_message\n        except ANY_GIT_ERROR as err:\n            self.io.tool_error(f\"Unable to commit: {err}\")\n\n    def get_head_commit(self):\n        try:\n            return self.repo.head.commit\n        except (ValueError, git.exc.ODBError, git.exc.GitError):\n            return None\n\n    def normalize_path(self, path):\n        orig_path = path\n        res = self.normalized_path.get(orig_path)\n        if res:\n            return res\n\n        path = str(Path(PurePosixPath((Path(self.root) / path).relative_to(self.root))))\n        self.normalized_path[orig_path] = path\n        return path\n\n    def ignored_file(self, fname):\n        self.refresh_aider_ignore()\n\n        if fname in self.ignore_file_cache:\n            return self.ignore_file_cache[fname]\n\n        try:\n            fname_path = Path(self.normalize_path(fname))\n            cwd_path = Path.cwd().resolve().relative_to(Path(self.root).resolve())\n            if cwd_path not in fname_path.parents and fname_path != cwd_path:\n                return True\n        except ValueError:\n            return True\n\n        return self.aider_ignore_spec.match_file(fname)",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.40677101922194464,
      "diff_lines": 446,
      "added_lines": 19,
      "removed_lines": 328,
      "diff_preview": "--- expected+++ predicted@@ -4,7 +4,6 @@ \n try:\n     import git\n-\n     ANY_GIT_ERROR = [\n         git.exc.ODBError,\n         git.exc.GitError,\n@@ -18,20 +17,9 @@ import pathspec\n \n from aider import prompts, utils\n+from aider.sendchat import simple_send_with_retries\n \n from .dump import dump  # noqa: F401\n-\n-ANY_GIT_ERROR += [\n-    OSError,\n-    IndexError,\n-    BufferError,\n-    TypeError,\n-    ValueError,\n-    AttributeError,\n-    AssertionError,\n-    TimeoutError,\n-]\n-ANY_GIT_ERROR = tuple(ANY_GIT_ERROR)\n \n \n class GitRepo:\n@@ -39,10 +27,8 @@     aider_ignore_file = None\n     aider_ignore_spec = None\n     aider_ignore_ts = 0\n-    aider_ignore_last_check = 0\n     subtree_only = False\n     ignore_file_cache = {}\n-    git_repo_error = None\n \n     def __init__(\n         self,\n@@ -62,9 +48,6 @@         self.io = io\n         self.models = models\n \n-        self.normalized_path = {}\n-        self.tree_files = {}\n-\n"
    },
    "llm_calls": 1,
    "elapsed_time": 27.913008213043213,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 51983,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_aider_run_cmd.py",
    "exact_match": false,
    "similarity": 0.7556904400606981,
    "diff_lines": 97,
    "predicted_output": "import os\nimport platform\nimport subprocess\nimport sys\nfrom io import BytesIO\nimport pexpect\nimport psutil\n\n\ndef run_cmd(command, verbose=False, error_print=None, cwd=None):\n    try:\n        if sys.stdin.isatty() and hasattr(pexpect, \"spawn\") and platform.system() != \"Windows\":\n            return run_cmd_pexpect(command, verbose, cwd)\n\n        return run_cmd_subprocess(command, verbose, cwd)\n    except OSError as e:\n        error_message = f\"Error occurred while running command '{command}': {str(e)}\"\n        if error_print is None:\n            print(error_message)\n        else:\n            error_print(error_message)\n        return 1, error_message\n\n\ndef get_windows_parent_process_name():\n    try:\n        kernel32 = ctypes.windll.kernel32\n        h_process = kernel32.OpenProcess(0x0400, False, os.getppid())\n        exe_path = ctypes.create_unicode_buffer(260)\n        kernel32.QueryFullProcessImageNameW(\n            h_process, 0, exe_path, ctypes.byref(ctypes.c_ulong(260))\n        )\n        kernel32.CloseHandle(h_process)\n        return os.path.basename(exe_path.value).lower()\n    except Exception:\n        return None\n\n\ndef run_cmd_subprocess(command, verbose=False, cwd=None):\n    if verbose:\n        print(\"Using run_cmd_subprocess:\", command)\n\n    try:\n        shell = os.environ.get(\"SHELL\", \"/bin/sh\")\n        parent_process = None\n\n        # Determine the appropriate shell\n        if platform.system() == \"Windows\":\n            parent_process = get_windows_parent_process_name()\n            if parent_process == \"powershell.exe\":\n                command = f\"powershell -Command {command}\"\n\n        if verbose:\n            print(\"Running command:\", command)\n            print(\"SHELL:\", shell)\n            if platform.system() == \"Windows\":\n                print(\"Parent process:\", parent_process)\n\n        process = subprocess.Popen(\n            command,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            text=True,\n            shell=True,\n            encoding=sys.stdout.encoding,\n            errors=\"replace\",\n            bufsize=0,  # Set bufsize to 0 for unbuffered output\n            cwd=cwd,\n        )\n\n        output = []\n        while True:\n            chunk = process.stdout.read(1)\n            if not chunk:\n                break\n            print(chunk, end=\"\", flush=True)  # Print the chunk in real-time\n            output.append(chunk)  # Store the chunk for later use\n\n        process.wait()\n        return process.returncode, \"\".join(output)\n    except Exception as e:\n        return 1, str(e)\n\n\ndef run_cmd_pexpect(command, verbose=False, cwd=None):\n    \"\"\"\n    Run a shell command interactively using pexpect, capturing all output.\n\n    :param command: The command to run as a string.\n    :param verbose: If True, print output in real-time.\n    :return: A tuple containing (exit_status, output)\n    \"\"\"\n    if verbose:\n        print(\"Running pexpect.spawn with shell:\", shell)\n\n    output = BytesIO()\n\n    def output_callback(b):\n        output.write(b)\n        if verbose:\n            print(b.decode(\"utf-8\", errors=\"replace\"), end=\"\", flush=True)\n        return b\n\n    try:\n        shell = os.environ.get(\"SHELL\", \"/bin/sh\")\n        if verbose:\n            print(\"Running pexpect.spawn with shell:\", shell)\n        child = pexpect.spawn(shell, args=[\"-c\", command], encoding=\"utf-8\", cwd=cwd)\n        child.interact(output_filter=output_callback)\n        child.close()\n        return child.exitstatus, output.getvalue().decode(\"utf-8\", errors=\"replace\")\n    except (pexpect.ExceptionPexpect, TypeError, ValueError) as e:\n        error_msg = f\"Error running command {command}: {e}\"\n        return 1, error_msg",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.7556904400606981,
      "diff_lines": 97,
      "added_lines": 15,
      "removed_lines": 33,
      "diff_preview": "--- expected+++ predicted@@ -3,7 +3,6 @@ import subprocess\n import sys\n from io import BytesIO\n-\n import pexpect\n import psutil\n \n@@ -25,21 +24,19 @@ \n def get_windows_parent_process_name():\n     try:\n-        current_process = psutil.Process()\n-        while True:\n-            parent = current_process.parent()\n-            if parent is None:\n-                break\n-            parent_name = parent.name().lower()\n-            if parent_name in [\"powershell.exe\", \"cmd.exe\"]:\n-                return parent_name\n-            current_process = parent\n-        return None\n+        kernel32 = ctypes.windll.kernel32\n+        h_process = kernel32.OpenProcess(0x0400, False, os.getppid())\n+        exe_path = ctypes.create_unicode_buffer(260)\n+        kernel32.QueryFullProcessImageNameW(\n+            h_process, 0, exe_path, ctypes.byref(ctypes.c_ulong(260))\n+        )\n+        kernel32.CloseHandle(h_process)\n+        return os.path.basename(exe_path.value).lower()\n     except Exception:\n         return None\n \n \n-def run_cmd_subprocess(command, verbose=False, cwd=None, encoding=sys.stdout.encoding):\n+def run_cmd_subprocess(command, verbose=False, cwd=None):\n     if verbose:\n         print(\"Using run_cmd_subprocess:\", command)\n \n@@ -65,10 +62,9 @@             stderr=subprocess.STDOUT,\n             text=True,\n             shell=True,\n-            encoding=encoding,\n+            encoding=sys.stdout.encoding,\n             errors=\"replace\",\n             bufsize=0,  # Set bufsize to 0 for unbuffered output\n-            universal_newlines=True,\n"
    },
    "llm_calls": 1,
    "elapsed_time": 21.02902603149414,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 11571,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_aider_scrape.py",
    "exact_match": false,
    "similarity": 0.6940397350993377,
    "diff_lines": 303,
    "predicted_output": "#!/usr/bin/env python\n\nimport re\nimport sys\nimport pypandoc\nfrom playwright.sync_api import sync_playwright\nfrom aider import __version__, urls, utils\nfrom aider.dump import dump  # noqa: F401\n\naider_user_agent = f\"Aider/{__version__} +{urls.website}\"\n\nclass Scraper:\n    pandoc_available = None\n    playwright_available = None\n\n    def __init__(self, print_error=None, playwright_available=None, verify_ssl=True):\n        if print_error:\n            self.print_error = print_error\n        else:\n            self.print_error = print\n\n        self.playwright_available = playwright_available\n        self.verify_ssl = verify_ssl\n\n    def scrape(self, url):\n        if self.playwright_available:\n            content, mime_type = self.scrape_with_playwright(url)\n        else:\n            content, mime_type = self.scrape_with_httpx(url)\n\n        if not content:\n            print(f\"Failed to retrieve content from {url}\")\n            return None\n\n        if (mime_type and mime_type.startswith(\"text/html\")) or (mime_type is None and self.looks_like_html(content)):\n            self.try_pandoc()\n            content = self.html_to_markdown(content)\n\n        return content\n\n    def looks_like_html(self, content):\n        if isinstance(content, str):\n            html_patterns = [\n                r'<!DOCTYPE\\s+html',\n                r'<html',\n                r'<head',\n                r'<body',\n                r'<div',\n                r'<p>',\n                r'<a\\s+href=',\n            ]\n            return any(re.search(pattern, content, re.IGNORECASE) for pattern in html_patterns)\n        return False\n\n    def scrape_with_playwright(self, url):\n        import playwright\n        from playwright.sync_api import Error as PlaywrightError\n        from playwright.sync_api import TimeoutError as PlaywrightTimeoutError\n\n        with sync_playwright() as p:\n            try:\n                browser = p.chromium.launch()\n            except Exception as e:\n                self.playwright_available = False\n                print(str(e))\n                return None, None\n\n            context = browser.new_context(ignore_https_errors=not self.verify_ssl)\n            page = context.new_page()\n\n            user_agent = page.evaluate(\"navigator.userAgent\")\n            user_agent = user_agent.replace(\"Headless\", \"\")\n            user_agent = user_agent.replace(\"headless\", \"\")\n            user_agent += \" \" + aider_user_agent\n\n            page.set_extra_http_headers({\"User-Agent\": user_agent})\n\n            response = None\n            try:\n                response = page.goto(url, wait_until=\"networkidle\", timeout=5000)\n            except PlaywrightTimeoutError:\n                print(f\"Page didn't quiesce, scraping content anyway: {url}\")\n                response = None\n            except PlaywrightError as e:\n                print(f\"Error navigating to {url}: {str(e)}\")\n                return None, None\n\n            try:\n                content = page.content()\n                mime_type = response.header_value(\"content-type\").split(\";\")[0] if response else None\n            except PlaywrightError as e:\n                print(f\"Error retrieving page content: {str(e)}\")\n                content = None\n                mime_type = None\n            finally:\n                browser.close()\n\n        return content, mime_type\n\n    def scrape_with_httpx(self, url):\n        import httpx\n\n        headers = {\"User-Agent\": f\"Mozilla./5.0 ({aider_user_agent})\"}\n        try:\n            with httpx.Client(headers=headers, verify=self.verify_ssl, follow_redirects=True) as client:\n                response = client.get(url)\n                response.raise_for_status()\n                return response.text, response.headers.get(\"content-type\", \"\").split(\";\")[0]\n        except httpx.HTTPError as http_err:\n            print(f\"HTTP error occurred: {http_err}\")\n        except Exception as err:\n            print(f\"An error occurred: {err}\")\n        return None, None\n\n    def try_pandoc(self):\n        if self.pandoc_available:\n            return\n\n        try:\n            pypandoc.get_pandoc_version()\n        except OSError:\n            pass\n\n        try:\n            pypandoc.download_pandoc(delete_installer=True)\n        except Exception as err:\n            self.print_error(f\"Unable to install pandoc: {err}\")\n            return\n\n        self.pandoc_available = True\n\n    def html_to_markdown(self, page_source):\n        if not self.pandoc_available:\n            return page_source\n\n        try:\n            md = pypandoc.convert_text(page_source, \"markdown\", format=\"html\")\n        except OSError:\n            return page_source\n\n        md = re.sub(r\"</div>\", \"      \", md)\n        md = re.sub(r\"<div>\", \"     \", md)\n\n        return md",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.6940397350993377,
      "diff_lines": 303,
      "added_lines": 42,
      "removed_lines": 172,
      "diff_preview": "--- expected+++ predicted@@ -2,81 +2,18 @@ \n import re\n import sys\n-\n import pypandoc\n-\n+from playwright.sync_api import sync_playwright\n from aider import __version__, urls, utils\n from aider.dump import dump  # noqa: F401\n \n aider_user_agent = f\"Aider/{__version__} +{urls.website}\"\n \n-# Playwright is nice because it has a simple way to install dependencies on most\n-# platforms.\n-\n-\n-def install_playwright(io):\n-    try:\n-        from playwright.sync_api import sync_playwright\n-\n-        has_pip = True\n-    except ImportError:\n-        has_pip = False\n-\n-    try:\n-        with sync_playwright() as p:\n-            p.chromium.launch()\n-            has_chromium = True\n-    except Exception:\n-        has_chromium = False\n-\n-    if has_pip and has_chromium:\n-        return True\n-\n-    pip_cmd = utils.get_pip_install([\"aider-chat[playwright]\"])\n-    chromium_cmd = \"-m playwright install --with-deps chromium\"\n-    chromium_cmd = [sys.executable] + chromium_cmd.split()\n-\n-    cmds = \"\"\n-    if not has_pip:\n-        cmds += \" \".join(pip_cmd) + \"\\n\"\n-    if not has_chromium:\n-        cmds += \" \".join(chromium_cmd) + \"\\n\"\n-\n-    text = f\"\"\"For the best web scraping, install Playwright:\n-\n-{cmds}\n"
    },
    "llm_calls": 1,
    "elapsed_time": 35.62352013587952,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 23410,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_aider_sendchat.py",
    "exact_match": false,
    "similarity": 0.5677484108397457,
    "diff_lines": 130,
    "predicted_output": "from aider.dump import dump  # noqa: F401\nfrom aider.utils import format_messages\n\ndef sanity_check_messages(messages):\n    \"\"\"Check if messages alternate between 'assistant' and 'user' roles.\n    Also verifies the last non-system message is from the user.\n    Returns True if valid, False otherwise.\"\"\"\n    last_role = None\n    last_non_system_role = None\n    \n    for msg in messages:\n        role = msg.get(\"role\")\n        if role == \"system\":\n            continue\n            \n        if last_role and role == last_role:\n            turns = format_messages(messages)\n            raise ValueError(\"Messages don't properly alternate user/assistant:\\n\\n\" + turns)\n        \n        last_role = role\n        last_non_system_role = role\n\n    return last_non_system_role == \"user\"\n\ndef ensure_alternating_roles(messages):\n    \"\"\"Ensure messages alternate between 'assistant' and 'user' roles.\n    Inserts empty messages of the opposite role when consecutive messages of the same role are found.\n\n    Args:\n        messages: List of message dictionaries with 'role' and 'content' keys.\n\n    Returns:\n        List of messages with alternating roles.\n    \"\"\"\n    if not messages:\n        return messages\n\n    fixed_messages = []\n    prev_role = None\n\n    for msg in messages:\n        current_role = msg.get(\"role\")  # Get 'role', None if missing\n\n        # If current role same as previous, insert empty message\n        # of the opposite role\n        if current_role == prev_role:\n            if current_role == \"user\":\n                fixed_messages.append({\"role\": \"assistant\", \"content\": \"\"})\n            else:\n                fixed_messages.append({\"role\": \"user\", \"content\": \"\"})\n\n        fixed_messages.append(msg)\n        prev_role = current_role\n\n    return fixed_messages\n\ndef send_completion(\n    model_name,\n    messages,\n    functions,\n    stream,\n    temperature=0,\n    extra_params=None,\n):\n    if \"deepseek-reasoner\" in model_name:\n        messages = ensure_alternating_roles(messages)\n\n    kwargs = dict(\n        model=model_name,\n        messages=messages,\n        stream=stream,\n    )\n    if temperature is not None:\n        kwargs[\"temperature\"] = temperature\n\n    if functions is not None:\n        function = functions[0]\n        kwargs[\"tools\"] = [dict(type=\"function\", function=function)]\n        kwargs[\"tool_choice\"] = {\"type\": \"function\", \"function\": {\"name\": function[\"name\"]}}\n\n    if extra_params is not None:\n        kwargs.update(extra_params)\n\n    key = json.dumps(kwargs, sort_keys=True).encode()\n\n    # Generate SHA1 hash of kwargs and append it to chat_completion_call_hashes\n    hash_object = hashlib.sha1(key)\n\n    res = litellm.completion(**kwargs)\n\n    return hash_object, res\n\ndef simple_send_with_retries(model, messages):\n    litellm_ex = LiteLLMExceptions()\n\n    if \"deepseek-reasoner\" in model.name:\n        messages = ensure_alternating_roles(messages)\n\n    retry_delay = 0.125\n    while True:\n        try:\n            kwargs = {\n                \"model_name\": model.name,\n                \"messages\": messages,\n                \"functions\": None,\n                \"stream\": False,\n                \"temperature\": None if not model.use_temperature else 0,\n                \"extra_params\": model.extra_params,\n            }\n\n            _hash, response = send_completion(**kwargs)\n            if not response or not hasattr(response, \"choices\") or not response.choices:\n                return None\n            return response.choices[0].message.content\n        except litellm_ex.exceptions_tuple() as err:\n            ex_info = litellm_ex.get_ex_info(err)\n\n            print(str(err))\n            if ex_info.description:\n                print(ex_info.description)\n\n            should_retry = ex_info.retry\n            if should_retry:\n                retry_delay *= 2\n                if retry_delay > RETRY_TIMEOUT:\n                    should_retry = False\n\n            if not should_retry:\n                return None\n\n            print(f\"Retrying in {retry_delay:.1f} seconds...\")\n            time.sleep(retry_delay)\n            continue\n        except AttributeError:\n            return None",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.5677484108397457,
      "diff_lines": 130,
      "added_lines": 86,
      "removed_lines": 12,
      "diff_preview": "--- expected+++ predicted@@ -1,36 +1,30 @@ from aider.dump import dump  # noqa: F401\n from aider.utils import format_messages\n \n-\n def sanity_check_messages(messages):\n-    \"\"\"Check if messages alternate between user and assistant roles.\n-    System messages can be interspersed anywhere.\n+    \"\"\"Check if messages alternate between 'assistant' and 'user' roles.\n     Also verifies the last non-system message is from the user.\n     Returns True if valid, False otherwise.\"\"\"\n     last_role = None\n     last_non_system_role = None\n-\n+    \n     for msg in messages:\n         role = msg.get(\"role\")\n         if role == \"system\":\n             continue\n-\n+            \n         if last_role and role == last_role:\n             turns = format_messages(messages)\n             raise ValueError(\"Messages don't properly alternate user/assistant:\\n\\n\" + turns)\n-\n+        \n         last_role = role\n         last_non_system_role = role\n \n-    # Ensure last non-system message is from user\n     return last_non_system_role == \"user\"\n-\n \n def ensure_alternating_roles(messages):\n     \"\"\"Ensure messages alternate between 'assistant' and 'user' roles.\n-\n-    Inserts empty messages of the opposite role when consecutive messages\n-    of the same role are found.\n+    Inserts empty messages of the opposite role when consecutive messages of the same role are found.\n \n     Args:\n         messages: List of message dictionaries with 'role' and 'content' keys.\n@@ -58,4 +52,84 @@         fixed_messages.append(msg)\n         prev_role = current_role\n \n-    return fixed_messages+    return fixed_messages\n"
    },
    "llm_calls": 1,
    "elapsed_time": 27.54512310028076,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 36327,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_aider_special.py",
    "exact_match": false,
    "similarity": 0.999207158228565,
    "diff_lines": 26,
    "predicted_output": "import os\n\nROOT_IMPORTANT_FILES = [\n    # Version Control\n    \".gitignore\",\n    \".gitattributes\",\n    # Documentation\n    \"README\",\n    \"README.md\",\n    \"README.txt\",\n    \"README.rst\",\n    \"CONTRIBUTING\",\n    \"CONTRIBUTING.md\",\n    \"CONTRIBUTING.txt\",\n    \"CONTRIBUTING.rst\",\n    \"LICENSE\",\n    \"LICENSE.md\",\n    \"LICENSE.txt\",\n    \"CHANGELOG\",\n    \"CHANGELOG.md\",\n    \"CHANGELOG.txt\",\n    \"CHANGELOG.rst\",\n    \"SECURITY\",\n    \"SECURITY.md\",\n    \"SECURITY.txt\",\n    \"CODEOWNERS\",\n    # Package Management and Dependencies\n    \"requirements.txt\",\n    \"Pipfile\",\n    \"Pipfile.lock\",\n    \"pyproject.toml\",\n    \"setup.py\",\n    \"setup.cfg\",\n    \"package.json\",\n    \"package-lock.json\",\n    \"yarn.lock\",\n    \"npm-shrinkwrap.json\",\n    \"Gemfile\",\n    \"Gemfile.lock\",\n    \"composer.json\",\n    \"composer.lock\",\n    \"pom.xml\",\n    \"build.gradle\",\n    \"build.gradle.kts\",\n    \"build.sbt\",\n    \"go.mod\",\n    \"go.sum\",\n    \"Cargo.toml\",\n    \"Cargo.lock\",\n    \"mix.exs\",\n    \"rebar.config\",\n    \"project.clj\",\n    \"Podfile\",\n    \"Cartfile\",\n    \"dub.json\",\n    \"dub.sdl\",\n    # Configuration and Settings\n    \".env\",\n    \".env.example\",\n    \".editorconfig\",\n    \"tsconfig.json\",\n    \"jsconfig.json\",\n    \".babelrc\",\n    \"babel.config.js\",\n    \".eslintrc\",\n    \".eslintignore\",\n    \".prettierrc\",\n    \".stylelintrc\",\n    \"tslint.json\",\n    \".pylintrc\",\n    \".flake8\",\n    \".rubocop.yml\",\n    \".scalafmt.conf\",\n    \".dockerignore\",\n    \".gitpod.yml\",\n    \"sonar-project.properties\",\n    \"renovate.json\",\n    \"dependabot.yml\",\n    \".pre-commit-config.yaml\",\n    \"mypy.ini\",\n    \"tox.ini\",\n    \".yamllint\",\n    \"pyrightconfig.json\",\n    # Build and Compilation\n    \"webpack.config.js\",\n    \"rollup.config.js\",\n    \"parcel.config.js\",\n    \"gulpfile.js\",\n    \"Gruntfile.js\",\n    \"build.xml\",\n    \"build.boot\",\n    \"project.json\",\n    \"build.cake\",\n    \"MANIFEST.in\",\n    # Testing\n    \"pytest.ini\",\n    \"phpunit.xml\",\n    \"karma.conf.js\",\n    \"jest.config.js\",\n    \"cypress.json\",\n    \".nycrc\",\n    \".nycrc.json\",\n    # CI/CD\n    \".travis.yml\",\n    \".gitlab-ci.yml\",\n    \"Jenkinsfile\",\n    \"azure-pipelines.yml\",\n    \"bitbucket-pipelines.yml\",\n    \"appveyor.yml\",\n    \"circle.yml\",\n    \".circleci/config.yml\",\n    \".github/dependabot.yml\",\n    \"codecov.yml\",\n    \".coveragerc\",\n    # Docker and Containers\n    \"Dockerfile\",\n    \"docker-compose.yml\",\n    \"docker-compose.override.yml\",\n    # Cloud and Serverless\n    \"serverless.yml\",\n    \"firebase.json\",\n    \"now.json\",\n    \"netlify.toml\",\n    \"vercel.json\",\n    \"app.yaml\",\n    \"terraform.tf\",\n    \"main.tf\",\n    \"cloudformation.yaml\",\n    \"cloudformation.json\",\n    \"ansible.cfg\",\n    \"kubernetes.yaml\",\n    \"k8s.yaml\",\n    # Database\n    \"schema.sql\",\n    \"liquibase.properties\",\n    \"flyway.conf\",\n    # Framework-specific\n    \"next.config.js\",\n    \"nuxt.config.js\",\n    \"vue.config.js\",\n    \"angular.json\",\n    \"gatsby-config.js\",\n    \"gridsome.config.js\",\n    # API Documentation\n    \"swagger.yaml\",\n    \"swagger.json\",\n    \"openapi.yaml\",\n    \"openapi.json\",\n    # Development environment\n    \".nvmrc\",\n    \".ruby-version\",\n    \".python-version\",\n    \"Vagrantfile\",\n    # Quality and metrics\n    \".codeclimate.yml\",\n    \"codecov.yml\",\n    # Documentation\n    \"mkdocs.yml\",\n    \"_config.yml\",\n    \"book.toml\",\n    \"readthedocs.yml\",\n    \".readthedocs.yaml\",\n    # Package registries\n    \".npmrc\",\n    \".yarnrc\",\n    # Linting and formatting\n    \".isort.cfg\",\n    \".markdownlint.json\",\n    \".markdownlint.yaml\",\n    # Security\n    \".bandit\",\n    \".secrets.baseline\",\n    # Misc\n    \".pypirc\",\n    \".gitkeep\",\n    \".npmignore\",\n]\n\n# Normalize the lists once\nNORMALIZED_ROOT_IMPORTANT_FILES = set(os.path.normpath(path) for path in ROOT_IMPORTANT_FILES)\n\ndef is_important(file_path):\n    file_name = os.path.basename(file_path)\n    dir_name = os.path.normpath(os.path.dirname(file_path))\n    normalized_path = os.path.normpath(file_path)\n\n    # Check for GitHub Actions workflow files\n    if dir_name == os.path.normpath(\".github/workflows\") and file_name.endswith(\".yml\"):\n        return True\n\n    return normalized_path in NORMALIZED_ROOT_IMPORTANT_FILES\n\ndef filter_important_files(file_paths):\n    \"\"\"\n    Filter a list of file paths to return only those that are commonly important in codebases.\n    \n    :param file_paths: List of file paths to check\n    :return: List of file paths that match important file patterns\n    \"\"\"\n    return list(filter(is_important, file_paths))",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.999207158228565,
      "diff_lines": 26,
      "added_lines": 1,
      "removed_lines": 4,
      "diff_preview": "--- expected+++ predicted@@ -176,10 +176,8 @@     \".npmignore\",\n ]\n \n-\n # Normalize the lists once\n NORMALIZED_ROOT_IMPORTANT_FILES = set(os.path.normpath(path) for path in ROOT_IMPORTANT_FILES)\n-\n \n def is_important(file_path):\n     file_name = os.path.basename(file_path)\n@@ -192,11 +190,10 @@ \n     return normalized_path in NORMALIZED_ROOT_IMPORTANT_FILES\n \n-\n def filter_important_files(file_paths):\n     \"\"\"\n     Filter a list of file paths to return only those that are commonly important in codebases.\n-\n+    \n     :param file_paths: List of file paths to check\n     :return: List of file paths that match important file patterns\n     \"\"\"\n"
    },
    "llm_calls": 1,
    "elapsed_time": 33.833776235580444,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 23015,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_aider_utils.py",
    "exact_match": false,
    "similarity": 0.31766525452222444,
    "diff_lines": 418,
    "predicted_output": "import itertools\nimport os\nimport platform\nimport shlex\nimport subprocess\nimport sys\nimport tempfile\nimport time\nfrom pathlib import Path\nimport git\nfrom aider.dump import dump  # noqa: F401\n\nIMAGE_EXTENSIONS = {\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\", \".tiff\", \".webp\", \".pdf\"}\n\n\nclass Spinner:\n    unicode_spinner = [\"\u280b\", \"\u2819\", \"\u2839\", \"\u2838\", \"\u283c\", \"\u2834\", \"\u2826\", \"\u2827\", \"\u2807\", \"\u280f\"]\n    ascii_spinner = [\"|\", \"/\", \"-\", \"\\\\\"]\n\n    def __init__(self, text):\n        self.text = text\n        self.start_time = time.time()\n        self.last_update = 0\n        self.visible = False\n        self.is_tty = sys.stdout.isatty()\n        \n        # Try unicode first, fall back to ascii if needed\n        try:\n            # Test if we can print unicode characters\n            print(self.unicode_spinner[0], end=\"\", flush=True)\n            print(\"\\r\", end=\"\", flush=True)\n            self.spinner_chars = itertools.cycle(self.unicode_spinner)\n        except UnicodeEncodeError:\n            self.spinner_chars = itertools.cycle(self.ascii_spinner)\n\n    def step(self):\n        if not self.is_tty:\n            return\n            \n        current_time = time.time()\n        if not self.visible and current_time - self.start_time >= 0.5:\n            self.visible = True\n        if self.visible:\n            print(f\"\\r{self.text} {next(self.spinner_chars)}\\r{self.text} \", end=\"\", flush=True)\n\n    def end(self):\n        if self.visible and self.is_tty:\n            print(\"\\r\" + \" \" * (len(self.text) + 3))\n\n\ndef safe_abs_path(res):\n    \"Gives an abs path, which safely returns a full (not 8.3) windows path\"\n    res = Path(res).resolve()\n    return str(res)\n\n\ndef make_repo(path=None):\n    if not path:\n        path = \".\"\n    repo = git.Repo.init(path)\n    return repo\n\n\ndef is_image_file(file_name):\n    \"\"\"\n    Check if the given file name has an image file extension.\n    \n    :param file_name: The name of the file to check.\n    :return: True if the file is an image, False otherwise.\n    \"\"\"\n    file_name = str(file_name)  # Convert file_name to string\n    return any(file_name.endswith(ext) for ext in IMAGE_EXTENSIONS)\n\n\ndef run_install(cmd):\n    print()\n    print(\"Installing:\", printable_shell_command(cmd))\n\n    try:\n        output = []\n        process = subprocess.Popen(\n            cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            text=True,\n            shell=True,\n            encoding=sys.stdout.encoding,\n            errors=\"replace\",\n        )\n        spinner = Spinner(\"Installing...\")\n\n        while True:\n            char = process.stdout.read(1)\n            if not char:\n                break\n\n            output.append(char)\n            spinner.step()\n\n        return_code = process.wait()\n        output = \"\".join(output)\n\n        if return_code == 0:\n            print(\"\\rInstallation complete.\")\n            print()\n            return True, output\n\n    except subprocess.CalledProcessError as e:\n        print(f\"\\nError running pip install: {e}\")\n\n    print(\"\\nInstallation failed.\\n\")\n    return False, output\n\n\ndef check_pip_install_extra(io, module, prompt, pip_install_cmd):\n    if module:\n        try:\n            __import__(module)\n            return True\n        except (ImportError, ModuleNotFoundError):\n            pass\n\n    cmd = get_pip_install(pip_install_cmd)\n\n    if prompt:\n        io.tool_warning(prompt)\n\n    if not io.confirm_ask(\"Run pip install?\", default=\"y\", subject=printable_shell_command(cmd)):\n        return\n\n    success, output = run_install(cmd)\n    if success:\n        if not module:\n            return True\n        try:\n            __import__(module)\n            return True\n        except (ImportError, ModuleNotFoundError, RuntimeError) as err:\n            io.tool_error(str(err))\n\n    io.tool_error(output)\n\n\ndef printable_shell_command(cmd_list):\n    \"\"\"\n    Convert a list of command arguments to a properly shell-escaped string.\n\n    Args:\n        cmd_list (list): List of command arguments.\n\n    Returns:\n        str: Shell-escaped command string.\n    \"\"\"\n    if platform.system() == \"Windows\":\n        return subprocess.list2cmdline(cmd_list)\n    else:\n        return shlex.join(cmd_list)\n\n\nif __name__ == \"__main__\":\n    main()",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.31766525452222444,
      "diff_lines": 418,
      "added_lines": 49,
      "removed_lines": 290,
      "diff_preview": "--- expected+++ predicted@@ -7,90 +7,45 @@ import tempfile\n import time\n from pathlib import Path\n-\n+import git\n from aider.dump import dump  # noqa: F401\n \n IMAGE_EXTENSIONS = {\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\", \".tiff\", \".webp\", \".pdf\"}\n \n \n-class IgnorantTemporaryDirectory:\n-    def __init__(self):\n-        if sys.version_info >= (3, 10):\n-            self.temp_dir = tempfile.TemporaryDirectory(ignore_cleanup_errors=True)\n-        else:\n-            self.temp_dir = tempfile.TemporaryDirectory()\n+class Spinner:\n+    unicode_spinner = [\"\u280b\", \"\u2819\", \"\u2839\", \"\u2838\", \"\u283c\", \"\u2834\", \"\u2826\", \"\u2827\", \"\u2807\", \"\u280f\"]\n+    ascii_spinner = [\"|\", \"/\", \"-\", \"\\\\\"]\n \n-    def __enter__(self):\n-        return self.temp_dir.__enter__()\n+    def __init__(self, text):\n+        self.text = text\n+        self.start_time = time.time()\n+        self.last_update = 0\n+        self.visible = False\n+        self.is_tty = sys.stdout.isatty()\n+        \n+        # Try unicode first, fall back to ascii if needed\n+        try:\n+            # Test if we can print unicode characters\n+            print(self.unicode_spinner[0], end=\"\", flush=True)\n+            print(\"\\r\", end=\"\", flush=True)\n+            self.spinner_chars = itertools.cycle(self.unicode_spinner)\n+        except UnicodeEncodeError:\n+            self.spinner_chars = itertools.cycle(self.ascii_spinner)\n \n-    def __exit__(self, exc_type, exc_val, exc_tb):\n-        self.cleanup()\n+    def step(self):\n+        if not self.is_tty:\n+            return\n+            \n+        current_time = time.time()\n+        if not self.visible and current_time - self.start_time >= 0.5:\n+            self.visible = True\n"
    },
    "llm_calls": 1,
    "elapsed_time": 30.413715362548828,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 57219,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_aider_voice.py",
    "exact_match": false,
    "similarity": 0.8173635063514764,
    "diff_lines": 165,
    "predicted_output": "import os\nimport queue\nimport tempfile\nimport time\nimport warnings\n\nfrom prompt_toolkit.shortcuts import prompt\nfrom pydub import AudioSegment  # noqa\nfrom aider.llm import litellm\n\nwarnings.filterwarnings(\n    \"ignore\", message=\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\"\n)\nwarnings.filterwarnings(\"ignore\", category=SyntaxWarning)\n\n\nclass SoundDeviceError(Exception):\n    pass\n\n\nclass Voice:\n    threshold = 0.15\n\n    def __init__(self, audio_format=\"wav\", device_name=None):\n        if sf is None:\n            raise SoundDeviceError\n        try:\n            import sounddevice as sd\n\n            self.sd = sd\n\n            devices = sd.query_devices()\n\n            if device_name:\n                # Find the device with matching name\n                device_id = None\n                for i, device in enumerate(devices):\n                    if device_name in device[\"name\"]:\n                        device_id = i\n                        break\n                if device_id is None:\n                    available_inputs = [d[\"name\"] for d in devices if d[\"max_input_channels\"] > 0]\n                    raise ValueError(\n                        f\"Device '{device_name}' not found. Available input devices:\"\n                        f\" {available_inputs}\"\n                    )\n\n                print(f\"Using input device: {device_name} (ID: {device_id})\")\n\n                self.device_id = device_id\n            else:\n                self.device_id = None\n\n        except (OSError, ModuleNotFoundError):\n            raise SoundDeviceError\n        if audio_format not in [\"wav\", \"mp3\", \"webm\"]:\n            raise ValueError(f\"Unsupported audio format: {audio_format}\")\n        self.audio_format = audio_format\n\n    def callback(self, indata, frames, time, status):\n        \"\"\"This is called (from a separate thread) for each audio block.\"\"\"\n        rms = np.sqrt(np.mean(indata**2))\n        self.max_rms = max(self.max_rms, rms)\n        self.min_rms = min(self.min_rms, rms)\n\n    def get_prompt(self):\n        num = 10\n        if math.isnan(self.pct) or self.pct < self.threshold:\n            cnt = 0\n        else:\n            cnt = int(self.pct * 10)\n\n        dur = time.time() - self.start_time\n        return f\"Recording, press ENTER when done... {dur:.1f}sec {'\u2588' * cnt}{'\u2591' * (num - cnt)}\"\n\n    def raw_record_and_transcribe(self, history, language):\n        self.q = queue.Queue()\n\n        temp_wav = tempfile.mktemp(suffix=\".wav\")\n\n        try:\n            sample_rate = int(self.sd.query_devices(self.device_id, \"input\")[\"default_samplerate\"])\n        except (TypeError, ValueError):\n            sample_rate = 16000  # fallback to 16kHz if unable to query device\n        except self.sd.PortAudioError:\n            raise SoundDeviceError(f\"Error accessing audio input device: {err}\")\n\n        self.start_time = time.time()\n\n        try:\n            with self.sd.InputStream(samplerate=sample_rate, channels=1, callback=self.callback,\n                                      device=self.device_id):\n                prompt(self.get_prompt, refresh_interval=0.1)\n        except self.sd.PortAudioError as err:\n            raise SoundDeviceError(f\"Error accessing audio input device: {err}\")\n\n        with sf.SoundFile(temp_wav, mode=\"x\", samplerate=sample_rate, channels=1) as file:\n            while not self.q.empty():\n                file.write(self.q.get())\n\n        # Check file size and offer to convert to mp3 if too large\n        file_size = os.path.getsize(temp_wav)\n        if file_size > 24.9 * 1024 * 1024 and self.audio_format == \"wav\":\n            print(\"\\nWarning: {temp_wav} is too large, switching to mp3 format.\")\n            use_audio_format = \"mp3\"\n        else:\n            use_audio_format = self.audio_format\n\n        if use_audio_format != \"wav\":\n            try:\n                new_filename = tempfile.mktemp(suffix=f\".{use_audio_format}\")\n                audio = AudioSegment.from_wav(temp_wav)\n                audio.export(new_filename, format=use_audio_format)\n                os.remove(temp_wav)\n                filename = new_filename\n            except (CouldntDecodeError, CouldntEncodeError) as e:\n                print(f\"Error converting audio: {e}\")\n                filename = temp_wav  # fall back to original file\n            except (OSError, FileNotFoundError) as e:\n                print(f\"File system error during conversion: {e}\")\n                filename = temp_wav  # fall back to original file\n            except Exception as e:\n                print(f\"Unexpected error during audio conversion: {e}\")\n                filename = temp_wav  # fall back to original file\n        else:\n            filename = temp_wav\n\n        with open(filename, \"rb\") as fh:\n            try:\n                transcript = litellm.transcription(\n                    model=\"whisper-1\", file=fh, prompt=history, language=language\n                )\n            except Exception as err:\n                print(f\"Unable to transcribe {filename}: {err}\")\n                return\n\n        if filename != temp_wav:\n            os.remove(filename)\n\n        text = transcript.text\n        return text\n\n    def record_and_transcribe(self, history=None, language=None):\n        try:\n            return self.raw_record_and_transcribe(history, language)\n        except KeyboardInterrupt:\n            return\n        except SoundDeviceError as e:\n            print(f\"Error: {e}\")\n            print(\"Please ensure you have a working audio input device connected and try again.\")\n            return\n\n\nif __name__ == \"__main__\":\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        raise ValueError(\"Please set the OPENAI_API_KEY environment variable.\")\n    print(Voice().record_and_transcribe())",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.8173635063514764,
      "diff_lines": 165,
      "added_lines": 22,
      "removed_lines": 51,
      "diff_preview": "--- expected+++ predicted@@ -1,4 +1,3 @@-import math\n import os\n import queue\n import tempfile\n@@ -6,10 +5,8 @@ import warnings\n \n from prompt_toolkit.shortcuts import prompt\n-\n+from pydub import AudioSegment  # noqa\n from aider.llm import litellm\n-\n-from .dump import dump  # noqa: F401\n \n warnings.filterwarnings(\n     \"ignore\", message=\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\"\n@@ -17,31 +14,17 @@ warnings.filterwarnings(\"ignore\", category=SyntaxWarning)\n \n \n-from pydub import AudioSegment  # noqa\n-from pydub.exceptions import CouldntDecodeError, CouldntEncodeError  # noqa\n-\n-try:\n-    import soundfile as sf\n-except (OSError, ModuleNotFoundError):\n-    sf = None\n-\n-\n class SoundDeviceError(Exception):\n     pass\n \n \n class Voice:\n-    max_rms = 0\n-    min_rms = 1e5\n-    pct = 0\n-\n     threshold = 0.15\n \n     def __init__(self, audio_format=\"wav\", device_name=None):\n         if sf is None:\n             raise SoundDeviceError\n         try:\n-            print(\"Initializing sound device...\")\n             import sounddevice as sd\n \n"
    },
    "llm_calls": 1,
    "elapsed_time": 34.94512581825256,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 23587,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_aider_watch.py",
    "exact_match": false,
    "similarity": 0.6219204220214378,
    "diff_lines": 295,
    "predicted_output": "import threading\nfrom pathlib import Path\nfrom typing import Optional\n\nfrom pathspec import PathSpec\nfrom pathspec.patterns import GitWildMatchPattern\nfrom watchfiles import watch\nfrom grep_ast import TreeContext\n\nfrom aider.dump import dump  # noqa\n\n\ndef load_gitignores(gitignore_paths: list[Path]) -> Optional[PathSpec]:\n    \"\"\"Load and parse multiple .gitignore files into a single PathSpec\"\"\"\n    if not gitignore_paths:\n        return None\n\n    patterns = [\n        \".aider*\",\n        \".git\",\n        # Common editor backup/temp files\n        \"*~\",  # Emacs/vim backup\n        \"*.bak\",  # Generic backup\n        \"*.swp\",  # Vim swap\n        \"*.swo\",  # Vim swap\n        \"#*#\",  # Emacs auto-save\n        \".#*\",  # Emacs lock files\n        \"*.tmp\",  # Generic temp files\n        \"*.temp\",  # Generic temp files\n        \"*.orig\",  # Merge conflict originals\n        \"*.pyc\",  # Python bytecode\n        \"__pycache__/\",  # Python cache dir\n        \".DS_Store\",  # macOS metadata\n        \"Thumbs.db\",  # Windows thumbnail cache\n        # IDE files\n        \".idea/\",  # JetBrains IDEs\n        \".vscode/\",  # VS Code\n        \"*.sublime-*\",  # Sublime Text\n        \".project\",  # Eclipse\n        \".settings/\",  # Eclipse\n        \"*.code-workspace\",  # VS Code workspace\n        # Environment files\n        \".env\",  # Environment variables\n        \".venv/\",  # Python virtual environments\n        \"node_modules/\",  # Node.js dependencies\n        \"vendor/\",  # Various dependencies\n        # Logs and caches\n        \"*.log\",  # Log files\n        \".cache/\",  # Cache directories\n        \".pytest_cache/\",  # Python test cache\n        \"coverage/\",  # Code coverage reports\n    ]  # Always ignore\n    for path in gitignore_paths:\n        if path.exists():\n            with open(path) as f:\n                patterns.extend(f.readlines())\n\n    return PathSpec.from_lines(GitWildMatchPattern, patterns) if patterns else None\n\n\nclass FileWatcher:\n    \"\"\"Watches source files for changes and AI comments\"\"\"\n\n    # Compiled regex pattern for AI comments\n    ai_comment_pattern = re.compile(r\"(?:#|//|--|;) *(ai\\b.*|ai\\b.*|.*\\bai[?!]?) *$\", re.IGNORECASE)\n\n    def __init__(self, coder, gitignores=None, verbose=False, analytics=None, root=None):\n        self.coder = coder\n        self.io = coder.io\n        self.root = Path(root) if root else Path(coder.root)\n        self.verbose = verbose\n        self.analytics = analytics\n        self.stop_event = None\n        self.watcher_thread = None\n        self.changed_files = set()\n        self.gitignore_spec = load_gitignores([Path(g) for g in gitignores] if gitignores else [])\n\n        coder.io.file_watcher = self\n\n    def filter_func(self, change_type, path):\n        \"\"\"Filter function for the file watcher\"\"\"\n        path_obj = Path(path)\n        path_abs = path_obj.absolute()\n\n        if not path_abs.is_relative_to(self.root.absolute()):\n            return False\n\n        rel_path = path_abs.relative_to(self.root)\n        if self.verbose:\n            dump(rel_path)\n\n        if self.gitignore_spec and self.gitignore_spec.match_file(rel_path.as_posix() + (\"/\" if path_abs.is_dir() else \"\")):\n            return False\n\n        if self.verbose:\n            dump(\"ok\", rel_path)\n\n        # Check if file contains AI markers\n        try:\n            content = self.io.read_text(str(path_abs), silent=True)\n            return bool(self.ai_comment_pattern.search(content))\n        except Exception:\n            return\n\n    def start(self):\n        \"\"\"Start watching for file changes\"\"\"\n        self.stop_event = threading.Event()\n        self.changed_files = set()\n\n        self.watcher_thread = threading.Thread(target=self.watch_files, daemon=True)\n        self.watcher_thread.start()\n\n    def watch_files(self):\n        \"\"\"Watch for file changes and process them\"\"\"\n        try:\n            roots_to_watch = self.get_roots_to_watch()\n            for changes in watch(*roots_to_watch, watch_filter=self.filter_func, stop_event=self.stop_event):\n                if self.handle_changes(changes):\n                    return\n        except Exception as e:\n            if self.verbose:\n                dump(f\"File watcher error: {e}\")\n            raise e\n\n    def handle_changes(self, changes):\n        \"\"\"Process the detected changes and update state\"\"\"\n        if not changes:\n            return False\n\n        changed_files = {str(Path(change[1])) for change in changes}\n        self.changed_files.update(changed_files)\n        self.io.interrupt_input()\n        return True\n\n    def get_roots_to_watch(self):\n        \"\"\"Determine which root paths to watch based on gitignore rules\"\"\"\n        if self.gitignore_spec:\n            roots = [\n                str(path)\n                for path in self.root.iterdir()\n                if not self.gitignore_spec.match_file(path.relative_to(self.root).as_posix() + (\"/\" if path.is_dir() else \"\"))\n            ]\n            # Fallback to watching root if all top-level items are filtered out\n            return roots if roots else [str(self.root)]\n        return [str(self.root)]\n\n    def process_changes(self):\n        \"\"\"Get any detected file changes\"\"\"\n        has_action = None\n        for fname in self.changed_files:\n            _, _, action = self.get_ai_comments(fname)\n            if action in (\"!\", \"?\"):\n                has_action = action\n\n            if fname in self.coder.abs_fnames:\n                continue\n            self.coder.abs_fnames.add(fname)\n            rel_fname = self.coder.get_rel_fname(fname)\n            self.io.tool_output(f\"Added {rel_fname} to the chat\")\n            self.io.tool_output()\n\n        if not has_action:\n            if added:\n                self.io.tool_output(\n                    \"End your comment with AI! to request changes or AI? to ask questions\"\n                )\n            return \"\"\n\n    def get_ai_comments(self, filepath):\n        \"\"\"Extract AI comment line numbers, comments, and action status from a file\"\"\"\n        line_nums = []\n        comments = []\n        has_action = None  # None, \"!\" or \"?\"\n        content = self.io.read_text(filepath, silent=True)\n        for i, line in enumerate(content.splitlines(), 1):\n            if match := self.ai_comment_pattern.search(line):\n                comment = match.group(0).strip()\n                if comment:\n                    line_nums.append(i)\n                    if comment.strip().endswith(\"!\"):\n                        has_action = \"!\"\n                    elif comment.strip().endswith(\"?\"):\n                        has_action = \"?\"\n        if not line_nums:\n            return None, None, None\n        return line_nums, comments, has_action\n\n\ndef main():\n    import argparse\n\n    parser = argparse.ArgumentParser(description=\"Watch source files for changes\")\n    parser.add_argument(\"directory\", help=\"Directory to watch\")\n    parser.add_argument(\"--gitignore\", action='append', help=\"Path to .gitignore file (can be specified multiple times)\")\n    args = parser.parse_args()\n\n    watcher = FileWatcher(args.directory, gitignores=args.gitignore)\n    try:\n        watcher.start()\n        while True:\n            if changes := watcher.get_changes():\n                for file in sorted(changes.keys()):\n                    print(file)\n                watcher.changed_files = None\n    except KeyboardInterrupt:\n        print(\"\\nStopped watching files\")\n        watcher.stop()\n\n\nif __name__ == \"__main__\":\n    main()",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.6219204220214378,
      "diff_lines": 295,
      "added_lines": 44,
      "removed_lines": 143,
      "diff_preview": "--- expected+++ predicted@@ -1,15 +1,13 @@-import re\n import threading\n from pathlib import Path\n from typing import Optional\n \n-from grep_ast import TreeContext\n from pathspec import PathSpec\n from pathspec.patterns import GitWildMatchPattern\n from watchfiles import watch\n+from grep_ast import TreeContext\n \n from aider.dump import dump  # noqa\n-from aider.watch_prompts import watch_ask_prompt, watch_code_prompt\n \n \n def load_gitignores(gitignore_paths: list[Path]) -> Optional[PathSpec]:\n@@ -25,7 +23,7 @@         \"*.bak\",  # Generic backup\n         \"*.swp\",  # Vim swap\n         \"*.swo\",  # Vim swap\n-        \"\\\\#*\\\\#\",  # Emacs auto-save\n+        \"#*#\",  # Emacs auto-save\n         \".#*\",  # Emacs lock files\n         \"*.tmp\",  # Generic temp files\n         \"*.temp\",  # Generic temp files\n@@ -64,7 +62,7 @@     \"\"\"Watches source files for changes and AI comments\"\"\"\n \n     # Compiled regex pattern for AI comments\n-    ai_comment_pattern = re.compile(r\"(?:#|//|--|;+) *(ai\\b.*|ai\\b.*|.*\\bai[?!]?) *$\", re.IGNORECASE)\n+    ai_comment_pattern = re.compile(r\"(?:#|//|--|;) *(ai\\b.*|ai\\b.*|.*\\bai[?!]?) *$\", re.IGNORECASE)\n \n     def __init__(self, coder, gitignores=None, verbose=False, analytics=None, root=None):\n         self.coder = coder\n@@ -75,11 +73,7 @@         self.stop_event = None\n         self.watcher_thread = None\n         self.changed_files = set()\n-        self.gitignores = gitignores\n-\n-        self.gitignore_spec = load_gitignores(\n-            [Path(g) for g in self.gitignores] if self.gitignores else []\n-        )\n+        self.gitignore_spec = load_gitignores([Path(g) for g in gitignores] if gitignores else [])\n \n         coder.io.file_watcher = self\n \n"
    },
    "llm_calls": 1,
    "elapsed_time": 44.95755100250244,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 56211,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_aider_website__includes_leaderboard_table.js",
    "exact_match": false,
    "similarity": 0.5199797673242287,
    "diff_lines": 425,
    "predicted_output": "document.addEventListener('DOMContentLoaded', function() {\n  let currentMode = 'view'; // 'view', 'select', 'detail'\n  let selectedRows = new Set(); // Store indices of selected rows\n  const MAX_DISPLAY_COST_CAP = 75; // Define the constant here\n\n  const allMainRows = document.querySelectorAll('tr[id^=\"main-row-\"]');\n  const allDetailsRows = document.querySelectorAll('tr[id^=\"details-\"]');\n  const searchInput = document.getElementById('editSearchInput');\n  const modeViewButton = document.getElementById('mode-view-btn');\n  const modeDetailButton = document.getElementById('mode-detail-btn');\n  const modeSelectButton = document.getElementById('mode-select-btn');\n  const modeButtons = [modeViewButton, modeSelectButton, modeDetailButton];\n  const selectAllCheckbox = document.getElementById('select-all-checkbox');\n  const leaderboardTitle = document.getElementById('leaderboard-title'); // Get title element\n  const defaultTitle = \"Aider polyglot coding leaderboard\";\n  const filteredTitle = \"Aider polyglot coding benchmark results (selected)\";\n\n  function applySearchFilter() {\n    const searchTerm = searchInput.value.toLowerCase();\n    allMainRows.forEach(row => {\n      const textContent = row.textContent.toLowerCase();\n      const detailsRow = document.getElementById(row.id.replace('main-row-', 'details-'));\n      const matchesSearch = textContent.includes(searchTerm);\n\n      if (matchesSearch) {\n        row.classList.remove('hidden-by-search');\n        if (detailsRow) detailsRow.classList.remove('hidden-by-search');\n      } else {\n        row.classList.add('hidden-by-search');\n        if (detailsRow) detailsRow.classList.add('hidden-by-search');\n      }\n    });\n    // After applying search filter, re-apply view mode filter and update select-all state\n    updateTableView(currentMode);\n    if (currentMode === 'select') {\n        updateSelectAllCheckboxState();\n    }\n  }\n\n  function getVisibleMainRows() {\n      // Helper to get rows currently visible (not hidden by search or mode)\n      return Array.from(allMainRows).filter(row =>\n          !row.classList.contains('hidden-by-search') && !row.classList.contains('hidden-by-mode')\n      );\n  }\n\n  function updateSelectAllCheckboxState() {\n      // Update the header checkbox based on the selection state of *visible* rows\n      if (currentMode !== 'select') return; // Only relevant in select mode\n\n      const visibleRows = getVisibleMainRows();\n      const visibleRowCount = visibleRows.length;\n      const selectedVisibleRowCount = visibleRows.filter(row => selectedRows.has(row.querySelector('.row-selector')?.dataset.rowIndex)).length;\n\n      if (visibleRowCount === 0) {\n          selectAllCheckbox.checked = false;\n          selectAllCheckbox.indeterminate = false;\n      } else if (selectedVisibleRowCount === visibleRowCount) {\n          selectAllCheckbox.checked = true;\n          selectAllCheckbox.indeterminate = false;\n      } else if (selectedVisibleRowCount > 0) {\n          selectAllCheckbox.checked = false;\n          selectAllCheckbox.indeterminate = true;\n      } else {\n          selectAllCheckbox.checked = false;\n          selectAllCheckbox.indeterminate = false;\n      }\n  }\n\n  function updateTableView(mode) {\n    currentMode = mode; // Update global state ('view', 'select', 'detail')\n\n    // Update button styles first\n    modeButtons.forEach(btn => {\n        btn.classList.remove('active');\n        // Reset specific styles potentially added by .active\n        btn.style.backgroundColor = '';\n        btn.style.color = '';\n    });\n    let activeButton;\n    if (mode === 'view') activeButton = modeViewButton;\n    else if (mode === 'select') activeButton = modeSelectButton;\n    else if (mode === 'detail') activeButton = modeDetailButton;\n\n    activeButton.classList.add('active');\n    activeButton.style.backgroundColor = '#e7f3ff'; // Use selected row highlight blue\n    activeButton.style.color = '#495057'; // Use dark text for contrast on light blue\n\n    // Get the first header cell (for the toggle/checkbox column)\n    const firstHeaderCell = document.querySelector('table thead th:first-child');\n\n    // Show/hide header checkbox based on mode\n    selectAllCheckbox.style.display = mode === 'select' ? 'inline-block' : 'none';\n\n    allMainRows.forEach(row => {\n      const rowIndex = row.querySelector('.row-selector')?.dataset.rowIndex;\n      const toggleButton = row.querySelector('.toggle-details');\n      const selectorCheckbox = row.querySelector('.row-selector');\n      const detailsRow = document.getElementById(`details-${rowIndex}`);\n      const isSelected = selectedRows.has(rowIndex);\n      \n      row.classList.remove('hidden-by-mode');\n      if (detailsRow) detailsRow.classList.remove('hidden-by-mode');\n\n      // Show/hide the first column (header and data cells) based on mode\n      if (firstHeaderCell) {\n          firstHeaderCell.style.display = mode === 'view' ? 'none' : '';\n      }\n      if (firstCell) {\n          firstCell.style.display = mode === 'view' ? 'none' : '';\n      }\n\n      // Apply mode-specific logic\n      if (mode === 'view') { // --- VIEW MODE ---\n          toggleButton.style.display = 'none'; // Hide toggle in view mode\n          selectorCheckbox.style.display = 'none';\n          row.classList.remove('row-selected'); // Ensure no selection highlight\n\n          // In 'view' mode, hide row if selections exist AND this row is NOT selected\n          if (selectedRows.size > 0 && !isSelected) {\n              row.classList.add('hidden-by-mode');\n              if (detailsRow) detailsRow.classList.add('hidden-by-mode');\n          } else {\n              // Ensure row is not hidden by mode if it's selected or no selections exist\n              row.classList.remove('hidden-by-mode');\n              if (detailsRow) detailsRow.classList.remove('hidden-by-mode');\n          }\n          // Always hide details row content in view mode regardless of visibility class\n          if (detailsRow) {\n              detailsRow.style.display = 'none';\n          }\n\n      } else if (mode === 'select') { // --- SELECT MODE ---\n          toggleButton.style.display = 'none';\n          selectorCheckbox.style.display = 'inline-block';\n          selectorCheckbox.checked = isSelected;\n          row.classList.toggle('row-selected', isSelected);\n\n          // Always hide details row in select mode\n          if (detailsRow) detailsRow.style.display = 'none';\n\n          // In 'select' mode, no rows should be hidden based on selection status\n          row.classList.remove('hidden-by-mode');\n          if (detailsRow) detailsRow.classList.remove('hidden-by-mode');\n\n      } else { // --- DETAIL MODE --- (mode === 'detail')\n          toggleButton.style.display = 'inline-block'; // Show toggle\n          selectorCheckbox.style.display = 'none';\n          row.classList.remove('row-selected'); // Clear selection highlight\n          // Details row visibility is controlled by the toggle button state, don't force hide/show here\n          // Ensure main row is visible if not hidden by search\n          row.classList.remove('hidden-by-mode');\n          if (detailsRow) {\n              detailsRow.classList.remove('hidden-by-mode');\n              // Preserve existing display state (controlled by toggle) unless hidden by search\n              if (detailsRow.classList.contains('hidden-by-search')) {\n                  detailsRow.style.display = 'none';\n              }\n          }\n      }\n    });\n\n    // Update the select-all checkbox state after updating the view\n    updateSelectAllCheckboxState();\n    \n    // Update leaderboard title based on mode and selection\n    if (leaderboardTitle) {\n      if (currentMode === 'view' && selectedRows.size > 0) {\n        leaderboardTitle.textContent = filteredTitle;\n      } else {\n        leaderboardTitle.textContent = defaultTitle;\n      }\n    }\n  }\n\n  // --- Existing Initializations ---\n  updateTableView('view'); // Initialize view to 'view' mode\n  applySearchFilter(); // Apply initial search filter (if any text is pre-filled or just to set initial state)\n\n  // New Event Listeners...\n});",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.5199797673242287,
      "diff_lines": 425,
      "added_lines": 9,
      "removed_lines": 343,
      "diff_preview": "--- expected+++ predicted@@ -35,10 +35,6 @@     if (currentMode === 'select') {\n         updateSelectAllCheckboxState();\n     }\n-    \n-    // Update cost bars and ticks since visible rows may have changed\n-    updateCostBars();\n-    updateCostTicks();\n   }\n \n   function getVisibleMainRows() {\n@@ -71,7 +67,6 @@       }\n   }\n \n-\n   function updateTableView(mode) {\n     currentMode = mode; // Update global state ('view', 'select', 'detail')\n \n@@ -101,11 +96,9 @@       const rowIndex = row.querySelector('.row-selector')?.dataset.rowIndex;\n       const toggleButton = row.querySelector('.toggle-details');\n       const selectorCheckbox = row.querySelector('.row-selector');\n-      const firstCell = row.querySelector('td:first-child'); // Get the first cell of the main row\n       const detailsRow = document.getElementById(`details-${rowIndex}`);\n       const isSelected = selectedRows.has(rowIndex);\n-\n-      // Reset visibility classes before applying mode logic\n+      \n       row.classList.remove('hidden-by-mode');\n       if (detailsRow) detailsRow.classList.remove('hidden-by-mode');\n \n@@ -122,7 +115,6 @@           toggleButton.style.display = 'none'; // Hide toggle in view mode\n           selectorCheckbox.style.display = 'none';\n           row.classList.remove('row-selected'); // Ensure no selection highlight\n-          // view-highlighted is handled by row click listener\n \n           // In 'view' mode, hide row if selections exist AND this row is NOT selected\n           if (selectedRows.size > 0 && !isSelected) {\n@@ -130,9 +122,8 @@               if (detailsRow) detailsRow.classList.add('hidden-by-mode');\n           } else {\n               // Ensure row is not hidden by mode if it's selected or no selections exist\n-              // This is handled by the reset at the start of the loop:\n-              // row.classList.remove('hidden-by-mode');\n-              // if (detailsRow) detailsRow.classList.remove('hidden-by-mode');\n+              row.classList.remove('hidden-by-mode');\n"
    },
    "llm_calls": 1,
    "elapsed_time": 39.679619789123535,
    "repo": "aider",
    "language": "JavaScript",
    "prompt_tokens": 18588,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_aider_website__includes_recording.js",
    "exact_match": false,
    "similarity": 0.3936112022754622,
    "diff_lines": 354,
    "predicted_output": "document.addEventListener('DOMContentLoaded', function() {\n  let player; // Store player reference to make it accessible to click handlers\n  let globalAudio; // Global audio element to be reused\n  \n  // Detect if device likely has no physical keyboard\n  function detectNoKeyboard() {\n    // Check if it's a touch device (most mobile devices)\n    const isTouchDevice = ('ontouchstart' in window) || \n                         (navigator.maxTouchPoints > 0) ||\n                         (navigator.msMaxTouchPoints > 0);\n                         \n    // Check common mobile user agents as additional signal\n    const isMobileUA = /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);\n    \n    // If it's a touch device and has a mobile user agent, likely has no physical keyboard\n    if (isTouchDevice && isMobileUA) {\n      document.body.classList.add('no-physical-keyboard');\n    }\n  }\n  \n  // Run detection\n  detectNoKeyboard();\n  \n  // Parse the transcript section to create markers and convert timestamps to links\n  function parseTranscript() {\n    const markers = [];\n    // Find the Commentary heading\n    const transcriptHeading = Array.from(document.querySelectorAll('h2')).find(el => el.textContent.trim() === 'Commentary');\n    \n    if (transcriptHeading) {\n      // Get all list items after the transcript heading\n      let currentElement = transcriptHeading.nextElementSibling;\n      \n      while (currentElement && currentElement.tagName === 'UL') {\n        const listItems = currentElement.querySelectorAll('li');\n        \n        listItems.forEach(item => {\n          const text = item.textContent.trim();\n          const match = text.match(/(\\d+):(\\d+)\\s+(.*)/);\n          \n          if (match) {\n            const minutes = parseInt(match[1], 10);\n            const seconds = parseInt(match[2], 10);\n            const timeInSeconds = minutes * 60 + seconds;\n            const formattedTime = `${minutes}:${seconds.toString().padStart(2, '0')}`;\n            const message = match[3].trim();\n            \n            // Create link for the timestamp\n            const timeLink = document.createElement('a');\n            timeLink.href = '#';\n            timeLink.textContent = formattedTime;\n            timeLink.className = 'timestamp-link';\n            timeLink.dataset.time = timeInSeconds;\n            timeLink.dataset.message = message;\n            \n            // Add click event to seek the player\n            timeLink.addEventListener('click', function(e) {\n              e.preventDefault();\n              if (player && typeof player.seek === 'function') {\n                player.seek(timeInSeconds);\n                player.play();\n                \n                // Also trigger toast and speech\n                showToast(message);\n                speakText(message, timeInSeconds);\n                \n                // Highlight this timestamp\n                highlightTimestamp(timeInSeconds);\n              }\n            });\n            \n            // Replace text with the link + message\n            item.textContent = '';\n            item.appendChild(timeLink);\n            item.appendChild(document.createTextNode(' ' + message));\n            \n            markers.push([timeInSeconds, message]);\n          }\n        });\n        \n        currentElement = currentElement.nextElementSibling;\n      }\n    }\n    \n    return markers;\n  }\n\n  const url = \"https://gist.githubusercontent.com/paul-gauthier/3011ab9455c2d28c0e5a60947202752f/raw/5a5b3dbf68a9c2b22b4954af287efedecdf79d52/tmp.redacted.cast\";\n  \n  // Parse transcript and create markers\n  const markers = parseTranscript();\n  \n  // Create player with a single call\n  player = AsciinemaPlayer.create(\n    url,\n    document.getElementById('demo'),\n    {\n      speed: 1.25,\n      idleTimeLimit: 1,\n      theme: \"aider\",\n      poster: \"npt:0:01\",\n      markers: markers,\n      controls: true\n    }\n  );\n});",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.3936112022754622,
      "diff_lines": 354,
      "added_lines": 3,
      "removed_lines": 325,
      "diff_preview": "--- expected+++ predicted@@ -74,30 +74,6 @@             item.appendChild(timeLink);\n             item.appendChild(document.createTextNode(' ' + message));\n             \n-            // Add class and click handler to the entire list item\n-            item.classList.add('transcript-item');\n-            item.dataset.time = timeInSeconds;\n-            item.dataset.message = message;\n-            \n-            item.addEventListener('click', function(e) {\n-              // Prevent click event if the user clicked directly on the timestamp link\n-              // This prevents double-firing of the event\n-              if (e.target !== timeLink) {\n-                e.preventDefault();\n-                if (player && typeof player.seek === 'function') {\n-                  player.seek(timeInSeconds);\n-                  player.play();\n-                  \n-                  // Also trigger toast and speech\n-                  showToast(message);\n-                  speakText(message, timeInSeconds);\n-                  \n-                  // Highlight this timestamp\n-                  highlightTimestamp(timeInSeconds);\n-                }\n-              }\n-            });\n-            \n             markers.push([timeInSeconds, message]);\n           }\n         });\n@@ -109,12 +85,14 @@     return markers;\n   }\n \n+  const url = \"https://gist.githubusercontent.com/paul-gauthier/3011ab9455c2d28c0e5a60947202752f/raw/5a5b3dbf68a9c2b22b4954af287efedecdf79d52/tmp.redacted.cast\";\n+  \n   // Parse transcript and create markers\n   const markers = parseTranscript();\n   \n   // Create player with a single call\n   player = AsciinemaPlayer.create(\n-    recording_url,\n+    url,\n     document.getElementById('demo'),\n     {\n       speed: 1.25,\n@@ -125,304 +103,4 @@"
    },
    "llm_calls": 1,
    "elapsed_time": 25.187837839126587,
    "repo": "aider",
    "language": "JavaScript",
    "prompt_tokens": 10768,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_benchmark_over_time.py",
    "exact_match": false,
    "similarity": 0.9275249072084392,
    "diff_lines": 81,
    "predicted_output": "import matplotlib.pyplot as plt\nimport yaml\nfrom imgcat import imgcat\nfrom matplotlib import rc\nfrom aider.dump import dump  # noqa: 401\n\nLABEL_FONT_SIZE = 16  # Font size for scatter plot dot labels\n\n\nclass ModelData:\n    name: str\n    release_date: date\n    pass_rate: float\n\n    @property\n    def color(self) -> str:\n        model = self.name.lower()\n        if \"gemini\" in model and \"pro\" in model:\n            return \"magenta\"\n        if \"qwen\" in model and \"pro\" in model:\n            return \"darkblue\"\n        if \"mistral\" in model:\n            return \"cyan\"\n        if \"haiku\" in model:\n            return \"pink\"\n        if \"deepseek\" in model:\n            return \"brown\"\n        if \"sonnet\" in model:\n            return \"orange\"\n        if \"-4o\" in model:\n            return \"purple\"\n        if \"gpt-4\" in model:\n            return \"red\"\n        if \"gpt-3.5\" in model:\n            return \"green\"\n        return \"lightblue\"\n\n    @property\n    def legend_label(self) -> str:\n        model = self.name.lower()\n        if \"claude-3-sonnet\" in model:\n            return \"Claude 3 Sonnet\"\n        if \"o1-preview\" in model:\n            return \"O1 Preview\"\n        if \"gpt-3.5\" in model:\n            return \"GPT-3.5 Turbo\"\n        if \"gpt-4-\" in model and \"-4o\" not in model:\n            return \"GPT-4\"\n        if \"qwen\" in model:\n            return \"Qwen\"\n        if \"-4o\" in model:\n            return \"GPT-4o\"\n        if \"haiku\" in model:\n            return \"Haiku\"\n        if \"deepseek\" in model:\n            return \"DeepSeek\"\n        if \"mistral\" in model:\n            return \"Mistral\"\n        return model\n\n\nclass BenchmarkPlotter:\n    LABEL_FONT_SIZE = 16\n\n    def __init__(self):\n        self.setup_plot_style()\n\n    def setup_plot_style(self):\n        plt.rcParams[\"hatch.linewidth\"] = 0.5\n        plt.rcParams[\"hatch.color\"] = \"#444444\"\n        rc(\"font\", **{\"family\": \"sans-serif\", \"sans-serif\": [\"Helvetica\"], \"size\": 10})\n        plt.rcParams[\"text.color\"] = \"#444444\"\n\n    def load_data(self, yaml_file: str) -> List[ModelData]:\n        with open(yaml_file, \"r\") as file:\n            data = yaml.safe_load(file)\n\n        models = []\n        for entry in data:\n            if \"released\" in entry and \"pass_rate_2\" in entry:\n                model = ModelData(\n                    name=entry[\"model\"].split(\"(\")[0].strip(),\n                    release_date=entry[\"released\"],\n                    pass_rate=entry[\"pass_rate_2\"],\n                )\n                models.append(model)\n        return models\n\n    def create_figure(self) -> Tuple[plt.Figure, plt.Axes]:\n        fig, ax = plt.subplots(figsize=(12, 8))\n        ax.grid(axis=\"y\", zorder=0, lw=0.2)\n        for spine in ax.spines.values():\n            spine.set_edgecolor(\"#DDDDDD\")\n            spine.set_linewidth(0.5)\n        return fig, ax\n\n    def plot_model_series(self, ax: plt.Axes, models: List[ModelData]):\n        color_groups: Dict[str, List[ModelData]] = {}\n        for model in models:\n            if model.color not in color_groups:\n                color_groups[model.color] = []\n            color_groups[model.color].append(model)\n\n        for color, group in color_groups.items():\n            sorted_group = sorted(group, key=lambda x: x.release_date)\n            dates = [m.release_date for m in sorted_group]\n            rates = [m.pass_rate for m in sorted_group]\n\n            ax.plot(dates, rates, c=color, alpha=0.5, linewidth=1)\n            ax.scatter(dates, rates, c=color, alpha=0.5, s=120)\n\n            first_model = sorted_group[0]\n            ax.annotate(\n                first_model.legend_label,\n                (first_model.release_date, first_model.pass_rate),\n                xytext=(10, 5),\n                textcoords=\"offset points\",\n                color=color,\n                alpha=0.8,\n                fontsize=self.LABEL_FONT_SIZE,\n            )\n\n    def set_labels_and_style(self, ax: plt.Axes):\n        ax.set_xlabel(\"Model release date\", fontsize=18, color=\"#555\")\n        ax.set_ylabel(\n            \"Aider code editing benchmark,\\npercent completed correctly\", fontsize=18, color=\"#555\"\n        )\n        ax.set_title(\"LLM code editing skill by model release date\", fontsize=20)\n        ax.set_ylim(30, 90)\n        plt.xticks(fontsize=14, rotation=45, ha=\"right\")\n        plt.tight_layout(pad=1.0)\n\n    def save_and_display(self, fig: plt.Figure):\n        plt.savefig(\"aider/website/assets/models-over-time.png\")\n        plt.savefig(\"aider/website/assets/models-over-time.svg\")\n        imgcat(fig)\n\n    def plot(self, yaml_file: str):\n        models = self.load_data(yaml_file)\n        fig, ax = self.create_figure()\n        self.plot_model_series(ax, models)\n        self.set_labels_and_style(ax)\n        self.save_and_display(fig)\n\n\ndef main():\n    plotter = BenchmarkPlotter()\n    plotter.plot(\"aider/website/_data/edit_leaderboard.yml\")\n\n\nif __name__ == \"__main__\":\n    main()",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9275249072084392,
      "diff_lines": 81,
      "added_lines": 5,
      "removed_lines": 21,
      "diff_preview": "--- expected+++ predicted@@ -1,14 +1,12 @@-from dataclasses import dataclass\n-from datetime import date\n-from typing import Dict, List, Tuple\n-\n import matplotlib.pyplot as plt\n import yaml\n from imgcat import imgcat\n from matplotlib import rc\n+from aider.dump import dump  # noqa: 401\n+\n+LABEL_FONT_SIZE = 16  # Font size for scatter plot dot labels\n \n \n-@dataclass\n class ModelData:\n     name: str\n     release_date: date\n@@ -19,7 +17,7 @@         model = self.name.lower()\n         if \"gemini\" in model and \"pro\" in model:\n             return \"magenta\"\n-        if \"qwen\" in model:\n+        if \"qwen\" in model and \"pro\" in model:\n             return \"darkblue\"\n         if \"mistral\" in model:\n             return \"cyan\"\n@@ -40,10 +38,8 @@     @property\n     def legend_label(self) -> str:\n         model = self.name.lower()\n-        if \"gemini\" in model and \"pro\" in model:\n-            return \"Gemini 1.5 Pro\"\n         if \"claude-3-sonnet\" in model:\n-            return \"Sonnet\"\n+            return \"Claude 3 Sonnet\"\n         if \"o1-preview\" in model:\n             return \"O1 Preview\"\n         if \"gpt-3.5\" in model:\n@@ -99,26 +95,20 @@         return fig, ax\n \n     def plot_model_series(self, ax: plt.Axes, models: List[ModelData]):\n-        # Group models by color\n         color_groups: Dict[str, List[ModelData]] = {}\n         for model in models:\n             if model.color not in color_groups:\n                 color_groups[model.color] = []\n"
    },
    "llm_calls": 1,
    "elapsed_time": 35.298487186431885,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 35454,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_benchmark_problem_stats.py",
    "exact_match": false,
    "similarity": 0.6993261004636047,
    "diff_lines": 232,
    "predicted_output": "#!/usr/bin/env python\n\nimport argparse\nimport json\nfrom collections import defaultdict\nfrom pathlib import Path\nimport yaml\n\nHARD_SET_NUM = 3  # Number of models that defines the hard set threshold\n\n\ndef get_dirs_from_leaderboard():\n    # Load the leaderboard data\n    with open(\"aider/website/_data/polyglot_leaderboard.yml\") as f:\n        leaderboard = yaml.safe_load(f)\n    return [(entry[\"dirname\"], entry[\"model\"]) for entry in leaderboard]\n\n\ndef load_results(dirname):\n    \"\"\"Load all result files from a benchmark directory\"\"\"\n    dirname = Path(dirname)\n    benchmark_dir = dirname\n    if not benchmark_dir.exists():\n        benchmark_dir = Path(\"tmp.benchmarks\") / dirname\n        if not benchmark_dir.exists():\n            return None\n\n    all_results = []\n    parse_errors = []  # Track which exercises had parse errors for this model\n\n    # Look in language subdirectories under exercises/practice\n    for fname in benchmark_dir.glob(\"*/exercises/practice/*/.aider.results.json\"):\n        error = False\n        try:\n            results = json.loads(fname.read_text())\n            error = \"testcase\" not in results\n            if not error:\n                # Add language info to results\n                lang = fname.parts[-5]  # Get language from path\n                results[\"language\"] = lang\n                all_results.append(results)\n\n        except json.JSONDecodeError:\n            error = True\n\n        if error:\n            # Track the parse error for this exercise/model combination\n            lang = fname.parts[-5]\n            exercise = f\"{fname.parts[-2]}/{lang}\"  # Use directory name as testcase\n            parse_errors.append(exercise)\n            print(f\"Bad results file {fname}\")\n            continue\n\n    return all_results, parse_errors\n\n\ndef analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):\n    PARSE_ERROR_M = 4  # Threshold for number of parse errors to DQ an exercise\n\n    if dirs is None:\n        # Use leaderboard data if no directories specified\n        dir_entries = get_dirs_from_leaderboard()\n    else:\n        # Use provided directories, with dirname as model name\n        dir_entries = [(d, d) for d in dirs]\n\n    # Filter out entries that don't load and sort by pass rate\n    valid_entries = []\n    parse_errors_by_model = {}  # Track which exercises had parse errors for each model\n\n    for dirname, model in dir_entries:\n        results_data = load_results(dirname)\n        if results_data:\n            results, model_parse_errors = results_data\n            parse_errors_by_model[model] = set(model_parse_errors)\n            # Calculate pass rate for sorting when using custom dirs\n            if dirs is not None:\n                pass_rate = sum(\n                    1 for r in results if r.get(\"tests_outcomes\", []) and r[\"tests_outcomes\"][-1]\n                ) / len(results)\n            else:\n                pass_rate = next(\n                    (entry[\"pass_rate_2\"] for entry in yaml.safe_load(open(\"aider/website/_data/polyglot_leaderboard.yml\") if entry[\"dirname\"] == dirname), 0)\n                )\n            valid_entries.append(((dirname, model), results, float(pass_rate)))\n\n    # Collect the hard set (exercises solved by HARD_SET_NUM or fewer models)\n    print(f\"\\nHard Set Analysis (exercises solved by \u2264{HARD_SET_NUM} models):\")\n    print(\"-\" * 60)\n    hard_set = {\n        ex for ex, models in exercise_solutions.items()\n        if len(models) <= HARD_SET_NUM and ex not in disqualified_exercises\n    }\n    print(f\"Total hard set exercises: {len(hard_set)}\")\n\n    # Count total problems, unsolved problems, and hard set problems by language\n    lang_totals = defaultdict(int)\n    lang_unsolved = defaultdict(int)\n    lang_hard_set = defaultdict(int)\n\n    for exercise in all_exercises:\n        lang = exercise.split(\"/\")[1]  # Get language from path\n        lang_totals[lang] += 1\n        if not exercise_solutions[exercise]:  # No models solved this exercise\n            lang_unsolved[lang] += 1\n        if exercise in hard_set:  # Exercise is in the hard set\n            lang_hard_set[lang] += 1\n\n    print(\"\\nUnsolved and hard set problems by language:\")\n    print(f\"{'Language':<12} {'Unsolved':>8} {'Hard Set':>9} {'Total':>7} {'% hard Unsolved':>8}\")\n    print(\"-\" * 47)\n    for lang in sorted(lang_totals.keys()):\n        count = lang_unsolved[lang]\n        hard = lang_hard_set[lang]\n        total = lang_totals[lang]\n        pct = (count / hard) * 100 if hard else -1\n        print(f\"{lang:<12} {count:>8} {hard:>9} {total:>7} {pct:>7.1f}%\")\n    print()\n\n    # For each model, compute performance on hard set\n    model_hard_stats = []\n    for (dirname, model), results, _ in valid_entries:\n        if not results:\n            continue\n\n        solved_hard = 0\n        for result in results:\n            testcase = result.get(\"testcase\")\n            if not testcase:\n                continue\n            lang = result.get(\"language\")\n            if not lang:\n                continue\n\n            testcase = f\"{testcase}/{lang}\"\n            if testcase in hard_set:\n                tests_outcomes = result.get(\"tests_outcomes\", [])\n                if tests_outcomes and tests_outcomes[-1]:\n                    solved_hard += 1\n\n        pct = (solved_hard / len(hard_set)) * 100\n        model_hard_stats.append((model, solved_hard, pct))\n\n    # Sort by number solved\n    model_hard_stats.sort(key=lambda x: x[1], reverse=True)\n\n    print(\"\\nModel performance on hard set:\")\n    print(f\"{'Model':<55} {'Solved':<8} {'Percent':>7}\")\n    print(\"-\" * 50)\n    for model, solved, pct in model_hard_stats:\n        print(f\"{model:<55} {solved:>6d}   {pct:>6.1f}%\")\n\n    if copy_hard_set:\n        # Create hard set directory\n        src_dir = Path(\"tmp.benchmarks/exercism\")\n        dst_dir = Path(\"tmp.benchmarks/exercism-polyglot\")\n\n        if dst_dir.exists():\n            print(f\"\\nError: Destination directory {dst_dir} already exists\")\n            return\n\n        print(f\"\\nCopying hard set problems to {dst_dir}...\")\n\n        # Create a set of (exercise, language) pairs from hard_set\n        hard_set_pairs = {tuple(exercise.split(\"/\")) for exercise in hard_set}\n\n        # Copy each hard set problem's directory\n        for lang_dir in src_dir.glob(\"*/exercises/practice\"):\n            if not lang_dir.is_dir():\n                continue\n\n            lang = lang_dir.parts[-3]  # Get language from path\n            for problem_dir in lang_dir.glob(\"*\"):\n                if (problem_dir.name, lang) in hard_set_pairs:\n                    rel_path = problem_dir.relative_to(src_dir)\n                    dst_path = dst_dir / rel_path\n                    dst_path.parent.mkdir(parents=True, exist_ok=True)\n                    shutil.copytree(problem_dir, dst_path)\n\n        print(\"Done copying hard set problems\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--topn\", type=int, help=\"Only consider top N models by pass rate\")\n    parser.add_argument(\n        \"dirs\", nargs=\"*\", help=\"Directories to analyze (optional, defaults to leaderboard entries)\"\n    )\n    parser.add_argument(\n        \"--copy-hard-set\",\n        action=\"store_true\",\n        help=\"Copy hard set problems to tmp.benchmarks/exercism-polygot\",\n    )\n    args = parser.parse_args()\n\n    analyze_exercise_solutions(args.dirs if args.dirs else None, args.topn, args.copy_hard_set)",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.6993261004636047,
      "diff_lines": 232,
      "added_lines": 4,
      "removed_lines": 163,
      "diff_preview": "--- expected+++ predicted@@ -2,13 +2,9 @@ \n import argparse\n import json\n-import shutil\n from collections import defaultdict\n from pathlib import Path\n-\n import yaml\n-\n-from aider.dump import dump  # noqa\n \n HARD_SET_NUM = 3  # Number of models that defines the hard set threshold\n \n@@ -23,7 +19,6 @@ def load_results(dirname):\n     \"\"\"Load all result files from a benchmark directory\"\"\"\n     dirname = Path(dirname)\n-\n     benchmark_dir = dirname\n     if not benchmark_dir.exists():\n         benchmark_dir = Path(\"tmp.benchmarks\") / dirname\n@@ -73,11 +68,8 @@     valid_entries = []\n     parse_errors_by_model = {}  # Track which exercises had parse errors for each model\n \n-    dump(dir_entries)\n-\n     for dirname, model in dir_entries:\n         results_data = load_results(dirname)\n-\n         if results_data:\n             results, model_parse_errors = results_data\n             parse_errors_by_model[model] = set(model_parse_errors)\n@@ -87,162 +79,16 @@                     1 for r in results if r.get(\"tests_outcomes\", []) and r[\"tests_outcomes\"][-1]\n                 ) / len(results)\n             else:\n-                # Use existing pass rate from leaderboard\n                 pass_rate = next(\n-                    (\n-                        entry[\"pass_rate_2\"]\n-                        for entry in yaml.safe_load(\n-                            open(\"aider/website/_data/polyglot_leaderboard.yml\")\n-                        )\n-                        if entry[\"dirname\"] == dirname\n-                    ),\n-                    0,\n"
    },
    "llm_calls": 1,
    "elapsed_time": 44.53807067871094,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 29665,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_scripts_blame.py",
    "exact_match": false,
    "similarity": 0.7756941940137035,
    "diff_lines": 259,
    "predicted_output": "#!/usr/bin/env python3\n\nimport argparse\nimport subprocess\nimport sys\nfrom collections import defaultdict\nfrom datetime import datetime\nfrom operator import itemgetter\nimport semver\nimport yaml\nfrom tqdm import tqdm\n\nwebsite_files = [\n    \"aider/website/share/index.md\",\n    \"aider/website/_includes/head_custom.html\",\n    \"aider/website/docs/leaderboards/index.md\",\n]\n\nexclude_files = [\n    \"aider/website/install.ps1\",\n    \"aider/website/install.sh\",\n]\n\ndef blame(start_tag, end_tag=None):\n    commits = get_all_commit_hashes_between_tags(start_tag, end_tag)\n    authors = get_commit_authors(commits)\n\n    revision = end_tag if end_tag else \"HEAD\"\n    files = run([\"git\", \"ls-tree\", \"-r\", \"--name-only\", revision]).strip().split(\"\\n\")\n    files = [\n        f\n        for f in files\n        if f.endswith((\".js\", \".py\", \".scm\", \".sh\", \"Dockerfile\", \"Gemfile\"))\n        or (f.startswith(\".github/workflows/\") and f.endswith(\".yml\"))\n        or f in website_files\n        or f in test_files\n    ]\n    files = [f for f in files if not f.endswith(\"prompts.py\")]\n    files = [f for f in files if not f.startswith(\"tests/fixtures/watch\")]\n    files = [f for f in files if f not in exclude_files]\n\n    all_file_counts = {}\n    grand_total = defaultdict(int)\n    aider_total = 0\n    for file in files:\n        file_counts = get_counts_for_file(start_tag, end_tag, authors, file)\n        if file_counts:\n            all_file_counts[file] = file_counts\n            for author, count in file_counts.items():\n                grand_total[author] += count\n                if \"(aider)\" in author.lower():\n                    aider_total += count\n\n    total_lines = sum(grand_total.values())\n    aider_percentage = (aider_total / total_lines) * 100 if total_lines > 0 else 0\n    end_date = get_tag_date(end_tag if end_tag else \"HEAD\")\n\n    return all_file_counts, grand_total, total_lines, aider_total, aider_percentage, end_date\n\ndef get_all_commit_hashes_between_tags(start_tag, end_tag=None):\n    if end_tag:\n        res = run([\"git\", \"rev-list\", f\"{start_tag}..{end_tag}\"])\n    else:\n        res = run([\"git\", \"rev-list\", f\"{start_tag}..HEAD\"])\n\n    if res:\n        commit_hashes = res.strip().split(\"\\n\")\n        return commit_hashes\n\ndef run(cmd):\n    result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n    return result.stdout\n\ndef get_commit_authors(commits):\n    commit_to_author = dict()\n    for commit in commits:\n        author = run([\"git\", \"show\", \"-s\", \"--format=%an\", commit]).strip()\n        commit_message = run([\"git\", \"show\", \"-s\", \"--format=%s\", commit]).strip()\n        if commit_message.lower().startswith(\"aider:\"):\n            author += \" (aider)\"\n        commit_to_author[commit] = author\n    return commit_to_author\n\nhash_len = len(\"44e6fefc2\")\n\ndef process_all_tags_since(start_tag):\n    tags = get_all_tags_since(start_tag)\n    results = []\n    for i in tqdm(range(len(tags) - 1), desc=\"Processing tags\"):\n        start_tag, end_tag = tags[i], tags[i + 1]\n        all_file_counts, grand_total, total_lines, aider_total, aider_percentage, end_date = blame(start_tag, end_tag)\n        results.append(\n            {\n                \"start_tag\": start_tag,\n                \"end_tag\": end_tag,\n                \"end_date\": end_date.strftime(\"%Y-%m-%d\"),\n                \"file_counts\": all_file_counts,\n                \"grand_total\": {\n                    author: count\n                    for author, count in sorted(grand_total.items(), key=itemgetter(1), reverse=True)\n                },\n                \"total_lines\": total_lines,\n                \"aider_total\": aider_total,\n                \"aider_percentage\": round(aider_percentage, 2),\n            }\n        )\n    return results\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Get aider/non-aider blame stats\")\n    parser.add_argument(\"start_tag\", nargs=\"?\", help=\"The tag to start from (optional)\")\n    parser.add_argument(\"--end-tag\", help=\"The tag to end at (default: HEAD)\", default=None)\n    parser.add_argument(\"--all-since\", action=\"store_true\", help=\"Find all tags since the specified tag and print aide percentage between each pair of successive tags\")\n    args = parser.parse_args()\n\n    if not args.start_tag:\n        args.start_tag = get_latest_version_tag()\n        if not args.start_tag:\n            print(\"Error: No valid vX.Y.0 tag found.\")\n            return\n\n    if args.all_since:\n        results = process_all_tags_since(args.start_tag)\n        print(yaml.dump(results, sort_keys=False))\n    else:\n        all_file_counts, grand_total, total_lines, aider_total, aider_percentage, end_date = blame(args.start_tag, args.end_tag)\n        result = {\n            \"start_tag\": args.start_tag,\n            \"end_tag\": args.end_tag or \"HEAD\",\n            \"end_date\": end_date.strftime(\"%Y-%m-%d\"),\n            \"file_counts\": all_file_counts,\n            \"grand_total\": {\n                author: count\n                for author, count in sorted(grand_total.items(), key=itemgetter(1), reverse=True)\n            },\n            \"total_lines\": total_lines,\n            \"aider_total\": aider_total,\n            \"aider_percentage\": round(aider_percentage, 2),\n        }\n        print(yaml.dump(result, sort_keys=False))\n\ndef get_counts_for_file(start_tag, end_tag, authors, fname):\n    try:\n        if end_tag:\n            text = run([\"git\", \"blame\", \"-M\", \"-C\", \"-C\", \"--abbrev=9\", f\"{start_tag}..{end_tag}\", \"--\", fname])\n        else:\n            text = run([\"git\", \"blame\", \"-M\", \"-C\", \"-C\", \"--abbrev=9\", f\"{start_tag}..HEAD\", \"--\", fname])\n        if not text:\n            return None\n        text = text.splitlines()\n        line_counts = defaultdict(int)\n        for line in text:\n            if line.startswith(\"^\"):\n                continue\n            hsh = line[:hash_len]\n            author = authors.get(hsh, \"Unknown\")\n            line_counts[author] += 1\n\n        return dict(line_counts)\n    except subprocess.CalledProcessError as e:\n        if \"no such path\" in str(e).lower():\n            return None\n        else:\n            print(f\"Warning: Unable to blame file {fname}. Error: {e}\", file=sys.stderr)\n            return None\n\ndef get_all_tags_since(start_tag):\n    all_tags = run([\"git\", \"tag\", \"--sort=v:refname\"]).strip().split(\"\\n\")\n    start_version = semver.Version.parse(start_tag[1:])  # Remove 'v' prefix\n    filtered_tags = [\n        tag\n        for tag in all_tags\n        if semver.Version.is_valid(tag[1:]) and semver.Version.parse(tag[1:]) >= start_version\n    ]\n    return [tag for tag in filtered_tags if tag.endswith(\".0\")]\n\ndef get_tag_date(tag):\n    date_str = run([\"git\", \"log\", \"-1\", \"--format=%ai\", tag]).strip()\n    return datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S %z\")\n\nif __name__ == \"__main__\":\n    main()",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.7756941940137035,
      "diff_lines": 259,
      "added_lines": 9,
      "removed_lines": 118,
      "diff_preview": "--- expected+++ predicted@@ -1,22 +1,18 @@ #!/usr/bin/env python3\n \n import argparse\n-import os\n import subprocess\n import sys\n from collections import defaultdict\n from datetime import datetime\n from operator import itemgetter\n-\n import semver\n import yaml\n from tqdm import tqdm\n \n website_files = [\n-    \"aider/website/index.html\",\n     \"aider/website/share/index.md\",\n     \"aider/website/_includes/head_custom.html\",\n-    \"aider/website/_includes/home.css\",\n     \"aider/website/docs/leaderboards/index.md\",\n ]\n \n@@ -25,22 +21,17 @@     \"aider/website/install.sh\",\n ]\n \n-\n def blame(start_tag, end_tag=None):\n     commits = get_all_commit_hashes_between_tags(start_tag, end_tag)\n-    commits = [commit[:hash_len] for commit in commits]\n-\n     authors = get_commit_authors(commits)\n \n     revision = end_tag if end_tag else \"HEAD\"\n     files = run([\"git\", \"ls-tree\", \"-r\", \"--name-only\", revision]).strip().split(\"\\n\")\n-    test_files = [f for f in files if f.startswith(\"tests/fixtures/languages/\") and \"/test.\" in f]\n     files = [\n         f\n         for f in files\n         if f.endswith((\".js\", \".py\", \".scm\", \".sh\", \"Dockerfile\", \"Gemfile\"))\n         or (f.startswith(\".github/workflows/\") and f.endswith(\".yml\"))\n-        or (f.startswith(\"aider/resources/\") and f.endswith(\".yml\"))\n         or f in website_files\n         or f in test_files\n     ]\n@@ -62,11 +53,9 @@ \n"
    },
    "llm_calls": 1,
    "elapsed_time": 48.26353621482849,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 47383,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_scripts_issues.py",
    "exact_match": false,
    "similarity": 0.26266529798925187,
    "diff_lines": 418,
    "predicted_output": "#!/usr/bin/env python3\nimport os\nimport re\nfrom collections import defaultdict\nfrom datetime import datetime\nfrom tqdm import tqdm\nimport requests\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\nBOT_SUFFIX = \"\"\"\nNote: A [bot script](https://github.com/Aider-AI/aider/blob/main/scripts/issues.py) made these updates to the issue.\n\"\"\"  # noqa\n\nDUPLICATE_COMMENT = (\n    \"\"\"Thanks for trying aider and filing this issue.\nThis looks like a duplicate of #{oldest_issue_number}. Please see the comments there for more information, and feel free to continue the discussion within that issue.\nI'm going to close this issue for now. But please let me know if you think this is actually a distinct issue and I will reopen this issue.\"\"\"  # noqa\n    + BOT_SUFFIX\n)\n\nSTALE_COMMENT = (\n    \"\"\"I'm labeling this issue as stale because it has been open for 2 weeks with no activity. If there are no additional comments, I will close it in 7 days.\"\"\"  # noqa\n    + BOT_SUFFIX\n)\n\nCLOSE_STALE_COMMENT = (\n    \"\"\"I'm closing this issue because it has been stalled for 3 weeks with no activity. Feel free to add a comment here and we can re-open it. Or feel free to file a new issue at any time.\"\"\" + BOT_SUFFIX\n)\n\nCLOSE_FIXED_ENHANCEMENT_COMMENT = (\n    \"\"\"I'm closing this enhancement request since it has been marked as 'fixed' for over \"\"\"\n    \"\"\"3 weeks. The requested feature should now be available in recent versions of aider.\\n\\n\"\"\"\n    \"\"\"If you find that this enhancement is still needed, please feel free to reopen this \"\"\"\n    \"\"\"issue or create a new one.\"\"\" + BOT_SUFFIX\n)\n\nCLOSE_FIXED_BUG_COMMENT = (\n    \"\"\"I'm closing this bug report since it has been marked as 'fixed' for over \"\"\"\n    \"\"\"3 weeks. This issue should be resolved in recent versions of aider.\\n\\n\"\"\"\n    \"\"\"If you find that this bug is still present, please feel free to reopen this \"\"\"\n    \"\"\"issue or create a new one with steps to reproduce.\"\"\" + BOT_SUFFIX\n)\n\n# GitHub API configuration\nGITHUB_API_URL = \"https://api.github.com\"\nREPO_OWNER = \"Aider-AI\"\nREPO_NAME = \"aider\"\nTOKEN = os.getenv(\"GITHUB_TOKEN\")\n\nheaders = {\"Authorization\": f\"token {TOKEN}\", \"Accept\": \"application/vnd.github.v3+json\"}\n\ndef has_been_reopened(issue_number):\n    timeline_url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue_number}/timeline\"\n    response = requests.get(timeline_url, headers=headers)\n    response.raise_for_status()\n    events = response.json()\n    return any(event[\"event\"] == \"reopened\" for event in events if \"event\" in event)\n\ndef get_issues(state=\"open\"):\n    issues = []\n    page = 1\n    with tqdm(desc=\"Collecting issues\", unit=\"page\") as pbar:\n        while True:\n            response = requests.get(\n                f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues\",\n                headers=headers,\n                params={\"state\": state, \"page\": page, \"per_page\": 100},\n            )\n            response.raise_for_status()\n            page_issues = response.json()\n            if not page_issues:\n                break\n            issues.extend(page_issues)\n            page += 1\n            pbar.update(1)\n    return issues\n\ndef group_issues_by_subject(issues):\n    grouped_issues = defaultdict(list)\n    for issue in issues:\n        if \"pull_request\" in issue:\n            continue\n        subject = issue[\"title\"].split(\":\")[0].strip()  # Assuming subject is before the first colon\n        grouped_issues[subject].append(issue)\n    return grouped_issues\n\ndef find_oldest_issue(subject, all_issues):\n    oldest_issue = None\n    oldest_date = datetime.now()\n\n    for issue in all_issues:\n        if issue[\"title\"] == subject and not has_been_reopened(issue[\"number\"]):\n            created_at = datetime.strptime(issue[\"created_at\"], \"%Y-%m-%dT%H:%M:%SZ\")\n            if created_at < oldest_date:\n                oldest_date = created_at\n                oldest_issue = issue\n\n    return oldest_issue\n\ndef comment_and_close_duplicate(issue, oldest_issue):\n    # Skip if issue is labeled as priority\n    if \"priority\" in [label[\"name\"] for label in issue[\"labels\"]]:\n        print(f\"  - Skipping priority issue #{issue['number']}\")\n        return\n\n    comment_url = (\n        f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}/comments\"\n    )\n    close_url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}\"\n    \n    comment_body = DUPLICATE_COMMENT.format(oldest_issue_number=oldest_issue[\"number\"])\n    \n    # Post comment\n    response = requests.post(comment_url, headers=headers, json={\"body\": comment_body})\n    response.raise_for_status()\n    \n    # Close issue\n    response = requests.patch(close_url, headers=headers, json={\"state\": \"closed\"})\n    response.raise_for_status()\n    \n    print(f\"  - Commented and closed issue #{issue['number']}\")\n\ndef find_unlabeled_with_paul_comments(issues):\n    unlabeled_issues = []\n    for issue in issues:\n        # Skip pull requests\n        if \"pull_request\" in issue:\n            continue\n        if not issue[\"labels\"] and issue[\"state\"] == \"open\":\n            # Get comments for this issue\n            comments_url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}/comments\"\n            response = requests.get(comments_url, headers=headers)\n            response.raise_for_status()\n            comments = response.json()\n            \n            # Check if paul-gauthier has commented\n            if any(comment[\"user\"][\"login\"] == \"paul-gauthier\" for comment in comments):\n                unlabeled_issues.append(issue)\n    return unlabeled_issues\n\ndef handle_unlabeled_issues(all_issues, auto_yes):\n    print(\"\\nFinding unlabeled issues with paul-gauthier comments...\")\n    unlabeled_issues = [\n        issue for issue in find_unlabeled_with_paul_comments(all_issues)\n        if \"priority\" not in [label[\"name\"] for label in issue[\"labels\"]]\n    ]\n\n    if not unlabeled_issues:\n        print(\"No unlabeled issues with paul-gauthier comments found.\")\n        return\n\n    print(f\"\\nFound {len(unlabeled_issues)} unlabeled issues with paul-gauthier comments:\")\n    for issue in unlabeled_issues:\n        print(f\"  - #{issue['number']}: {issue['title']} {issue['html_url']}\")\n\n    if not auto_yes:\n        confirm = input(\"\\nDo you want to add the 'question' label to these issues? (y/n): \")\n        if confirm.lower() != \"y\":\n            print(\"Skipping labeling.\")\n            return\n\n    print(\"\\nAdding 'question' label to issues...\")\n    for issue in unlabeled_issues:\n        url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}\"\n        response = requests.patch(url, headers=headers, json={\"labels\": [\"question\"]})\n        response.raise_for_status()\n        print(f\"  - Added 'question' label to #{issue['number']}\")\n\ndef handle_stale_issues(all_issues, auto_yes):\n    print(\"\\nChecking for stale question issues...\")\n    \n    for issue in all_issues:\n        # Skip if not open, not a question, already stale, or has been reopened\n        labels = [label[\"name\"] for label in issue[\"labels\"]]\n        if (\n            issue[\"state\"] != \"open\"\n            or \"question\" not in labels\n            or \"stale\" in labels\n            or \"priority\" in labels\n            or has_been_reopened(issue[\"number\"])\n        ):\n            continue\n        \n        # Get latest activity timestamp from issue or its comments\n        latest_activity = datetime.strptime(issue[\"updated_at\"], \"%Y-%m-%dT%H:%M:%SZ\")\n        \n        # Check if issue is stale (no activity for 14 days)\n        days_inactive = (datetime.now() - latest_activity).days\n        if days_inactive >= 14:\n            print(f\"\\nStale issue found: #{issue['number']}: {issue['title']}\\n{issue['html_url']}\")\n            print(f\"  No activity for {days_inactive} days\")\n            \n            if not auto_yes:\n                confirm = input(\"Add stale label and comment? (y/n): \")\n                if confirm.lower() != \"y\":\n                    print(\"Skipping this issue.\")\n                    continue\n\n            # Add stale label\n            url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}\"\n            response = requests.patch(url, headers=headers, json={\"labels\": [\"question\", \"stale\"]})\n            response.raise_for_status()\n\n            # Add comment\n            comment_url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}/comments\"\n            response = requests.post(comment_url, headers=headers, json={\"body\": STALE_COMMENT})\n            response.raise_for_status()\n            \n            print(f\"  Added stale label and comment to #{issue['number']}\")\n\ndef handle_stale_closing(all_issues, auto_yes):\n    print(\"\\nChecking for issues to close or unstale...\")\n\n    for issue in all_issues:\n        # Skip if not open, not stale, or is priority\n        labels = [label[\"name\"] for label in issue[\"labels\"]]\n        if issue[\"state\"] != \"open\" or \"stale\" not in labels or \"priority\" in labels:\n            continue\n\n        # Get the timeline to find when the stale label was last added\n        timeline_url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}/timeline\"\n        response = requests.get(timeline_url, headers=headers)\n        response.raise_for_status()\n        events = response.json()\n\n        # Find the most recent stale label addition\n        stale_events = [\n            event\n            for event in events\n            if event.get(\"event\") == \"labeled\" and event.get(\"label\", {}).get(\"name\") == \"stale\"\n        ]\n\n        if not stale_events:\n            continue\n\n        latest_stale = datetime.strptime(stale_events[-1][\"created_at\"], \"%Y-%m-%dT%H:%M:%SZ\")\n\n        # Get comments since the stale label\n        comments_url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}/comments\"\n        response = requests.get(comments_url, headers=headers)\n        response.raise_for_status()\n        comments = response.json()\n\n        # Check for comments newer than the stale label\n        new_comments = [\n            comment\n            for comment in comments\n            if datetime.strptime(comment[\"created_at\"], \"%Y-%m-%dT%H:%M:%SZ\") > latest_stale\n        ]\n\n        if new_comments:\n            print(f\"\\nFound new activity on stale issue #{issue['number']}: {issue['title']}\")\n            print(f\"  {len(new_comments)} new comments since stale label\")\n            \n            if not auto_yes:\n                confirm = input(\"Remove stale label? (y/n): \")\n                if confirm.lower() != \"y\":\n                    print(\"Skipping this issue.\")\n                    continue\n\n            # Remove stale label but keep question label\n            url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}\"\n            response = requests.patch(url, headers=headers, json={\"labels\": [\"question\"]})\n            response.raise_for_status()\n            print(f\"  Removed stale label from #{issue['number']}\")\n        else:\n            # Check if it's been 7 days since stale label\n            days_stale = (datetime.now() - latest_stale).days\n            if days_stale >= 7:\n                print(f\"\\nStale issue ready for closing #{issue['number']}: {issue['title']}\")\n                print(f\"  No activity for {days_stale} days since stale label\")\n\n                if not auto_yes:\n                    confirm = input(\"Close this issue? (y/n): \")\n                    if confirm.lower() != \"y\":\n                        print(\"Skipping this issue.\")\n                        continue\n\n                # Add closing comment\n                comment_url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}/comments\"\n                response = requests.post(comment_url, headers=headers, json={\"body\": CLOSE_STALE_COMMENT})\n                response.raise_for_status()\n\n                # Close the issue\n                url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}\"\n                response = requests.patch(url, headers=headers, json={\"state\": \"closed\"})\n                response.raise_for_status()\n                print(f\"  Closed issue #{issue['number']}\")\n\ndef handle_fixed_issues(all_issues, auto_yes):\n    print(\"\\nChecking for fixed enhancement and bug issues to close...\")\n\n    for issue in all_issues:\n        # Skip if not open, doesn't have fixed label, or is priority\n        labels = [label[\"name\"] for label in issue[\"labels\"]]\n        if issue[\"state\"] != \"open\" or \"fixed\" not in labels or \"priority\" in labels:\n            continue\n\n        # Check if it's an enhancement or bug\n        is_enhancement = \"enhancement\" in labels\n        is_bug = \"bug\" in labels\n        if not (is_enhancement or is_bug):\n            continue\n\n        # Find when the fixed label was added\n        timeline_url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}/timeline\"\n        response = requests.get(timeline_url, headers=headers)\n        response.raise_for_status()\n        events = response.json()\n\n        # Find the most recent fixed label addition\n        fixed_events = [\n            event\n            for event in events\n            if event.get(\"event\") == \"labeled\" and event.get(\"label\", {}).get(\"name\") == \"fixed\"\n        ]\n\n        if not fixed_events:\n            continue\n\n        latest_fixed = datetime.strptime(fixed_events[-1][\"created_at\"], \"%Y-%m-%dT%H:%M:%SZ\")\n        days_fixed = (datetime.now() - latest_fixed).days\n\n        if days_fixed >= 21:\n            issue_type = \"enhancement\" if is_enhancement else \"bug\"\n            print(f\"\\nFixed {issue_type} ready for closing #{issue['number']}: {issue['title']}\")\n            print(f\"  Has been marked fixed for {days_fixed} days\")\n\n            if not auto_yes:\n                confirm = input(\"Close this issue? (y/n): \")\n                if confirm.lower() != \"y\":\n                    print(\"Skipping this issue.\")\n                    continue\n\n            # Add closing comment\n            comment_url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}/comments\"\n            comment = (\n                CLOSE_FIXED_ENHANCEMENT_COMMENT if is_enhancement else CLOSE_FIXED_BUG_COMMENT\n            )\n            response = requests.post(comment_url, headers=headers, json={\"body\": comment})\n            response.raise_for_status()\n\n            # Close the issue\n            url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}\"\n            response = requests.patch(url, headers=headers, json={\"state\": \"closed\"})\n            response.raise_for_status()\n            print(f\"  Closed issue #{issue['number']}\")\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Handle duplicate GitHub issues\")\n    parser.add_argument(\n        \"--yes\", action=\"store_true\", help=\"Automatically close duplicates without prompting\"\n    )\n    args = parser.parse_args()\n\n    if not TOKEN:\n        print(\"Error: Missing GITHUB_TOKEN environment variable. Please check your .env file.\")\n        return\n\n    all_issues = get_issues(\"all\")\n\n    handle_unlabeled_issues(all_issues, args.yes)\n    handle_stale_issues(all_issues, args.yes)\n    handle_stale_closing(all_issues, args.yes)\n    handle_duplicate_issues(all_issues, args.yes)\n    handle_fixed_issues(all_issues, args.yes)\n\nif __name__ == \"__main__\":\n    main()",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.26266529798925187,
      "diff_lines": 418,
      "added_lines": 77,
      "removed_lines": 163,
      "diff_preview": "--- expected+++ predicted@@ -1,15 +1,56 @@ #!/usr/bin/env python3\n-\n-import argparse\n import os\n import re\n from collections import defaultdict\n from datetime import datetime\n-\n+from tqdm import tqdm\n import requests\n from dotenv import load_dotenv\n-from tqdm import tqdm\n-\n+\n+# Load environment variables from .env file\n+load_dotenv()\n+\n+BOT_SUFFIX = \"\"\"\n+Note: A [bot script](https://github.com/Aider-AI/aider/blob/main/scripts/issues.py) made these updates to the issue.\n+\"\"\"  # noqa\n+\n+DUPLICATE_COMMENT = (\n+    \"\"\"Thanks for trying aider and filing this issue.\n+This looks like a duplicate of #{oldest_issue_number}. Please see the comments there for more information, and feel free to continue the discussion within that issue.\n+I'm going to close this issue for now. But please let me know if you think this is actually a distinct issue and I will reopen this issue.\"\"\"  # noqa\n+    + BOT_SUFFIX\n+)\n+\n+STALE_COMMENT = (\n+    \"\"\"I'm labeling this issue as stale because it has been open for 2 weeks with no activity. If there are no additional comments, I will close it in 7 days.\"\"\"  # noqa\n+    + BOT_SUFFIX\n+)\n+\n+CLOSE_STALE_COMMENT = (\n+    \"\"\"I'm closing this issue because it has been stalled for 3 weeks with no activity. Feel free to add a comment here and we can re-open it. Or feel free to file a new issue at any time.\"\"\" + BOT_SUFFIX\n+)\n+\n+CLOSE_FIXED_ENHANCEMENT_COMMENT = (\n+    \"\"\"I'm closing this enhancement request since it has been marked as 'fixed' for over \"\"\"\n+    \"\"\"3 weeks. The requested feature should now be available in recent versions of aider.\\n\\n\"\"\"\n+    \"\"\"If you find that this enhancement is still needed, please feel free to reopen this \"\"\"\n+    \"\"\"issue or create a new one.\"\"\" + BOT_SUFFIX\n+)\n+\n+CLOSE_FIXED_BUG_COMMENT = (\n+    \"\"\"I'm closing this bug report since it has been marked as 'fixed' for over \"\"\"\n+    \"\"\"3 weeks. This issue should be resolved in recent versions of aider.\\n\\n\"\"\"\n"
    },
    "llm_calls": 1,
    "elapsed_time": 88.25070881843567,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 31218,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_scripts_redact-cast.py",
    "exact_match": false,
    "similarity": 0.9781149693609571,
    "diff_lines": 54,
    "predicted_output": "#!/usr/bin/env python3\nimport json\nimport os\nimport re\nimport sys\n\nimport pyte\nfrom tqdm import tqdm\n\ndef main():\n    if len(sys.argv) != 3:\n        print(f\"Usage: {sys.argv[0]} input_cast_file output_cast_file\")\n        sys.exit(1)\n    \n    input_file = sys.argv[1]\n    output_file = sys.argv[2]\n    \n    # Count total lines for progress bar\n    total_lines = sum(1 for _ in open(input_file, \"r\"))\n\n    with open(input_file, \"r\") as fin, open(output_file, \"w\") as fout:\n        # Process header\n        header = fin.readline().strip()\n        fout.write(header + \"\\n\")\n        \n        # Parse header for terminal dimensions\n        header_data = json.loads(header)\n        width = header_data.get(\"width\", 80)\n        height = header_data.get(\"height\", 24)\n        print(f\"Terminal dimensions: {width}x{height}\")\n        \n        screen = pyte.Screen(width, height)\n        stream = pyte.Stream(screen)\n\n        # Process events line by line\n        for line in tqdm(fin, desc=\"Processing events\", total=total_lines - 1):\n            if not line.strip():\n                continue\n            \n            event = json.loads(line)\n\n            if not (len(event) >= 3 and event[1] == \"o\"):\n                fout.write(line)\n                continue\n\n            output_text = event[2]\n            stream.feed(output_text)\n\n            # Check if \"Atuin\" is visible on screen\n            atuin_visible = False\n            for display_line in screen.display:\n                if \"Atuin\" in display_line or \"[    GLOBAL    ]\" in display_line:\n                    atuin_visible = True\n                    break\n\n            if not atuin_visible:\n                fout.write(line)\n\nif __name__ == \"__main__\":\n    main()",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9781149693609571,
      "diff_lines": 54,
      "added_lines": 5,
      "removed_lines": 9,
      "diff_preview": "--- expected+++ predicted@@ -7,17 +7,14 @@ import pyte\n from tqdm import tqdm\n \n-from aider.dump import dump  # noqa\n-\n-\n def main():\n     if len(sys.argv) != 3:\n         print(f\"Usage: {sys.argv[0]} input_cast_file output_cast_file\")\n         sys.exit(1)\n-\n+    \n     input_file = sys.argv[1]\n     output_file = sys.argv[2]\n-\n+    \n     # Count total lines for progress bar\n     total_lines = sum(1 for _ in open(input_file, \"r\"))\n \n@@ -25,13 +22,13 @@         # Process header\n         header = fin.readline().strip()\n         fout.write(header + \"\\n\")\n-\n+        \n         # Parse header for terminal dimensions\n         header_data = json.loads(header)\n         width = header_data.get(\"width\", 80)\n         height = header_data.get(\"height\", 24)\n         print(f\"Terminal dimensions: {width}x{height}\")\n-\n+        \n         screen = pyte.Screen(width, height)\n         stream = pyte.Stream(screen)\n \n@@ -39,7 +36,7 @@         for line in tqdm(fin, desc=\"Processing events\", total=total_lines - 1):\n             if not line.strip():\n                 continue\n-\n+            \n             event = json.loads(line)\n \n             if not (len(event) >= 3 and event[1] == \"o\"):\n@@ -59,6 +56,5 @@             if not atuin_visible:\n                 fout.write(line)\n"
    },
    "llm_calls": 1,
    "elapsed_time": 10.263912916183472,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 13688,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_scripts_versionbump.py",
    "exact_match": false,
    "similarity": 0.9641193720890116,
    "diff_lines": 110,
    "predicted_output": "#!/usr/bin/env python\n\nimport argparse\nimport datetime\nimport re\nimport subprocess\nimport sys\n\nfrom packaging import version\n\n\n# Function to check if we are on the main branch\ndef check_branch():\n    branch = subprocess.run(\n        [\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"], capture_output=True, text=True\n    ).stdout.strip()\n    if branch != \"main\":\n        print(\"Error: Not on the main branch.\")\n        sys.exit(1)\n\n\n# Function to check if the working directory is clean\ndef check_working_directory_clean():\n    status = subprocess.run(\n        [\"git\", \"status\", \"--porcelain\"], capture_output=True, text=True\n    ).stdout\n    if status:\n        print(\"Error: Working directory is not clean.\")\n        sys.exit(1)\n\n\n# Function to fetch the latest changes and check if the main branch is up to date\ndef check_main_branch_up_to_date():\n    subprocess.run([\"git\", \"fetch\", \"origin\"], check=True)\n    local_main = subprocess.run(\n        [\"git\", \"rev-parse\", \"main\"], capture_output=True, text=True\n    ).stdout.strip()\n    print(f\"Local main commit hash: {local_main}\")\n    origin_main = subprocess.run(\n        [\"git\", \"rev-parse\", \"origin/main\"], capture_output=True, text=True\n    ).stdout.strip()\n    print(f\"Origin main commit hash: {origin_main}\")\n    if local_main != origin_main:\n        local_date = subprocess.run(\n            [\"git\", \"show\", \"-s\", \"--format=%ci\", \"main\"], capture_output=True, text=True\n        ).stdout.strip()\n        origin_date = subprocess.run(\n            [\"git\", \"show\", \"-s\", \"--format=%ci\", \"origin/main\"], capture_output=True, text=True\n        ).stdout.strip()\n        local_date = datetime.datetime.strptime(local_date, \"%Y-%m-%d %H:%M:%S %z\")\n        origin_date = datetime.datetime.strptime(origin_date, \"%Y-%m-%d %H:%M:%S %z\")\n        if local_date < origin_date:\n            print(\n                \"Error: The local main branch is behind origin/main. Please pull the latest changes.\"\n            )\n        elif local_date > origin_date:\n            print(\n                \"Error: The origin/main branch is behind the local main branch. Please push your changes.\"\n            )\n        else:\n            print(\"Error: The main branch and origin/main have diverged.\")\n        sys.exit(1)\n\n\n# Function to check if we can push to the origin repository\ndef check_ok_to_push():\n    print(\"Checking if it's ok to push to origin repository...\")\n    result = subprocess.run([\"git\", \"push\", \"--dry-run\", \"origin\"], capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Error: Cannot push to origin repository.\")\n        sys.exit(1)\n    print(\"Push to origin repository is possible.\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Bump version\")\n    parser.add_argument(\"new_version\", help=\"New version in x.y.z format\")\n    parser.add_argument(\"--dry-run\", action=\"store_true\", help=\"Print each step without actually executing them\")\n    parser.add_argument(\"--force\", action=\"store_true\", help=\"Skip pre-push checks\")\n\n    args = parser.parse_args()\n    dry_run = args.dry_run\n    force = args.force\n\n    # Perform checks before proceeding unless --force is used\n    if not force:\n        check_branch()\n        check_working_directory_clean()\n        check_main_branch_up_to_date()\n        check_ok_to_push()\n    else:\n        print(\"Skipping pre-push checks due to --force flag.\")\n\n    new_version_str = args.new_version\n    if not re.match(r\"^\\d+\\.\\d+\\.\\d+$\", new_version_str):\n        raise ValueError(f\"Invalid version format, must be x.y.z: {new_version_str}\")\n\n    new_version = version.parse(new_version_str)\n    incremented_version = version.Version(\n        f\"{new_version.major}.{new_version.minor}.{new_version.micro + 1}\"\n    )\n\n    with open(\"aider/__init__.py\", \"r\") as f:\n        content = f.read()\n\n    updated_content = re.sub(r'__version__ = \".+?\"', f'__version__ = \"{new_version}\"', content)\n\n    print(\"Updating aider/__init__.py with new version:\")\n    print(updated_content)\n\n    if not dry_run:\n        with open(\"aider/__init__.py\", \"w\") as f:\n            f.write(updated_content)\n\n    git_commands = [\n        [\"git\", \"add\", \"aider/__init__.py\"],\n        [\"git\", \"commit\", \"-m\", f\"version bump to {new_version}\"],\n        [\"git\", \"tag\", f\"v{new_version}\"],\n        [\"git\", \"push\", \"origin\", \"--no-verify\"],\n        [\"git\", \"push\", \"origin\", f\"v{new_version}\", \"--no-verify\"],\n    ]\n\n    for cmd in git_commands:\n        print(f\"Running: {' '.join(cmd)}\")\n        if not dry_run:\n            subprocess.run(cmd, check=True)\n\n    new_dev_version = f\"{incremented_version}.dev\"\n    updated_dev_content = re.sub(\n        r'__version__ = \".+?\"', f'__version__ = \"{new_dev_version}\"', content\n    )\n\n    print()\n    print(\"Updating aider/__init__.py with new dev version:\")\n    print(updated_dev_content)\n\n    if not dry_run:\n        with open(\"aider/__init__.py\", \"w\") as f:\n            f.write(updated_dev_content)\n\n    git_commands_dev = [\n        [\"git\", \"add\", \"aider/__init__.py\"],\n        [\"git\", \"commit\", \"-m\", f\"set version to {new_dev_version}\"],\n        [\"git\", \"tag\", f\"v{new_dev_version}\"],\n        [\"git\", \"push\", \"origin\", \"--no-verify\"],\n        [\"git\", \"push\", \"origin\", f\"v{new_dev_version}\", \"--no-verify\"],\n    ]\n\n    for cmd in git_commands_dev:\n        print(f\"Running: {' '.join(cmd)}\")\n        if not dry_run:\n            subprocess.run(cmd, check=True)\n\n    # Remove aider/_version.py if it exists\n    version_file = \"aider/_version.py\"\n    if os.path.exists(version_file):\n        print(f\"Removing {version_file}\")\n        if not dry_run:\n            os.remove(version_file)\n\nif __name__ == \"__main__\":\n    main()",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9641193720890116,
      "diff_lines": 110,
      "added_lines": 11,
      "removed_lines": 24,
      "diff_preview": "--- expected+++ predicted@@ -2,7 +2,6 @@ \n import argparse\n import datetime\n-import os\n import re\n import subprocess\n import sys\n@@ -22,7 +21,9 @@ \n # Function to check if the working directory is clean\n def check_working_directory_clean():\n-    status = subprocess.run([\"git\", \"status\", \"--porcelain\"], capture_output=True, text=True).stdout\n+    status = subprocess.run(\n+        [\"git\", \"status\", \"--porcelain\"], capture_output=True, text=True\n+    ).stdout\n     if status:\n         print(\"Error: Working directory is not clean.\")\n         sys.exit(1)\n@@ -50,13 +51,11 @@         origin_date = datetime.datetime.strptime(origin_date, \"%Y-%m-%d %H:%M:%S %z\")\n         if local_date < origin_date:\n             print(\n-                \"Error: The local main branch is behind origin/main. Please pull the latest\"\n-                \" changes.\"\n+                \"Error: The local main branch is behind origin/main. Please pull the latest changes.\"\n             )\n         elif local_date > origin_date:\n             print(\n-                \"Error: The origin/main branch is behind the local main branch. Please push\"\n-                \" your changes.\"\n+                \"Error: The origin/main branch is behind the local main branch. Please push your changes.\"\n             )\n         else:\n             print(\"Error: The main branch and origin/main have diverged.\")\n@@ -66,21 +65,17 @@ # Function to check if we can push to the origin repository\n def check_ok_to_push():\n     print(\"Checking if it's ok to push to origin repository...\")\n-    result = subprocess.run([\"git\", \"push\", \"--dry-run\", \"origin\"])\n-\n+    result = subprocess.run([\"git\", \"push\", \"--dry-run\", \"origin\"], capture_output=True, text=True)\n     if result.returncode != 0:\n         print(\"Error: Cannot push to origin repository.\")\n         sys.exit(1)\n-\n     print(\"Push to origin repository is possible.\")\n \n"
    },
    "llm_calls": 1,
    "elapsed_time": 30.19280219078064,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 22874,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_tests_basic_test_coder.py",
    "exact_match": false,
    "similarity": 0.0798131438153797,
    "diff_lines": 1281,
    "predicted_output": "import os\nimport tempfile\nimport unittest\nfrom pathlib import Path\nfrom unittest.mock import MagicMock, patch\n\nimport git\n\nfrom aider.coders import Coder\nfrom aider.coders.base_coder import FinishReasonLength, UnknownEditFormat\nfrom aider.dump import dump  # noqa: F401\nfrom aider.io import InputOutput\nfrom aider.models import Model\nfrom aider.repo import GitRepo\nfrom aide.utils import GitTemporaryDirectory\nfrom aider.sendchat import sanity_check_messages\n\nclass TestCoder(unittest.TestCase):\n    def setUp(self):\n        self.GPT35 = Model(\"gpt-3.5-turbo\")\n        self.webbrowser_patcher = patch(\"aider.io.webbrowser.open\")\n        self.mock_webbrowser = self.webbrowser_patcher.start()\n\n    def test_allowed_to_edit(self):\n        with GitTemporaryDirectory():\n            repo = git.Repo()\n            fname = Path(\"added.txt\")\n            fname.touch()\n            repo.git.add(str(fname))\n\n            io = InputOutput(yes=True)\n            coder = Coder.create(self.GPT35, None, io, fnames=[\"added.txt\"])\n            self.assertTrue(coder.allowed_to_edit(\"added.txt\"))\n\n    def test_get_file_mentions_path_formats(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n\n            # Create test files\n            test_files = [\n                \"file1.txt\",\n                \"file2.py\",\n                \"dir/nested_file.js\",\n                \"dir/subdir/deep_file.html\",\n                \"file with spaces.txt\",\n                \"special_chars!@#.md\",\n            ]\n\n            for fname in test_files:\n                fpath = Path(fname)\n                fpath.parent.mkdir(parents=True, exist_ok=True)\n                fpath.touch()\n\n            # Mock get_addable_relative_files to return our test files\n            coder.get_addable_relative_files = MagicMock(return_value=set(test_files))\n\n            # Test different mention formats\n            test_cases = [\n                (f\"You should edit {test_files[0]} first\", {test_files[0]}),\n                (f\"Edit both {test_files[0]} and {test_files[1]}\", {test_files[0], test_files[1]}),\n                (f\"Check the file `{test_files[2]}`\", {test_files[2]}),\n                (f\"",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.0798131438153797,
      "diff_lines": 1281,
      "added_lines": 7,
      "removed_lines": 1236,
      "diff_preview": "--- expected+++ predicted@@ -12,9 +12,8 @@ from aider.io import InputOutput\n from aider.models import Model\n from aider.repo import GitRepo\n+from aide.utils import GitTemporaryDirectory\n from aider.sendchat import sanity_check_messages\n-from aider.utils import GitTemporaryDirectory\n-\n \n class TestCoder(unittest.TestCase):\n     def setUp(self):\n@@ -25,267 +24,15 @@     def test_allowed_to_edit(self):\n         with GitTemporaryDirectory():\n             repo = git.Repo()\n-\n             fname = Path(\"added.txt\")\n             fname.touch()\n             repo.git.add(str(fname))\n \n-            fname = Path(\"repo.txt\")\n-            fname.touch()\n-            repo.git.add(str(fname))\n+            io = InputOutput(yes=True)\n+            coder = Coder.create(self.GPT35, None, io, fnames=[\"added.txt\"])\n+            self.assertTrue(coder.allowed_to_edit(\"added.txt\"))\n \n-            repo.git.commit(\"-m\", \"init\")\n-\n-            # YES!\n-            # Use a completely mocked IO object instead of a real one\n-            io = MagicMock()\n-            io.confirm_ask = MagicMock(return_value=True)\n-            coder = Coder.create(self.GPT35, None, io, fnames=[\"added.txt\"])\n-\n-            self.assertTrue(coder.allowed_to_edit(\"added.txt\"))\n-            self.assertTrue(coder.allowed_to_edit(\"repo.txt\"))\n-            self.assertTrue(coder.allowed_to_edit(\"new.txt\"))\n-\n-            self.assertIn(\"repo.txt\", str(coder.abs_fnames))\n-            self.assertIn(\"new.txt\", str(coder.abs_fnames))\n-\n-            self.assertFalse(coder.need_commit_before_edits)\n-\n-    def test_allowed_to_edit_no(self):\n-        with GitTemporaryDirectory():\n-            repo = git.Repo()\n-\n"
    },
    "llm_calls": 1,
    "elapsed_time": 30.8182430267334,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 37799,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_tests_basic_test_editblock.py",
    "exact_match": false,
    "similarity": 0.03487397306209132,
    "diff_lines": 616,
    "predicted_output": "\"\"\"\n        edits = list(eb.find_original_update_blocks(edit))\n        search_text = 'Console.WriteLine(\"Hello World!\");\\n'\n        replace_text = 'Console.WriteLine(\"Hello, C# World!\");\\n'\n        self.assertEqual(edits, [(\"Program.cs\", search_text, replace_text)])\n\n\nif __name__ == \"__main__\":\n    unittest.main()",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.03487397306209132,
      "diff_lines": 616,
      "added_lines": 0,
      "removed_lines": 609,
      "diff_preview": "--- expected+++ predicted@@ -1,613 +1,4 @@-# flake8: noqa: E501\n-\n-import tempfile\n-import unittest\n-from pathlib import Path\n-from unittest.mock import MagicMock, patch\n-\n-from aider.coders import Coder\n-from aider.coders import editblock_coder as eb\n-from aider.dump import dump  # noqa: F401\n-from aider.io import InputOutput\n-from aider.models import Model\n-from aider.utils import ChdirTemporaryDirectory\n-\n-\n-class TestUtils(unittest.TestCase):\n-    def setUp(self):\n-        self.GPT35 = Model(\"gpt-3.5-turbo\")\n-\n-    def test_find_filename(self):\n-        fence = (\"```\", \"```\")\n-        valid_fnames = [\"file1.py\", \"file2.py\", \"dir/file3.py\", r\"\\windows\\__init__.py\"]\n-\n-        # Test with filename on a single line\n-        lines = [\"file1.py\", \"```\"]\n-        self.assertEqual(eb.find_filename(lines, fence, valid_fnames), \"file1.py\")\n-\n-        # Test with filename in fence\n-        lines = [\"```python\", \"file3.py\", \"```\"]\n-        self.assertEqual(eb.find_filename(lines, fence, valid_fnames), \"dir/file3.py\")\n-\n-        # Test with no valid filename\n-        lines = [\"```\", \"invalid_file.py\", \"```\"]\n-        self.assertEqual(\"invalid_file.py\", eb.find_filename(lines, fence, valid_fnames))\n-\n-        # Test with multiple fences\n-        lines = [\"```python\", \"file1.py\", \"```\", \"```\", \"file2.py\", \"```\"]\n-        self.assertEqual(eb.find_filename(lines, fence, valid_fnames), \"file2.py\")\n-\n-        # Test with filename having extra characters\n-        lines = [\"# file1.py\", \"```\"]\n-        self.assertEqual(eb.find_filename(lines, fence, valid_fnames), \"file1.py\")\n-\n-        # Test with fuzzy matching\n-        lines = [\"file1_py\", \"```\"]\n-        self.assertEqual(eb.find_filename(lines, fence, valid_fnames), \"file1.py\")\n-\n"
    },
    "llm_calls": 1,
    "elapsed_time": 37.88373613357544,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 10489,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_tests_basic_test_exceptions.py",
    "exact_match": false,
    "similarity": 0.9967882108445116,
    "diff_lines": 52,
    "predicted_output": "import from aider.exceptions import ExInfo, LiteLLMExceptions\n\ndef test_litellm_exceptions_load():\n    \"\"\"Test that LiteLLMExceptions loads without errors\"\"\"\n    ex = LiteLLMExceptions()\n    assert len(ex.exceptions) > 0\n\ndef test_exceptions_tuple():\n    \"\"\"Test that exceptions_tuple returns a non-empty tuple\"\"\"\n    ex = LiteLLMExceptions()\n    assert isinstance(ex.exceptions_tuple(), tuple)\n    assert len(ex.exceptions_tuple()) > 0\n\ndef test_get_ex_info():\n    \"\"\"Test get_ex_info returns correct ExInfo\"\"\"\n    ex = LiteLLMExceptions()\n    \n    # Test with a known exception type\n    from litellm import AuthenticationError\n\n    auth_error = AuthenticationError(\n        message=\"Invalid API key\", llm_provider=\"openai\", model=\"gpt-4\"\n    )\n    ex_info = ex.get_ex_info(auth_error)\n    assert isinstance(ex_info, ExInfo)\n    assert ex_info.name == \"AuthenticationError\"\n    assert ex_info.retry is False\n    assert \"API key\" in ex_info.description\n\n    # Test with unknown exception type\n    class UnknownError(Exception):\n        pass\n\n    unknown = UnknownError()\n    ex_info = ex.get_ex_info(unknown)\n    assert isinstance(ex_info, ExInfo)\n    assert ex_info.name is None\n    assert ex_info.retry is None\n    assert ex_info.description is None\n\ndef test_rate_limit_error():\n    \"\"\"Test specific handling of RateLimitError\"\"\"\n    ex = LiteLLMExceptions()\n    from litellm import RateLimitError\n\n    rate_error = RateLimitError(message=\"Rate limit exceeded\", llm_provider=\"openai\", model=\"gpt-4\")\n    ex_info = ex.get_ex_info(rate_error)\n    assert ex_info.retry is True\n    assert \"rate limited\" in ex_info.description.lower()\n\ndef test_context_window_error():\n    \"\"\"Test specific handling of ContextWindowExceededError\"\"\"\n    ex = LiteLLMExceptions()\n    from litellm import ContextWindowExceededError\n\n    ctx_error = ContextWindowExceededError(\n        message=\"Context length exceeded\", model=\"gpt-4\", llm_provider=\"openai\"\n    )\n    ex_info = ex.get_ex_info(ctx_error)\n    assert ex_info.retry is False\n\ndef test_openrouter_error():\n    \"\"\"Test specific handling of OpenRouter API errors\"\"\"\n    ex = LiteLLMExceptions()\n    from litellm import APIConnectionError\n\n    # Create an APIConnectionError with OpenrouterException message\n    openrouter_error = APIConnectionError(\n        message=\"APIConnectionError: OpenrouterException - 'choices'\",\n        model=\"openrouter/model\",\n        llm_provider=\"openrouter\",\n    )\n\n    ex_info = ex.get_ex_info(openrouter_error)\n    assert ex_info.retry is True\n    assert \"OpenRouter\" in ex_info.description\n    assert \"overloaded\" in ex_info.description\n    assert \"rate\" in ex_info.description",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9967882108445116,
      "diff_lines": 52,
      "added_lines": 2,
      "removed_lines": 8,
      "diff_preview": "--- expected+++ predicted@@ -1,11 +1,9 @@-from aider.exceptions import ExInfo, LiteLLMExceptions\n-\n+import from aider.exceptions import ExInfo, LiteLLMExceptions\n \n def test_litellm_exceptions_load():\n     \"\"\"Test that LiteLLMExceptions loads without errors\"\"\"\n     ex = LiteLLMExceptions()\n     assert len(ex.exceptions) > 0\n-\n \n def test_exceptions_tuple():\n     \"\"\"Test that exceptions_tuple returns a non-empty tuple\"\"\"\n@@ -13,11 +11,10 @@     assert isinstance(ex.exceptions_tuple(), tuple)\n     assert len(ex.exceptions_tuple()) > 0\n \n-\n def test_get_ex_info():\n     \"\"\"Test get_ex_info returns correct ExInfo\"\"\"\n     ex = LiteLLMExceptions()\n-\n+    \n     # Test with a known exception type\n     from litellm import AuthenticationError\n \n@@ -41,7 +38,6 @@     assert ex_info.retry is None\n     assert ex_info.description is None\n \n-\n def test_rate_limit_error():\n     \"\"\"Test specific handling of RateLimitError\"\"\"\n     ex = LiteLLMExceptions()\n@@ -51,7 +47,6 @@     ex_info = ex.get_ex_info(rate_error)\n     assert ex_info.retry is True\n     assert \"rate limited\" in ex_info.description.lower()\n-\n \n def test_context_window_error():\n     \"\"\"Test specific handling of ContextWindowExceededError\"\"\"\n@@ -63,7 +58,6 @@     )\n     ex_info = ex.get_ex_info(ctx_error)\n     assert ex_info.retry is False\n-\n \n"
    },
    "llm_calls": 1,
    "elapsed_time": 17.607041120529175,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 3360,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_tests_basic_test_io.py",
    "exact_match": false,
    "similarity": 0.31070530633849414,
    "diff_lines": 436,
    "predicted_output": "import os\nimport unittest\nfrom pathlib import Path\nfrom unittest.mock import MagicMock, patch\n\nfrom aider.dump import dump  # noqa: F401\nfrom aider.io import AutoCompleter, ConfirmGroup, InputOutput\nfrom aider.utils import ChdirTemporaryDirectory\n\n\nclass TestInputOutput(unittest.TestCase):\n    def test_line_endings_validation(self):\n        # Test valid line endings\n        for ending in [\"platform\", \"lf\", \"crlf\"]:\n            io = InputOutput(line_endings=ending)\n            self.assertEqual(io.newline, None if ending == \"platform\" else \"\\n\" if ending == \"lf\" else \"\\r\\n\")\n\n        # Test invalid line endings\n        with self.assertRaises(ValueError) as cm:\n            io = InputOutput(line_endings=\"invalid\")\n        self.assertIn(\"Invalid line_endings value: invalid\", str(cm.exception))\n        # Check each valid option is in the error message\n        self.assertIn(\"platform\", str(cm.exception))\n        self.assertIn(\"crlf\", str(cm.exception))\n        self.assertIn(\"lf\", str(cm.exception))\n\n    def test_no_color_environment_variable(self):\n        with patch.dict(os.environ, {\"NO_COLOR\": \"1\"}):\n            io = InputOutput(fancy_input=False)\n            self.assertFalse(io.pretty)\n\n    def test_dumb_terminal(self):\n        with patch.dict(os.environ, {\"TERM\": \"dumb\"}):\n            io = InputOutput(fancy_input=True)\n            self.assertTrue(io.is_dumb_terminal)\n            self.assertFalse(io.pretty)\n            self.assertIsNone(io.prompt_session)\n\n    def test_autocompleter_get_command_completions(self):\n        # Step 3: Mock the commands object\n        commands = MagicMock()\n        commands.get_commands.return_value = [\"/help\", \"/add\", \"/drop\"]\n        commands.matching_commands.side_effect = lambda inp: (\n            [cmd for cmd in commands.get_commands() if cmd.startswith(inp.strip().split()[0])],\n            inp.strip().split()[0],\n            \" \".join(inp.strip().split()[1:]),\n        )\n        commands.get_raw_completions.return_value = None\n        commands.get_completions.side_effect = lambda cmd: [\"file1.txt\", \"file2.txt\"] if cmd == \"/add\" else None\n\n        # Step 4: Create an instance of AutoCompleter\n        root = \"\"\n        rel_fnames = []\n        addable_rel_fnames = []\n        autocompleter = AutoCompleter(\n            root=root,\n            rel_fnames=rel_fnames,\n            addable_rel_fnames=addable_rel_fnames,\n            commands=commands,\n            encoding=\"utf-8\",\n        )\n\n        # Step 5: Set up test cases\n        test_cases = [\n            # Input text, Expected completion texts\n            (\"/\", [\"/help\", \"/add\", \"/drop\"]),\n            (\"/a\", [\"/add\"]),\n            (\"/add \", [\"file1.txt\", \"file2.txt\"]),\n        ]\n\n        # Step 6: Iterate through test cases\n        for text, expected_completions in test_cases:\n            document = Document(text=text)\n            complete_event = CompleteEvent()\n            words = text.strip().split()\n\n            # Call get_command_completions\n            completions = list(\n                autocompleter.get_command_completions(\n                    document,\n                    complete_event,\n                    text,\n                    words,\n                )\n            )\n\n            # Extract completion texts\n            completion_texts = [comp.text for comp in completions]\n\n            # Assert that the completions match expected results\n            self.assertEqual(set(completion_texts), set(expected_completions))\n\n    # ... other tests\n\nif __name__ == \"__main__\":\n    unittest.main()",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.31070530633849414,
      "diff_lines": 436,
      "added_lines": 4,
      "removed_lines": 388,
      "diff_preview": "--- expected+++ predicted@@ -2,9 +2,6 @@ import unittest\n from pathlib import Path\n from unittest.mock import MagicMock, patch\n-\n-from prompt_toolkit.completion import CompleteEvent\n-from prompt_toolkit.document import Document\n \n from aider.dump import dump  # noqa: F401\n from aider.io import AutoCompleter, ConfirmGroup, InputOutput\n@@ -16,9 +13,7 @@         # Test valid line endings\n         for ending in [\"platform\", \"lf\", \"crlf\"]:\n             io = InputOutput(line_endings=ending)\n-            self.assertEqual(\n-                io.newline, None if ending == \"platform\" else \"\\n\" if ending == \"lf\" else \"\\r\\n\"\n-            )\n+            self.assertEqual(io.newline, None if ending == \"platform\" else \"\\n\" if ending == \"lf\" else \"\\r\\n\")\n \n         # Test invalid line endings\n         with self.assertRaises(ValueError) as cm:\n@@ -33,35 +28,6 @@         with patch.dict(os.environ, {\"NO_COLOR\": \"1\"}):\n             io = InputOutput(fancy_input=False)\n             self.assertFalse(io.pretty)\n-\n-    def test_color_initialization(self):\n-        \"\"\"Test that color values are properly initialized with # prefix\"\"\"\n-        # Test with hex colors without #\n-        io = InputOutput(\n-            user_input_color=\"00cc00\",\n-            tool_error_color=\"FF2222\",\n-            tool_warning_color=\"FFA500\",\n-            assistant_output_color=\"0088ff\",\n-            pretty=True,\n-        )\n-\n-        # Check that # was added to hex colors\n-        self.assertEqual(io.user_input_color, \"#00cc00\")\n-        self.assertEqual(io.tool_error_color, \"#FF2222\")\n-        self.assertEqual(io.tool_warning_color, \"#FFA500\")  # Already had #\n-        self.assertEqual(io.assistant_output_color, \"#0088ff\")\n-\n-        # Test with named colors (should be unchanged)\n-        io = InputOutput(user_input_color=\"blue\", tool_error_color=\"red\", pretty=True)\n-\n-        self.assertEqual(io.user_input_color, \"blue\")\n-        self.assertEqual(io.tool_error_color, \"red\")\n"
    },
    "llm_calls": 1,
    "elapsed_time": 18.94010090827942,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 21186,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_tests_basic_test_main.py",
    "exact_match": false,
    "similarity": 0.3420541309761638,
    "diff_lines": 1313,
    "predicted_output": "import os\nimport subprocess\nimport tempfile\nfrom io import StringIO\nfrom pathlib import Path\nfrom unittest import TestCase\nfrom unittest.mock import MagicMock, patch\n\nimport git\nfrom prompt_toolkit.input import DummyInput\nfrom prompt_toolkit.output import DummyOutput\n\nfrom aider.dump import dump  # noqa: F401\nfrom aider.io import InputOutput\nfrom aider.main import check_gitignore, load_dotenv_files, main, setup_git\nfrom aider.utils import GitTemporaryDirectory, IgnorantTemporaryDirectory, make_repo\n\n\nclass TestMain(TestCase):\n    def setUp(self):\n        self.original_env = os.environ.copy()\n        os.environ[\"OPENAI_API_KEY\"] = \"deadbeef\"\n        os.environ[\"AIDER_CHECK_UPDATE\"] = \"false\"\n        os.environ[\"AIDER_ANALYTICS\"] = \"false\"\n        self.original_cwd = os.getcwd()\n        self.tempdir_obj = IgnorantTemporaryDirectory()\n        self.tempdir = self.tempdir_obj.name\n        os.chdir(self.tempdir)\n        self.homedir_obj = IgnorantTemporaryDirectory()\n        os.environ[\"HOME\"] = self.homedir_obj.name\n        self.input_patcher = patch(\"builtins.input\", return_value=None)\n        self.mock_input = self.input_patcher.start()\n        self.webbrowser_patcher = patch(\"aider.io.webbrowser.open\")\n        self.mock_webbrowser = self.webbrowser_patcher.start()\n\n    def tearDown(self):\n        os.chdir(self.original_cwd)\n        self.tempdir_obj.cleanup()\n        self.homedir_obj.cleanup()\n        os.environ.clear()\n        os.environ.update(self.original_env)\n        self.input_patcher.stop()\n        self.webbrowser_patcher.stop()\n\n    def test_main_with_empty_dir_no_files_on_command(self):\n        main([\"--no-git\", \"--exit\", \"--yes\"], input=DummyInput(), output=DummyOutput())\n\n    def test_main_with_emptqy_dir_new_file(self):\n        main([\"foo.txt\", \"--yes\", \"--no-git\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n        self.assertTrue(os.path.exists(\"foo.txt\"))\n\n    @patch(\"aider.repo.GitRepo.get_commit_message\", return_value=\"mock commit message\")\n    def test_main_with_empty_git_dir_new_file(self, _):\n        make_repo()\n        main([\"--yes\", \"foo.txt\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n        self.assertTrue(os.path.exists(\"foo.txt\"))\n\n    @patch(\"aider.repo.GitRepo.get_commit_message\", return_value=\"mock commit message\")\n    def test_main_with_empty_git_dir_new_files(self, _):\n        make_repo()\n        main([\"--yes\", \"foo.txt\", \"bar.txt\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n        self.assertTrue(os.path.exists(\"foo.txt\"))\n        self.assertTrue(os.path.exists(\"bar.txt\"))\n\n    def test_main_with_dname_and_fname(self):\n        subdir = Path(\"subdir\")\n        subdir.mkdir()\n        make_repo(str(subdir))\n        res = main([\"subdir\", \"foo.txt\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n        self.assertNotEqual(res, None)\n\n    @patch(\"aider.repo.GitRepo.get_commit_message\", return_value=\"mock commit message\")\n    def test_main_with_subdir_repo_fnames(self, _):\n        subdir = Path(\"subdir\")\n        subdir.mkdir()\n        make_repo(str(subdir))\n        main(\n            [\"--yes\", str(subdir / \"foo.txt\"), str(subdir / \"bar.txt\"), \"--exit\"],\n            input=DummyInput(),\n            output=DummyOutput(),\n        )\n        self.assertTrue((subdir / \"foo.txt\").exists())\n        self.assertTrue((subdir / \"bar.txt\").exists())\n\n    def test_main_with_git_config_yml(self):\n        make_repo()\n\n        Path(\".aider.conf.yml\").write_text(\"auto-commits: false\\n\")\n        with patch(\"aider.main.InputOutput\") as MockInputOutput:\n            main([\"--yes\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n            _, kwargs = MockInputOutput.call_args\n            assert kwargs[\"auto_commits\"] is False\n\n        Path(\".aider.conf.yml\").write_text(\"auto-commits: true\\n\")\n        with patch(\"aider.main.InputOutput\") as MockInputOutput:\n            main([], input=DummyInput(), output=DummyOutput())\n            _, kwargs = MockInputOutput.call_args\n            assert kwargs[\"auto_commits\"] is True\n\n    def test_main_with_empty_git_dir_new_subdir_file(self):\n        make_repo()\n        subdir = Path(\"subdir\")\n        subdir.mkdir()\n        fname = subdir / \"foo.txt\"\n        fname.touch()\n        subprocess.run([\"git\", \"add\", str(subdir)])\n        subprocess.run([\"git\", \"commit\", \"-m\", \"added\"])\n\n        main([\"--yes\", str(fname), \"--exit\"], input=DummyInput(), output=DummyOutput())\n\n    def test_setup_git(self):\n        io = InputOutput(pretty=False, yes=True)\n        git_root = setup_git(None, io)\n        git_root = Path(git_root).resolve()\n        self.assertEqual(git_root, Path(self.tempdir).resolve())\n\n        self.assertTrue(git.Repo(self.tempdir))\n\n        gitignore = Path.cwd() / \".gitignore\"\n        self.assertTrue(gitignore.exists())\n        self.assertEqual(\".aider*\", gitignore.read_text().splitlines()[0])\n\n    def test_check_gitignore(self):\n        with GitTemporaryDirectory():\n            os.environ[\"GIT_CONFIG_GLOBAL\"] = \"globalgitconfig\"\n\n            io = InputOutput(pretty=False, yes=True)\n            cwd = Path.cwd()\n            gitignore = cwd / \".gitignore\"\n\n            self.assertFalse(gitignore.exists())\n            check_gitignore(cwd, io)\n            self.assertTrue(gitignore.exists())\n\n            self.assertEqual(\".aider*\", gitignore.read_text().splitlines()[0])\n\n            gitignore.write_text(\"one\\ntwo\\n\")\n            check_gitignore(cwd, io)\n            self.assertEqual(\"one\\ntwo\\n.aider*\\n\", gitignore.read_text())\n            del os.environ[\"GIT_CONFIG_GLOBAL\"]\n\n    def test_main_git_ignore(self):\n        cwd = Path().cwd()\n        self.assertFalse((cwd / \".git\").exists())\n        self.assertFalse((cwd / \".gitignore\").exists())\n\n        with patch(\"aider.main.InputOutput\") as MockInputOutput:\n            main([\"--yes\"], input=DummyInput())\n        self.assertTrue((cwd / \".git\").exists())\n        self.assertTrue((cwd / \".gitignore\").exists())\n\n    def test_main_args(self):\n        with patch(\"aider.main.InputOutput\") as MockInputOutput:\n            main([\"--no-auto-commits\", \"--yes\"], input=DummyInput())\n            _, kwargs = MockInputOutput.call_args\n            assert kwargs[\"auto_commits\"] is False\n\n        with patch(\"aider.main.InputOutput\") as MockInputOutput:\n            main([\"--auto-commits\"], input=DummyInput())\n            _, kwargs = MockInputOutput.call_args\n            assert kwargs[\"auto_commits\"] is True\n\n        with patch(\"aider.main.InputOutput\") as MockInputOutput:\n            main([], input=DummyInput())\n            _, kwargs = MockInputOutput.call_args\n            assert kwargs[\"dirty_commits\"] is True\n            assert kwargs[\"auto_commits\"] is True\n            assert kwargs[\"pretty\"] is True\n\n        with patch(\"aider.main.InputOutput\") as MockInputOutput:\n            main([\"--no-pretty\"], input=DummyInput())\n            _, kwargs = MockInputOutput.call_args\n            assert kwargs[\"pretty\"] is False\n\n        with patch(\"aider.main.InputOutput\") as MockInputOutput:\n            main([\"--pretty\"], input=DummyInput())\n            _, kwargs = MockInputOutput.call_args\n            assert kwargs[\"pretty\"] is True\n\n        with patch(\"aider.main.InputOutput\") as MockInputOutput:\n            main([\"--no-dirty-commits\"], input=DummyInput())\n            _, kwargs = MockInputOutput.call_args\n            assert kwargs[\"dirty_commits\"] is False\n\n        with patch(\"aider.main.InputOutput\") as MockInputOutput:\n            main([\"--dirty-commits\"], input=DummyInput())\n            _, kwargs = MockInputOutput.call_args\n            assert kwargs[\"dirty_commits\"] is True\n\n    def test_message_file_flag(self):\n        message_file_content = \"This is a test message from a file.\"\n        message_file_path = tempfile.mktemp()\n\n        with patch(\"aider.main.InputOutput\") as MockInputOutput:\n            MockInputOutput.return_value.run = MagicMock()\n            main(\n                [\"--yes\", \"--message-file\", message_file_path],\n                input=DummyInput(),\n                output=DummyOutput(),\n            )\n            MockInputOutput.return_value.run.assert_called_once_with(with_message=message_file_content)\n\n        os.remove(message_file_path)\n\n    def test_encodings_arg(self):\n        fname = \"foo.py\"\n\n        with GitTemporaryDirectory():\n            with patch(\"aider.main.InputOutput\") as MockSend:\n                def side_effect(*args, **kwargs):\n                    self.assertEqual(kwargs[\"encoding\"], \"iso-8859-15\")\n                    return MagicMock()\n\n                MockSend.side_effect = side_effect\n\n                main([\"--yes\", fname, \"--encoding\", \"iso-8859-15\"])\n\n    @patch(\"aider.main.InputOutput\")\n    @patch(\"aider.coders.base_coder.Coder.run\")\n    def test_main_message_adds_to_input_history(self, mock_run, MockInputOutput):\n        test_message = \"test message\"\n        mock_io_instance = MockInputOutput.return_value\n\n        main([\"--message\", test_message], input=DummyInput(), output=DummyOutput())\n\n        mock_io_instance.add_to_input_history.assert_called_once_with(test_message)\n\n    @patch(\"aider.main.InputOutput\")\n    @patch(\"aider.coders.base_coder.Coder.run\")\n    def test_yes(self, mock_run, MockInputOutput):\n        test_message = \"test message\"\n\n        main([\"--yes\", \"--message\", test_message])\n        args, kwargs = MockInputOutput.call_args\n        self.assertTrue(args[1])\n\n    @patch(\"aider.main.InputOutput\")\n    @patch(\"aider.coders.base_coder.Coder.run\")\n    def test_default_yes(self, mock_run, MockInputOutput):\n        test_message = \"test message\"\n\n        main([\"--message\", test_message])\n        args, kwargs = MockInputOutput.call_args\n        self.assertEqual(args[1], None)\n\n    def test_dark_mode_sets_code_theme(self):\n        with patch(\"aider.main.InputOutput\") as MockInputOutput:\n            main([\"--dark-mode\", \"--no-git\"], input=DummyInput(), output=DummyOutput())\n            MockInputOutput.assert_called_once()\n            _, kwargs = MockInputOutput.call_args\n            self.assertEqual(kwargs[\"code_theme\"], \"monokai\")\n\n    def test_light_mode_sets_code_theme(self):\n        with patch(\"aider.main.InputOutput\") as MockInputOutput:\n            main([\"--light-mode\", \"--no-git\"], input=DummyInput(), output=DummyOutput())\n            MockInputOutput.assert_called_once()\n            _, kwargs = MockInputOutput.call_args\n            self.assertEqual(kwargs[\"code_theme\"], \"default\")\n\n    def create_env_file(self, file_name, content):\n        env_file_path = Path(self.tempdir) / file_name\n        env_file_path.write_text(content)\n        return env_file_path\n\n    def test_env_file_flag_sets_automatic_variable(self):\n        env_file_path = self.create_env_file(\".env.test\", \"AIDER_DARK_MODE=True\")\n        with patch(\"aider.main.InputOutput\") as MockInputOutput:\n            main(\n                [\"--env-file\", str(env_file_path), \"--no-git\"],\n                input=DummyInput(),\n                output=DummyOutput(),\n            )\n            MockInputOutput.assert_called_once()\n            _, kwargs = MockInputOutput.call_args\n            self.assertEqual(kwargs[\"code_theme\"], \"monokai\")\n\n    def test_default_env_file_sets_automatic_variable(self):\n        self.create_env_file(\".env\", \"AIDER_DARK_MODE=True\")\n        with patch(\"aider.main.InputOutput\") as MockInputOutput:\n            main([\"--no-git\"], input=DummyInput(), output=DummyOutput())\n            MockInputOutput.assert_called_once()\n            _, kwargs = MockInputOutput.call_args\n            self.assertEqual(kwargs[\"code_theme\"], \"monokai\")\n\n    def test_false_vals_in_env_file(self):\n        self.create_env_file(\".env\", \"AIDER_SHOW_DIFFS=off\")\n        with patch(\"aider.coders.Coder.create\") as MockCoder:\n            main([\"--no-git\", \"--yes\"], input=DummyInput(), output=DummyOutput())\n            MockCoder.assert_called_once()\n            _, kwargs = MockCoder.call_args\n            self.assertEqual(kwargs[\"show_diffs\"], False)\n\n    def test_true_vals_in_env_file(self):\n        self.create_env_file(\".env\", \"AIDER_SHOW_DIFFS=on\")\n        with patch(\"aider.coders.Coder.create\") as MockCoder:\n            main([\"--no-git\", \"--yes\"], input=DummyInput(), output=DummyOutput())\n            MockCoder.assert_called_once()\n            _, kwargs = MockCoder.call_args\n            self.assertEqual(kwargs[\"show_diffs\"], True)\n\n    def test_verbose_mode_lists_env_vars(self):\n        self.create_env_file(\".env\", \"AIDER_DARK_MODE=on\")\n        with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n            main([\"--no-git\", \"--verbose\", \"--exit\", \"--yes\"], input=DummyInput(), output=DummyOutput())\n            output = mock_stdout.getvalue()\n            relevant_output = \"\\n\".join(\n                line\n                for line in output.splitlines()\n                if \"AIDER_DARK_MODE\" in line or \"dark_mode\" in line\n            )\n            self.assertIn(\"AIDER_DARK_MODE\", relevant_output)\n            self.assertIn(\"dark_mode\", relevant_output)\n            self.assertRegex(relevant_output, r\"AIDER_DARK_MODE:\\s+on\")\n            self.assertRegex(relevant_output, r\"dark_mode:\\s+True\")\n\n    # Add tests for the new features incorporated\n    def test_git_config_include_directive(self):\n        with GitTemporaryDirectory() as git_dir:\n            git_dir = Path(git_dir)\n            include_config = git_dir / \"included.gitconfig\"\n            include_config.write_text(\n                \"[user]\\n    name = Directive User\\n    email = directive@example.com\\n\"\n            )\n            # Set up main git config with include directive\n            git_config = git_dir / \".git\" / \"config\"\n            with open(git_config, \"a\") as f:\n                f.write(f\"\\n[include]\\n    path = {str(include_config).replace('\\\\', '/')}\\n\")\n            # Read the modified config file\n            modified_config_content = git_config.read_text()\n            # Verify the include directive was added correctly\n            self.assertIn(\"[include]\", modified_config_content)\n            # Verify the config is set up correctly using git command\n            repo = git.Repo(git_dir)\n            self.assertEqual(repo.git.config(\"user.name\"), \"Directive User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"directive@example.com\")\n            # Run aider and verify it doesn't change the git config\n            main([\"--yes\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n            # Check that the user settings are still the same using git command\n            repo = git.Repo(git_dir)  # Re-open repo to ensure we get fresh config\n            self.assertEqual(repo.git.config(\"user.name\"), \"Directive User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"directive@example.com\")\n\n    def test_git_config_include(self):\n        # Test that aider respects git config includes for user.name and user.email\n        with GitTemporaryDirectory() as git_dir:\n            git_dir = Path(git_dir)\n            include_config = git_dir / \"included.gitconfig\"\n            include_config.write_text(\n                \"[user]\\n    name = Included User\\n    email = included@example.com\\n\"\n            )\n            # Set up main git config to include the other file\n            repo = git.Repo(git_dir)\n            repo.git.config(\"--local\", \"include.path\", str(include_config))\n            # Verify the config is set up correctly\n            self.assertEqual(repo.git.config(\"user.name\"), \"Included User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"included@example.com\")\n            # Run aider and verify it doesn't change the git config\n            main([\"--yes\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n            # Check that the user settings are still the same\n            repo = git.Repo(git_dir)  # Re-open repo to ensure we get fresh config\n            self.assertEqual(repo.git.config(\"user.name\"), \"Included User\")\n            self.assertEqual(repo.git.config(\"user.email\"), \"included@example.com\")",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.3420541309761638,
      "diff_lines": 1313,
      "added_lines": 86,
      "removed_lines": 1071,
      "diff_preview": "--- expected+++ predicted@@ -1,4 +1,3 @@-import json\n import os\n import subprocess\n import tempfile\n@@ -11,7 +10,6 @@ from prompt_toolkit.input import DummyInput\n from prompt_toolkit.output import DummyOutput\n \n-from aider.coders import Coder\n from aider.dump import dump  # noqa: F401\n from aider.io import InputOutput\n from aider.main import check_gitignore, load_dotenv_files, main, setup_git\n@@ -28,7 +26,6 @@         self.tempdir_obj = IgnorantTemporaryDirectory()\n         self.tempdir = self.tempdir_obj.name\n         os.chdir(self.tempdir)\n-        # Fake home directory prevents tests from using the real ~/.aider.conf.yml file:\n         self.homedir_obj = IgnorantTemporaryDirectory()\n         os.environ[\"HOME\"] = self.homedir_obj.name\n         self.input_patcher = patch(\"builtins.input\", return_value=None)\n@@ -69,7 +66,7 @@         subdir = Path(\"subdir\")\n         subdir.mkdir()\n         make_repo(str(subdir))\n-        res = main([\"subdir\", \"foo.txt\"], input=DummyInput(), output=DummyOutput())\n+        res = main([\"subdir\", \"foo.txt\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n         self.assertNotEqual(res, None)\n \n     @patch(\"aider.repo.GitRepo.get_commit_message\", return_value=\"mock commit message\")\n@@ -89,15 +86,15 @@         make_repo()\n \n         Path(\".aider.conf.yml\").write_text(\"auto-commits: false\\n\")\n-        with patch(\"aider.coders.Coder.create\") as MockCoder:\n-            main([\"--yes\"], input=DummyInput(), output=DummyOutput())\n-            _, kwargs = MockCoder.call_args\n+        with patch(\"aider.main.InputOutput\") as MockInputOutput:\n+            main([\"--yes\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n+            _, kwargs = MockInputOutput.call_args\n             assert kwargs[\"auto_commits\"] is False\n \n         Path(\".aider.conf.yml\").write_text(\"auto-commits: true\\n\")\n-        with patch(\"aider.coders.Coder.create\") as MockCoder:\n+        with patch(\"aider.main.InputOutput\") as MockInputOutput:\n             main([], input=DummyInput(), output=DummyOutput())\n-            _, kwargs = MockCoder.call_args\n+            _, kwargs = MockInputOutput.call_args\n"
    },
    "llm_calls": 1,
    "elapsed_time": 94.10125803947449,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 77009,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_tests_basic_test_models.py",
    "exact_match": false,
    "similarity": 0.4365793507965074,
    "diff_lines": 516,
    "predicted_output": "import unittest\nfrom unittest.mock import MagicMock, patch\nfrom aider.models import (\n    ANTHROPIC_BETA_HEADER,\n    Model,\n    ModelInfoManager,\n    register_models,\n)\n\n\nclass TestModels(unittest.TestCase):\n    def setUp(self):\n        \"\"\"Reset MODEL_SETTINGS before each test\"\"\"\n        from aider.models import MODEL_SETTINGS\n\n        self._original_settings = MODEL_SETTINGS.copy()\n\n    def tearDown(self):\n        \"\"\"Restore original MODEL_SETTINGS after each test\"\"\"\n        from aider.models import MODEL_SETTINGS\n\n        MODEL_SETTINGS.clear()\n        MODEL_SETTINGS.extend(self._original_settings)\n\n    def test_get_model_info_nonexistent(self):\n        manager = ModelInfoManager()\n        info = manager.get_model_info(\"non-existent-model\")\n        self.assertEqual(info, {})\n\n    def test_max_context_tokens(self):\n        model = Model(\"gpt-3.5-turbo\")\n        self.assertEqual(model.info[\"max_input_tokens\"], 16385)\n\n        model = Model(\"gpt-3.5-turbo-16k\")\n        self.assertEqual(model.info[\"max_input_tokens\"], 16385)\n\n        model = Model(\"gpt-3.5-turbo-1106\")\n        self.assertEqual(model.info[\"max_input_tokens\"], 16385)\n\n        model = Model(\"gpt-4\")\n        self.assertEqual(model.info[\"max_input_tokens\"], 8 * 1024)\n\n        model = Model(\"gpt-4-32k\")\n        self.assertEqual(model.info[\"max_input_tokens\"], 32 * 1024)\n\n        model = Model(\"gpt-4-0613\")\n        self.assertEqual(model.info[\"max_input_tokens\"], 8 * 1024)\n\n    @patch(\"aider.models.check_for_dependencies\")\n    def test_sanity_check_model_calls_check_dependencies(self, mock_check_deps):\n        \"\"\"Test that sanity_check_model calls check_for_dependencies\"\"\"\n        mock_io = MagicMock()\n        model = MagicMock()\n        model.name = \"test-model\"\n        model.missing_keys = []\n        model.keys_in_environment = True\n        model.info = {\"some\": \"info\"}\n\n        sanity_check_model(mock_io, model)\n\n        # Verify check_for_dependencies was called with the model name\n        mock_check_deps.assert_called_once_with(mock_io, \"test-model\")\n\n    def test_model_aliases(self):\n        # Test common aliases\n        model = Model(\"4\")\n        self.assertEqual(model.name, \"gpt-4-0613\")\n\n        model = Model(\"4o\")\n        self.assertEqual(model.name, \"gpt-4o\")\n\n        model = Model(\"35turbo\")\n        self.assertEqual(model.name, \"gpt-3.5-turbo\")\n\n        model = Model(\"35-turbo\")\n        self.assertEqual(model.name, \"gpt-3.5-turbo\")\n\n        model = Model(\"3\")\n        self.assertEqual(model.name, \"gpt-3.5-turbo\")\n\n        model = Model(\"sonnet\")\n        self.assertEqual(model.name, \"anthropic/claude-3-7-sonnet-20250219\")\n\n        model = Model(\"haiku\")\n        self.assertEqual(model.name, \"claude-3-5-haiku-20241022\")\n\n        model = Model(\"opus\")\n        self.assertEqual(model.name, \"claude-3-opus-20240229\")\n\n        # Test GitHub Copilot models\n        model = Model(\"github/o1-mini\")\n        self.assertEqual(model.name, \"github/o1-mini\")\n        self.assertEqual(model.use_temperature, False)\n\n        model = Model(\"github/o1-preview\")\n        self.assertEqual(model.name, \"github/o1-preview\")\n        self.assertEqual(model.use_temperature, False)\n\n    @patch(\"aider.models.check_pip_install_extra\")\n    def test_check_for_dependencies_bedrock(self, mock_check_pip):\n        \"\"\"Test that check_for_dependencies calls check_pip_install_extra for Bedrock models\"\"\"\n        from aider.io import InputOutput\n\n        io = InputOutput()\n        # Test with a Bedrock model\n        from aider.models import check_for_dependencies\n\n        check_for_dependencies(io, \"bedrock/anthropic.claude-3-sonnet-20240229-v1:0\")\n\n        # Verify check_pip_install_extra was called with correct arguments\n        mock_check_pip.assert_called_once_with(\n            io, \"boto3\", \"AWS Bedrock models require the boto3 package.\", [\"boto3\"]\n        )\n\n    @patch(\"aider.models.check_pip_install_extra\")\n    def test_check_for_dependencies_vertex_ai(self, mock_check_pip):\n        \"\"\"Test that check_for_dependencies calls check_pip_install_extra for Vertex AI models\"\"\"\n        from aider.io import InputOutput\n\n        io = InputOutput()\n        # Test with a Vertex AI model\n        from aider.models import check_for_dependencies\n\n        check_for_dependencies(io, \"vertex_ai/gemini-1.5-pro\")\n\n        # Verify check_pip_install_extra was called with correct arguments\n        mock_check_pip.assert_called_once_with(\n            io,\n            \"google.cloud.aiplatform\",\n            \"Google Vertex AI models require the google-cloud-aiplatform package.\",\n            [\"google-cloud-aiplatform\"],\n        )\n\n    @patch(\"aider.models.check_pip_install_extra\")\n    def test_check_for_dependencies_other_model(self, mock_check_pip):\n        \"\"\"Test that check_for_dependencies doesn't call check_pip_install_extra for other models\"\"\"\n        from aider.io import InputOutput\n\n        io = InputOutput()\n        # Test with a non-Bedrock, non-Vertex AI model\n        from aider.models import check_for_dependencies\n\n        check_for_dependencies(io, \"gpt-4\")\n\n        # Verify check_pip_install_extra was not called\n        mock_check_pip.assert_not_called()\n\n    def test_get_repo_map_tokens(self):\n        # Test default case (no max_input_tokens in info)\n        model = Model(\"gpt-4\")\n        model.info = {}\n        self.assertEqual(model.get_repo_map_tokens(), 1024)\n\n        # Test minimum boundary (max_input_tokens < 8192)\n        model.info = {\"max_input_tokens\": 4096}\n        self.assertEqual(model.get_repo_map_tokens(), 1024)\n\n        # Test middle range (max_input_tokens = 16384)\n        model.info = {\"max_input_tokens\": 16384}\n        self.assertEqual(model.get_repo_map_tokens(), 2048)\n\n        # Test maximum boundary (max_input_tokens > 32768)\n        model.info = {\"max_input_tokens\": 65536}\n        self.assertEqual(model.get_repo_map_tokens(), 4096)\n\n        # Test exact boundary values\n        model.info = {\"max_input_tokens\": 8192}\n        self.assertEqual(model.get_repo_map_tokens(), 1024)\n\n        model.info = {\"max_input_tokens\": 32768}\n        self.assertEqual(model.get_repo_map_tokens(), 4096)\n\n    def test_parse_token_value(self):\n        # Create a model instance to test the parse_token_value method\n        model = Model(\"gpt-4\")\n\n        # Test integer inputs\n        self.assertEqual(model.parse_token_value(8096), 8096)\n        self.assertEqual(model.parse_token_value(1000), 1000)\n\n        # Test string inputs\n        self.assertEqual(model.parse_token_value(\"8096\"), 8096)\n\n        # Test k/K suffix (kilobytes)\n        self.assertEqual(model.parse_token_value(\"8k\"), 8 * 1024)\n        self.assertEqual(model.parse_token_value(\"8K\"), 8 * 1024)\n        self.assertEqual(model.parse_token_value(\"10.5k\"), 10.5 * 1024)\n        self.assertEqual(model.parse_token_value(\"0.5K\"), 0.5 * 1024)\n\n        # Test m/M suffix (megabytes)\n        self.assertEqual(model.parse_token_value(\"1m\"), 1 * 1024 * 1024)\n        self.assertEqual(model.parse_token_value(\"1M\"), 1 * 1024 * 1024)\n        self.assertEqual(model.parse_token_value(\"0.5M\"), 0.5 * 1024 * 1024)\n\n        # Test with spaces\n        self.assertEqual(model.parse_token_value(\" 8k \"), 8 * 1024)\n\n        # Test conversion from other types\n        self.assertEqual(model.parse_token_value(8.0), 8)\n\n    def test_set_thinking_tokens(self):\n        # Test that set_thinking_tokens correctly sets the tokens with different formats\n        model = Model(\"gpt-4\")\n\n        # Test with integer\n        model.set_thinking_tokens(8096)\n        self.assertEqual(model.extra_params[\"thinking\"][\"budget_tokens\"], 8096)\n        self.assertFalse(model.use_temperature)\n\n        # Test with string\n        model.set_thinking_tokens(\"10k\")\n        self.assertEqual(model.extra_params[\"thinking\"][\"budget_tokens\"], 10 * 1024)\n\n        # Test with decimal value\n        model.set_thinking_tokens(\"0.5M\")\n        self.assertEqual(model.extra_params[\"thinking\"][\"budget_tokens\"], 0.5 * 1024 * 1024)\n\n    @patch(\"aider.models.check_pip_install_extra\")\n    def test_check_for_dependencies_bedrock(self, mock_check_pip):\n        \"\"\"Test that check_for_dependencies calls check_pip_install_extra for Bedrock models\"\"\"\n        from aider.io import InputOutput\n        io = InputOutput()\n        check_for_dependencies(io, \"bedrock/anthropic.claude-3-sonnet-20240229-v1:0\")\n        mock_check_pip.assert_called_once_with(io, \"boto3\", \"AWS Bedrock models require the boto3 package.\", [\"boto3\"])\n\n    @patch(\"aider.models.check_pip_install_extra\")\n    def test_check_for_dependencies_vertex_ai(self, mock_check_pip):\n        \"\"\"Test that check_for_dependencies calls check_pip_install_extra for Vertex AI models\"\"\"\n        from aider.io import InputOutput\n        io = InputOutput()\n        check_for_dependencies(io, \"vertex_ai/gemini-1.5-pro\")\n        mock_check_pip.assert_called_once_with(io, \"google.cloud.aiplatform\", \"Google Vertex AI models require the google-cloud-aiplatform package.\", [\"google-cloud-aiplatform\"])\n\n    @patch(\"aider.models.check_pip_install_extra\")\n    def test_check_for_dependencies_other_model(self, mock_check_pip):\n        \"\"\"Test that check_for_dependencies doesn't call check_pip_install_extra for other models\"\"\"\n        from aider.io import InputOutput\n        io = InputOutput()\n        check_for_dependencies(io, \"gpt-4\")\n        mock_check_pip.assert_not_called()\n\nif __name__ == \"__main__\":\n    unittest.main()",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.4365793507965074,
      "diff_lines": 516,
      "added_lines": 67,
      "removed_lines": 387,
      "diff_preview": "--- expected+++ predicted@@ -1,13 +1,10 @@ import unittest\n-from unittest.mock import ANY, MagicMock, patch\n-\n+from unittest.mock import MagicMock, patch\n from aider.models import (\n     ANTHROPIC_BETA_HEADER,\n     Model,\n     ModelInfoManager,\n     register_models,\n-    sanity_check_model,\n-    sanity_check_models,\n )\n \n \n@@ -48,62 +45,6 @@ \n         model = Model(\"gpt-4-0613\")\n         self.assertEqual(model.info[\"max_input_tokens\"], 8 * 1024)\n-\n-    @patch(\"os.environ\")\n-    def test_sanity_check_model_all_set(self, mock_environ):\n-        mock_environ.get.return_value = \"dummy_value\"\n-        mock_io = MagicMock()\n-        model = MagicMock()\n-        model.name = \"test-model\"\n-        model.missing_keys = [\"API_KEY1\", \"API_KEY2\"]\n-        model.keys_in_environment = True\n-        model.info = {\"some\": \"info\"}\n-\n-        sanity_check_model(mock_io, model)\n-\n-        mock_io.tool_output.assert_called()\n-        calls = mock_io.tool_output.call_args_list\n-        self.assertIn(\"- API_KEY1: Set\", str(calls))\n-        self.assertIn(\"- API_KEY2: Set\", str(calls))\n-\n-    @patch(\"os.environ\")\n-    def test_sanity_check_model_not_set(self, mock_environ):\n-        mock_environ.get.return_value = \"\"\n-        mock_io = MagicMock()\n-        model = MagicMock()\n-        model.name = \"test-model\"\n-        model.missing_keys = [\"API_KEY1\", \"API_KEY2\"]\n-        model.keys_in_environment = True\n-        model.info = {\"some\": \"info\"}\n-\n-        sanity_check_model(mock_io, model)\n"
    },
    "llm_calls": 1,
    "elapsed_time": 65.28935694694519,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 34611,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_tests_basic_test_onboarding.py",
    "exact_match": false,
    "similarity": 0.9639593908629441,
    "diff_lines": 149,
    "predicted_output": "import argparse\nimport base64\nimport hashlib\nimport os\nimport unittest\nfrom unittest.mock import MagicMock, patch\n\nimport requests\n\n# Mock the Analytics class as it's used in some functions\nclass DummyAnalytics:\n    def event(self, *args, **kwargs):\n        pass\n\n\n# Mock the InputOutput class\nclass DummyIO:\n    def tool_output(self, *args, **kwargs):\n        pass\n\n    def tool_warning(self, *args, **kwargs):\n        pass\n\n    def tool_error(self, *args, **kwargs):\n        pass\n\n    def confirm_ask(self, *args, **kwargs):\n        return False  # Default to no confirmation\n\n    def offer_url(self, *args, **kwargs):\n        pass\n\n\n# Import the functions to be tested\nfrom aider.onboarding import (\n    check_openrouter_tier,\n    exchange_code_for_key,\n    find_available_port,\n    generate_pkce_codes,\n    offer_openrouter_oauth,\n    select_default_model,\n    start_openrouter_oauth_flow,\n    try_to_select_default_model,\n)\n\n\nclass TestOnboarding(unittest.TestCase):\n    @patch(\"requests.get\")\n    def test_check_openrouter_tier_free(self, mock_get):\n        \"\"\"Test check_openrouter_tier identifies free tier.\"\"\"\n        mock_response = MagicMock()\n        mock_response.json.return_value = {\"data\": {\"is_free_tier\": True}}\n        mock_response.raise_for_status.return_value = None\n        mock_get.return_value = mock_response\n        self.assertTrue(check_openrouter_tier(\"fake_key\"))\n        mock_get.assert_called_once_with(\n            \"https://openrouter.ai/api/v1/auth/key\",\n            headers={\"Authorization\": \"Bearer fake_key\"},\n            timeout=5,\n        )\n\n    @patch(\"requests.get\")\n    def test_check_openrouter_tier_paid(self, mock_get):\n        \"\"\"Test check_openrouter_tier identifies paid tier.\"\"\"\n        mock_response = MagicMock()\n        mock_response.json.return_value = {\"data\": {\"is_free_tier\": False}}\n        mock_response.raise_for_status.return_value = None\n        mock_get.return_value = mock_response\n        self.assertFalse(check_openrouter_tier(\"fake_key\"))\n\n    @patch(\"requests.get\")\n    def test_check_openrouter_tier_api_error(self, mock_get):\n        \"\"\"Test check_openrouter_tier defaults to free on API error.\"\"\"\n        mock_get.side_effect = requests.exceptions.RequestException(\"API Error\")\n        self.assertTrue(check_openrouter_tier(\"fake_key\"))\n\n    @patch(\"requests.get\")\n    def test_check_openrouter_tier_missing_key(self, mock_get):\n        \"\"\"Test check_openrouter_tier defaults to free if key is missing in response.\"\"\"\n        mock_response = MagicMock()\n        mock_response.json.return_value = {\"data\": {}}  # Missing 'is_free_tier'\n        mock_response.raise_for_status.return_value = None\n        mock_get.return_value = mock_response\n        self.assertTrue(check_openrouter_tier(\"fake_key\"))\n\n    @patch(\"aider.onboarding.check_openrouter_tier\")\n    @patch.dict(os.environ, {}, clear=True)\n    def test_try_select_default_model_no_keys(self, mock_check_tier):\n        \"\"\"Test no model is selected when no keys are present.\"\"\"\n        self.assertIsNone(try_to_select_default_model())\n        mock_check_tier.assert_not_called()\n\n    @patch(\"aider.onboarding.check_openrouter_tier\", return_value=True)  # Assume free tier\n    @patch.dict(os.environ, {\"OPENROUTER_API_KEY\": \"or_key\"}, clear=True)\n    def test_try_select_default_model_openrouter_free(self, mock_check_tier):\n        \"\"\"Test OpenRouter free model selection.\"\"\"\n        self.assertEqual(\n            try_to_select_default_model(), \"openrouter/google/gemini-2.5-pro-exp-03-25:free\"\n        )\n        mock_check_tier.assert_called_once_with(\"or_key\")\n\n    @patch(\"aider.onboarding.check_openrouter_tier\", return_value=False)  # Assume paid tier\n    @patch.dict(os.environ, {\"OPENROUTER_API_KEY\": \"or_key\"}, clear=True)\n    def test_try_select_default_model_openrouter_paid(self, mock_check_tier):\n        \"\"\"Test OpenRouter paid model selection.\"\"\"\n        self.assertEqual(try_to_select_default_model(), \"openrouter/anthropic/claude-3.7-sonnet\")\n        mock_check_tier.assert_called_once_with(\"or_key\")\n\n    @patch(\"aider.onboarding.check_openrouter_tier\")\n    @patch.dict(os.environ, {\"ANTHROPIC_API_KEY\": \"an_key\"}, clear=True)\n    def test_try_select_default_model_anthropic(self, mock_check_tier):\n        \"\"\"Test Anthropic model selection.\"\"\"\n        self.assertEqual(try_to_select_default_model(), \"sonnet\")\n        mock_check_tier.assert_not_called()\n\n    @patch(\"aider.onboarding.check_openrouter_tier\")\n    @patch.dict(os.environ, {\"DEEPSEEK_API_KEY\": \"ds_key\"}, clear=True)\n    def test_try_select_default_model_deepseek(self, mock_check_tier):\n        \"\"\"Test Deepseek model selection.\"\"\"\n        self.assertEqual(try_to_select_default_model(), \"deepseek\")\n        mock_check_tier.assert_not_called()\n\n    @patch(\"aider.onboarding.check_openrouter_tier\")\n    @patch.dict(os.environ, {\"OPENAI_API_KEY\": \"oa_key\"}, clear=True)\n    def test_try_select_default_model_openai(self, mock_check_tier):\n        \"\"\"Test OpenAI model selection.\"\"\"\n        self.assertEqual(try_to_select_default_model(), \"gpt-4o\")\n        mock_check_tier.assert_not_called()\n\n    @patch(\"aider.onboarding.check_openrouter_tier\")\n    @patch.dict(os.environ, {\"GEMINI_API_KEY\": \"gm_key\"}, clear=True)\n    def test_try_select_default_model_gemini(self, mock_check_tier):\n        \"\"\"Test Gemini model selection.\"\"\"\n        self.assertEqual(try_to_select_default_model(), \"gemini/gemini-2.5-pro-exp-03-25\")\n        mock_check_tier.assert_not_called()\n\n    @patch(\"aider.onboarding.check_openrouter_tier\")\n    @patch.dict(os.environ, {\"VERTEXAI_PROJECT\": \"vx_proj\"}, clear=True)\n    def test_try_select_default_model_vertex(self, mock_check_tier):\n        \"\"\"Test Vertex AI model selection.\"\"\"\n        self.assertEqual(try_to_select_default_model(), \"vertex_ai/gemini-2.5-pro-exp-03-25\")\n        mock_check_tier.assert_not_called()\n\n    @patch(\"aider.onboarding.check_openrouter_tier\", return_value=False)  # Paid\n    @patch.dict(os.environ, {\"OPENROUTER_API_KEY\": \"or_key\", \"OPENAI_API_KEY\": \"oa_key\"}, clear=True)\n    def test_try_select_default_model_priority_openrouter(self, mock_check_tier):\n        \"\"\"Test OpenRouter key takes priority.\"\"\"\n        self.assertEqual(try_to_select_default_model(), \"openrouter/anthropic/claude-3.7-sonnet\")\n        mock_check_tier.assert_called_once_with(\"or_key\")\n\n    @patch(\"aider.onboarding.check_openrouter_tier\")\n    @patch.dict(os.environ, {\"ANTHROPIC_API_KEY\": \"an_key\", \"OPENAI_API_KEY\": \"oa_key\"}, clear=True)\n    def test_try_select_default_model_priority_anthropic(self, mock_check_tier):\n        \"\"\"Test Anthropic key takes priority over OpenAI.\"\"\"\n        self.assertEqual(try_to_select_default_model(), \"sonnet\")\n        mock_check_tier.assert_not_called()\n\n    @patch(\"socketserver.TCPServer\")\n    def test_find_available_port_success(self, mock_tcp_server):\n        \"\"\"Test finding an available port.\"\"\"\n        # Simulate port 8484 being available\n        mock_tcp_server.return_value.__enter__.return_value = None  # Allow context manager\n        port = find_available_port(start_port=8484, end_port=8484)\n        self.assertEqual(port, 8484)\n        mock_tcp_server.assert_called_once_with((\"localhost\", 8484), None)\n\n    @patch(\"socketserver.TCPServer\")\n    def test_find_available_port_in_use(self, mock_tcp_server):\n        \"\"\"Test finding the next available port if the first is in use.\"\"\"\n        # Simulate port 8484 raising OSError, 8485 being available\n        mock_tcp_server.side_effect = [OSError, MagicMock()]\n        mock_tcp_server.return_value.__enter__.return_value = None  # Allow context manager\n        port = find_available_port(start_port=8484, end_port=8485)\n        self.assertEqual(port, 8485)\n        self.assertEqual(mock_tcp_server.call_count, 2)\n        mock_tcp_server.assert_any_call((\"localhost\", 8484), None)\n        mock_tcp_server.assert_any_call((\"localhost\", 8485), None)\n\n    @patch(\"socketserver.TCPServer\", side_effect=OSError)\n    def test_find_available_port_none_available(self, mock_tcp_server):\n        \"\"\"Test returning None if no ports are available in the range.\"\"\"\n        port = find_available_port(start_port=8484, end_port=8485)\n        self.assertIsNone(port)\n        self.assertEqual(mock_tcp_server.call_count, 2)  # Tried 8484 and 8485\n\n    def test_generate_pkce_codes(self):\n        \"\"\"Test PKCE code generation.\"\"\"\n        verifier, challenge = generate_pkce_codes()\n        self.assertIsInstance(verifier, str)\n        self.assertIsInstance(challenge, str)\n        self.assertGreater(len(verifier), 40)  # Check reasonable length\n        self.assertGreater(len(challenge), 40)\n        # Verify the challenge is the SHA256 hash of the verifier, base64 encoded\n        hasher = hashlib.sha256()\n        hasher.update(verifier.encode(\"utf-8\"))\n        expected_challenge = base64.urlsafe_b64encode(hasher.digest()).rstrip(b\"=\").decode(\"utf-8\")\n        self.assertEqual(challenge, expected_challenge)\n\n    @patch(\"requests.post\")\n    def test_exchange_code_for_key_success(self, mock_post):\n        \"\"\"Test successful code exchange for API key.\"\"\"\n        mock_response = MagicMock()\n        mock_response.json.return_value = {\"key\": \"test_api_key\"}\n        mock_response.raise_for_status.return_value = None\n        mock_post.return_value = mock_response\n        io_mock = DummyIO()\n\n        api_key = exchange_code_for_key(\"auth_code\", \"verifier\", io_mock)\n\n        self.assertEqual(api_key, \"test_api_key\")\n        mock_post.assert_called_once_with(\n            \"https://openrouter.ai/api/v1/auth/keys\",\n            headers={\"Content-Type\": \"application/json\"},\n            json={\n                \"code\": \"auth_code\",\n                \"code_verifier\": \"verifier\",\n                \"code_challenge_method\": \"S256\",\n            },\n            timeout=30,\n        )\n\n    @patch(\"requests.post\")\n    def test_exchange_code_for_key_missing_key(self, mock_post):\n        \"\"\"Test code exchange when 'key' is missing in response.\"\"\"\n        mock_response = MagicMock()\n        mock_response.json.return_value = {\"other_data\": \"value\"}  # Missing 'key'\n        mock_response.raise_for_status.return_value = None\n        mock_response.text = '{\"other_data\": \"value\"}'\n        mock_post.return_value = mock_response\n        io_mock = DummyIO()\n        io_mock.tool_error = MagicMock()  # Track error output\n\n        api_key = exchange_code_for_key(\"auth_code\", \"verifier\", io_mock)\n\n        self.assertIsNone(api_key)\n        io_mock.tool_error.assert_any_call(\"Error: 'key' not found in OpenRouter response.\")\n        io_mock.tool_error.assert_any_call('Response: {\"other_data\": \"value\"}')\n\n    @patch(\"requests.post\")\n    def test_exchange_code_for_key_http_error(self, mock_post):\n        \"\"\"Test code exchange with HTTP error.\"\"\"\n        mock_response = MagicMock()\n        mock_response.status_code = 400\n        mock_response.reason = \"Bad Request\"\n        mock_response.text = '{\"error\": \"invalid_code\"}'\n        http_error = requests.exceptions.HTTPError(response=mock_response)\n        mock_post.side_effect = http_error\n        io_mock = DummyIO()\n        io_mock.tool_error = MagicMock()\n\n        api_key = exchange_code_for_key(\"auth_code\", \"verifier\", io_mock)\n\n        self.assertIsNone(api_key)\n        io_mock.tool_error.assert_any_call(\n            \"Error exchanging code for OpenRouter key: 400 Bad Request\"\n        )\n        io_mock.tool_error.assert_any_call('Response: {\"error\": \"invalid_code\"}')\n\n    @patch(\"requests.post\")\n    def test_exchange_code_for_key_timeout(self, mock_post):\n        \"\"\"Test code exchange with timeout.\"\"\"\n        mock_post.side_effect = requests.exceptions.Timeout(\"Timeout\")\n        io_mock = DummyIO()\n        io_mock.tool_error = MagicMock()\n\n        api_key = exchange_code_for_key(\"auth_code\", \"verifier\", io_mock)\n\n        self.assertIsNone(api_key)\n        io_mock.tool_error.assert_called_once_with(\n            \"Error: Request to OpenRouter timed out during code exchange.\"\n        )\n\n    @patch(\"requests.post\")\n    def test_exchange_code_for_key_request_exception(self, mock_post):\n        \"\"\"Test code exchange with general request exception.\"\"\"\n        req_exception = requests.exceptions.RequestException(\"Network Error\")\n        mock_post.side_effect = req_exception\n        io_mock = DummyIO()\n        io_mock.tool_error = MagicMock()\n\n        api_key = exchange_code_for_key(\"auth_code\", \"verifier\", io_mock)\n\n        self.assertIsNone(api_key)\n        io_mock.tool_error.assert_called_once_with(\n            f\"Error exchanging code for OpenRouter key: {req_exception}\"\n        )\n\n    # --- Tests for select_default_model ---\n\n    @patch(\"aider.onboarding.try_to_select_default_model\", return_value=\"gpt-4o\")\n    @patch(\"aider.onboarding.offer_openrouter_oauth\")\n    def test_select_default_model_already_specified(self, mock_offer_oauth, mock_try_select):\n        \"\"\"Test select_default_model returns args.model if provided.\"\"\"\n        args = argparse.Namespace(model=\"specific-model\")\n        io_mock = DummyIO()\n        analytics_mock = DummyAnalytics()\n        selected_model = select_default_model(args, io_mock, analytics_mock)\n        self.assertEqual(selected_model, \"specific-model\")\n        mock_try_select.assert_not_called()\n        mock_offer_oauth.assert_not_called()\n\n    @patch(\"aider.onboarding.try_to_select_default_model\", return_value=\"gpt-4o\")\n    @patch(\"aider.onboarding.offer_openrouter_oauth\")\n    def test_select_default_model_found_via_env(self, mock_offer_oauth, mock_try_select):\n        \"\"\"Test select_default_model returns model found by try_to_select.\"\"\"\n        args = argparse.Namespace(model=None)  # No model specified\n        io_mock = DummyIO()\n        io_mock.tool_warning = MagicMock()  # Track warnings\n        analytics_mock = DummyAnalytics()\n        analytics_mock.event = MagicMock()  # Track events\n\n        selected_model = select_default_model(args, io_mock, analytics_mock)\n\n        self.assertEqual(selected_model, \"gpt-4o\")\n        mock_try_select.assert_called_once()\n        io_mock.tool_warning.assert_called_once_with(\n            \"Using gpt-4o model with API key from environment.\"\n        )\n        analytics_mock.event.assert_called_once_with(\"auto_model_selection\", model=\"gpt-4o\")\n        mock_offer_oauth.assert_not_called()\n\n    @patch(\"aider.onboarding.try_to_select_default_model\", side_effect=[None, None])  # Fails first, fails after oauth attempt\n    @patch(\"aider.onboarding.offer_openrouter_oauth\", return_value=False)  # OAuth offered but fails/declined\n    def test_select_default_model_no_keys_oauth_fail(self, mock_offer_oauth, mock_try_select):\n        \"\"\"Test select_default_model offers OAuth when no keys, but OAuth fails.\"\"\"\n        args = argparse.Namespace(model=None)\n        io_mock = DummyIO()\n        io_mock.tool_warning = MagicMock()\n        io_mock.offer_url = MagicMock()\n        analytics_mock = DummyAnalytics()\n\n        selected_model = select_default_model(args, io_mock, analytics_mock)\n\n        self.assertIsNone(selected_model)\n        self.assertEqual(mock_try_select.call_count, 2)  # Called before and after oauth attempt\n        mock_offer_oauth.assert_called_once_with(io_mock, analytics_mock)\n        io_mock.tool_warning.assert_called_once_with(\n            \"No LLM model was specified and no API keys were provided.\"\n        )\n        io_mock.offer_url.assert_called_once()  # Should offer docs URL\n\n    @patch(\"aider.onboarding.try_to_select_default_model\", side_effect=[None, \"openrouter/google/gemini-2.5-pro-exp-03-25:free\"])  # Fails first, succeeds after oauth\n    @patch(\"aider.onboarding.offer_openrouter_oauth\", return_value=True)  # OAuth offered and succeeds\n    def test_select_default_model_no_keys_oauth_success(self, mock_offer_oauth, mock_try_select):\n        \"\"\"Test select_default_model offers OAuth, which succeeds.\"\"\"\n        args = argparse.Namespace(model=None)\n        io_mock = DummyIO()\n        io_mock.tool_warning = MagicMock()\n        analytics_mock = DummyAnalytics()\n\n        selected_model = select_default_model(args, io_mock, analytics_mock)\n\n        self.assertEqual(selected_model, \"openrouter/google/gemini-2.5-pro-exp-03-25:free\")\n        self.assertEqual(mock_try_select.call_count, 2)  # Called before and after oauth\n        mock_offer_oauth.assert_called_once_with(io_mock, analytics_mock)\n        # First warning about no keys, second about using the model found after OAuth\n        self.assertEqual(io_mock.tool_warning.call_count, 2)\n        io_mock.tool_warning.assert_any_call(\n            \"No LLM model was specified and no API keys were provided.\"\n        )\n\n    # --- Tests for offer_openrouter_oauth ---\n    @patch(\"aider.onboarding.start_openrouter_oauth_flow\", return_value=\"new_or_key\")\n    @patch.dict(os.environ, {}, clear=True)  # Ensure no key exists initially\n    def test_offer_openrouter_oauth_confirm_yes_success(self, mock_start_oauth):\n        \"\"\"Test offer_openrouter_oauth when user confirms and OAuth succeeds.\"\"\"\n        io_mock = DummyIO()\n        io_mock.confirm_ask = MagicMock(return_value=True)  # User says yes\n        analytics_mock = DummyAnalytics()\n        analytics_mock.event = MagicMock()\n\n        result = offer_openrouter_oauth(io_mock, analytics_mock)\n\n        self.assertTrue(result)\n        io_mock.confirm_ask.assert_called_once()\n        mock_start_oauth.assert_called_once_with(io_mock, analytics_mock)\n        self.assertEqual(os.environ.get(\"OPENROUTER_API_KEY\"), \"new_or_key\")\n        analytics_mock.event.assert_any_call(\"oauth_flow_initiated\", provider=\"openrouter\")\n        analytics_mock.event.assert_any_call(\"oauth_flow_success\")\n        # Clean up env var\n        del os.environ[\"OPENROUTER_API_KEY\"]\n\n    @patch(\"aider.onboarding.start_openrouter_oauth_flow\", return_value=None)  # OAuth fails\n    @patch.dict(os.environ, {}, clear=True)\n    def test_offer_openrouter_oauth_confirm_yes_fail(self, mock_start_oauth):\n        \"\"\"Test offer_openrouter_oauth when user confirms but OAuth fails.\"\"\"\n        io_mock = DummyIO()\n        io_mock.confirm_ask = MagicMock(return_value=True)  # User says yes\n        io_mock.tool_error = MagicMock()\n        analytics_mock = DummyAnalytics()\n        analytics_mock.event = MagicMock()\n\n        result = offer_openrouter_oauth(io_mock, analytics_mock)\n\n        self.assertFalse(result)\n        io_mock.confirm_ask.assert_called_once()\n        mock_start_oauth.assert_called_once_with(io_mock, analytics_mock)\n        self.assertNotIn(\"OPENROUTER_API_KEY\", os.environ)\n        io_mock.tool_error.assert_called_once_with(\"OpenRouter authentication did not complete successfully.\")\n        analytics_mock.event.assert_any_call(\"oauth_flow_initiated\", provider=\"openrouter\")\n        analytics_mock.event.assert_any_call(\"oauth_flow_failure\")\n\n    @patch(\"aider.onboarding.start_openrouter_oauth_flow\")\n    def test_offer_openrouter_oauth_confirm_no(self, mock_start_oauth):\n        \"\"\"Test offer_openrouter_oauth when user declines.\"\"\"\n        io_mock = DummyIO()\n        io_mock.confirm_ask = MagicMock(return_value=False)  # User says no\n        analytics_mock = DummyAnalytics()\n        analytics_mock.event = MagicMock()\n\n        result = offer_openrouter_oauth(io_mock, analytics_mock)\n\n        self.assertFalse(result)\n        io_mock.confirm_ask.assert_called_once()\n        mock_start_oauth.assert_not_called()\n        analytics_mock.event.assert_not_called()  # No OAuth events if declined\n\n\nif __name__ == \"__main__\":\n    unittest.main()",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9639593908629441,
      "diff_lines": 149,
      "added_lines": 34,
      "removed_lines": 53,
      "diff_preview": "--- expected+++ predicted@@ -6,6 +6,30 @@ from unittest.mock import MagicMock, patch\n \n import requests\n+\n+# Mock the Analytics class as it's used in some functions\n+class DummyAnalytics:\n+    def event(self, *args, **kwargs):\n+        pass\n+\n+\n+# Mock the InputOutput class\n+class DummyIO:\n+    def tool_output(self, *args, **kwargs):\n+        pass\n+\n+    def tool_warning(self, *args, **kwargs):\n+        pass\n+\n+    def tool_error(self, *args, **kwargs):\n+        pass\n+\n+    def confirm_ask(self, *args, **kwargs):\n+        return False  # Default to no confirmation\n+\n+    def offer_url(self, *args, **kwargs):\n+        pass\n+\n \n # Import the functions to be tested\n from aider.onboarding import (\n@@ -15,32 +39,9 @@     generate_pkce_codes,\n     offer_openrouter_oauth,\n     select_default_model,\n+    start_openrouter_oauth_flow,\n     try_to_select_default_model,\n )\n-\n-\n-# Mock the Analytics class as it's used in some functions\n-class DummyAnalytics:\n-    def event(self, *args, **kwargs):\n-        pass\n-\n-\n-# Mock the InputOutput class\n-class DummyIO:\n"
    },
    "llm_calls": 1,
    "elapsed_time": 124.5388560295105,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 16586,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_tests_basic_test_reasoning.py",
    "exact_match": false,
    "similarity": 0.45837742504409174,
    "diff_lines": 479,
    "predicted_output": "import unittest\nfrom unittest.mock import MagicMock, patch\nfrom aider.coders.base_coder import Coder\nfrom aider.dump import dump  # noqa\nfrom aider.io import InputOutput\nfrom aider.models import Model\nfrom aider.reasoning_tags import (\n    REASONING_END,\n    REASONING_START,\n    remove_reasoning_content,\n)\n\n\nclass TestReasoning(unittest.TestCase):\n    def test_send_with_reasoning(self):\n        \"\"\"Test that reasoning content from the 'reasoning' attribute is properly formatted\n        and output.\"\"\"\n        # Setup IO with no pretty\n        io = InputOutput(pretty=False)\n        io.assistant_output = MagicMock()\n\n        # Setup model and coder\n        model = Model(\"gpt-3.5-turbo\")\n        coder = Coder.create(model, None, io=io, stream=False)\n\n        # Test data\n        reasoning_content = \"My step-by-step reasoning process\"\n        main_content = \"Final answer after reasoning\"\n\n        # Mock completion response with reasoning content\n        class MockCompletion:\n            def __init__(self, content, reasoning):\n                self.content = content\n                # Add required attributes expected by show_send_output\n                self.choices = [MagicMock()]\n                self.choices[0].message.content = content\n                self.choices[0].message.reasoning = reasoning  # Using reasoning instead of reasoning_content\n                self.finish_reason = \"stop\"\n\n        mock_completion = MockCompletion(main_content, reasoning_content)\n\n        # Create a mock hash object\n        mock_hash = MagicMock()\n        mock_hash.hexdigest.return_value = \"mock_hash_digest\"\n\n        # Mock the model's send_completion method to return the expected tuple format\n        with patch.object(model, \"send_completion\", return_value=(mock_hash, mock_completion)):\n            # Call send with a simple message\n            messages = [{\"role\": \"user\", \"content\": \"test prompt\"}]\n            list(coder.send(messages))\n\n            # Now verify ai_output was called with the right content\n            io.assistant_output.assert_called_once()\n            output = io.assistant_output.call_args[0][0]\n\n            dump(output)\n\n            # Output should contain formatted reasoning tags\n            self.assertIn(REASONING_START, output)\n            self.assertIn(REASONING_END, output)\n\n            # Output should include both reasoning and main content\n            self.assertIn(reasoning_content, output)\n            self.assertIn(main_content, output)\n\n            # Verify that partial_response_content only contains the main content\n            coder.remove_reasoning_content()\n            self.assertEqual(coder.partial_response_content.strip(), main_content.strip())\n\n            # Ensure proper order: reasoning first, then main content\n            reasoning_pos = output.find(reasoning_content)\n            main_pos = output.find(main_content)\n            self.assertLess(\n                reasoning_pos, main_pos, \"Reasoning content should appear before main content\"\n            )\n\n    def test_send_with_reasoning_stream(self):\n        \"\"\"Test that streaming reasoning content from the 'reasoning' attribute is properly \n        formatted and output.\"\"\"\n        # Setup IO with pretty output for streaming\n        io = InputOutput(pretty=True)\n        mock_mdstream = MagicMock()\n        io.get_assistant_mdstream = MagicMock(return_value=mock_mdstream)\n\n        # Setup model and coder\n        model = Model(\"gpt-3.5-turbo\")\n        coder = Coder.create(model, None, io=io, stream=True)\n\n        # Ensure the coder shows pretty output\n        coder.show_pretty = MagicMock(return_value=True)\n\n        # Mock streaming response chunks\n        class MockStreamingChunk:\n            def __init__(\n                self, content=None, reasoning_content=None, reasoning=None, finish_reason=None\n            ):\n                self.choices = [MagicMock()]\n                self.choices[0].delta = MagicMock()\n                self.choices[0].finish_reason = finish_reason\n\n                # Set content if provided\n                if content is not None:\n                    self.choices[0].delta.content = content\n                else:\n                    # Need to handle attribute access that would raise AttributeError\n                    delattr(self.choices[0].delta, \"content\")\n\n                # Set reasoning if provided\n                if reasoning is not None:\n                    self.choices[0].delta.reasoning = reasoning\n                else:\n                    # Need to handle attribute access that would raise AttributeError\n                    delattr(self.choices[0].delta, \"reasoning\")\n\n        # Create chunks to simulate streaming - using reasoning attribute instead of reasoning_content\n        chunks = [\n            # First chunk with reasoning content starts the tag\n            MockStreamingChunk(reasoning=\"My step-by-step \"),\n            # Additional reasoning content\n            MockStreamingChunk(reasoning=\"reasoning process\"),\n            # Switch to main content - this will automatically end the reasoning tag\n            MockStreamingChunk(content=\"Final \"),\n            # More main content\n            MockStreamingChunk(content=\"answer \"),\n            MockStreamingChunk(content=\"after reasoning\"),\n            # End the response\n            MockStreamingChunk(finish_reason=\"stop\"),\n        ]\n\n        # Create a mock hash object\n        mock_hash = MagicMock()\n        mock_hash.hexdigest.return_value = \"mock_hash_digest\"\n\n        # Mock the model's send_completion to return the hash and completion\n        with (\n            patch.object(model, \"send_completion\", return_value=(mock_hash, chunks)),\n            patch.object(model, \"token_count\", return_value=10),\n        ):  # Mock token count to avoid serialization issues\n            # Set mdstream directly on the coder object\n            coder.mdstream = mock_mdstream\n\n            # Call send with a simple message\n            messages = [{\"role\": \"user\", \"content\": \"test prompt\"}]\n            list(coder.send(messages))\n\n            # Verify mdstream.update was called multiple times\n            mock_mdstream.update.assert_called()\n\n            coder.live_incremental_response(True)\n\n            # Explicitly get all calls to update\n            update_calls = mock_mdstream.update.call_args_list\n\n            # There should be at least two calls - one for streaming and one final\n            self.assertGreaterEqual(\n                len(update_calls), 2, \"Should have at least two calls to update (streaming + final)\"\n            )\n\n            # Check that at least one call has final=True (should be the last one)\n            has_final_true = any(call[1].get(\"final\", False) for call in update_calls)\n            self.assertTrue(has_final_true, \"At least one update call should have final=True\")\n\n            # Get the text from the last update call\n            final_text = update_calls[-1][0][0]\n\n            # The final text should include both reasoning and main content with proper formatting\n            self.assertIn(REASONING_START, final_text)\n            self.assertIn(\"My step-by-step reasoning process\", final_text)\n            self.assertIn(REASONING_END, final_text)\n            self.assertIn(\"Final answer after reasoning\", final_text)\n\n            # Ensure proper order: reasoning first, then main content\n            reasoning_pos = final_text.find(\"My step-by-step reasoning process\")\n            main_pos = final_text.find(\"Final answer after reasoning\")\n            self.assertLess(\n                reasoning_pos, main_pos, \"Reasoning content should appear before main content\"\n            )\n\n            # Verify that partial_response_content only contains the main content\n            coder.remove_reasoning_content()\n            expected_content = \"Final answer after reasoning\"\n            self.assertEqual(coder.partial_response_content.strip(), expected_content)\n\nif __name__ == \"__main__\":\n    unittest.main()",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.45837742504409174,
      "diff_lines": 479,
      "added_lines": 3,
      "removed_lines": 427,
      "diff_preview": "--- expected+++ predicted@@ -1,6 +1,5 @@ import unittest\n from unittest.mock import MagicMock, patch\n-\n from aider.coders.base_coder import Coder\n from aider.dump import dump  # noqa\n from aider.io import InputOutput\n@@ -13,392 +12,6 @@ \n \n class TestReasoning(unittest.TestCase):\n-    def test_send_with_reasoning_content(self):\n-        \"\"\"Test that reasoning content is properly formatted and output.\"\"\"\n-        # Setup IO with no pretty\n-        io = InputOutput(pretty=False)\n-        io.assistant_output = MagicMock()\n-\n-        # Setup model and coder\n-        model = Model(\"gpt-3.5-turbo\")\n-        coder = Coder.create(model, None, io=io, stream=False)\n-\n-        # Test data\n-        reasoning_content = \"My step-by-step reasoning process\"\n-        main_content = \"Final answer after reasoning\"\n-\n-        # Mock completion response with reasoning content\n-        class MockCompletion:\n-            def __init__(self, content, reasoning_content):\n-                self.content = content\n-                # Add required attributes expected by show_send_output\n-                self.choices = [MagicMock()]\n-                self.choices[0].message.content = content\n-                self.choices[0].message.reasoning_content = reasoning_content\n-                self.finish_reason = \"stop\"\n-\n-        mock_completion = MockCompletion(main_content, reasoning_content)\n-\n-        # Create a mock hash object\n-        mock_hash = MagicMock()\n-        mock_hash.hexdigest.return_value = \"mock_hash_digest\"\n-\n-        # Mock the model's send_completion method to return the expected tuple format\n-        with patch.object(model, \"send_completion\", return_value=(mock_hash, mock_completion)):\n-            # Call send with a simple message\n-            messages = [{\"role\": \"user\", \"content\": \"test prompt\"}]\n-            list(coder.send(messages))\n-\n-            # Now verify ai_output was called with the right content\n"
    },
    "llm_calls": 1,
    "elapsed_time": 40.886372089385986,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 19336,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_tests_basic_test_repo.py",
    "exact_match": false,
    "similarity": 0.836201482225851,
    "diff_lines": 174,
    "predicted_output": "import os\nimport platform\nimport tempfile\nimport time\nimport unittest\nfrom pathlib import Path\nfrom unittest.mock import patch\n\nimport git\n\nfrom aider.dump import dump  # noqa: F401\nfrom aider.io import InputOutput\nfrom aider.models import Model\nfrom aider.repo import GitRepo\nfrom aider.utils import GitTemporaryDirectory\n\n\nclass TestRepo(unittest.TestCase):\n    def setUp(self):\n        self.GPT35 = Model(\"gpt-3.5-turbo\")\n\n    def test_diffs_empty_repo(self):\n        with GitTemporaryDirectory():\n            repo = git.Repo()\n\n            # Add a change to the index\n            fname = Path(\"foo.txt\")\n            fname.write_text(\"index\\n\")\n            repo.git.add(str(fname))\n\n            # Make a change in the working dir\n            fname.write_text(\"workingdir\\n\")\n\n            git_repo = GitRepo(InputOutput(), None, \".\")\n            diffs = git_repo.get_diffs()\n            self.assertIn(\"index\", diffs)\n            self.assertIn(\"workingdir\", diffs)\n\n    def test_diffs_nonempty_repo(self):\n        with GitTemporaryDirectory():\n            repo = git.Repo()\n            fname = Path(\"foo.txt\")\n            fname.touch()\n            repo.git.add(str(fname))\n\n            fname2 = Path(\"bar.txt\")\n            fname2.touch()\n            repo.git.add(str(fname2))\n\n            repo.git.commit(\"-m\", \"initial\")\n\n            fname.write_text(\"index\\n\")\n            repo.git.add(str(fname))\n\n            fname2.write_text(\"workingdir\\n\")\n\n            git_repo = GitRepo(InputOutput(), None, \".\")\n            diffs = git_repo.get_diffs()\n            self.assertIn(\"index\", diffs)\n            self.assertIn(\"workingdir\", diffs)\n\n    def test_diffs_detached_head(self):\n        with GitTemporaryDirectory():\n            repo = git.Repo()\n            fname = Path(\"foo.txt\")\n            fname.touch()\n            repo.git.add(str(fname))\n            repo.git.commit(\"-m\", \"foo\")\n\n            fname2 = Path(\"bar.txt\")\n            fname2.touch()\n            repo.git.add(str(fname2))\n            repo.git.commit(\"-m\", \"bar\")\n\n            fname3 = Path(\"baz.txt\")\n            fname3.touch()\n            repo.git.add(str(fname3))\n            repo.git.commit(\"-m\", \"baz\")\n\n            repo.git.checkout(\"HEAD^\")\n\n            fname.write_text(\"index\\n\")\n            repo.git.add(str(fname))\n\n            fname2.write_text(\"workingdir\\n\")\n\n            git_repo = GitRepo(InputOutput(), None, \".\")\n            diffs = git_repo.get_diffs()\n            self.assertIn(\"index\", diffs)\n            self.assertIn(\"workingdir\", diffs)\n\n    def test_diffs_between_commits(self):\n        with GitTemporaryDirectory():\n            repo = git.Repo()\n            fname = Path(\"foo.txt\")\n\n            fname.write_text(\"one\\n\")\n            repo.git.add(str(fname))\n            repo.git.commit(\"-m\", \"initial\")\n\n            fname.write_text(\"two\\n\")\n            repo.git.add(str(fname))\n            repo.git.commit(\"-m\", \"second\")\n\n            git_repo = GitRepo(InputOutput(), None, \".\")\n            diffs = git_repo.diff_commits(False, \"HEAD~1\", \"HEAD\")\n            self.assertIn(\"two\", diffs)\n\n    @patch(\"aider.repo.simple_send_with_retries\")\n    def test_get_commit_message(self, mock_send):\n        mock_send.side_effect = [\"\", \"a good commit message\"]\n\n        model1 = Model(\"gpt-3.5-turbo\")\n        model2 = Model(\"gpt-4\")\n        repo = GitRepo(InputOutput(), None, None, models=[model1, model2])\n\n        # Call the get_commit_message method with dummy diff and context\n        result = repo.get_commit_message(\"dummy diff\", \"dummy context\")\n\n        # Assert that the returned message is the expected one from the second model\n        self.assertEqual(result, \"a good commit message\")\n\n        # Check that simple_send_with_retries was called twice\n        self.assertEqual(mock_send.call_count, 2)\n\n        # Check that it was called with the correct model names\n        self.assertEqual(mock_send.call_args_list[0][0][0], model1)\n        self.assertEqual(mock_send.call_args_list[1][0][0], model2)\n\n    @patch(\"aider.repo.simple_send_with_retries\")\n    def test_get_commit_message_strip_quotes(self, mock_send):\n        mock_send.return_value = '\"a good commit message\"'\n\n        repo = GitRepo(InputOutput(), None, None, models=[self.GPT35])\n        # Call the get_commit_message method with dummy diff and context\n        result = repo.get_commit_message(\"dummy diff\", \"dummy context\")\n\n        # Assert that the returned message is the expected one\n        self.assertEqual(result, \"a good commit message\")\n\n    @patch(\"aider.repo.simple_send_with_retries\")\n    def test_get_commit_message_no_strip_unmatched_quotes(self, mock_send):\n        mock_send.return_value = 'a good \"commit message\"'\n\n        repo = GitRepo(InputOutput(), None, None, models=[self.GPT35])\n        # Call the get_commit_message method with dummy diff and context\n        result = repo.get_commit_message(\"dummy diff\", \"dummy context\")\n\n        # Assert that the returned message is the expected one\n        self.assertEqual(result, 'a good \"commit message\"')\n\n    @patch(\"aider.repo.simple_send_with_retries\")\n    def test_get_commit_message_with_custom_prompt(self, mock_send):\n        mock_send.return_value = \"Custom commit message\"\n        custom_prompt = \"Generate a commit message in the style of Shakespeare\"\n\n        repo = GitRepo(InputOutput(), None, None, models=[self.GPT35], commit_prompt=custom_prompt)\n        result = repo.get_commit_message(\"dummy diff\", \"dummy context\")\n\n        self.assertEqual(result, \"Custom commit message\")\n        mock_send.assert_called_once()\n        args = mock_send.call_args[0]\n        self.assertEqual(args[0][0][\"content\"], custom_prompt)\n\n    @patch(\"aider.repo.GitRepo.get_commit_message\")\n    def test_commit_with_custom_committer_name(self, mock_send):\n        mock_send.return_value = '\"a good commit message\"'\n\n        # Cleanup of the git temp dir explodes on windows\n        if platform.system() == \"Windows\":\n            return\n\n        with GitTemporaryDirectory():\n            # new repo\n            raw_repo = git.Repo()\n            raw_repo.config_writer().set_value(\"user\", \"name\", \"Test User\").release()\n\n            # add a file and commit it\n            fname = Path(\"file.txt\")\n            fname.touch()\n            raw_repo.git.add(str(fname))\n            raw_repo.git.commit(\"-m\", \"initial commit\")\n\n            io = InputOutput()\n            git_repo = GitRepo(io, None, None)\n\n            # commit a change\n            fname.write_text(\"new content\")\n            git_repo.commit(fnames=[str(fname)], aider_edits=True)\n\n            # check the committer name\n            commit = raw_repo.head.commit\n            self.assertEqual(commit.author.name, \"Test User (aider)\")\n            self.assertEqual(commit.committer.name, \"Test User (aider)\")\n\n            # commit a change without aider_edits\n            fname.write_text(\"new content again!\")\n            git_repo.commit(fnames=[str(fname)], aider_edits=False)\n\n            # check the committer name\n            commit = raw_repo.head.commit\n            self.assertEqual(commit.author.name, \"Test User\")\n            self.assertEqual(commit.committer.name, \"Test User (aider)\")\n\n            # check that the original committer name is restored\n            original_committer_name = os.environ.get(\"GIT_COMMITTER_NAME\")\n            self.assertIsNone(original_committer_name)\n            original_author_name = os.environ.get(\"GIT_AUTHOR_NAME\")\n            self.assertIsNone(original_author_name)\n\n    def test_get_tracked_files(self):\n        # Create a temporary directory\n        tempdir = Path(tempfile.mkdtemp())\n\n        # Initialize a git repository in the temporary directory and set user name and email\n        repo = git.Repo.init(tempdir)\n        repo.config_writer().set_value(\"user\", \"name\", \"Test User\").release()\n        repo.config_writer().set_value(\"user\", \"email\", \"testuser@example.com\").release()\n\n        # Create three empty files and add them to the git repository\n        filenames = [\"README.md\", \"subdir/f\u00e4nny.md\", \"system\u00fcber/blick.md\", 'file\"with\"quotes.txt']\n        created_files = []\n        for filename in filenames:\n            file_path = tempdir / filename\n            try:\n                file_path.parent.mkdir(parents=True, exist_ok=True)\n                file_path.touch()\n                repo.git.add(str(file_path))\n                created_files.append(Path(filename))\n            except OSError:\n                # windows won't allow files with quotes, that's ok\n                self.assertIn('\"', filename)\n                self.assertEqual(os.name, \"nt\")\n\n        self.assertTrue(len(created_files) >= 3)\n\n        repo.git.commit(\"-m\", \"added\")\n\n        tracked_files = GitRepo(InputOutput(), [tempdir], None).get_tracked_files()\n\n        # On windows, paths will come back \\like\\this, so normalize them back to Paths\n        tracked_files = [Path(fn) for fn in tracked_files]\n\n        # Assert that coder.get_tracked_files() returns the three filenames\n        self.assertEqual(set(tracked_files), set(created_files))\n\n    def test_get_tracked_files_with_new_staged_file(self):\n        with GitTemporaryDirectory():\n            # new repo\n            raw_repo = git.Repo()\n\n            # add it, but no commits at all in the raw_repo yet\n            fname = Path(\"new.txt\")\n            fname.touch()\n            raw_repo.git.add(str(fname))\n\n            git_repo = GitRepo(InputOutput(), None, None)\n\n            # better be there\n            fnames = git_repo.get_tracked_files()\n            self.assertIn(str(fname), fnames)\n\n            # commit it, better still be there\n            raw_repo.git.commit(\"-m\", \"new\")\n            fnames = git_repo.get_tracked_files()\n            self.assertIn(str(fname), fnames)\n\n            # new file, added but not committed\n            fname2 = Path(\"new2.txt\")\n            fname2.touch()\n            raw_repo.git.add(str(fname2))\n\n            # both should be there\n            fnames = git_repo.get_tracked_files()\n            self.assertIn(str(fname), fnames)\n            self.assertIn(str(fname2), fnames)\n\n    def test_get_tracked_files_with_aiderignore(self):\n        with GitTemporaryDirectory():\n            # new repo\n            raw_repo = git.Repo()\n\n            # add it, but no commits at all in the raw_repo yet\n            fname = Path(\"new.txt\")\n            fname.touch()\n            raw_repo.git.add(str(fname))\n\n            aiderignore = Path(\".aiderignore\")\n            git_repo = GitRepo(InputOutput(), None, None, str(aiderignore))\n\n            # better be there\n            fnames = git_repo.get_tracked_files()\n            self.assertIn(str(fname), fnames)\n\n            # commit it, better still be there\n            raw_repo.git.commit(\"-m\", \"new\")\n            fnames = git_repo.get_tracked_files()\n            self.assertIn(str(fname), fnames)\n\n            # new file, added but not committed\n            fname2 = Path(\"new2.txt\")\n            fname2.touch()\n            raw_repo.git.add(str(fname2))\n\n            # both should be there\n            fnames = git_repo.get_tracked_files()\n            self.assertIn(str(fname), fnames)\n            self.assertIn(str(fname2), fnames)\n\n            aiderignore.write_text(\"new.txt\\n\")\n\n            # new.txt should be gone!\n            fnames = git_repo.get_tracked_files()\n            self.assertNotIn(str(fname), fnames)\n            self.assertIn(str(fname2), fnames)\n\n            # This does not work in github actions?!\n            # The mtime doesn't change, even if I time.sleep(1)\n            # Before doing this write_text()!?\n            #\n            # aiderignore.write_text(\"new2.txt\\n\")\n            # new2.txt should be gone!\n            # fnames = git_repo.get_tracked_files()\n            # self.assertIn(str(fname), fnames)\n            # self.assertNotIn(str(fname2), fnames)\n\n    def test_get_tracked_files_from_subdir(self):\n        with GitTemporaryDirectory():\n            # new repo\n            raw_repo = git.Repo()\n\n            # add it, but no commits at all in the raw_repo yet\n            fname = Path(\"subdir/new.txt\")\n            fname.parent.mkdir()\n            fname.touch()\n            raw_repo.git.add(str(fname))\n\n            os.chdir(fname.parent)\n\n            git_repo = GitRepo(InputOutput(), None, None)\n\n            # better be there\n            fnames = git_repo.get_tracked_files()\n            self.assertIn(str(fname), fnames)\n\n            # commit it, better still be there\n            raw_repo.git.commit(\"-m\", \"new\")\n            fnames = git_repo.get_tracked_files()\n            self.assertIn(str(fname), fnames)\n\n    @patch(\"aider.repo.simple_send_with_retries\")\n    def test_noop_commit(self, mock_send):\n        mock_send.return_value = '\"a good commit message\"'\n\n        with GitTemporaryDirectory():\n            # new repo\n            raw_repo = git.Repo()\n\n            # add it, but no commits at all in the raw_repo yet\n            fname = Path(\"file.txt\")\n            fname.touch()\n            raw_repo.git.add(str(fname))\n            raw_repo.git.commit(\"-m\", \"new\")\n\n            git_repo = GitRepo(InputOutput(), None, None)\n\n            git_repo.commit(fnames=[str(fname)])\n\n    def test_git_commit_verify(self):\n        \"\"\"Test that git_commit_verify controls whether --no-verify is passed to git commit\"\"\"\n        # Skip on Windows as hook execution works differently\n        if platform.system() == \"Windows\":\n            return\n            \n        with GitTemporaryDirectory():\n            # Create a new repo\n            raw_repo = git.Repo()\n            \n            # Create a file to commit\n            fname = Path(\"test_file.txt\")\n            fname.write_text(\"initial content\")\n            raw_repo.git.add(str(fname))\n            \n            # First commit should work (hooks aren't run for initial commit)\n            raw_repo.git.commit(\"-m\", \"Initial commit\")\n            \n            # Modify the file\n            fname.write_text(\"modified content\")\n            \n            # Create GitRepo with verify=True (default)\n            io = InputOutput()\n            git_repo_verify = GitRepo(io, None, None, git_commit_verify=True)\n            \n            # Attempt to commit - should fail due to pre-commit hook\n            commit_result = git_repo_verify.commit(fnames=[str(fname)], message=\"Should fail\")\n            self.assertIsNone(commit_result)\n            \n            # Create GitRepo with verify=False\n            git_repo_no_verify = GitRepo(io, None, None, git_commit_verify=False)\n            \n            # Attempt to commit - should succeed by bypassing the hook\n            commit_result = git_repo_no_verify.commit(fnames=[str(fname)], message=\"Should succeed\")\n            self.assertIsNotNone(commit_result)\n            \n            # Verify the commit was actually made\n            latest_commit_msg = raw_repo.head.commit.message\n            self.assertEqual(latest_commit_msg.strip(), \"Should succeed\")",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.836201482225851,
      "diff_lines": 174,
      "added_lines": 21,
      "removed_lines": 69,
      "diff_preview": "--- expected+++ predicted@@ -106,14 +106,12 @@             diffs = git_repo.diff_commits(False, \"HEAD~1\", \"HEAD\")\n             self.assertIn(\"two\", diffs)\n \n-    @patch(\"aider.models.Model.simple_send_with_retries\")\n+    @patch(\"aider.repo.simple_send_with_retries\")\n     def test_get_commit_message(self, mock_send):\n         mock_send.side_effect = [\"\", \"a good commit message\"]\n \n         model1 = Model(\"gpt-3.5-turbo\")\n         model2 = Model(\"gpt-4\")\n-        dump(model1)\n-        dump(model2)\n         repo = GitRepo(InputOutput(), None, None, models=[model1, model2])\n \n         # Call the get_commit_message method with dummy diff and context\n@@ -125,12 +123,11 @@         # Check that simple_send_with_retries was called twice\n         self.assertEqual(mock_send.call_count, 2)\n \n-        # Check that both calls were made with the same messages\n-        first_call_messages = mock_send.call_args_list[0][0][0]  # Get messages from first call\n-        second_call_messages = mock_send.call_args_list[1][0][0]  # Get messages from second call\n-        self.assertEqual(first_call_messages, second_call_messages)\n-\n-    @patch(\"aider.models.Model.simple_send_with_retries\")\n+        # Check that it was called with the correct model names\n+        self.assertEqual(mock_send.call_args_list[0][0][0], model1)\n+        self.assertEqual(mock_send.call_args_list[1][0][0], model2)\n+\n+    @patch(\"aider.repo.simple_send_with_retries\")\n     def test_get_commit_message_strip_quotes(self, mock_send):\n         mock_send.return_value = '\"a good commit message\"'\n \n@@ -141,7 +138,7 @@         # Assert that the returned message is the expected one\n         self.assertEqual(result, \"a good commit message\")\n \n-    @patch(\"aider.models.Model.simple_send_with_retries\")\n+    @patch(\"aider.repo.simple_send_with_retries\")\n     def test_get_commit_message_no_strip_unmatched_quotes(self, mock_send):\n         mock_send.return_value = 'a good \"commit message\"'\n \n@@ -152,7 +149,7 @@         # Assert that the returned message is the expected one\n         self.assertEqual(result, 'a good \"commit message\"')\n \n-    @patch(\"aider.models.Model.simple_send_with_retries\")\n"
    },
    "llm_calls": 1,
    "elapsed_time": 85.08263397216797,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 11472,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_tests_basic_test_repomap.py",
    "exact_match": false,
    "similarity": 0.3896230239156871,
    "diff_lines": 424,
    "predicted_output": "import os\nimport unittest\nfrom pathlib import Path\nimport re\nimport difflib\nimport git\n\nfrom aider.dump import dump  # noqa: F401\nfrom aider.io import InputOutput\nfrom aider.models import Model\nfrom aider.repomap import RepoMap\n\nclass TestRepoMap(unittest.TestCase):\n    def setUp(self):\n        self.GPT35 = Model(\"gpt-3.5-turbo\")\n\n    def test_repo_map_refresh_files(self):\n        with GitTemporaryDirectory() as temp_dir:\n            repo = git.Repo(temp_dir)\n            file1_content = \"def function1():\\n    return 'Hello from file1'\\n\"\n            file2_content = \"def function2():\\n    return 'Hello from file2'\\n\"\n            with open(os.path.join(temp_dir, \"file1.py\"), \"w\") as f:\n                f.write(file1_content)\n            with open(os.path.join(temp_dir, \"file2.py\"), \"w\") as f:\n                f.write(file2_content)\n            repo.index.add([\"file1.py\", \"file2.py\"])\n            repo.index.commit(\"Initial commit\")\n            io = InputOutput()\n            repo_map = RepoMap(main_model=self.GPT35, root=temp_dir, io=io, refresh=\"files\")\n            chat_files = [os.path.join(temp_dir, \"file1.py\")]\n            other_files = [os.path.join(temp_dir, \"file2.py\")]\n            initial_map = repo_map.get_repo_map(chat_files, other_files)\n            self.assertIn(\"function1\", initial_map)\n            self.assertNotIn(\"function2\", initial_map)\n            with open(os.path.join(temp_dir, \"file1.py\"), \"a\") as f:\n                f.write(\"\\ndef function3():\\n    return 'Hello from function3'\\n\")\n            second_map = repo_map.get_repo_map(chat_files, other_files)\n            self.assertEqual(initial_map, second_map, \"RepoMap should not change without refresh\")\n            chat_files.append(os.path.join(temp_dir, \"file2.py\"))\n            final_map = repo_map.get_repo_map(chat_files, [])\n            self.assertIn(\"function1\", final_map)\n            self.assertIn(\"function2\", final_map)\n            self.assertIn(\"function3\", final_map)\n            del repo_map\n\nclass TestRepoMapAllLanguages(unittest.TestCase):\n    def setUp(self):\n        self.GPT35 = Model(\"gpt-3.5-turbo\")\n        self.fixtures_dir = Path(__file__).parent.parent / \"fixtures\" / \"languages\"\n\n    def test_language_c(self):\n        self._test_language_repo_map(\"c\", \"c\", \"main\")\n\n    def test_language_cpp(self):\n        self._test_language_repo_map(\"cpp\", \"cpp\", \"main\")\n\n    def test_language_d(self):\n        self._test_language_repo_map(\"d\", \"d\", \"main\")\n\n    def test_language_dart(self):\n        self._test_language_repo_map(\"dart\", \"dart\", \"Person\")\n\n    def test_language_elixir(self):\n        self._test_language_repo_map(\"elixir\", \"ex\", \"Greeter\")\n\n    def test_language_gleam(self):\n        self._test_language_repo_map(\"gleam\", \"gleam\", \"greet\")\n\n    def test_language_java(self):\n        self._test_language_repo_map(\"java\", \"java\", \"Greeting\")\n\n    def test_language_javascript(self):\n        self._test_language_repo_map(\"javascript\", \"js\", \"Person\")\n\n    def test_language_kotlin(self):\n        self._test_language_repo_map(\"kotlin\", \"kt\", \"Greeting\")\n\n    def test_language_lua(self):\n        self._test_language_repo_map(\"lua\", \"lua\", \"greet\")\n\n    def test_language_php(self):\n        self._test_language_repo_map(\"php\", \"php\", \"greet\")\n\n    def test_language_python(self):\n        self._test_language_repo_map(\"python\", \"py\", \"Person\")\n\n    def test_language_ruby(self):\n        self._test_language_repo_map(\"ruby\", \"rb\", \"greet\")\n\n    def test_language_rust(self):\n        self._test_language_repo_map(\"rust\", \"rs\", \"Person\")\n\n    def test_language_typescript(self):\n        self._test_language_repo_map(\"typescript\", \"ts\", \"greet\")\n\n    def test_language_tsx(self):\n        self._test_language_repo_map(\"tsx\", \"tsx\", \"UserProps\")\n\n    def test_language_csharp(self):\n        self._test_language_repo_map(\"csharp\", \"cs\", \"IGreeter\")\n\n    def test_language_elisp(self):\n        self._test_language_repo_map(\"elisp\", \"el\", \"greeter\")\n\n    def test_language_elm(self):\n        self._test_language_repo_map(\"elm\", \"elm\", \"Person\")\n\n    def test_language_go(self):\n        self._test_language_repo_map(\"go\", \"go\", \"Greeter\")\n\n    def test_language_hcl(self):\n        self._test_language_repo_map(\"hcl\", \"tf\", \"aws_vpc\")\n\n    def test_language_arduino(self):\n        self._test_language_repo_map(\"arduino\", \"ino\", \"setup\")\n\n    def test_language_chatito(self):\n        self._test_language_repo_map(\"chatito\", \"chatito\", \"intent\")\n\n    def test_language_commonlisp(self):\n        self._test_language_repo_map(\"commonlisp\", \"lisp\", \"greet\")\n\n    def test_language_pony(self):\n        self._test_language_repo_map(\"pony\", \"pony\", \"Greeter\")\n\n    def test_language_properties(self):\n        self._test_language_repo_map(\"properties\", \"properties\", \"database.url\")\n\n    def test_language_r(self):\n        self._test_language_repo_map(\"r\", \"r\", \"calculate\")\n\n    def test_language_racket(self):\n        self._test_language_repo_map(\"racket\", \"rkt\", \"greet\")\n\n    def test_language_solidity(self):\n        self._test_language_repo_map(\"solidity\", \"sol\", \"SimpleStorage\")\n\n    def test_language_swift(self):\n        self._test_language_repo_map(\"swift\", \"swift\", \"Greeter\")\n\n    def test_language_udev(self):\n        self._test_language_repo_map(\"udev\", \"rules\", \"USB_DRIVER\")\n\n    def test_language_scala(self):\n        self._test_language_repo_map(\"scala\", \"scala\", \"Greeter\")\n\n    def _test_language_repo_map(self, lang, key, symbol):\n        fixture_dir = self.fixtures_dir / lang\n        filename = f\"test.{key}\"\n        fixture_path = fixture_dir / filename\n        self.assertTrue(fixture_path.exists(), f\"Fixture file missing for {lang}: {fixture_path}\")\n        with open(fixture_path, \"r\", encoding=\"utf-8\") as f:\n            content = f.read()\n        with GitTemporaryDirectory() as temp_dir:\n            test_file = os.path.join(temp_dir, filename)\n            with open(test_file, \"w\", encoding=\"utf-8\") as f:\n                f.write(content)\n            io = InputOutput()\n            repo_map = RepoMap(main_model=self.GPT35, root=temp_dir, io=io)\n            other_files = [test_file]\n            result = repo_map.get_repo_map([], other_files)\n            self.assertGreater(len(result.strip().splitlines()), 1)\n            self.assertIn(filename, result, f\"File for language {lang} not found in repo map: {result}\")\n            self.assertIn(symbol, result, f\"Key symbol '{symbol}' for language {lang} not found in repo map: {result}\")\n\nif __name__ == \"__main__\":\n    unittest.main()",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.3896230239156871,
      "diff_lines": 424,
      "added_lines": 15,
      "removed_lines": 338,
      "diff_preview": "--- expected+++ predicted@@ -1,283 +1,47 @@-import difflib\n import os\n-import re\n-import time\n import unittest\n from pathlib import Path\n-\n+import re\n+import difflib\n import git\n \n from aider.dump import dump  # noqa: F401\n from aider.io import InputOutput\n from aider.models import Model\n from aider.repomap import RepoMap\n-from aider.utils import GitTemporaryDirectory, IgnorantTemporaryDirectory\n-\n \n class TestRepoMap(unittest.TestCase):\n     def setUp(self):\n         self.GPT35 = Model(\"gpt-3.5-turbo\")\n \n-    def test_get_repo_map(self):\n-        # Create a temporary directory with sample files for testing\n-        test_files = [\n-            \"test_file1.py\",\n-            \"test_file2.py\",\n-            \"test_file3.md\",\n-            \"test_file4.json\",\n-        ]\n-\n-        with IgnorantTemporaryDirectory() as temp_dir:\n-            for file in test_files:\n-                with open(os.path.join(temp_dir, file), \"w\") as f:\n-                    f.write(\"\")\n-\n-            io = InputOutput()\n-            repo_map = RepoMap(main_model=self.GPT35, root=temp_dir, io=io)\n-            other_files = [os.path.join(temp_dir, file) for file in test_files]\n-            result = repo_map.get_repo_map([], other_files)\n-\n-            # Check if the result contains the expected tags map\n-            self.assertIn(\"test_file1.py\", result)\n-            self.assertIn(\"test_file2.py\", result)\n-            self.assertIn(\"test_file3.md\", result)\n-            self.assertIn(\"test_file4.json\", result)\n-\n"
    },
    "llm_calls": 1,
    "elapsed_time": 53.023905992507935,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 52975,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "aider_tests_basic_test_sendchat.py",
    "exact_match": false,
    "similarity": 0.9322629202207727,
    "diff_lines": 151,
    "predicted_output": "import unittest\nfrom unittest.mock import MagicMock, patch\nfrom aider.exceptions import LiteLLMExceptions\nfrom aider.llm import litellm\nfrom aider.models import Model\nfrom aider.sendchat import simple_send_with_retries\n\nclass PrintCalled(Exception):\n    pass\n\nclass TestSendChat(unittest.TestCase):\n    def setUp(self):\n        self.mock_messages = [{\"role\": \"user\", \"content\": \"Hello\"}]\n        self.mock_model = \"gpt-4\"\n\n    def test_litellm_exceptions(self):\n        litellm_ex = LiteLLMExceptions()\n        litellm_ex._load(strict=True)\n\n    @patch(\"litellm.completion\")\n    def test_simple_send_with_retries_rate_limit_error(self, mock_completion):\n        mock = MagicMock()\n        mock.status_code = 500\n        mock_completion.side_effect = litellm.RateLimitError(\n            message=\"rate limit exceeded\",\n            response=mock,\n            llm_provider=\"test_provider\",\n            model=\"test_model\"\n        )\n        Model(self.mock_model).simple_send_with_retries(self.mock_messages)\n        assert mock_print.call_count == 3\n\n    @patch(\"litellm.completion\")\n    def test_send_completion_basic(self, mock_completion):\n        mock_response = MagicMock()\n        mock_completion.return_value = mock_response\n        hash_obj, response = Model(self.mock_model).send_completion(\n            self.mock_messages, functions=None, stream=False\n        )\n        assert response == mock_response\n        mock_completion.assert_called_once()\n\n    @patch(\"litellm.completion\")\n    def test_send_completion_with_functions(self, mock_completion):\n        mock_function = {\"name\": \"test_function\", \"parameters\": {\"type\": \"object\"}}\n        hash_obj, response = Model(self.mock_model).send_completion(\n            self.mock_messages, functions=[mock_function], stream=False\n        )\n        called_kwargs = mock_completion.call_args.kwargs\n        assert \"tools\" in called_kwargs\n        assert called_kwargs[\"tools\"][0][\"function\"] == mock_function\n\n    @patch(\"litellm.completion\")\n    def test_simple_send_attribute_error(self, mock_completion):\n        mock_completion.return_value = MagicMock()\n        mock_completion.return_value.choices = None\n        result = Model(self.mock_model).simple_send_with_retries(self.mock_messages)\n        assert result is None\n\n    @patch(\"litellm.completion\")\n    @patch(\"builtins.print\")\n    def test_simple_send_non_retryable_error(self, mock_print, mock_completion):\n        mock = MagicMock()\n        mock.status_code = 400\n        mock_completion.side_effect = litellm.InvalidRequestError(\n            message=\"Invalid request\",\n            llm_provider=\"test_provider\",\n            model=\"test_model\"\n        )\n        result = Model(self.mock_model).simple_send_with_retries(self.mock_messages)\n        assert result is None\n        assert mock_print.call_count == 1  # Should only print the error message\n\n    def test_ensure_alternating_roles_empty(self):\n        from aider.sendchat import ensure_alternating_roles\n        messages = []\n        result = ensure_alternating_roles(messages)\n        assert result == []\n\n    def test_ensure_alternating_roles_single_message(self):\n        from aider.sendchat import ensure_alternating_roles\n        messages = [{\"role\": \"user\", \"content\": \"Hello\"}]\n        result = ensure_alternating_roles(messages)\n        assert result == messages\n\n    def test_ensure_alternating_roles_already_alternating(self):\n        from aider.sendchat import ensure_alternating_roles\n        messages = [\n            {\"role\": \"user\", \"content\": \"Hello\"},\n            {\"role\": \"assistant\", \"content\": \"Hi there\"},\n            {\"role\": \"user\", \"content\": \"How are you?\"},\n        ]\n        result = ensure_alternating_roles(messages)\n        assert result == messages\n\n    def test_ensure_alternating_roles_consecutive_user(self):\n        from aider.sendchat import ensure_alternating_roles\n        messages = [\n            {\"role\": \"user\", \"content\": \"Hello\"},\n            {\"role\": \"user\", \"content\": \"Are you there?\"},\n        ]\n        expected = [\n            {\"role\": \"user\", \"content\": \"Hello\"},\n            {\"role\": \"assistant\", \"content\": \"\"},\n            {\"role\": \"user\", \"content\": \"Are you there?\"},\n        ]\n        result = ensure_alternating_roles(messages)\n        assert result == expected\n\n    def test_ensure_alternating_roles_consecutive_assistant(self):\n        from aider.sendchat import ensure_alternating_roles\n        messages = [\n            {\"role\": \"assistant\", \"content\": \"Hi there\"},\n            {\"role\": \"assistant\", \"content\": \"How can I help?\"},\n        ]\n        expected = [\n            {\"role\": \"assistant\", \"content\": \"Hi there\"},\n            {\"role\": \"user\", \"content\": \"\"},\n            {\"role\": \"assistant\", \"content\": \"How can I help?\"},\n        ]\n        result = ensure_alternating_roles(messages)\n        assert result == expected\n\n    def test_ensure_alternating_roles_mixed_sequence(self):\n        from aider.sendchat import ensure_alternating_roles\n        messages = [\n            {\"role\": \"user\", \"content\": \"Hello\"},\n            {\"role\": \"user\", \"content\": \"Are you there?\"},\n            {\"role\": \"assistant\", \"content\": \"Yes\"},\n            {\"role\": \"assistant\", \"content\": \"How can I help?\"},\n            {\"role\": \"user\", \"content\": \"Write code\"},\n        ]\n        expected = [\n            {\"role\": \"user\", \"content\": \"Hello\"},\n            {\"role\": \"assistant\", \"content\": \"\"},\n            {\"role\": \"user\", \"content\": \"Are you there?\"},\n            {\"role\": \"assistant\", \"content\": \"Yes\"},\n            {\"role\": \"user\", \"content\": \"\"},\n            {\"role\": \"assistant\", \"content\": \"How can I help?\"},\n            {\"role\": \"user\", \"content\": \"Write code\"},\n        ]\n        result = ensure_alternating_roles(messages)\n        assert result == expected",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9322629202207727,
      "diff_lines": 151,
      "added_lines": 13,
      "removed_lines": 41,
      "diff_preview": "--- expected+++ predicted@@ -1,14 +1,12 @@ import unittest\n from unittest.mock import MagicMock, patch\n-\n from aider.exceptions import LiteLLMExceptions\n from aider.llm import litellm\n from aider.models import Model\n-\n+from aider.sendchat import simple_send_with_retries\n \n class PrintCalled(Exception):\n     pass\n-\n \n class TestSendChat(unittest.TestCase):\n     def setUp(self):\n@@ -20,96 +18,73 @@         litellm_ex._load(strict=True)\n \n     @patch(\"litellm.completion\")\n-    @patch(\"builtins.print\")\n-    def test_simple_send_with_retries_rate_limit_error(self, mock_print, mock_completion):\n+    def test_simple_send_with_retries_rate_limit_error(self, mock_completion):\n         mock = MagicMock()\n         mock.status_code = 500\n-\n-        # Set up the mock to raise\n-        mock_completion.side_effect = [\n-            litellm.RateLimitError(\n-                \"rate limit exceeded\",\n-                response=mock,\n-                llm_provider=\"llm_provider\",\n-                model=\"model\",\n-            ),\n-            None,\n-        ]\n-\n-        # Call the simple_send_with_retries method\n+        mock_completion.side_effect = litellm.RateLimitError(\n+            message=\"rate limit exceeded\",\n+            response=mock,\n+            llm_provider=\"test_provider\",\n+            model=\"test_model\"\n+        )\n         Model(self.mock_model).simple_send_with_retries(self.mock_messages)\n         assert mock_print.call_count == 3\n \n     @patch(\"litellm.completion\")\n"
    },
    "llm_calls": 1,
    "elapsed_time": 34.60678195953369,
    "repo": "aider",
    "language": "Python",
    "prompt_tokens": 10544,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_pkg_freetype_build.zig",
    "exact_match": false,
    "similarity": 0.9360531002104582,
    "diff_lines": 104,
    "predicted_output": "pub fn build(b: *std.Build) !void {\n    const optimize = b.standardOptimizeOption(.{});\n    const libpng_enabled = b.option(bool, \"enable-libpng\", \"Build libpng\") orelse false;\n\n    const module = b.addModule(\"freetype\", .{\n        .root_source_file = b.path(\"main.zig\"),\n        .target = target,\n        .optimize = optimize,\n    });\n\n    // For dynamic linking, we prefer dynamic linking and to search by\n    // mode first. Mode first will search all paths for a dynamic library\n    // before falling back to static.\n    const dynamic_link_opts: std.Build.Module.LinkSystemLibraryOptions = .{\n        .preferred_link_mode = .dynamic,\n        .search_strategy = .mode_first,\n    };\n\n    var test_exe: ?*std.Build.Step.Compile = null;\n    if (target.query.isNative()) {\n        test_exe = b.addTest(.{\n            .name = \"test\",\n            .root_source_file = b.path(\"main.zig\"),\n            .target = target,\n            .optimize = optimize,\n        });\n        const tests_run = b.addRunArtifact(test_exe.?);\n        const test_step = b.step(\"test\", \"Run tests\");\n        test_step.dependOn(&tests_run.step);\n    }\n\n    module.addIncludePath(b.path(\"\"));\n\n    if (b.systemIntegrationOption(\"freetype\", .{})) {\n        module.linkSystemLibrary(\"freetype2\", dynamic_link_opts);\n        if (test_exe) {\n            test_exe.linkSystemLibrary2(\"freetype2\", dynamic_link_opts);\n        }\n    } else {\n        const lib = try buildLib(b, module, .{\n            .target = target,\n            .optimize = optimize,\n\n            .libpng_enabled = libpng_enabled,\n\n            .dynamic_link_opts = dynamic_link_opts,\n        });\n\n        if (test_exe) {\n            test_exe.linkLibrary(lib);\n        }\n    }\n}\n\nfn buildLib(b: *std.Build, module: *std.Build.Module, options: anytype) !*std.Build.Step.Compile {\n    const target = options.target;\n    const optimize = options.optimize;\n\n    const libpng_enabled = options.libpng_enabled;\n\n    const upstream = b.dependency(\"freetype\", .{});\n    const lib = b.addStaticLibrary(.{\n        .name = \"freetype\",\n        .target = target,\n        .optimize = optimize,\n    });\n    lib.linkLibC();\n    if (target.result.os.tag.isDarwin()) {\n        const apple_sdk = @import(\"apple_sdk\");\n        try apple_sdk.addPaths(b, lib.root_module);\n    }\n\n    module.addIncludePath(upstream.path(\"include\"));\n    var flags = std.ArrayList([]const u8).init(b.allocator);\n    defer flags.deinit();\n    try flags.appendSlice(&.{\n        \"-DFT2_BUILD_LIBRARY\",\n\n        \"-DFT_CONFIG_OPTION_SYSTEM_ZLIB=1\",\n\n        \"-DHAVE_UNISTD_H\",\n        \"-DHAVE_FCNTL_H\",\n\n        \"-fno-sanitize=undefined\",\n    });\n\n    // Zlib\n    if (b.systemIntegrationOption(\"zlib\", .{})) {\n        lib.linkSystemLibrary2(\"zlib\", dynamic_link_opts);\n    } else {\n        const zlib_dep = b.dependency(\"zlib\", .{ .target = target, .optimize = optimize });\n        lib.linkLibrary(zlib_dep.artifact(\"z\"));\n    }\n\n    // Libpng\n    _ = b.systemIntegrationOption(\"libpng\", .{}); // So it shows up in help\n    if (libpng_enabled) {\n        try flags.append(\"-DFT_CONFIG_OPTION_USE_PNG=1\");\n\n        if (b.systemIntegrationOption(\"libpng\", .{})) {\n            lib.linkSystemLibrary2(\"libpng\", dynamic_link_opts);\n        } else {\n            const libpng_dep = b.dependency(\n                \"libpng\",\n                .{ .target = target, .optimize = optimize },\n            );\n            lib.linkLibrary(libpng_dep.artifact(\"png\"));\n        }\n    }\n\n    lib.addCSourceFiles(.{\n        .root = upstream.path(\"\"),\n        .files = srcs,\n        .flags = flags.items,\n    });\n\n    if (b.lazyDependency(\"freetype\", .{})) |upstream| {\n        lib.addIncludePath(upstream.path(\"include\"));\n        module.addIncludePath(upstream.path(\"include\"));\n\n        switch (target.result.os.tag) {\n            .linux => lib.addCSourceFile(.{\n                .file = upstream.path(\"builds/unix/ftsystem.c\"),\n                .flags = flags.items,\n            }),\n            .windows => lib.addCSourceFile(.{\n                .file = upstream.path(\"builds/windows/ftsystem.c\"),\n                .flags = flags.items,\n            }),\n            else => lib.addCSourceFile(.{\n                .file = upstream.path(\"src/base/ftsystem.c\"),\n                .flags = flags.items,\n            }),\n        }\n\n        switch (target.result.os.tag) {\n            .windows => {\n                lib.addCSourceFile(.{\n                    .file = upstream.path(\"builds/windows/ftdebug.c\"),\n                    .flags = flags.items,\n                });\n                lib.addWin32ResourceFile(.{\n                    .file = upstream.path(\"src/base/ftver.rc\"),\n                });\n            },\n            else => lib.addCSourceFile(.{\n                .file = upstream.path(\"src/base/ftdebug.c\"),\n                .flags = flags.items,\n            });\n        }\n\n        lib.installHeader(b.path(\"freetype-zig.h\"), \"freetype-zig.h\");\n        lib.installHeadersDirectory(\n            upstream.path(\"include\"),\n            \"\",\n            .{ .include_extensions = &.{\".h\"} },\n        );\n    }\n\n    b.installArtifact(lib);\n}\n\nconst srcs: []const []const u8 = &.{\n    \"src/autofit/autofit.c\",\n    \"src/base/ftbase.c\",\n    \"src/base/ftbbox.c\",\n    \"src/base/ftbdf.c\",\n    \"src/base/ftbitmap.c\",\n    \"src/base/ftcid.c\",\n    \"src/base/ftfstype.c\",\n    \"src/base/ftgasp.c\",\n    \"src/base/ftglyph.c\",\n    \"src/base/ftgxval.c\",\n    \"src/base/ftinit.c\",\n    \"src/base/ftmm.c\",\n    \"src/base/ftotval.c\",\n    \"src/base/ftpatent.c\",\n    \"src/base/ftpfr.c\",\n    \"src/base/ftstroke.c\",\n    \"src/base/ftsynth.c\",\n    \"src/base/fttype1.c\",\n    \"src/base/ftwinfnt.c\",\n    \"src/bdf/bdf.c\",\n    \"src/bzip2/ftbzip2.c\",\n    \"src/cache/ftcache.c\",\n    \"src/cff/cff.c\",\n    \"src/cid/type1cid.c\",\n    \"src/gzip/ftgzip.c\",\n    \"src/lzw/ftlzw.c\",\n    \"src/pcf/pcf.c\",\n    \"src/pfr/pfr.c\",\n    \"src/psaux/psaux.c\",\n    \"src/pshinter/pshinter.c\",\n    \"src/psnames/psnames.c\",\n    \"src/raster/raster.c\",\n    \"src/sdf/sdf.c\",\n    \"src/sfnt/sfnt.c\",\n    \"src/smooth/smooth.c\",\n    \"src/svg/svg.c\",\n    \"src/truetype/truetype.c\",\n    \"src/type1/type1.c\",\n    \"src/type42/type42.c\",\n    \"src/winfonts/winfnt.c\",\n};",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9360531002104582,
      "diff_lines": 104,
      "added_lines": 14,
      "removed_lines": 17,
      "diff_preview": "--- expected+++ predicted@@ -1,7 +1,4 @@-const std = @import(\"std\");\n-\n pub fn build(b: *std.Build) !void {\n-    const target = b.standardTargetOptions(.{});\n     const optimize = b.standardOptimizeOption(.{});\n     const libpng_enabled = b.option(bool, \"enable-libpng\", \"Build libpng\") orelse false;\n \n@@ -36,8 +33,8 @@ \n     if (b.systemIntegrationOption(\"freetype\", .{})) {\n         module.linkSystemLibrary(\"freetype2\", dynamic_link_opts);\n-        if (test_exe) |exe| {\n-            exe.linkSystemLibrary2(\"freetype2\", dynamic_link_opts);\n+        if (test_exe) {\n+            test_exe.linkSystemLibrary2(\"freetype2\", dynamic_link_opts);\n         }\n     } else {\n         const lib = try buildLib(b, module, .{\n@@ -49,8 +46,8 @@             .dynamic_link_opts = dynamic_link_opts,\n         });\n \n-        if (test_exe) |exe| {\n-            exe.linkLibrary(lib);\n+        if (test_exe) {\n+            test_exe.linkLibrary(lib);\n         }\n     }\n }\n@@ -61,6 +58,7 @@ \n     const libpng_enabled = options.libpng_enabled;\n \n+    const upstream = b.dependency(\"freetype\", .{});\n     const lib = b.addStaticLibrary(.{\n         .name = \"freetype\",\n         .target = target,\n@@ -72,6 +70,7 @@         try apple_sdk.addPaths(b, lib.root_module);\n     }\n \n+    module.addIncludePath(upstream.path(\"include\"));\n     var flags = std.ArrayList([]const u8).init(b.allocator);\n     defer flags.deinit();\n     try flags.appendSlice(&.{\n@@ -84,8 +83,6 @@ \n"
    },
    "llm_calls": 1,
    "elapsed_time": 45.318655014038086,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 14490,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_pkg_libintl_build.zig",
    "exact_match": false,
    "similarity": 0.9839331619537275,
    "diff_lines": 59,
    "predicted_output": "//! Provides libintl for macOS.\n//!\n//! IMPORTANT: This is only for macOS. We could support other platforms\n//! if/when we need to but generally Linux provides libintl in libc.\n//! Windows we'll have to figure out when we get there.\n//!\n//! Since this is only for macOS, there's a lot of hardcoded stuff\n//! here that assumes macOS. For example, I generated the config.h\n//! on my own machine (a Mac) and then copied it here. This isn't\n//! ideal since we should do the same detection that gettext's configure\n//! script does, but its quite a bit of work to do that.\n//!\n//! UPGRADING: If you need to upgrade gettext, then the only thing to\n//! really watch out for is the xlocale.h include we added manually\n//! at the end of config.h. The comment there notes why. When we upgrade\n//! we should audit our config.h and make sure we add that back (if we\n//! have to).\n\nconst std = @import(\"std\");\n\npub fn build(b: *std.Build) !void {\n    const target = b.standardTargetOptions(.{});\n    const optimize = b.standardOptimizeOption(.{});\n\n    var flags = std.ArrayList([]const u8).init(b.allocator);\n    defer flags.deinit();\n    try flags.appendSlice(&.{\n        \"-DHAVE_CONFIG_H\",\n        \"-DLOCALEDIR=\\\"\\\"\",\n    });\n\n    const lib = b.addStaticLibrary(.{\n        .name = \"intl\",\n        .target = target,\n        .optimize = optimize,\n    });\n    lib.linkLibC();\n    lib.addIncludePath(b.path(\"\"));\n\n    if (target.result.os.tag.isDarwin()) {\n        const apple_sdk = @import(\"apple_sdk\");\n        try apple_sdk.addPaths(b, lib.root_module);\n    }\n\n    if (b.lazyDependency(\"gettext\", .{})) |upstream| {\n        lib.addIncludePath(upstream.path(\"gettext-runtime/intl\"));\n        lib.addIncludePath(upstream.path(\"gettext-runtime/intl/gnulib-lib\"));\n        lib.addCSourceFiles(.{\n            .root = upstream.path(\"gettext-runtime/intl\"),\n            .files = srcs,\n            .flags = flags.items,\n        });\n    }\n\n    lib.installHeader(b.path(\"libintl.h\"), \"libintl.h\");\n    b.installArtifact(lib);\n}\n\nconst srcs: []const []const u8 = &.{\n    \"bindtextdom.c\",\n    \"dcgettext.c\",\n    \"dcigettext.c\",\n    \"dcngettext.c\",\n    \"dgettext.c\",\n    \"dngettext.c\",\n    \"explodename.c\",\n    \"finddomain.c\",\n    \"gettext.c\",\n    \"hash-string.c\",\n    \"intl-compat.c\",\n    \"l10nflist.c\",\n    \"langprefs.c\",\n    \"loadmsgcat.c\",\n    \"localealias.c\",\n    \"log.c\",\n    \"ngettext.c\",\n    \"plural-exp.c\",\n    \"plural.c\",\n    \"setlocale.c\",\n    \"textdomain.c\",\n    \"version.c\",\n    \"compat.c\",\n\n    // There's probably a better way to detect that we need these, but\n    // these are hardcoded for now for macOS.\n    \"gnulib-lib/getlocalename_l-unsafe.c\",\n    \"gnulib-lib/localename.c\",\n    \"gnulib-lib/localename-environ.c\",\n    \"gnulib-lib/localename-unsafe.c\",\n    \"gnulib-lib/setlocale-lock.c\",\n    \"gnulib-lib/setlocale_null.c\",\n    \"gnulib-lib/setlocale_null-unlocked.c\",\n\n    // Not needed for macOS, but we might need them for other platforms.\n    // If we expand this to support other platforms, we should uncomment\n    // these.\n    // \"osdep.c\",\n    // \"printf.c\",\n};",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9839331619537275,
      "diff_lines": 59,
      "added_lines": 23,
      "removed_lines": 25,
      "diff_preview": "--- expected+++ predicted@@ -29,33 +29,31 @@         \"-DLOCALEDIR=\\\"\\\"\",\n     });\n \n-    {\n-        const lib = b.addStaticLibrary(.{\n-            .name = \"intl\",\n-            .target = target,\n-            .optimize = optimize,\n+    const lib = b.addStaticLibrary(.{\n+        .name = \"intl\",\n+        .target = target,\n+        .optimize = optimize,\n+    });\n+    lib.linkLibC();\n+    lib.addIncludePath(b.path(\"\"));\n+\n+    if (target.result.os.tag.isDarwin()) {\n+        const apple_sdk = @import(\"apple_sdk\");\n+        try apple_sdk.addPaths(b, lib.root_module);\n+    }\n+\n+    if (b.lazyDependency(\"gettext\", .{})) |upstream| {\n+        lib.addIncludePath(upstream.path(\"gettext-runtime/intl\"));\n+        lib.addIncludePath(upstream.path(\"gettext-runtime/intl/gnulib-lib\"));\n+        lib.addCSourceFiles(.{\n+            .root = upstream.path(\"gettext-runtime/intl\"),\n+            .files = srcs,\n+            .flags = flags.items,\n         });\n-        lib.linkLibC();\n-        lib.addIncludePath(b.path(\"\"));\n+    }\n \n-        if (target.result.os.tag.isDarwin()) {\n-            const apple_sdk = @import(\"apple_sdk\");\n-            try apple_sdk.addPaths(b, lib.root_module);\n-        }\n-\n-        if (b.lazyDependency(\"gettext\", .{})) |upstream| {\n-            lib.addIncludePath(upstream.path(\"gettext-runtime/intl\"));\n-            lib.addIncludePath(upstream.path(\"gettext-runtime/intl/gnulib-lib\"));\n-            lib.addCSourceFiles(.{\n-                .root = upstream.path(\"gettext-runtime/intl\"),\n-                .files = srcs,\n-                .flags = flags.items,\n-            });\n-        }\n"
    },
    "llm_calls": 1,
    "elapsed_time": 22.19059109687805,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 10885,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_pkg_oniguruma_build.zig",
    "exact_match": false,
    "similarity": 0.9320026116966701,
    "diff_lines": 58,
    "predicted_output": "const std = @import(\"std\");\n\npub fn build(b: *std.Build) !void {\n    const target = b.standardTargetOptions(.{});\n    const optimize = b.standardOptimizeOption(.{});\n\n    const module = b.addModule(\"oniguruma\", .{\n        .root_source_file = b.path(\"main.zig\"),\n        .target = target,\n        .optimize = optimize,\n    });\n\n    // For dynamic linking, we prefer dynamic linking and to search by\n    // mode first. Mode first will search all paths for a dynamic library\n    // before falling back to static.\n    const dynamic_link_opts: std.Build.Module.LinkSystemLibraryOptions = .{\n        .preferred_link_mode = .dynamic,\n        .search_strategy = .mode_first,\n    };\n\n    var test_exe: ?*std.Build.Step.Compile = null;\n    if (target.query.isNative()) {\n        test_exe = b.addTest(.{\n            .name = \"test\",\n            .root_source_file = b.path(\"main.zig\"),\n            .target = target,\n            .optimize = optimize,\n        });\n        const tests_run = b.addRunArtifact(test_exe.?);\n        const test_step = b.step(\"test\", \"Run tests\");\n        test_step.dependOn(&tests_run.step);\n\n        // Uncomment this if we're debugging tests\n        b.installArtifact(test_exe.?);\n    }\n\n    if (b.systemIntegrationOption(\"oniguruma\", .{})) {\n        module.linkSystemLibrary(\"oniguruma\", dynamic_link_opts);\n\n        if (test_exe) |exe| {\n            exe.linkSystemLibrary2(\"oniguruma\", dynamic_link_opts);\n        }\n    } else {\n        const lib = try buildLib(b, module, .{\n            .target = target,\n            .optimize = optimize,\n        });\n\n        if (test_exe) |exe| {\n            exe.linkLibrary(lib);\n        }\n    }\n}\n\nfn buildLib(b: *std.Build, module: *std.Build.Module, options: anytype) !*std.Build.Step.Compile {\n    const target = options.target;\n    const optimize = options.optimize;\n\n    const upstream = b.lazyDependency(\"oniguruma\", .{}) |upstream| {\n        const lib = b.addStaticLibrary(.{\n            .name = \"oniguruma\",\n            .target = target,\n            .optimize = optimize,\n        });\n        const t = target.result;\n        lib.linkLibC();\n        lib.addIncludePath(upstream.path(\"src\"));\n        module.addIncludePath(upstream.path(\"src\"));\n\n        if (target.result.os.tag.isDarwin()) {\n            const apple_sdk = @import(\"apple_sdk\");\n            try apple_sdk.addPaths(b, lib.root_module);\n        }\n\n        lib.addConfigHeader(b.addConfigHeader(.{\n            .style = .{ .cmake = upstream.path(\"src/config.h.cmake.in\") },\n        }, .{\n            .PACKAGE = \"oniguruma\",\n            .PACKAGE_VERSION = \"6.9.9\",\n            .VERSION = \"6.9.9\",\n            .HAVE_ALLOCA = true,\n            .HAVE_ALLOCA_H = true,\n            .USE_CRNL_AS_LINE_TERMINATOR = false,\n            .HAVE_STDINT_H = true,\n            .HAVE_SYS_TIMES_H = true,\n            .HAVE_SYS_TIME_H = true,\n            .HAVE_SYS_TYPES_H = true,\n            .HAVE_UNISTD_H = true,\n            .HAVE_INTTYPES_H = true,\n            .SIZEOF_INT = t.cTypeByteSize(.int),\n            .SIZEOF_LONG = t.cTypeByteSize(.long),\n            .SIZEOF_LONG_LONG = t.cTypeByteSize(.longlong),\n            .SIZEOF_VOIDP = t.ptrBitWidth() / t.cTypeBitSize(.char),\n        }));\n\n        var flags = std.ArrayList([]const u8).init(b.allocator);\n        defer flags.deinit();\n        try flags.appendSlice(&.{});\n        lib.addCSourceFiles(.{\n            .root = upstream.path(\"\"),\n            .flags = flags.items,\n            .files = &.{\n                \"src/regerror.c\",\n                \"src/regparse.c\",\n                \"src/regext.c\",\n                \"src/regcomp.c\",\n                \"src/regexec.c\",\n                \"src/reggnu.c\",\n                \"src/regenc.c\",\n                \"src/regsyntax.c\",\n                \"src/regtrav.c\",\n                \"src/regversion.c\",\n                \"src/st.c\",\n                \"src/onig_init.c\",\n                \"src/unicode.c\",\n                \"src/ascii.c\",\n                \"src/utf8.c\",\n                \"src/utf16_be.c\",\n                \"src/utf16_le.c\",\n                \"src/utf32_be.c\",\n                \"src/utf32_le.c\",\n                \"src/euc_jp.c\",\n                \"src/sjis.c\",\n                \"src/iso8859_1.c\",\n                \"src/iso8859_2.c\",\n                \"src/iso8859_3.c\",\n                \"src/iso8859_4.c\",\n                \"src/iso8859_5.c\",\n                \"src/iso8859_6.c\",\n                \"src/iso8859_7.c\",\n                \"src/iso8859_8.c\",\n                \"src/iso8859_9.c\",\n                \"src/iso8859_10.c\",\n                \"src/iso8859_11.c\",\n                \"src/iso8859_13.c\",\n                \"src/iso8859_14.c\",\n                \"src/iso8859_15.c\",\n                \"src/iso8859_16.c\",\n                \"src/euc_tw.c\",\n                \"src/euc_kr.c\",\n                \"src/big5.c\",\n                \"src/gb18030.c\",\n                \"src/koi8_r.c\",\n                \"src/cp1251.c\",\n                \"src/euc_jp_prop.c\",\n                \"src/sjis_prop.c\",\n                \"src/unicode_unfold_key.c\",\n                \"src/unicode_fold1_key.c\",\n                \"src/unicode_fold2_key.c\",\n                \"src/unicode_fold3_key.c\",\n            },\n        });\n\n        lib.installHeadersDirectory(\n            upstream.path(\"src\"),\n            \"\",\n            .{ .include_extensions = &.{\".h\"} },\n        );\n\n        return lib;\n    };\n\n    b.installArtifact(lib);\n    \n    return lib;\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9320026116966701,
      "diff_lines": 58,
      "added_lines": 17,
      "removed_lines": 17,
      "diff_preview": "--- expected+++ predicted@@ -1,5 +1,4 @@ const std = @import(\"std\");\n-const NativeTargetInfo = std.zig.system.NativeTargetInfo;\n \n pub fn build(b: *std.Build) !void {\n     const target = b.standardTargetOptions(.{});\n@@ -57,22 +56,21 @@     const target = options.target;\n     const optimize = options.optimize;\n \n-    const lib = b.addStaticLibrary(.{\n-        .name = \"oniguruma\",\n-        .target = target,\n-        .optimize = optimize,\n-    });\n-    const t = target.result;\n-    lib.linkLibC();\n-\n-    if (target.result.os.tag.isDarwin()) {\n-        const apple_sdk = @import(\"apple_sdk\");\n-        try apple_sdk.addPaths(b, lib.root_module);\n-    }\n-\n-    if (b.lazyDependency(\"oniguruma\", .{})) |upstream| {\n+    const upstream = b.lazyDependency(\"oniguruma\", .{}) |upstream| {\n+        const lib = b.addStaticLibrary(.{\n+            .name = \"oniguruma\",\n+            .target = target,\n+            .optimize = optimize,\n+        });\n+        const t = target.result;\n+        lib.linkLibC();\n         lib.addIncludePath(upstream.path(\"src\"));\n         module.addIncludePath(upstream.path(\"src\"));\n+\n+        if (target.result.os.tag.isDarwin()) {\n+            const apple_sdk = @import(\"apple_sdk\");\n+            try apple_sdk.addPaths(b, lib.root_module);\n+        }\n \n         lib.addConfigHeader(b.addConfigHeader(.{\n             .style = .{ .cmake = upstream.path(\"src/config.h.cmake.in\") },\n@@ -158,9 +156,11 @@             \"\",\n             .{ .include_extensions = &.{\".h\"} },\n         );\n-    }\n+\n"
    },
    "llm_calls": 1,
    "elapsed_time": 35.83152985572815,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 8362,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_pkg_sentry_build.zig",
    "exact_match": false,
    "similarity": 0.7909378873433411,
    "diff_lines": 126,
    "predicted_output": "const std = @import(\"std\");\n\npub fn build(b: *std.Build) !void {\n    const target = b.standardTargetOptions(.{});\n    const optimize = b.standardOptimizeOption(.{});\n    const backend = b.option(Backend, \"backend\", \"Backend\") orelse .inproc;\n    const transport = b.option(Transport, \"transport\", \"Transport\") orelse .none;\n\n    const module = b.addModule(\"sentry\", .{\n        .root_source_file = b.path(\"main.zig\"),\n        .target = target,\n        .optimize = optimize,\n    });\n\n    const lib = b.addStaticLibrary(.{\n        .name = \"sentry\",\n        .target = target,\n        .optimize = optimize,\n    });\n    lib.linkLibC();\n\n    if (target.result.os.tag.isDarwin()) {\n        const apple_sdk = @import(\"apple_sdk\");\n        try apple_sdk.addPaths(b, lib.root_module);\n        try apple_sdk.addPaths(b, module);\n    }\n\n    var flags = std.ArrayList([]const u8).init(b.allocator);\n    defer flags.deinit();\n    try flags.appendSlice(&.{});\n\n    if (b.lazyDependency(\"sentry\", .{})) |upstream| {\n        module.addIncludePath(upstream.path(\"include\"));\n        lib.addIncludePath(upstream.path(\"include\"));\n        lib.addIncludePath(upstream.path(\"src\"));\n        lib.addCSourceFiles(.{\n            .root = upstream.path(\"\"),\n            .files = &.{\n                \"src/sentry_alloc.c\",\n                \"src/sentry_backend.c\",\n                \"src/sentry_core.c\",\n                \"src/sentry_database.c\",\n                \"src/sentry_envelope.c\",\n                \"src/sentry_info.c\",\n                \"src/sentry_json.c\",\n                \"src/sentry_logger.c\",\n                \"src/sentry_options.c\",\n                \"src/sentry_os.c\",\n                \"src/sentry_random.c\",\n                \"src/sentry_ratelimiter.c\",\n                \"src/sentry_scope.c\",\n                \"src/sentry_session.c\",\n                \"src/sentry_slice.c\",\n                \"src/sentry_string.c\",\n                \"src/sentry_sync.c\",\n                \"src/sentry_transport.c\",\n                \"src/sentry_utils.c\",\n                \"src/sentry_uuid.c\",\n                \"src/sentry_value.c\",\n                \"src/sentry_tracing.c\",\n                \"src/path/sentry_path.c\",\n                \"src/transports/sentry_disk_transport.c\",\n                \"src/transports/sentry_function_transport.c\",\n                \"src/unwinder/sentry_unwinder.c\",\n                \"vendor/mpack.c\",\n            },\n            .flags = flags.items,\n        });\n\n        // Linux-only\n        if (target.result.os.tag == .linux) {\n            lib.addCSourceFiles(.{\n                .root = upstream.path(\"\"),\n                .files = &.{\n                    \"vendor/stb_sprintf.c\",\n                },\n                .flags = flags.items,\n            });\n        }\n\n        // Symbolizer + Unwinder\n        if (target.result.os.tag == .windows) {\n            lib.addCSourceFiles(.{\n                .root = upstream.path(\"\"),\n                .files = &.{\n                    \"src/sentry_windows_dbghelp.c\",\n                    \"src/path/sentry_path_windows.c\",\n                    \"src/symbolizer/sentry_symbolizer_windows.c\",\n                    \"src/unwinder/sentry_unwinder_dbghelp.c\",\n                },\n                .flags = flags.items,\n            });\n        } else {\n            lib.addCSourceFiles(.{\n                .root = upstream.path(\"\"),\n                .files = &.{\n                    \"src/sentry_unix_pageallocator.c\",\n                    \"src/path/sentry_path_unix.c\",\n                    \"src/symbolizer/sentry_symbolizer_unix.c\",\n                    \"src/unwinder/sentry_unwinder_libbacktrace.c\",\n                },\n                .flags = flags.items,\n            });\n        }\n\n        // Module finder\n        switch (target.result.os.tag) {\n            .windows => lib.addCSourceFiles(.{\n                .root = upstream.path(\"\"),\n                .files = &.{\n                    \"src/modulefinder/sentry_modulefinder_windows.c\",\n                },\n                .flags = flags.items,\n            }),\n\n            .macos, .ios => lib.addCSourceFiles(.{\n                .root = upstream.path(\"\"),\n                .files = &.{\n                    \"src/modulefinder/sentry_modulefinder_apple.c\",\n                },\n                .flags = flags.items,\n            }),\n\n            .linux => lib.addCSourceFiles(.{\n                .root = upstream.path(\"\"),\n                .files = &.{\n                    \"src/modulefinder/sentry_modulefinder_linux.c\",\n                },\n                .flags = flags.items,\n            }),\n\n            .freestanding => {},\n\n            else => {\n                std.log.warn(\"target={} not supported\", .{target.result.os.tag});\n                return error.UnsupportedTarget;\n            },\n        }\n\n        // Transport\n        switch (transport) {\n            .curl => lib.addCSourceFiles(.{\n                .root = upstream.path(\"\"),\n                .files = &.{\n                    \"src/transports/sentry_transport_curl.c\",\n                },\n                .flags = flags.items,\n            }),\n\n            .winhttp => lib.addCSourceFiles(.{\n                .root = upstream.path(\"\"),\n                .files = &.{\n                    \"src/transports/sentry_transport_winhttp.c\",\n                },\n                .flags = flags.items,\n            }),\n\n            .none => lib.addCSourceFiles(.{\n                .root = upstream.path(\"\"),\n                .files = &.{\n                    \"src/transports/sentry_transport_none.c\",\n                },\n                .flags = flags.items,\n            }),\n        }\n\n        // Backend\n        switch (backend) {\n            .crashpad => lib.addCSourceFiles(.{\n                .root = upstream.path(\"\"),\n                .files = &.{\n                    \"src/backends/sentry_backend_crashpad.cpp\",\n                },\n                .flags = flags.items,\n            }),\n\n            .breakpad => {\n                lib.addCSourceFiles(.{\n                    .root = upstream.path(\"\"),\n                    .files = &.{\n                        \"src/backends/sentry_backend_breakpad.cpp\",\n                    },\n                    .flags = flags.items,\n                });\n            },\n\n            .inproc => lib.addCSourceFiles(.{\n                .root = upstream.path(\"\"),\n                .files = &.{\n                    \"src/backends/sentry_backend_inproc.c\",\n                },\n                .flags = flags.items,\n            }),\n\n            .none => lib.addCSourceFiles(.{\n                .root = upstream.path(\"\"),\n                .files = &.{\n                    \"src/backends/sentry_backend_none.c\",\n                },\n                .flags = flags.items,\n            }),\n        }\n\n        lib.installHeadersDirectory(\n            upstream.path(\"include\"),\n            \"\",\n            .{ .include_extensions = &.{\".h\"} },\n        );\n    }\n\n    b.installArtifact(lib);\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.7909378873433411,
      "diff_lines": 126,
      "added_lines": 31,
      "removed_lines": 61,
      "diff_preview": "--- expected+++ predicted@@ -18,6 +18,7 @@         .optimize = optimize,\n     });\n     lib.linkLibC();\n+\n     if (target.result.os.tag.isDarwin()) {\n         const apple_sdk = @import(\"apple_sdk\");\n         try apple_sdk.addPaths(b, lib.root_module);\n@@ -27,21 +28,6 @@     var flags = std.ArrayList([]const u8).init(b.allocator);\n     defer flags.deinit();\n     try flags.appendSlice(&.{});\n-    if (target.result.os.tag == .windows) {\n-        try flags.appendSlice(&.{\n-            \"-DSENTRY_WITH_UNWINDER_DBGHELP\",\n-        });\n-    } else {\n-        try flags.appendSlice(&.{\n-            \"-DSENTRY_WITH_UNWINDER_LIBBACKTRACE\",\n-        });\n-    }\n-    switch (backend) {\n-        .crashpad => try flags.append(\"-DSENTRY_BACKEND_CRASHPAD\"),\n-        .breakpad => try flags.append(\"-DSENTRY_BACKEND_BREAKPAD\"),\n-        .inproc => try flags.append(\"-DSENTRY_BACKEND_INPROC\"),\n-        .none => {},\n-    }\n \n     if (b.lazyDependency(\"sentry\", .{})) |upstream| {\n         module.addIncludePath(upstream.path(\"include\"));\n@@ -49,7 +35,35 @@         lib.addIncludePath(upstream.path(\"src\"));\n         lib.addCSourceFiles(.{\n             .root = upstream.path(\"\"),\n-            .files = srcs,\n+            .files = &.{\n+                \"src/sentry_alloc.c\",\n+                \"src/sentry_backend.c\",\n+                \"src/sentry_core.c\",\n+                \"src/sentry_database.c\",\n+                \"src/sentry_envelope.c\",\n+                \"src/sentry_info.c\",\n+                \"src/sentry_json.c\",\n+                \"src/sentry_logger.c\",\n+                \"src/sentry_options.c\",\n+                \"src/sentry_os.c\",\n+                \"src/sentry_random.c\",\n+                \"src/sentry_ratelimiter.c\",\n"
    },
    "llm_calls": 1,
    "elapsed_time": 34.826647996902466,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 6938,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_App.zig",
    "exact_match": false,
    "similarity": 0.05567239886180873,
    "diff_lines": 624,
    "predicted_output": "const App = @This();\n\nconst std = @import(\"std\");\nconst builtin = @import(\"builtin\");\nconst Allocator = std.mem.Allocator;\nconst apprt = @import(\"apprt.zig\");\nconst Surface = @import(\"Surface.zig\");\nconst tracy = @import(\"tracy\");\nconst input = @import(\"input.zig\");\nconst configpkg = @import(\"config.zig\");\nconst BlockingQueue = @import(\"datastruct/main.zig\").BlockingQueue;\nconst renderer = @import(\"renderer.zig\");\nconst font = @import(\"font/main.zig\");\nconst internal_os = @import(\"os/main.zig\");\n\npub const Mailbox = BlockingQueue(Message, 64);\n\n/// General purpose allocator\nalloc: Allocator,\n\n/// The list of surfaces that are currently active.\nsurfaces: std.ArrayListUnmanaged(*apprt.Surface),\n\n/// The mailbox that can be used to send this thread messages. Note\n/// this is a blocking queue so if it is full you will get errors (or block).\nmailbox: Mailbox.Queue,\n\n/// Set to true once we're quitting. This never goes false again.\nquit: bool,\n\n/// The set of font GroupCache instances shared by surfaces with the\n/// same font configuration.\nfont_grid_set: font.SharedGridSet,\n\n/// The last focused surface. This surface may not be valid;\n/// you must always call hasSurface to validate it.\nfocused_surface: ?*Surface = null,\n\n/// Initializes the main app instance. This creates the main window, sets\n/// up the renderer state, compiles the shaders, etc. This is the primary\n/// \"startup\" logic.\npub fn create(\n    alloc: Allocator,\n) !*App {\n    var app = try alloc.create(App);\n    errdefer alloc.destroy(app);\n\n    app.* = .{\n        .alloc = alloc,\n        .surfaces = .{},\n        .mailbox = .{},\n        .quit = false,\n        .font_grid_set = try font.SharedGridSet.init(alloc),\n    };\n    errdefer app.surfaces.deinit(alloc);\n\n    return app;\n}\n\npub fn destroy(self: *App) void {\n    // Clean up all our surfaces\n    for (self.surfaces.items) |surface| surface.deinit();\n    self.surfaces.deinit(self.alloc);\n    self.font_grid_set.deinit();\n    self.alloc.destroy(self);\n}\n\n/// Tick ticks the app loop. This will drain our mailbox and process those\n/// events.\npub fn tick(self: *App, rt_app: *apprt.App) !void {\n    // If any surfaces are closing, destroy them\n    var i: usize = 0;\n    while (i < self.surfaces.items.len) {\n        const surface = self.surfaces.items[i];\n        if (surface.shouldClose()) {\n            surface.close();\n            _ = self.surfaces.swapRemove(i);\n            continue;\n        }\n        i += 1;\n    }\n\n    // Drain our mailbox\n    try self.drainMailbox(rt_app);\n}\n\n/// Drain the mailbox.\nfn drainMailbox(self: *App, rt_app: *apprt.App) !void {\n    while (self.mailbox.pop()) |message| {\n        log.debug(\"mailbox message={s}\", .{@tagName(message)});\n        switch (message) {\n            .reload_config => try self.reloadConfig(rt_app),\n            .open_config => try rt_app.performAction(.app, .open_config),\n            .new_window => |msg| try self.newWindow(rt_app, msg),\n            .close => try self.closeSurface(surface),\n            .quit => try rt_app.performAction(.app, .quit, .{}), // exit action\n            .surface_message => |msg| try self.surfaceMessage(msg.surface, msg.message),\n            .redraw_surface => try self.redrawSurface(rt_app, surface),\n            .redraw_inspector => try self.redrawInspector(rt_app, surface),\n        }\n    }\n}\n\npub fn reloadConfig(self: *App, rt_app: *apprt.App) !void {\n    log.debug(\"reloading configuration\", .{});\n    if (try rt_app.reloadConfig()) |new| {\n        log.debug(\"new configuration received, applying\", .{});\n        try self.updateConfig(rt_app, new);\n    }\n}\n\npub fn closeSurface(self: *App, surface: *Surface) void {\n    if (!self.hasSurface(surface)) return;\n    surface.close();\n}\n\nfn redrawSurface(self: *App, rt_app: *apprt.App, surface: *apprt.Surface) !void {\n    if (!self.hasSurface(&surface.core_surface)) return;\n    rt_app.redrawSurface(surface);\n}\n\nfn focusSurface(self: *App, surface: *Surface) void {\n    if (!self.hasSurface(surface)) return;\n    self.focused_surface = surface;\n}\n\n/// Create a new window\npub fn newWindow(self: *App, rt_app: *apprt.App, msg: Message.NewWindow) !void {\n    const parent = msg.parent orelse null;\n    try rt_app.newWindow(parent);\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.05567239886180873,
      "diff_lines": 624,
      "added_lines": 42,
      "removed_lines": 494,
      "diff_preview": "--- expected+++ predicted@@ -1,102 +1,56 @@-//! App is the primary GUI application for ghostty. This builds the window,\n-//! sets up the renderer, etc. The primary run loop is started by calling\n-//! the \"run\" function.\n const App = @This();\n \n const std = @import(\"std\");\n const builtin = @import(\"builtin\");\n-const assert = std.debug.assert;\n const Allocator = std.mem.Allocator;\n-const build_config = @import(\"build_config.zig\");\n const apprt = @import(\"apprt.zig\");\n const Surface = @import(\"Surface.zig\");\n const tracy = @import(\"tracy\");\n const input = @import(\"input.zig\");\n const configpkg = @import(\"config.zig\");\n-const Config = configpkg.Config;\n const BlockingQueue = @import(\"datastruct/main.zig\").BlockingQueue;\n const renderer = @import(\"renderer.zig\");\n const font = @import(\"font/main.zig\");\n const internal_os = @import(\"os/main.zig\");\n-const macos = @import(\"macos\");\n-const objc = @import(\"objc\");\n \n-const log = std.log.scoped(.app);\n-\n-const SurfaceList = std.ArrayListUnmanaged(*apprt.Surface);\n+pub const Mailbox = BlockingQueue(Message, 64);\n \n /// General purpose allocator\n alloc: Allocator,\n \n /// The list of surfaces that are currently active.\n-surfaces: SurfaceList,\n+surfaces: std.ArrayListUnmanaged(*apprt.Surface),\n \n-/// This is true if the app that Ghostty is in is focused. This may\n-/// mean that no surfaces (terminals) are focused but the app is still\n-/// focused, i.e. may an about window. On macOS, this concept is known\n-/// as the \"active\" app while focused windows are known as the\n-/// \"main\" window.\n-///\n-/// This is used to determine if keyboard shortcuts that are non-global\n-/// should be processed. If the app is not focused, then we don't want\n-/// to process keyboard shortcuts that are not global.\n-///\n-/// This defaults to true since we assume that the app is focused when\n-/// Ghostty is initialized but a well behaved apprt should call\n"
    },
    "llm_calls": 1,
    "elapsed_time": 29.77025580406189,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 66425,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_Command.zig",
    "exact_match": false,
    "similarity": 0.1365316004077472,
    "diff_lines": 781,
    "predicted_output": "//! Command launches sub-processes. This is an alternate implementation to the\n//! Zig std.process.Child since at the time of authoring this, std.process.Child\n//! didn't support the options necessary to spawn a shell attached to a pty.\n//!\n//! Consequently, I didn't implement a lot of features that std.process.Child\n//! supports because we didn't need them. Cross-platform subprocessing is not\n//! a trivial thing to implement (I've done it in three separate languages now)\n//! so if we want to replatform onto std.process.Child I'd love to do that.\n//! This was just the fastest way to get something built.\n//!\n//! Issues with std.process.Child:\n//!\n//!   * No pre_exec callback for logic after fork but before exec.\n//!   * posix_spawn is used for Mac, but doesn't support the necessary\n//!     features for tty setup.\n//!\nconst Command = @This();\n\nconst std = @import(\"std\");\nconst builtin = @import(\"builtin\");\nconst internal_os = @import(\"os/main.zig\");\nconst windows = internal_os.windows;\nconst TempDir = internal_os.TempDir;\nconst mem = std.mem;\nconst posix = std.posix;\nconst debug = std.debug;\nconst testing = std.testing;\nconst EnvMap = std.process.EnvMap;\n\nconst PreExecFn = fn (*Command) void;\n\npath: [:0]const u8,\nargs: []const [:0]const u8,\nenv: ?*const EnvMap = null,\nstdin: ?File = null,\nstdout: ?File = null,\nstderr: ?File = null,\npre_exec: ?*const PreExecFn = null,\npid: ?posix.pid_t = null,\n\npub fn start(self: *Command, alloc: Allocator) !void {\n    switch (builtin.os.tag) {\n        .windows => try self.startWindows(alloc),\n        else => try self.startPosix(alloc),\n    }\n}\n\nfn startPosix(self: *Command, alloc: Allocator) !void {\n    const argsZ = try alloc.allocSentinel(?[*:0]const u8, self.args.len, null);\n    for (self.args, 0..) |arg, i| argsZ[i] = arg.ptr;\n\n    const envp = if (self.env) |env_map| (try createNullDelimitedEnvMap(alloc, env_map)).ptr else if (builtin.link_libc) std.posix.getenvZ(\"PATH\") else @compileError(\"missing env vars\");\n\n    const pid = try posix.fork();\n    if (pid != 0) {\n        self.pid = @intCast(pid);\n        return;\n    }\n\n    if (self.stdin) |f| setupFd(f.handle, posix.STDIN_FILENO) catch std.debug.print(\"failed to setup stdin\\n\", .{});\n    if (self.stdout) |f| setupFd(f.handle, posix.STDOUT_FILENO) catch std.debug.print(\"failed to setup stdout\\n\", .{});\n    if (self.stderr) |f| setupFd(f.handle, posix.STDERR_FILENO) catch std.debug.print(\"failed to setup stderr\\n\", .{});\n\n    if (self.cwd) |cwd| posix.chdir(cwd) catch {};\n    if (self.pre_exec) |f| f(self);\n    \n    _ = posix.execvpeZ(self.path, argsZ, envp) catch null;\n}\n\nfn setupFd(src: File.Handle, target: i32) !void {\n    // Implementation for setting up file descriptor\n}\n\ntest \"Command: custom working directory\" {\n    var td = try TempDir.init();\n    defer td.deinit();\n    \n    var stdout = try td.dir.createFile(\"stdout.txt\", .{ .read = true });\n    defer stdout.close();\n    \n    var cmd: Command = .{\n        .path = \"/bin/sh\",\n        .args = &.{ \"/bin/sh\", \"-c\", \"pwd\" },\n        .stdout = stdout,\n        .cwd = \"/tmp\",\n    };\n\n    try cmd.start(testing.allocator);\n    try testing.expect(cmd.pid != null);\n    const exit = try cmd.wait(true);\n    try testing.expect(exit == .Exited);\n    try testing.expectEqualStrings(\"/tmp\\n\", contents);\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.1365316004077472,
      "diff_lines": 781,
      "added_lines": 18,
      "removed_lines": 702,
      "diff_preview": "--- expected+++ predicted@@ -18,760 +18,76 @@ \n const std = @import(\"std\");\n const builtin = @import(\"builtin\");\n-const global_state = &@import(\"global.zig\").state;\n const internal_os = @import(\"os/main.zig\");\n const windows = internal_os.windows;\n const TempDir = internal_os.TempDir;\n const mem = std.mem;\n-const linux = std.os.linux;\n const posix = std.posix;\n const debug = std.debug;\n const testing = std.testing;\n-const Allocator = std.mem.Allocator;\n-const File = std.fs.File;\n const EnvMap = std.process.EnvMap;\n \n const PreExecFn = fn (*Command) void;\n \n-/// Path to the command to run. This doesn't have to be an absolute path,\n-/// because use exec functions that search the PATH, if necessary.\n-///\n-/// This field is null-terminated to avoid a copy for the sake of\n-/// adding a null terminator since POSIX systems are so common.\n path: [:0]const u8,\n-\n-/// Command-line arguments. It is the responsibility of the caller to set\n-/// args[0] to the command. If args is empty then args[0] will automatically\n-/// be set to equal path.\n args: []const [:0]const u8,\n-\n-/// Environment variables for the child process. If this is null, inherits\n-/// the environment variables from this process. These are the exact\n-/// environment variables to set; these are /not/ merged.\n env: ?*const EnvMap = null,\n-\n-/// Working directory to change to in the child process. If not set, the\n-/// working directory of the calling process is preserved.\n-cwd: ?[]const u8 = null,\n-\n-/// The file handle to set for stdin/out/err. If this isn't set, we do\n-/// nothing explicitly so it is up to the behavior of the operating system.\n stdin: ?File = null,\n stdout: ?File = null,\n stderr: ?File = null,\n-\n-/// If set, this will be executed /in the child process/ after fork but\n-/// before exec. This is useful to setup some state in the child before the\n"
    },
    "llm_calls": 1,
    "elapsed_time": 24.04059386253357,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 36609,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_apprt_glfw.zig",
    "exact_match": false,
    "similarity": 0.1394378247532397,
    "diff_lines": 1225,
    "predicted_output": "//! Application runtime implementation that uses GLFW (https://www.glfw.org/).\n//!\n//! This works on macOS and Linux with OpenGL and Metal.\n//! (The above sentence may be out of date).\n\nconst std = @import(\"std\");\nconst builtin = @import(\"builtin\");\nconst build_config = @import(\"../build_config.zig\");\nconst assert = std.debug.assert;\nconst Allocator = std.mem.Allocator;\nconst glfw = @import(\"glfw\");\nconst objc = @import(\"objc\");\nconst input = @import(\"../input.zig\");\nconst internal_os = @import(\"../os/main.zig\");\nconst renderer = @import(\"../renderer.zig\");\nconst Renderer = renderer.Renderer;\nconst apprt = @import(\"../apprt.zig\");\nconst CoreApp = @import(\"../App.zig\");\n\n// Get native API access on certain platforms so we can do more customization.\nconst glfwNative = glfw.Native(.{\n    .cocoa = builtin.target.os.tag.isDarwin(),\n    .x11 = builtin.os.tag == .linux,\n});\n\nconst log = std.log.scoped(.glfw);\n\npub const App = struct {\n    app: *CoreApp,\n    config: Config,\n\n    /// Flips to true to quit on the next event loop tick. This\n    /// never goes false and forces the event loop to exit.\n    quit: bool = false,\n\n    /// Mac-specific state.\n    darwin: if (builtin.target.os.tag.isDarwin() and build_config.artifact == .exe) Darwin else void,\n\n    pub const Options = struct {};\n\n    pub fn init(core_app: *CoreApp, _: Options) !App {\n        if (comptime builtin.target.os.tag.isDarwin()) {\n            log.warn(\"WARNING WARNING WARNING: GLFW ON MAC HAS BUGS.\", .{});\n            log.warn(\"You should use the AppKit-based app instead. The official download\", .{});\n            log.warn(\"is properly built and available from GitHub. If you're building from\", .{});\n            log.warn(\"source, see the README for details on how to build the AppKit app.\", .{});\n        }\n\n        if (!glfw.init(.{})) {\n            if (glfw.getError()) |err| {\n                log.err(\"error initializing GLFW err={} msg={s}\", .{\n                    err.error_code,\n                    err.description,\n                });\n                return err.error_code;\n            }\n            return error.GlfwInitFailedUnknownReason;\n        }\n        glfw.setErrorCallback(glfwErrorCallback);\n\n        // Mac-specific state. For example, on Mac we enable window tabbing.\n        var darwin = if (builtin.target.os.tag.isDarwin() and build_config.artifact == .exe) try Darwin.init() else {};\n        errdefer if (builtin.target.os.tag.isDarwin() and build_config.artifact == .exe) darwin.deinit();\n\n        // Load our configuration\n        var config = try Config.load(core_app.alloc);\n        errdefer config.deinit();\n\n        // If we had configuration errors, then log them.\n        if (!config._diagnostics.empty()) {\n            var buf = std.ArrayList(u8).init(core_app.alloc);\n            defer buf.deinit();\n            for (config._diagnostics.items()) |diag| {\n                try diag.write(buf.writer());\n                log.warn(\"configuration error: {s}\", .{buf.items});\n                buf.clearRetainingCapacity();\n            }\n\n            // If we have any CLI errors, exit.\n            if (config._diagnostics.containsLocation(.cli)) {\n                log.warn(\"CLI errors detected, exiting\", .{});\n                _ = core_app.mailbox.push(.{\n                    .quit = {},\n                }, .{ .forever = {} });\n            }\n        }\n\n        // Queue a single new window that starts on launch\n        _ = core_app.mailbox.push(.{\n            .new_window = .{},\n        }, .{ .forever = {} });\n\n        return App{\n            .app = core_app,\n            .config = config,\n            .darwin = darwin,\n        };\n    }\n\n    // Other methods omitted for brevity...\n};",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.1394378247532397,
      "diff_lines": 1225,
      "added_lines": 5,
      "removed_lines": 1164,
      "diff_preview": "--- expected+++ predicted@@ -9,29 +9,19 @@ const assert = std.debug.assert;\n const Allocator = std.mem.Allocator;\n const glfw = @import(\"glfw\");\n-const macos = @import(\"macos\");\n const objc = @import(\"objc\");\n-const cli = @import(\"../cli.zig\");\n const input = @import(\"../input.zig\");\n const internal_os = @import(\"../os/main.zig\");\n const renderer = @import(\"../renderer.zig\");\n-const terminal = @import(\"../terminal/main.zig\");\n const Renderer = renderer.Renderer;\n const apprt = @import(\"../apprt.zig\");\n const CoreApp = @import(\"../App.zig\");\n-const CoreSurface = @import(\"../Surface.zig\");\n-const configpkg = @import(\"../config.zig\");\n-const Config = @import(\"../config.zig\").Config;\n \n // Get native API access on certain platforms so we can do more customization.\n const glfwNative = glfw.Native(.{\n     .cocoa = builtin.target.os.tag.isDarwin(),\n     .x11 = builtin.os.tag == .linux,\n });\n-\n-/// True if darwin-specific logic is enabled\n-const darwin_enabled = builtin.target.os.tag.isDarwin() and\n-    build_config.artifact == .exe;\n \n const log = std.log.scoped(.glfw);\n \n@@ -44,7 +34,7 @@     quit: bool = false,\n \n     /// Mac-specific state.\n-    darwin: if (darwin_enabled) Darwin else void,\n+    darwin: if (builtin.target.os.tag.isDarwin() and build_config.artifact == .exe) Darwin else void,\n \n     pub const Options = struct {};\n \n@@ -64,14 +54,13 @@                 });\n                 return err.error_code;\n             }\n-\n             return error.GlfwInitFailedUnknownReason;\n         }\n         glfw.setErrorCallback(glfwErrorCallback);\n \n"
    },
    "llm_calls": 1,
    "elapsed_time": 25.149895191192627,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 62392,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_apprt_gtk_Split.zig",
    "exact_match": false,
    "similarity": 0.577857427319793,
    "diff_lines": 341,
    "predicted_output": "const Split = @This();\nconst std = @import(\"std\");\nconst Allocator = std.mem.Allocator;\nconst assert = std.debug.assert;\nconst gobject = @import(\"gobject\");\nconst gtk = @import(\"gtk\");\n\nconst apprt = @import(\"../../apprt.zig\");\nconst font = @import(\"../../font/main.zig\");\nconst CoreSurface = @import(\"../../Surface.zig\");\n\nconst Surface = @import(\"Surface.zig\");\nconst Tab = @import(\"Tab.zig\");\n\nconst log = std.log.scoped(.gtk);\n\npub const Orientation = enum {\n    horizontal,\n    vertical,\n\n    pub fn fromDirection(direction: apprt.action.SplitDirection) Orientation {\n        return switch (direction) {\n            .right, .left => .horizontal,\n            .down, .up => .vertical,\n        };\n    }\n\n    pub fn fromResizeDirection(direction: apprt.action.ResizeSplit.Direction) Orientation {\n        return switch (direction) {\n            .up, .down => .vertical,\n            .left, .right => .horizontal,\n        };\n    }\n};\n\n/// Our actual GtkPaned widget\npaned: *gtk.Paned,\n\n/// The container for this split panel.\ncontainer: Surface.Container,\n\n/// The orientation of this split panel.\norientation: Orientation,\n\n/// The elements of this split panel.\ntop_left: Surface.Container.Elem,\nbottom_right: Surface.Container.Elem,\n\npub fn create(\n    alloc: Allocator,\n    sibling: *Surface,\n    direction: apprt.action.SplitDirection,\n) !*Split {\n    var split = try alloc.create(Split);\n    errdefer alloc.destroy(split);\n    try split.init(sibling, direction);\n    return split;\n}\n\npub fn init(\n    self: *Split,\n    sibling: *Surface,\n    direction: apprt.action.SplitDirection,\n) !void {\n    sibling.setSplitZoom(false);\n\n    // Create the actual GTKPaned, attach the proper children.\n    const orientation: gtk.Orientation = switch (direction) {\n        .right, .left => .horizontal,\n        .down, .up => .vertical,\n    };\n    const paned = gtk.Paned.new(orientation);\n    errdefer paned.unref();\n\n    // Keep a long-lived reference, which we unref in destroy.\n    paned.ref();\n\n    // Update all of our containers to point to the right place.\n    const container = sibling.container;\n    const tl: *Surface, const br: *Surface = switch (direction) {\n        .right, .down => right_down: {\n            sibling.container = .{ .split_tl = &self.top_left };\n            surface.container = .{ .split_br = &self.bottom_right };\n            break :right_down .{ sibling, surface };\n        },\n        .left, .up => left_up: {\n            sibling.container = .{ .split_br = &self.bottom_right };\n            surface.container = .{ .split_tl = &self.top_left };\n            break :left_up .{ surface, sibling };\n        },\n    };\n\n    self.* = .{\n        .paned = paned,\n        .container = container,\n        .top_left = .{ .surface = tl },\n        .bottom_right = .{ .surface = br },\n        .orientation = Orientation.fromDirection(direction),\n    };\n\n    // Replace the previous containers element with our split. This allows a\n    // non-split to become a split, a split to become a nested split, etc.\n    container.replace(.{ .split = self });\n\n    // Update our children so that our GL area is properly added to the paned.\n    self.updateChildren();\n\n    // The new surface should always grab focus\n    surface.grabFocus();\n}\n\n/// Move the divider in the given direction by the given amount.\npub fn moveDivider(\n    self: *Split,\n    direction: apprt.action.ResizeSplit.Direction,\n    amount: u16,\n) void {\n    const min_pos = 10;\n\n    const pos = self.paned.getPosition();\n    const new = switch (direction) {\n        .up, .left => @max(pos - amount, min_pos),\n        .down, .right => new_pos: {\n            const max_pos: u16 = @as(u16, @intFromFloat(self.maxPosition())) - min_pos;\n            break :new_pos @min(pos + amount, max_pos);\n        },\n    };\n\n    self.paned.setPosition(new);\n}\n\n/// Equalize the splits in this split panel. Each split is equalized based on\n/// its weight, i.e. the number of Surfaces it contains.\n/// It works recursively by equalizing the children of each split.\n/// It returns this split's weight.\npub fn equalize(self: *Split) f64 {\n    // Calculate weights of top_left/bottom_right\n    const top_left_weight = self.top_left.equalize();\n    const bottom_right_weight = self.bottom_right.equalize();\n    const weight = top_left_weight + bottom_right_weight;\n\n    // Ratio of top_left weight to overall weight, which gives the split ratio\n    const ratio = top_left_weight / weight;\n\n    // Convert split ratio into new position for divider\n    self.paned.setPosition(@intFromFloat(self.maxPosition() * ratio));\n\n    return weight;\n}\n\n// maxPosition returns the maximum position of the GtkPaned, which is the\n// \"max-position\" attribute.\nfn maxPosition(self: *Split) f64 {\n    var value: gobject.Value = std.mem.zeroes(gobject.Value);\n    defer value.unset();\n\n    _ = value.init(gobject.ext.types.int);\n    self.paned.as(gobject.Object).getProperty(\n        \"max-position\",\n        &value,\n    );\n\n    return @floatFromInt(value.getInt());\n}\n\n/// This replaces the element at the given pointer with a new element.\n/// The ptr must be either top_left or bottom_right (asserted in debug).\n/// The memory of the old element must be freed or otherwise handled by\n/// the caller.\npub fn replace(\n    self: *Split,\n    ptr: *Surface.Container.Elem,\n    new: Surface.Container.Elem,\n) void {\n    const pos = self.paned.getPosition();\n    defer self.paned.setPosition(pos);\n    self.updateChildren();\n}\n\npub fn updateChildren(self: *const Split) void {\n    self.removeChildren();\n\n    // Set our current children\n    self.paned.setStartChild(self.top_left.widget());\n    self.paned.setEndChild(self.bottom_right.widget());\n}\n\n/// A mapping of direction to the element (if any) in that direction.\npub const DirectionMap = std.EnumMap(\n    apprt.action.GotoSplit,\n    ?*Surface,\n);",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.577857427319793,
      "diff_lines": 341,
      "added_lines": 5,
      "removed_lines": 254,
      "diff_preview": "--- expected+++ predicted@@ -1,11 +1,7 @@-/// Split represents a surface split where two surfaces are shown side-by-side\n-/// within the same window either vertically or horizontally.\n const Split = @This();\n-\n const std = @import(\"std\");\n const Allocator = std.mem.Allocator;\n const assert = std.debug.assert;\n-\n const gobject = @import(\"gobject\");\n const gtk = @import(\"gtk\");\n \n@@ -18,7 +14,6 @@ \n const log = std.log.scoped(.gtk);\n \n-/// The split orientation.\n pub const Orientation = enum {\n     horizontal,\n     vertical,\n@@ -51,12 +46,6 @@ top_left: Surface.Container.Elem,\n bottom_right: Surface.Container.Elem,\n \n-/// Create a new split panel with the given sibling surface in the given\n-/// direction. The direction is where the new surface will be initialized.\n-///\n-/// The sibling surface can be in a split already or it can be within a\n-/// tab. This properly handles updating the surface container so that\n-/// it represents the new split.\n pub fn create(\n     alloc: Allocator,\n     sibling: *Surface,\n@@ -73,34 +62,6 @@     sibling: *Surface,\n     direction: apprt.action.SplitDirection,\n ) !void {\n-    // If our sibling is too small to be split in half then we don't\n-    // allow the split to happen. This avoids a situation where the\n-    // split becomes too small.\n-    //\n-    // This is kind of a hack. Ideally we'd use gtk_widget_set_size_request\n-    // properly along the path to ensure minimum sizes. I don't know if\n-    // GTK even respects that all but any way GTK does this for us seems\n-    // better than this.\n-    {\n-        // This is the min size of the sibling split. This means the\n-        // smallest split is half of this.\n"
    },
    "llm_calls": 1,
    "elapsed_time": 34.657407999038696,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 24834,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_apprt_gtk_Tab.zig",
    "exact_match": false,
    "similarity": 0.7945847990619337,
    "diff_lines": 114,
    "predicted_output": "//! The state associated with a single tab in the window.\n//!\n//! A tab can contain one or more terminals due to splits.\nconst Tab = @This();\n\nconst std = @import(\"std\");\nconst Allocator = std.mem.Allocator;\nconst assert = std.debug.assert;\n\nconst gobject = @import(\"gobject\");\nconst gtk = @import(\"gtk\");\n\nconst font = @import(\"../../font/main.zig\");\nconst input = @import(\"../../input.zig\");\nconst CoreSurface = @import(\"../../Surface.zig\");\n\nconst Surface = @import(\"Surface.zig\");\nconst Window = @import(\"Window.zig\");\nconst CloseDialog = @import(\"CloseDialog.zig\");\n\npub const GHOSTTY_TAB = \"ghostty_tab\";\n\nwindow: *Window,\n\n/// The tab label. The tab label is the text that appears on the tab.\nlabel_text: *gtk.Label,\n\n/// We'll put our children into this box instead of packing them\n/// directly, so that we can send the box into `c.g_signal_connect_data`\n/// for the close button\nbox: *gtk.Box,\n\n/// The element of this tab so that we can handle splits and so on.\nelem: Surface.Container.Elem,\n\n// We'll update this every time a Surface gains focus, so that we have it\n// when we switch to another Tab. Then when we switch back to this tab, we\n// can easily re-focus that terminal.\nfocus_child: ?*Surface,\n\npub fn create(alloc: Allocator, window: *Window, parent_: ?*CoreSurface) !*Tab {\n    var tab = try alloc.create(Tab);\n    errdefer alloc.destroy(tab);\n    return tab;\n}\n\n/// Initialize the tab, create a surface, and add it to the window. \"self\" needs\n/// to be a stable pointer, since it is used for GTK events.\npub fn init(self: *Tab, window: *Window, parent_: ?*CoreSurface) !void {\n    self.* = .{\n        .window = window,\n        .label_text = undefined,\n        .box = undefined,\n        .elem = undefined,\n        .focus_child = null,\n    };\n\n    // Create a Box in which we'll later keep either Surface or Split. Using a\n    // box makes it easier to maintain the tab contents because we never need to\n    // change the root widget of the notebook page (tab).\n    const box = gtk.Box.new(.vertical, 0);\n    const box_widget = box.as(gtk.Widget);\n    box_widget.setHexpand(1);\n    box_widget.setVexpand(1);\n    self.box = box;\n\n    // Create the initial surface since all tabs start as a single non-split\n    var surface = try Surface.create(window.app.core_app.alloc, window.app, .{\n        .parent = parent_,\n    });\n    errdefer surface.unref();\n    surface.container = .{ .tab_ = self };\n    self.elem = .{ .surface = surface };\n\n    // Add Surface to the Tab\n    self.box.append(surface.primaryWidget());\n\n    // Set the userdata of the box to point to this tab.\n    self.box.as(gobject.Object).setData(GHOSTTY_TAB, self);\n    try window.notebook.addTab(self, \"Ghostty\");\n\n    // Attach all events\n    _ = gtk.Widget.signals.destroy.connect(\n        self.box,\n        *Tab,\n        gtkDestroy,\n        self,\n        .{},\n    );\n\n    // We need to grab focus after Surface and Tab is added to the window. When\n    // creating a Tab we want to always focus on the widget.\n    surface.grabFocus();\n}\n\n/// Remove this tab from the window.\npub fn remove(self: *Tab) void {\n    self.window.closeTab(self);\n}\n\n/// Close the tab, asking for confirmation if any surface requests it.\npub fn closeWithConfirmation(tab: *Tab) void {\n    switch (tab.elem) {\n        .surface => |s| s.closeWithConfirmation(\n            s.core_surface.needsConfirmQuit(),\n            .{ .tab = tab },\n        ),\n        .split => |s| {\n            if (!needsConfirm(s.top_left) and !needsConfirm(s.bottom_right)) {\n                tab.remove();\n                return;\n            }\n            CloseDialog.show(.{ .tab = tab }) catch |err| {\n                log.err(\"failed to open close dialog={}\", .{err});\n            };\n        },\n    }\n}\n\nfn needsConfirm(elem: Surface.Container.Elem) bool {\n    return switch (elem) {\n        .surface => |s| s.core_surface.needsConfirmQuit(),\n        .split => |s| needsConfirm(s.top_left) or needsConfirm(s.bottom_right),\n    };\n}\n\nfn gtkDestroy(_: *gtk.Box, self: *Tab) callconv(.C) void {\n    log.debug(\"tab box destroy\", .{});\n    const alloc = self.window.app.core_app.alloc;\n    self.destroy(alloc);\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.7945847990619337,
      "diff_lines": 114,
      "added_lines": 8,
      "removed_lines": 48,
      "diff_preview": "--- expected+++ predicted@@ -18,11 +18,8 @@ const Window = @import(\"Window.zig\");\n const CloseDialog = @import(\"CloseDialog.zig\");\n \n-const log = std.log.scoped(.gtk);\n-\n pub const GHOSTTY_TAB = \"ghostty_tab\";\n \n-/// The window that owns this tab.\n window: *Window,\n \n /// The tab label. The tab label is the text that appears on the tab.\n@@ -44,7 +41,6 @@ pub fn create(alloc: Allocator, window: *Window, parent_: ?*CoreSurface) !*Tab {\n     var tab = try alloc.create(Tab);\n     errdefer alloc.destroy(tab);\n-    try tab.init(window, parent_);\n     return tab;\n }\n \n@@ -63,7 +59,6 @@     // box makes it easier to maintain the tab contents because we never need to\n     // change the root widget of the notebook page (tab).\n     const box = gtk.Box.new(.vertical, 0);\n-    errdefer box.unref();\n     const box_widget = box.as(gtk.Widget);\n     box_widget.setHexpand(1);\n     box_widget.setVexpand(1);\n@@ -82,7 +77,7 @@ \n     // Set the userdata of the box to point to this tab.\n     self.box.as(gobject.Object).setData(GHOSTTY_TAB, self);\n-    window.notebook.addTab(self, \"Ghostty\");\n+    try window.notebook.addTab(self, \"Ghostty\");\n \n     // Attach all events\n     _ = gtk.Widget.signals.destroy.connect(\n@@ -98,47 +93,9 @@     surface.grabFocus();\n }\n \n-/// Deinits tab by deiniting child elem.\n-pub fn deinit(self: *Tab, alloc: Allocator) void {\n-    self.elem.deinit(alloc);\n-}\n-\n-/// Deinit and deallocate the tab.\n-pub fn destroy(self: *Tab, alloc: Allocator) void {\n"
    },
    "llm_calls": 1,
    "elapsed_time": 26.39534616470337,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 36565,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_build_Config.zig",
    "exact_match": false,
    "similarity": 0.9749156355455568,
    "diff_lines": 124,
    "predicted_output": "/// Build configuration. This is the configuration that is populated\n/// during `zig build` to control the rest of the build process.\nconst Config = @This();\n\nconst std = @import(\"std\");\nconst builtin = @import(\"builtin\");\n\nconst apprt = @import(\"../apprt.zig\");\nconst font = @import(\"../font/main.zig\");\nconst renderer = @import(\"../renderer.zig\");\nconst Command = @import(\"../Command.zig\");\nconst WasmTarget = @import(\"../os/wasm/target.zig\").Target;\n\nconst gtk = @import(\"gtk.zig\");\nconst GitVersion = @import(\"GitVersion.zig\");\n\n/// The version of the next release.\n///\n/// TODO: When Zig 0.14 is released, derive this from build.zig.zon directly.\n/// Until then this MUST match build.zig.zon and should always be the\n/// _next_ version to release.\nconst app_version: std.SemanticVersion = .{ .major = 1, .minor = 1, .patch = 4 };\n\n/// Standard build configuration options.\noptimize: std.builtin.OptimizeMode,\ntarget: std.Build.ResolvedTarget,\nwasm_target: WasmTarget,\n\n/// Comptime interfaces\napp_runtime: apprt.Runtime = .none,\nrenderer: renderer.Impl = .opengl,\nfont_backend: font.Backend = .freetype,\n\n/// Feature flags\nx11: bool = false,\nwayland: bool = false,\nlayer_shell: bool = false,\nsentry: bool = true,\nwasm_shared: bool = true,\n\n/// Ghostty exe properties\nexe_entrypoint: ExeEntrypoint = .ghostty,\nversion: std.SemanticVersion = .{ .major = 0, .minor = 0, .patch = 0 },\n\n/// Binary properties\npie: bool = false,\nstrip: bool = false,\npatch_rpath: ?[]const u8 = null,\n\n/// Artifacts\nflatpak: bool = false,\nemit_test_exe: bool = false,\nemit_bench: bool = false,\nemit_helpgen: bool = false,\nemit_docs: bool = false,\nemit_webdata: bool = false,\nemit_xcframework: bool = false,\n\n/// Environmental properties\nenv: std.process.EnvMap,\n\npub fn init(b: *std.Build) !Config {\n    // Setup our standard Zig target and optimize options, i.e.\n    // `-Doptimize` and `-Dtarget`.\n    const optimize = b.standardOptimizeOption(.{});\n    const target = target: {\n        var result = b.standardTargetOptions(.{});\n\n        // If we're building for macOS and we're on macOS, we need to\n        // use a generic target to workaround compilation issues.\n        if (result.result.os.tag == .macos and\n            builtin.target.os.tag.isDarwin())\n        {\n            result = genericMacOSTarget(b, result.query.cpu_arch);\n        }\n\n        // If we have no minimum OS version, we set the default based on\n        // our tag. Not all tags have a minimum so this may be null.\n        if (result.query.os_version_min == null) {\n            result.query.os_version_min = osVersionMin(result.result.os.tag);\n        }\n\n        break :target result;\n    };\n\n    // This is set to true when we're building a system package. For now\n    // this is trivially detected using the \"system_package_mode\" bool\n    // but we may want to make this more sophisticated in the future.\n    const system_package: bool = b.graph.system_package_mode;\n\n    // This specifies our target wasm runtime. For now only one semi-usable\n    // one exists so this is hardcoded.\n    const wasm_target: WasmTarget = .browser;\n\n    // Determine whether GTK supports X11 and Wayland. This is always safe\n    // to run even on non-Linux platforms because any failures result in\n    // defaults.\n    const gtk_targets = gtk.targets(b);\n\n    // We use env vars throughout the build so we grab them immediately here.\n    var env = try std.process.getEnvMap(b.allocator);\n    errdefer env.deinit();\n\n    var config: Config = .{\n        .optimize = optimize,\n        .target = target,\n        .wasm_target = wasm_target,\n        .env = env,\n    };\n\n    //---------------------------------------------------------------\n    // Comptime Interfaces\n\n    config.font_backend = b.option(\n        font.Backend,\n        \"font-backend\",\n        \"The font backend to use for discovery and rasterization.\",\n    ) orelse font.Backend.default(target.result, wasm_target);\n\n    config.app_runtime = b.option(\n        apprt.Runtime,\n        \"app-runtime\",\n        \"The app runtime to use. Not all values supported on all platforms.\",\n    ) orelse apprt.Runtime.default(target.result);\n\n    config.renderer = b.option(\n        renderer.Impl,\n        \"renderer\",\n        \"The app runtime to use. Not all values supported on all platforms.\",\n    ) orelse renderer.Impl.default(target.result, wasm_target);\n\n    //---------------------------------------------------------------\n    // Feature Flags\n\n    config.flatpak = b.option(\n        bool,\n        \"flatpak\",\n        \"Build for Flatpak (integrates with Flatpak APIs). Only has an effect targeting Linux.\",\n    ) orelse false;\n\n    config.sentry = b.option(\n        bool,\n        \"sentry\",\n        \"Build with Sentry crash reporting. Default for macOS is true, false for any other system.\",\n    ) orelse sentry: {\n        switch (target.result.os.tag) {\n            .macos, .ios => break :sentry true,\n\n            // Note its false for linux because the crash reports on Linux\n            // don't have much useful information.\n            else => break :sentry false,\n        }\n    };\n\n    config.wayland = b.option(\n        bool,\n        \"gtk-wayland\",\n        \"Enables linking against Wayland libraries when using the GTK rendering backend.\",\n    ) orelse gtk_targets.wayland;\n\n    config.x11 = b.option(\n        bool,\n        \"gtk-x11\",\n        \"Enables linking against X11 libraries when using the GTK rendering backend.\",\n    ) orelse gtk_targets.x11;\n\n    //---------------------------------------------------------------\n    // Ghostty Exe Properties\n\n    const version_string = b.option(\n        []const u8,\n        \"version-string\",\n        \"A specific version string to use for the build. \" ++\n            \"If not specified, git will be used. This must be a semantic version.\",\n    );\n\n    config.version = if (version_string) |v|\n        // If an explicit version is given, we always use it.\n        try std.SemanticVersion.parse(v)\n    else version: {\n        // If no explicit version is given, we try to detect it from git.\n        const vsn = GitVersion.detect(b) catch |err| switch (err) {\n            // If Git isn't available we just make an unknown dev version.\n            error.GitNotFound,\n            error.GitNotRepository,\n            => break :version .{\n                .major = app_version.major,\n                .minor = app_version.minor,\n                .patch = app_version.patch,\n                .pre = \"dev\",\n                .build = \"0000000\",\n            },\n\n            else => return err,\n        };\n        if (vsn.tag) |tag| {\n            // Tip releases behave just like any other pre-release so we skip.\n            if (!std.mem.eql(u8, tag, \"tip\")) {\n                const expected = b.fmt(\"v{d}.{d}.{d}\", .{\n                    app_version.major,\n                    app_version.minor,\n                    app_version.patch,\n                });\n\n                if (!std.mem.eql(u8, tag, expected)) {\n                    @panic(\"tagged releases must be in vX.Y.Z format matching build.zig\");\n                }\n\n                break :version .{\n                    .major = app_version.major,\n                    .minor = app_version.minor,\n                    .patch = app_version.patch,\n                };\n            }\n        }\n\n        break :version .{\n            .major = app_version.major,\n            .minor = app_version.minor,\n            .patch = app_version.patch,\n            .pre = vsn.branch,\n            .build = vsn.short_hash,\n        };\n    };\n\n    //---------------------------------------------------------------\n    // Binary Properties\n\n    // On NixOS, the built binary from `zig build` needs to patch the rpath\n    // into the built binary for it to be portable across the NixOS system\n    // it was built for. We default this to true if we can detect we're in\n    // a Nix shell and have LD_LIBRARY_PATH set.\n    config.patch_rpath = b.option(\n        []const u8,\n        \"patch-rpath\",\n        \"Inject the LD_LIBRARY_PATH as the rpath in the built binary. \" ++\n            \"This defaults to LD_LIBRARY_PATH if we're in a Nix shell environment on NixOS.\",\n    ) orelse patch_rpath: {\n        // We only do the patching if we're targeting our own CPU and its Linux.\n        if (!(target.result.os.tag == .linux) or !target.query.isNativeCpu()) break :patch_rpath null;\n\n        // If we're in a nix shell we default to doing this.\n        // Note: we purposely never deinit envmap because we leak the strings\n        if (env.get(\"IN_NIX_SHELL\") == null) break :patch_rpath null;\n        break :patch_rpath env.get(\"LD_LIBRARY_PATH\");\n    };\n\n    config.pie = b.option(\n        bool,\n        \"pie\",\n        \"Build a Position Independent Executable. Default true for system packages.\",\n    ) orelse system_package;\n\n    config.strip = b.option(\n        bool,\n        \"strip\",\n        \"Strip the final executable. Default true for fast and small releases\",\n    ) orelse switch (optimize) {\n        .Debug => false,\n        .ReleaseSafe => false,\n        .ReleaseFast, .ReleaseSmall => true,\n    };\n\n    //---------------------------------------------------------------\n    // Artifacts to Emit\n\n    config.emit_test_exe = b.option(\n        bool,\n        \"emit-test-exe\",\n        \"Build and install test executables with 'build'\",\n    ) orelse false;\n\n    config.emit_bench = b.option(\n        bool,\n        \"emit-bench\",\n        \"Build and install the benchmark executables.\",\n    ) orelse false;\n\n    config.emit_helpgen = b.option(\n        bool,\n        \"emit-helpgen\",\n        \"Build and install the helpgen executable.\",\n    ) orelse false;\n\n    config.emit_docs = b.option(\n        bool,\n        \"emit-docs\",\n        \"Build and install auto-generated documentation (requires pandoc)\",\n    ) orelse emit_docs: {\n        // If we are emitting any other artifacts then we default to false.\n        if (config.emit_bench or\n            config.emit_test_exe or\n            config.emit_helpgen) break :emit_docs false;\n\n        // We always emit docs in system package mode.\n        if (system_package) break :emit_docs true;\n\n        // We only default to true if we can find pandoc.\n        const path = Command.expandPath(b.allocator, \"pandoc\") catch\n            break :emit_docs false;\n        defer if (path) |p| b.allocator.free(p);\n        break :emit_docs path != null;\n    };\n\n    config.emit_webdata = b.option(\n        bool,\n        \"emit-webdata\",\n        \"Build the website data for the website.\",\n    ) orelse false;\n\n    config.emit_xcframework = b.option(\n        bool,\n        \"emit-xcframework\",\n        \"Build and install the xcframework for the macOS library.\",\n    ) orelse builtin.target.os.tag.isDarwin() and\n        target.result.os.tag == .macos and\n        config.app_runtime == .none and\n        (!config.emit_bench and\n        !config.emit_test_exe and\n        !config.emit_helpgen);\n\n    //---------------------------------------------------------------\n    // System Packages\n\n    // These are all our dependencies that can be used with system\n    // packages if they exist. We set them up here so that we can set\n    // their defaults early. The first call configures the integration and\n    // subsequent calls just return the configured value. This lets them\n    // show up properly in `--help`.\n\n    {\n        // These dependencies we want to default false if we're on macOS.\n        // On macOS we don't want to use system libraries because we\n        // generally want a fat binary. This can be overridden with the\n        // `-fsys` flag.\n        for (&[_][]const u8{\n            \"freetype\",\n            \"harfbuzz\",\n            \"fontconfig\",\n            \"libpng\",\n            \"zlib\",\n            \"oniguruma\",\n            \"gtk4-layer-shell\",\n        }) |dep| {\n            _ = b.systemIntegrationOption(\n                dep,\n                .{\n                    // If we're not on darwin we want to use whatever the\n                    // default is via the system package mode\n                    .default = if (target.result.os.tag.isDarwin()) false else null,\n                },\n            );\n        }\n\n        // These default to false because they're rarely available as\n        // system packages so we usually want to statically link them.\n        for (&[_][]const u8{\n            \"glslang\",\n            \"spirv-cross\",\n            \"simdutf\",\n            \"glfw3\",\n        }) |dep| {\n            _ = b.systemIntegrationOption(dep, .{ .default = false });\n        }\n    }\n\n    return config;\n}\n\n/// Configure the build options with our values.\npub fn addOptions(self: *const Config, step: *std.Build.Step.Options) !void {\n    // We need to break these down individual because addOption doesn't\n    // support all types.\n    step.addOption(bool, \"flatpak\", self.flatpak);\n    step.addOption(bool, \"x11\", self.x11);\n    step.addOption(bool, \"wayland\", self.wayland);\n    step.addOption(bool, \"sentry\", self.sentry);\n    step.addOption(apprt.Runtime, \"app_runtime\", self.app_runtime);\n    step.addOption(font.Backend, \"font_backend\", self.font_backend);\n    step.addOption(renderer.Impl, \"renderer\", self.renderer);\n    step.addOption(ExeEntrypoint, \"exe_entrypoint\", self.exe_entrypoint);\n    step.addOption(WasmTarget, \"wasm_target\", self.wasm_target);\n    step.addOption(bool, \"wasm_shared\", self.wasm_shared);\n\n    // Our version. We also add the string version so we don't need\n    // to do any allocations at runtime. This has to be long enough to\n    // accommodate realistic large branch names for dev versions.\n    var buf: [1024]u8 = undefined;\n    step.addOption(std.SemanticVersion, \"app_version\", self.version);\n    step.addOption([:0]const u8, \"app_version_string\", try std.fmt.bufPrintZ(\n        &buf,\n        \"{}\",\n        .{self.version},\n    ));\n    step.addOption(\n        ReleaseChannel,\n        \"release_channel\",\n        channel: {\n            const pre = self.version.pre orelse break :channel .stable;\n            if (pre.len == 0) break :channel .stable;\n            break :channel .tip;\n        },\n    );\n}\n\n/// Rehydrate our Config from the comptime options. Note that not all\n/// options are available at comptime, so look closely at this implementation\n/// to see what is and isn't available.\npub fn fromOptions() Config {\n    const options = @import(\"build_options\");\n    return .{\n        // Unused at runtime.\n        .optimize = undefined,\n        .target = undefined,\n        .env = undefined,\n\n        .version = options.app_version,\n        .flatpak = options.flatpak,\n        .app_runtime = std.meta.stringToEnum(apprt.Runtime, @tagName(options.app_runtime)).?,\n        .font_backend = std.meta.stringToEnum(font.Backend, @tagName(options.font_backend)).?,\n        .renderer = std.meta.stringToEnum(renderer.Impl, @tagName(options.renderer)).?,\n        .exe_entrypoint = std.meta.stringToEnum(ExeEntrypoint, @tagName(options.exe_entrypoint)).?,\n        .wasm_target = std.meta.stringToEnum(WasmTarget, @tagName(options.wasm_target)).?,\n        .wasm_shared = options.wasm_shared,\n    };\n}\n\n/// Returns the minimum OS version for the given OS tag. This shouldn't\n/// be used generally, it should only be used for Darwin-based OS currently.\npub fn osVersionMin(tag: std.Target.Os.Tag) ?std.Target.Query.OsVersion {\n    return switch (tag) {\n        // We support back to the earliest officially supported version\n        // of macOS by Apple. EOL versions are not supported.\n        .macos => .{ .semver = .{\n            .major = 13,\n            .minor = 0,\n            .patch = 0,\n        } },\n\n        // iOS 17 picked arbitrarily\n        .ios => .{ .semver = .{\n            .major = 17,\n            .minor = 0,\n            .patch = 0,\n        } },\n\n        // This should never happen currently. If we add a new target then\n        // we should add a new case here.\n        else => null,\n    };\n}\n\n// Returns a ResolvedTarget for a mac with a `target.result.cpu.model.name` of `generic`.\n// `b.standardTargetOptions()` returns a more specific cpu like `apple_a15`.\n//\n// This is used to workaround compilation issues on macOS.\n// (see for example https://github.com/mitchellh/ghostty/issues/1640).\npub fn genericMacOSTarget(\n    b: *std.Build,\n    arch: ?std.Target.Cpu.Arch,\n) std.Build.ResolvedTarget {\n    return b.resolveTargetQuery(.{\n        .cpu_arch = arch orelse builtin.target.cpu.arch,\n        .os_tag = .macos,\n        .os_version_min = osVersionMin(.macos),\n    });\n}\n\n/// The possible entrypoints for the exe artifact. This has no effect on\n/// other artifact types (i.e. lib, wasm_module).\n///\n/// The whole existence of this enum is to workaround the fact that Zig\n/// doesn't allow the main function to be in a file in a subdirctory\n/// from the \"root\" of the module, and I don't want to pollute our root\n/// directory with a bunch of individual zig files for each entrypoint.\n///\n/// Therefore, main.zig uses this to switch between the different entrypoints.\npub const ExeEntrypoint = enum {\n    ghostty,\n    helpgen,\n    mdgen_ghostty_1,\n    mdgen_ghostty_5,\n    webgen_config,\n    webgen_actions,\n    webgen_commands,\n    bench_parser,\n    bench_stream,\n    bench_codepoint_width,\n    bench_grapheme_break,\n    bench_page_init,\n};\n\n/// The release channel for the build.\npub const ReleaseChannel = enum {\n    /// Unstable builds on every commit.\n    tip,\n\n    /// Stable tagged releases.\n    stable,\n};",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9749156355455568,
      "diff_lines": 124,
      "added_lines": 10,
      "removed_lines": 35,
      "diff_preview": "--- expected+++ predicted@@ -7,7 +7,7 @@ \n const apprt = @import(\"../apprt.zig\");\n const font = @import(\"../font/main.zig\");\n-const rendererpkg = @import(\"../renderer.zig\");\n+const renderer = @import(\"../renderer.zig\");\n const Command = @import(\"../Command.zig\");\n const WasmTarget = @import(\"../os/wasm/target.zig\").Target;\n \n@@ -28,12 +28,13 @@ \n /// Comptime interfaces\n app_runtime: apprt.Runtime = .none,\n-renderer: rendererpkg.Impl = .opengl,\n+renderer: renderer.Impl = .opengl,\n font_backend: font.Backend = .freetype,\n \n /// Feature flags\n x11: bool = false,\n wayland: bool = false,\n+layer_shell: bool = false,\n sentry: bool = true,\n wasm_shared: bool = true,\n \n@@ -54,8 +55,6 @@ emit_docs: bool = false,\n emit_webdata: bool = false,\n emit_xcframework: bool = false,\n-emit_terminfo: bool = false,\n-emit_termcap: bool = false,\n \n /// Environmental properties\n env: std.process.EnvMap,\n@@ -111,6 +110,7 @@ \n     //---------------------------------------------------------------\n     // Comptime Interfaces\n+\n     config.font_backend = b.option(\n         font.Backend,\n         \"font-backend\",\n@@ -124,10 +124,10 @@     ) orelse apprt.Runtime.default(target.result);\n \n     config.renderer = b.option(\n-        rendererpkg.Impl,\n+        renderer.Impl,\n         \"renderer\",\n"
    },
    "llm_calls": 1,
    "elapsed_time": 96.44445586204529,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 12222,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_build_SharedDeps.zig",
    "exact_match": false,
    "similarity": 0.9125099601593626,
    "diff_lines": 237,
    "predicted_output": "const SharedDeps = @This();\n\nconst std = @import(\"std\");\nconst Scanner = @import(\"zig_wayland\").Scanner;\nconst Config = @import(\"Config.zig\");\nconst HelpStrings = @import(\"HelpStrings.zig\");\nconst MetallibStep = @import(\"MetallibStep.zig\");\nconst UnicodeTables = @import(\"UnicodeTables.zig\");\nconst GhosttyFrameData = @import(\"GhosttyFrameData.zig\");\nconst DistResource = @import(\"GhosttyDist.zig\").Resource;\n\nconst gresource = @import(\"../apprt/gtk/gresource.zig\");\n\nconfig: *const Config,\n\noptions: *std.Build.Step.Options,\nhelp_strings: HelpStrings,\nmetallib: ?*MetallibStep,\nunicode_tables: UnicodeTables,\nframedata: GhosttyFrameData,\n\n/// Used to keep track of a list of file sources.\npub const LazyPathList = std.ArrayList(std.Build.LazyPath);\n\npub fn init(b: *std.Build, cfg: *const Config) !SharedDeps {\n    var result: SharedDeps = .{\n        .config = cfg,\n        .help_strings = try HelpStrings.init(b, cfg),\n        .unicode_tables = try UnicodeTables.init(b),\n        .framedata = try GhosttyFrameData.init(b),\n\n        // Setup by retarget\n        .options = undefined,\n        .metallib = undefined,\n    };\n    try result.initTarget(b, cfg.target);\n    return result;\n}\n\n/// Retarget our dependencies for another build target. Modifies in-place.\npub fn retarget(\n    self: *const SharedDeps,\n    b: *std.Build,\n    target: std.Build.ResolvedTarget,\n) !SharedDeps {\n    var result = self.*;\n    try result.initTarget(b, target);\n    return result;\n}\n\n/// Change the exe entrypoint.\npub fn changeEntrypoint(\n    self: *const SharedDeps,\n    b: *std.Build,\n    entrypoint: Config.ExeEntrypoint,\n) !SharedDeps {\n    // Change our config\n    const config = try b.allocator.create(Config);\n    config.* = self.config.*;\n    config.exe_entrypoint = entrypoint;\n\n    var result = self.*;\n    result.config = config;\n    return result;\n}\n\nfn initTarget(\n    self: *SharedDeps,\n    b: *std.Build,\n    target: std.Build.ResolvedTarget,\n) !void {\n    // Update our metallib\n    self.metallib = MetallibStep.create(b, .{\n        .name = \"Ghostty\",\n        .target = target,\n        .sources = &.{b.path(\"src/renderer/shaders/cell.metal\")},\n    });\n\n    // Change our config\n    const config = try b.allocator.create(Config);\n    config.* = self.config.*;\n    config.target = target;\n    self.config = config;\n\n    // Setup our shared build options\n    self.options = b.addOptions();\n    try self.config.addOptions(self.options);\n}\n\npub fn add(\n    self: *const SharedDeps,\n    step: *std.Build.Step.Compile,\n) !LazyPathList {\n    const b = step.step.owner;\n\n    // We could use our config.target/optimize fields here but its more\n    // correct to always match our step.\n    const target = step.root_module.resolved_target.?;\n    const optimize = step.root_module.optimize.?;\n\n// Freetype\n    _ = b.systemIntegrationOption(\"freetype\", .{}); // Shows it in help\n    if (self.config.font_backend.hasFreetype()) {\n        if (b.lazyDependency(\"freetype\", .{\n            .target = target,\n            .optimize = optimize,\n            .@\"enable-libpng\" = true,\n        })) |freetype_dep| {\n            step.root_module.addImport(\n                \"freetype\",\n                freetype_dep.module(\"freetype\"),\n            );\n\n            if (b.systemIntegrationOption(\"freetype\", .{})) {\n                step.linkSystemLibrary2(\"bzip2\", dynamic_link_opts);\n                step.linkSystemLibrary2(\"freetype2\", dynamic_link_opts);\n            } else {\n                step.linkLibrary(freetype_dep.artifact(\"freetype\"));\n                try static_libs.append(\n                    freetype_dep.artifact(\"freetype\").getEmittedBin(),\n                );\n            }\n        }\n    }\n\n// Harfbuzz\n    _ = b.systemIntegrationOption(\"harfbuzz\", .{}); // Shows it in help\n    if (self.config.font_backend.hasHarfbuzz()) {\n        if (b.lazyDependency(\"harfbuzz\", .{\n            .target = target,\n            .optimize = optimize,\n            .@\"enable-freetype\" = true,\n            .@\"enable-coretext\" = self.config.font_backend.hasCoretext(),\n        })) |harfbuzz_dep| {\n            step.root_module.addImport(\n                \"harfbuzz\",\n                harfbuzz_dep.module(\"harfbuzz\"),\n            );\n            if (b.systemIntegrationOption(\"harfbuzz\", .{})) {\n                step.linkSystemLibrary2(\"harfbuzz\", dynamic_link_opts);\n            } else {\n                step.linkLibrary(harfbuzz_dep.artifact(\"harfbuzz\"));\n                try static_libs.append(\n                    harfbuzz_dep.artifact(\"harfbuzz\").getEmittedBin(),\n                );\n            }\n        }\n    }\n\n    // Fontconfig\n    _ = b.systemIntegrationOption(\"fontconfig\", .{}); // Shows it in help\n    if (self.config.font_backend.hasFontconfig()) {\n        if (b.lazyDependency(\"fontconfig\", .{\n            .target = target,\n            .optimize = optimize,\n        })) |fontconfig_dep| {\n            step.root_module.addImport(\n                \"fontconfig\",\n                fontconfig_dep.module(\"fontconfig\"),\n            );\n\n            if (b.systemIntegrationOption(\"fontconfig\", .{})) {\n                step.linkSystemLibrary2(\"fontconfig\", dynamic_link_opts);\n            } else {\n                step.linkLibrary(fontconfig_dep.artifact(\"fontconfig\"));\n                try static_libs.append(\n                    fontconfig_dep.artifact(\"fontconfig\").getEmittedBin(),\n                );\n            }\n        }\n    }\n\n    // Libpng - Ghostty doesn't actually use this directly, its only used\n    // through dependencies, so we only need to add it to our static\n    // libs list if we're not using system integration. The dependencies\n    // will handle linking it.\n    if (!b.systemIntegrationOption(\"libpng\", .{})) {\n        if (b.lazyDependency(\"libpng\", .{\n            .target = target,\n            .optimize = optimize,\n        })) |libpng_dep| {\n            step.linkLibrary(libpng_dep.artifact(\"png\"));\n            try static_libs.append(\n                libpng_dep.artifact(\"png\").getEmittedBin(),\n            );\n        }\n    }\n\n    // Zlib - same as libpng, only used through dependencies.\n    if (!b.systemIntegrationOption(\"zlib\", .{})) {\n        if (b.lazyDependency(\"zlib\", .{\n            .target = target,\n            .optimize = optimize,\n        })) |zlib_dep| {\n            step.linkLibrary(zlib_dep.artifact(\"z\"));\n            try static_libs.append(\n                zlib_dep.artifact(\"z\").getEmittedBin(),\n            );\n        }\n    }\n\n    // Oniguruma\n    if (b.lazyDependency(\"oniguruma\", .{\n        .target = target,\n        .optimize = optimize,\n    })) |oniguruma_dep| {\n        step.root_module.addImport(\n            \"oniguruma\",\n            oniguruma_dep.module(\"oniguruma\"),\n        );\n        if (b.systemIntegrationOption(\"oniguruma\", .{})) {\n            step.linkSystemLibrary2(\"oniguruma\", dynamic_link_opts);\n        } else {\n            step.linkLibrary(oniguruma_dep.artifact(\"oniguruma\"));\n            try static_libs.append(\n                oniguruma_dep.artifact(\"oniguruma\").getEmittedBin(),\n            );\n        }\n    }\n\n    // Glslang\n    if (b.lazyDependency(\"glslang\", .{\n        .target = target,\n        .optimize = optimize,\n    })) |glslang_dep| {\n        step.root_module.addImport(\"glslang\", glslang_dep.module(\"glslang\"));\n        if (b.systemIntegrationOption(\"glslang\", .{})) {\n            step.linkSystemLibrary2(\"glslang\", dynamic_link_opts);\n            step.linkSystemLibrary2(\n                \"glslang-default-resource-limits\",\n                dynamic_link_opts,\n            );\n        } else {\n            step.linkLibrary(glslang_dep.artifact(\"glslang\"));\n            try static_libs.append(\n                glslang_dep.artifact(\"glslang\").getEmittedBin(),\n            );\n        }\n    }\n\n    // Spirv-cross\n    if (b.lazyDependency(\"spirv_cross\", .{\n        .target = target,\n        .optimize = optimize,\n    })) |spirv_cross_dep| {\n        step.root_module.addImport(\n            \"spirv_cross\",\n            spirv_cross_dep.module(\"spirv_cross\"),\n        );\n        if (b.systemIntegrationOption(\"spirv-cross\", .{})) {\n            step.linkSystemLibrary2(\"spirv-cross\", dynamic_link_opts);\n        } else {\n            step.linkLibrary(spirv_cross_dep.artifact(\"spirv_cross\"));\n            try static_libs.append(\n                spirv_cross_dep.artifact(\"spirv_cross\").getEmittedBin(),\n            );\n        }\n    }\n\n    // Simdutf\n    if (b.systemIntegrationOption(\"simdutf\", .{})) {\n        step.linkSystemLibrary2(\"simdutf\", dynamic_link_opts);\n    } else {\n        if (b.lazyDependency(\"simdutf\", .{\n            .target = target,\n            .optimize = optimize,\n        })) |simdutf_dep| {\n            step.linkLibrary(simdutf_dep.artifact(\"simdutf\"));\n            try static_libs.append(\n                simdutf_dep.artifact(\"simdutf\").getEmittedBin(),\n            );\n        }\n    }\n\n    // Sentry\n    if (self.config.sentry) {\n        if (b.lazyDependency(\"sentry\", .{\n            .target = target,\n            .optimize = optimize,\n            .backend = .breakpad,\n        })) |sentry_dep| {\n            step.root_module.addImport(\n                \"sentry\",\n                sentry_dep.module(\"sentry\"),\n            );\n            step.linkLibrary(sentry_dep.artifact(\"sentry\"));\n            try static_libs.append(\n                sentry_dep.artifact(\"sentry\").getEmittedBin(),\n            );\n\n            // We also need to include breakpad in the static libs.\n            if (sentry_dep.builder.lazyDependency(\"breakpad\", .{\n                .target = target,\n                .optimize = optimize,\n            })) |breakpad_dep| {\n                try static_libs.append(\n                    breakpad_dep.artifact(\"breakpad\").getEmittedBin(),\n                );\n            }\n        }\n    }\n\n    // Wasm we do manually since it is such a different build.\n    if (step.rootModuleTarget().cpu.arch == .wasm32) {\n        if (b.lazyDependency(\"zig_js\", .{\n            .target = target,\n            .optimize = optimize,\n        })) |js_dep| {\n            step.root_module.addImport(\n                \"zig-js\",\n                js_dep.module(\"zig-js\"),\n            );\n        }\n\n        return static_libs;\n    }\n\n    // Other dependencies, mostly pure Zig\n    if (b.lazyDependency(\"opengl\", .{})) |dep| {\n        step.root_module.addImport(\"opengl\", dep.module(\"opengl\"));\n    }\n    if (b.lazyDependency(\"vaxis\", .{})) |dep| {\n        step.root_module.addImport(\"vaxis\", dep.module(\"vaxis\"));\n    }\n    if (b.lazyDependency(\"wuffs\", .{\n        .target = target,\n        .optimize = optimize,\n    })) |dep| {\n        step.root_module.addImport(\"wuffs\", dep.module(\"wuffs\"));\n    }\n    if (b.lazyDependency(\"libxev\", .{\n        .target = target,\n        .optimize = optimize,\n    })) |dep| {\n        step.root_module.addImport(\"xev\", dep.module(\"xev\"));\n    }\n    if (b.lazyDependency(\"z2d\", .{})) |dep| {\n        step.root_module.addImport(\"z2d\", b.addModule(\"z2d\", .{\n            .root_source_file = dep.path(\"src/z2d.zig\"),\n            .target = target,\n            .optimize = optimize,\n        }));\n    }\n    if (b.lazyDependency(\"ziglyph\", .{\n        .target = target,\n        .optimize = optimize,\n    })) |dep| {\n        step.root_module.addImport(\"ziglyph\", dep.module(\"ziglyph\"));\n    }\n    if (b.lazyDependency(\"zf\", .{\n        .target = target,\n        .optimize = optimize,\n        .with_tui = false,\n    })) |dep| {\n        step.root_module.addImport(\"zf\", dep.module(\"zf\"));\n    }\n\n    // Mac Stuff\n    if (step.rootModuleTarget().os.tag.isDarwin()) {\n        if (b.lazyDependency(\"zig_objc\", .{\n            .target = target,\n            .optimize = optimize,\n        })) |objc_dep| {\n            step.root_module.addImport(\n                \"objc\",\n                objc_dep.module(\"objc\"),\n            );\n        }\n\n        if (b.lazyDependency(\"macos\", .{\n            .target = target,\n            .optimize = optimize,\n        })) |macos_dep| {\n            step.root_module.addImport(\n                \"macos\",\n                macos_dep.module(\"macos\"),\n            );\n            step.linkLibrary(macos_dep.artifact(\"macos\"));\n            try static_libs.append(\n                macos_dep.artifact(\"macos\").getEmittedBin(),\n            );\n        }\n\n        if (self.config.renderer == .opengl) {\n            step.linkFramework(\"OpenGL\");\n        }\n\n        // Apple platforms do not include libc libintl so we bundle it.\n        // This is LGPL but since our source code is open source we are\n        // in compliance with the LGPL since end users can modify this\n        // build script to replace the bundled libintl with their own.\n        if (b.lazyDependency(\"libintl\", .{\n            .target = target,\n            .optimize = optimize,\n        })) |libintl_dep| {\n            step.linkLibrary(libintl_dep.artifact(\"intl\"));\n            try static_libs.append(\n                libintl_dep.artifact(\"intl\").getEmittedBin(),\n            );\n        }\n    }\n\n    // cimgui\n    if (b.lazyDependency(\"cimgui\", .{\n        .target = target,\n        .optimize = optimize,\n    })) |cimgui_dep| {\n        step.root_module.addImport(\"cimgui\", cimgui_dep.module(\"cimgui\"));\n        step.linkLibrary(cimgui_dep.artifact(\"cimgui\"));\n        try static_libs.append(cimgui_dep.artifact(\"cimgui\").getEmittedBin());\n    }\n\n    // Highway\n    if (b.lazyDependency(\"highway\", .{\n        .target = target,\n        .optimize = optimize,\n    })) |highway_dep| {\n        step.linkLibrary(highway_dep.artifact(\"highway\"));\n        try static_libs.append(highway_dep.artifact(\"highway\").getEmittedBin());\n    }\n\n    // utfcpp - This is used as a dependency on our hand-written C++ code\n    if (b.lazyDependency(\"utfcpp\", .{\n        .target = target,\n        .optimize = optimize,\n    })) |utfcpp_dep| {\n        step.linkLibrary(utfcpp_dep.artifact(\"utfcpp\"));\n        try static_libs.append(utfcpp_dep.artifact(\"utfcpp\").getEmittedBin());\n    }\n\n    // If we're building an exe then we have additional dependencies.\n    if (step.kind != .lib) {\n        // We always statically compile glad\n        const glad = b.dependency(\"glad\", .{});\n        step.addIncludePath(b.path(\"vendor/glad/include/\"));\n        step.addCSourceFile(.{\n            .file = b.path(\"vendor/glad/src/gl.c\"),\n            .flags = &.{},\n        });\n\n        // When we're targeting flatpak we ALWAYS link GTK so we\n        // get access to glib for dbus.\n        if (self.config.flatpak) step.linkSystemLibrary2(\"gtk4\", dynamic_link_opts);\n\n        switch (self.config.app_runtime) {\n            .none => {},\n\n            .glfw => {\n                const glfw_dep = b.dependency(\"glfw\", .{\n                    .target = target,\n                    .optimize = optimize,\n                });\n                step.root_module.addImport(\n                    \"glfw\",\n                    glfw_dep.module(\"glfw\"),\n                );\n            },\n\n            .gtk => try self.addGTK(step),\n        }\n    }\n\n    self.help_strings.addImport(step);\n    self.unicode_tables.addImport(step);\n    self.framedata.addImport(step);\n\n    return static_libs;\n}\n\n// Setup the dependencies for the GTK apprt build. The GTK apprt\n// is particularly involved compared to others so we pull this out\n// into a dedicated function.\nfn addGTK(\n    self: *const SharedDeps,\n    step: *std.Build.Step.Compile,\n) !void {\n    const b = step.step.owner;\n    const target = step.root_module.resolved_target.?;\n    const optimize = step.root_module.optimize.?;\n\n// GObject dependencies\n    const gobject_ = b.lazyDependency(\"gobject\", .{\n        .target = target,\n        .optimize = optimize,\n    });\n    if (gobject_) |gobject| {\n        const gobject_imports = .{\n            .{ \"adw\", \"adw1\" },\n            .{ \"gdk\", \"gdk4\" },\n            .{ \"gio\", \"gio2\" },\n            .{ \"glib\", \"glib2\" },\n            .{ \"gobject\", \"gobject2\" },\n            .{ \"gtk\", \"gtk4\" },\n        };\n        inline for (gobject_imports) |import| {\n            const name, const module = import;\n            step.root_module.addImport(name, gobject.module(module));\n        }\n    }\n\n    step.linkSystemLibrary2(\"gtk4\", dynamic_link_opts);\n    step.linkSystemLibrary2(\"libadwaita-1\", dynamic_link_opts);\n\n    if (self.config.x11) {\n        step.linkSystemLibrary2(\"X11\", dynamic_link_opts);\n        if (gobject_) |gobject| {\n            step.root_module.addImport(\n                \"gdk_x11\",\n                gobject.module(\"gdkx114\"),\n            );\n        }\n    }\n\n    if (self.config.wayland) wayland: {\n        // These need to be all be called to note that we need them.\n        const wayland_dep_ = b.lazyDependency(\"wayland\", .{});\n        const wayland_protocols_dep_ = b.lazyDependency(\n            \"wayland_protocols\",\n            .{},\n        );\n        const plasma_wayland_protocols_dep_ = b.lazyDependency(\n            \"plasma_wayland_protocols\",\n            .{},\n        );\n\n        // Unwrap or return, there are no more dependencies below.\n        const wayland_dep = wayland_dep_ orelse break :wayland;\n        const wayland_protocols_dep = wayland_protocols_dep_ orelse break :wayland;\n        const plasma_wayland_protocols_dep = plasma_wayland_protocols_dep_ orelse break :wayland;\n\n        // Note that zig_wayland cannot be lazy because lazy dependencies\n        // can't be imported since they don't exist and imports are\n        // resolved at compile time of the build.\n        const zig_wayland_dep = b.dependency(\"zig_wayland\", .{});\n        const Scanner = @import(\"zig_wayland\").Scanner;\n        const scanner = Scanner.create(zig_wayland_dep.builder, .{\n            .wayland_xml = wayland_dep.path(\"protocol/wayland.xml\"),\n            .wayland_protocols = wayland_protocols_dep.path(\"\"),\n        });\n\n        // FIXME: replace with `zxdg_decoration_v1` once GTK merges https://gitlab.gnome.org/GNOME/gtk/-/merge_requests/6398\n        scanner.addCustomProtocol(\n            plasma_wayland_protocols_dep.path(\"src/protocols/blur.xml\"),\n        );\n        scanner.addCustomProtocol(\n            plasma_wayland_protocols_dep.path(\"src/protocols/server-decoration.xml\"),\n        );\n        scanner.addCustomProtocol(\n            plasma_wayland_protocols_dep.path(\"src/protocols/slide.xml\"),\n        );\n\n        scanner.generate(\"wl_compositor\", 1);\n        scanner.generate(\"org_kde_kwin_blur_manager\", 1);\n        scanner.generate(\"org_kde_kwin_server_decoration_manager\", 1);\n        scanner.generate(\"org_kde_kwin_slide_manager\", 1);\n\n        step.root_module.addImport(\"wayland\", b.createModule(.{\n            .root_source_file = scanner.result,\n        }));\n        if (gobject_) |gobject| step.root_module.addImport(\n            \"gdk_wayland\",\n            gobject.module(\"gdkwayland4\"),\n        );\n\n        if (b.lazyDependency(\"gtk4_layer_shell\", .{\n            .target = target,\n            .optimize = optimize,\n        })) |gtk4_layer_shell| {\n            const layer_shell_module = gtk4_layer_shell.module(\"gtk4-layer-shell\");\n            if (gobject_) |gobject| layer_shell_module.addImport(\n                \"gtk\",\n                gobject.module(\"gtk4\"),\n            );\n            step.root_module.addImport(\n                \"gtk4-layer-shell\",\n                layer_shell_module,\n            );\n\n            // IMPORTANT: gtk4-layer-shell must be linked BEFORE\n            // wayland-client, as it relies on shimming libwayland's APIs.\n            if (b.systemIntegrationOption(\"gtk4-layer-shell\", .{})) {\n                step.linkSystemLibrary2(\n                    \"gtk4-layer-shell-0\",\n                    dynamic_link_opts,\n                );\n            } else {\n                // gtk4-layer-shell *must* be dynamically linked,\n                // so we don't add it as a static library\n                step.linkLibrary(gtk4_layer_shell.artifact(\"gtk4-layer-shell\"));\n            }\n        }\n\n        step.linkSystemLibrary2(\"wayland-client\", dynamic_link_opts);\n    }\n\n    // Get our gresource c/h files and add them to our build.\n    const dist = gtkDistResources(b);\n    step.addCSourceFile(.{ .file = dist.resources_c.path(b), .flags = &.{} });\n    step.addIncludePath(dist.resources_h.path(b).dirname());\n}\n\n/// Creates the resources that can be prebuilt for our dist build.\npub fn gtkDistResources(\n    b: *std.Build,\n) struct {\n    resources_c: DistResource,\n    resources_h: DistResource,\n} {\n    const gresource_xml = gresource_xml: {\n        const xml_exe = b.addExecutable(.{\n            .name = \"generate_gresource_xml\",\n            .root_source_file = b.path(\"src/apprt/gtk/gresource.zig\"),\n            .target = b.graph.host,\n        });\n\n        const xml_run = b.addRunArtifact(xml_exe);\n\n        const blueprint_exe = b.addExecutable(.{\n            .name = \"gtk_blueprint_compiler\",\n            .root_source_file = b.path(\"src/apprt/gtk/blueprint_compiler.zig\"),\n            .target = b.graph.host,\n        });\n\n        blueprint_exe.linkLibC();\n        blueprint_exe.linkSystemLibrary2(\"gtk4\", dynamic_link_opts);\n        blueprint_exe.linkSystemLibrary2(\"libadwaita-1\", dynamic_link_opts);\n\n        for (gresource.blueprint_files) |blueprint_file| {\n            const blueprint_run = b.addRunArtifact(blueprint_exe);\n            blueprint_run.addArgs(&.{\n                b.fmt(\"{d}\", .{blueprint_file.major}),\n                b.fmt(\"{d}\", .{blueprint_file.minor}),\n            });\n            const ui_file = blueprint_run.addOutputFileArg(b.fmt(\n                \"{d}.{d}/{s}.ui\",\n                .{\n                    blueprint_file.major,\n                    blueprint_file.minor,\n                    blueprint_file.name,\n                },\n            ));\n            blueprint_run.addFileArg(b.path(b.fmt(\n                \"src/apprt/gtk/ui/{d}.{d}/{s}.blp\",\n                .{\n                    blueprint_file.major,\n                    blueprint_file.minor,\n                    blueprint_file.name,\n                },\n            )));\n            xml_run.addFileArg(ui_file);\n        }\n\n        break :gresource_xml xml_run.captureStdOut();\n    };\n\n    const generate_c = b.addSystemCommand(&.{\n        \"glib-compile-resources\",\n        \"--c-name\",\n        \"ghostty\",\n        \"--generate-source\",\n        \"--target\",\n    });\n    const resources_c = generate_c.addOutputFileArg(\"ghostty_resources.c\");\n    generate_c.addFileArg(gresource_xml);\n\n    const generate_h = b.addSystemCommand(&.{\n        \"glib-compile-resources\",\n        \"--c-name\",\n        \"ghostty\",\n        \"--generate-header\",\n        \"--target\",\n    });\n    const resources_h = generate_h.addOutputFileArg(\"ghostty_resources.h\");\n    generate_h.addFileArg(gresource_xml);\n\n    return .{\n        .resources_c = .{\n            .dist = \"src/apprt/gtk/ghostty_resources.c\",\n            .generated = resources_c,\n        },\n        .resources_h = .{\n            .dist = \"src/apprt/gtk/ghostty_resources.h\",\n            .generated = resources_h,\n        },\n    };\n}\n\n// For dynamic linking, we prefer dynamic linking and to search by\n// mode first. Mode first will search all paths for a dynamic library\n// before falling back to static.\nconst dynamic_link_opts: std.Build.Module.LinkSystemLibraryOptions = .{\n    .preferred_link_mode = .dynamic,\n    .search_strategy = .mode_first,\n};",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9125099601593626,
      "diff_lines": 237,
      "added_lines": 20,
      "removed_lines": 117,
      "diff_preview": "--- expected+++ predicted@@ -1,6 +1,7 @@ const SharedDeps = @This();\n \n const std = @import(\"std\");\n+const Scanner = @import(\"zig_wayland\").Scanner;\n const Config = @import(\"Config.zig\");\n const HelpStrings = @import(\"HelpStrings.zig\");\n const MetallibStep = @import(\"MetallibStep.zig\");\n@@ -97,15 +98,7 @@     const target = step.root_module.resolved_target.?;\n     const optimize = step.root_module.optimize.?;\n \n-    // We maintain a list of our static libraries and return it so that\n-    // we can build a single fat static library for the final app.\n-    var static_libs = LazyPathList.init(b.allocator);\n-    errdefer static_libs.deinit();\n-\n-    // Every exe gets build options populated\n-    step.root_module.addOptions(\"build_options\", self.options);\n-\n-    // Freetype\n+// Freetype\n     _ = b.systemIntegrationOption(\"freetype\", .{}); // Shows it in help\n     if (self.config.font_backend.hasFreetype()) {\n         if (b.lazyDependency(\"freetype\", .{\n@@ -130,7 +123,7 @@         }\n     }\n \n-    // Harfbuzz\n+// Harfbuzz\n     _ = b.systemIntegrationOption(\"harfbuzz\", .{}); // Shows it in help\n     if (self.config.font_backend.hasHarfbuzz()) {\n         if (b.lazyDependency(\"harfbuzz\", .{\n@@ -322,67 +315,6 @@         return static_libs;\n     }\n \n-    // On Linux, we need to add a couple common library paths that aren't\n-    // on the standard search list. i.e. GTK is often in /usr/lib/x86_64-linux-gnu\n-    // on x86_64.\n-    if (step.rootModuleTarget().os.tag == .linux) {\n-        const triple = try step.rootModuleTarget().linuxTriple(b.allocator);\n-        const path = b.fmt(\"/usr/lib/{s}\", .{triple});\n-        if (std.fs.accessAbsolute(path, .{})) {\n-            step.addLibraryPath(.{ .cwd_relative = path });\n-        } else |_| {}\n-    }\n"
    },
    "llm_calls": 1,
    "elapsed_time": 138.73179602622986,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 26602,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_build_bash_completions.zig",
    "exact_match": false,
    "similarity": 0.6187601236931233,
    "diff_lines": 184,
    "predicted_output": "const std = @import(\"std\");\n\nconst Config = @import(\"../config/Config.zig\");\nconst Action = @import(\"../cli/action.zig\").Action;\n\n/// A bash completions configuration that contains all the available commands\n/// and options.\n///\n/// Notes: bash completion support for --<key>=<value> depends on setting the completion\n/// system to _not_ print a space following each successful completion (see -o nospace).\n/// This results leading or tailing spaces being necessary to move onto the next match.\n///\n/// bash completion will read = as it's own completiong word regardless of whether or not\n/// it's part of an on going completion like --<key>=. Working around this requires looking\n/// backward in the command line args to pretend the = is an empty string\n/// see: https://www.gnu.org/software/gnuastro/manual/html_node/Bash-TAB-completion-tutorial.html\npub const completions = comptimeGenerateBashCompletions();\n\nfn comptimeGenerateBashCompletions() []const u8 {\n    comptime {\n        @setEvalBranchQuota(50000);\n        var counter = std.io.countingWriter(std.io.null_writer);\n        try writeBashCompletions(&counter.writer());\n\n        var buf: [counter.bytes_written]u8 = undefined;\n        var stream = std.io.fixedBufferStream(&buf);\n        try writeBashCompletions(stream.writer());\n        const final = buf;\n        return final[0..stream.getWritten().len];\n    }\n}\n\nfn writeBashCompletions(writer: anytype) !void {\n    const pad1 = \"  \";\n    const pad2 = pad1 ++ pad1;\n    const pad3 = pad2 ++ pad1;\n    const pad4 = pad3 ++ pad1;\n    const pad5 = pad4 ++ pad1;\n\n    try writer.writeAll(\n        \\\\\n        \\\\_ghostty() {\n        \\\\\n        \\\\  # -o nospace requires we add back a space when a completion is finished\n        \\\\  # and not part of a --key= completion\n        \\\\  _add_spaces() {\n        \\\\    for idx in \"${!COMPREPLY[@]}\"; do\n        \\\\      [ -n \"${COMPREPLY[idx]}\" ] && COMPREPLY[idx]=\"${COMPREPLY[idx]} \";\n        \\\\    done\n        \\\\  }\n        \\\\\n        \\\\  _fonts() {\n        \\\\    local IFS=$'\\n'\n        \\\\    mapfile -t COMPREPLY < <( compgen -P '\"' -S '\"' -W \"$($ghostty +list-fonts | grep '^[A-Z]' )\" -- \"$cur\")\n        \\\\  }\n        \\\\\n        \\\\  _themes() {\n        \\\\    local IFS=$'\\n'\n        \\\\    mapfile -t COMPREPLY < <( compgen -P '\"' -S '\"' -W \"$($ghostty +list-themes | sed -E 's/^(.*) \\(.*$/\\1/')\" -- \"$cur\")\n        \\\\  }\n        \\\\\n        \\\\  _files() {\n        \\\\    mapfile -t COMPREPLY < <( compgen -o filenames -f -- \"$cur\" )\n        \\\\    for i in \"${!COMPREPLY[@]}\"; do\n        \\\\      if [[ -d \"${COMPREPLY[i]}\" ]]; then\n        \\\\        COMPREPLY[i]=\"${COMPREPLY[i]}/\";\n        \\\\      fi\n        \\\\      if [[ -f \"${COMPREPLY[i]}\" ]]; then\n        \\\\        COMPREPLY[i]=\"${COMPREPLY[i]} \";\n        \\\\      fi\n        \\\\    done\n        \\\\  }\n        \\\\\n        \\\\  _dirs() {\n        \\\\    mapfile -t COMPREPLY < <( compgen -o dirnames -d -- \"$cur\" )\n        \\\\    for i in \"${!COMPREPLY[@]}\"; do\n        \\\\      if [[ -d \"${COMPREPLY[i]}\" ]]; then\n        \\\\        COMPREPLY[i]=\"${COMPREPLY[i]}/\";\n        \\\\      fi\n        \\\\    done\n        \\\\    if [[ \"${#COMPREPLY[@]}\" == 0 && -d \"$cur\" ]]; then\n        \\\\      COMPREPLY=( \"$cur \" )\n        \\\\    fi\n        \\\\  }\n        \\\\\n        \\\\  _handle_config() {\n        \\\\    local config=\"--help\"\n        \\\\    config+=\" --version\"\n        \\\\  }\n    );\n\n    for (@typeInfo(Config).@\"struct\".fields) |field| {\n        if (field.name[0] == '_') continue;\n        switch (field.type) {\n            bool, ?bool => try writer.writeAll(pad2 ++ \"config+=\\\" '--\" ++ field.name ++ \" '\\\"\\n\"),\n            else => try writer.writeAll(pad2 ++ \"config+=\\\" --\" ++ field.name ++ \"=\\\"\\n\"),\n        }\n    }\n\n    try writer.writeAll(\n        \\\\\n        \\\\    case \"$prev\" in\n        \\\\\n    );\n\n    for (@typeInfo(Config).@\"struct\".fields) |field| {\n        if (field.name[0] == '_') continue;\n        try writer.writeAll(pad3 ++ \"--\" ++ field.name ++ \") \");\n\n        if (std.mem.startsWith(u8, field.name, \"font-family\"))\n            try writer.writeAll(\"_fonts ;;\")\n        else if (std.mem.eql(u8, \"theme\", field.name))\n            try writer.writeAll(\"_themes ;;\")\n        else if (std.mem.eql(u8, \"working-directory\", field.name))\n            try writer.writeAll(\"_dirs ;;\")\n        else if (field.type == Config.RepeatablePath)\n            try writer.writeAll(\"_files ;;\")\n        else {\n            const compgenPrefix = \"mapfile -t COMPREPLY < <( compgen -W \\\"\";\n            const compgenSuffix = \"\\\" -- \\\"$cur\\\" ); _add_spaces ;;\";\n            switch (@typeInfo(field.type)) {\n                .bool => try writer.writeAll(\"return ;;\"),\n                .@\"enum\" => |info| {\n                    try writer.writeAll(compgenPrefix);\n                    for (info.fields, 0..) |f, i| {\n                        if (i > 0) try writer.writeAll(\" \");\n                        try writer.writeAll(f.name);\n                    }\n                    try writer.writeAll(compgenSuffix);\n                },\n                .@\"struct\" => |info| {\n                    if (!@hasDecl(field.type, \"parseCLI\") and info.layout == .@\"packed\") {\n                        try writer.writeAll(compgenPrefix);\n                        for (info.fields, 0..) |f, i| {\n                            if (i > 0) try writer.writeAll(\" \");\n                            try writer.writeAll(f.name ++ \" no-\" ++ f.name);\n                        }\n                        try writer.writeAll(compgenSuffix);\n                    } else {\n                        try writer.writeAll(\"return ;;\");\n                    }\n                },\n                else => try writer.writeAll(\"return ;;\"),\n            }\n        }\n\n        try writer.writeAll(\"\\n\");\n    }\n\n    try writer.writeAll(\n        \\\\      *) mapfile -t COMPREPLY < <( compgen -W \"$config\" -- \"$cur\" ) ;;\n        \\\\    esac\n        \\\\\n        \\\\    return 0\n        \\\\  }\n        \\\\\n        \\\\  # begin main logic\n        \\\\  local topLevel=\"-e\"\n        \\\\  topLevel+=\" --help\"\n        \\\\  topLevel+=\" --version\"\n    );\n\n    for (@typeInfo(Action).@\"enum\".fields) |field| {\n        if (std.mem.eql(u8, \"help\", field.name)) continue;\n        if (std.mem.eql(u8, \"version\", field.name)) continue;\n\n        try writer.writeAll(pad1 ++ \"topLevel+=\\\" +\" ++ field.name ++ \"\\\"\\n\");\n    }\n\n    try writer.writeAll(\n        \\\\\n        \\\\  local cur=\"\"; local prev=\"\"; local prevWasEq=false; COMPREPLY=()\n        \\\\  local ghostty=\"$1\"\n        \\\\\n        \\\\  # script assumes default COMP_WORDBREAKS of roughly $' \\t\\n\"\\'><=;|&(:'\n        \\\\  # if = is missing this script will degrade to matching on keys only.\n        \\\\  # eg: --key=\n        \\\\  # this can be improved if needed see: https://github.com/ghostty-org/ghostty/discussions/2994\n        \\\\\n        \\\\  if [ \"$2\" = \"=\" ]; then cur=\"\"\n        \\\\  else                    cur=\"$2\"\n        \\\\  fi\n        \\\\\n        \\\\  case \"$COMP_CWORD\" in\n        \\\\    1)\n        \\\\      case \"${COMP_WORDS[1]}\" in\n        \\\\        -e | --help | --version) return 0 ;;\n        \\\\        --*) _handle_config ;;\n        \\\\        *) mapfile -t COMPREPLY < <( compgen -W \"${topLevel}\" -- \"$cur\" ); _add_spaces ;;\n        \\\\      esac\n        \\\\      ;;\n        \\\\    *)\n        \\\\      case \"$prev\" in\n        \\\\        -e | --help | --version) return 0 ;;\n        \\\\        *)\n        \\\\          case \"${COMP_WORDS[1]}\" in\n        \\\\            --*) _handle_config ;;\n        \\\\            +*) _handle_actions ;;\n        \\\\          esac\n        \\\\          ;;\n        \\\\      esac\n        \\\\      ;;\n        \\\\  esac\n    );\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.6187601236931233,
      "diff_lines": 184,
      "added_lines": 2,
      "removed_lines": 135,
      "diff_preview": "--- expected+++ predicted@@ -38,6 +38,7 @@     const pad5 = pad4 ++ pad1;\n \n     try writer.writeAll(\n+        \\\\\n         \\\\_ghostty() {\n         \\\\\n         \\\\  # -o nospace requires we add back a space when a completion is finished\n@@ -85,7 +86,7 @@         \\\\  _handle_config() {\n         \\\\    local config=\"--help\"\n         \\\\    config+=\" --version\"\n-        \\\\\n+        \\\\  }\n     );\n \n     for (@typeInfo(Config).@\"struct\".fields) |field| {\n@@ -153,122 +154,10 @@         \\\\    return 0\n         \\\\  }\n         \\\\\n-        \\\\  _handle_actions() {\n-        \\\\\n-    );\n-\n-    for (@typeInfo(Action).@\"enum\".fields) |field| {\n-        if (std.mem.eql(u8, \"help\", field.name)) continue;\n-        if (std.mem.eql(u8, \"version\", field.name)) continue;\n-\n-        const options = @field(Action, field.name).options();\n-        // assumes options will never be created with only <_name> members\n-        if (@typeInfo(options).@\"struct\".fields.len == 0) continue;\n-\n-        var buffer: [field.name.len]u8 = undefined;\n-        const bashName: []u8 = buffer[0..field.name.len];\n-        @memcpy(bashName, field.name);\n-\n-        std.mem.replaceScalar(u8, bashName, '-', '_');\n-        try writer.writeAll(pad2 ++ \"local \" ++ bashName ++ \"=\\\"\");\n-\n-        {\n-            var count = 0;\n-            for (@typeInfo(options).@\"struct\".fields) |opt| {\n-                if (opt.name[0] == '_') continue;\n-                if (count > 0) try writer.writeAll(\" \");\n-                switch (opt.type) {\n-                    bool, ?bool => try writer.writeAll(\"'--\" ++ opt.name ++ \" '\"),\n-                    else => try writer.writeAll(\"--\" ++ opt.name ++ \"=\"),\n"
    },
    "llm_calls": 1,
    "elapsed_time": 48.8596568107605,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 16286,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_build_gtk.zig",
    "exact_match": true,
    "similarity": 1.0,
    "diff_lines": 0,
    "predicted_output": "const std = @import(\"std\");\n\npub const Targets = packed struct {\n    x11: bool = false,\n    wayland: bool = false,\n};\n\n/// Returns the targets that GTK4 was compiled with.\npub fn targets(b: *std.Build) Targets {\n    // Run pkg-config. We allow it to fail so that zig build --help\n    // works without all dependencies. The build will fail later when\n    // GTK isn't found anyways.\n    var code: u8 = undefined;\n    const output = b.runAllowFail(\n        &.{ \"pkg-config\", \"--variable=targets\", \"gtk4\" },\n        &code,\n        .Ignore,\n    ) catch return .{};\n\n    const x11 = std.mem.indexOf(u8, output, \"x11\") != null;\n    const wayland = std.mem.indexOf(u8, output, \"wayland\") != null;\n\n    return .{\n        .x11 = x11,\n        .wayland = wayland,\n    };\n}",
    "evaluation": {
      "exact_match": true,
      "similarity": 1.0,
      "diff_lines": 0,
      "added_lines": 0,
      "removed_lines": 0,
      "diff_preview": null
    },
    "llm_calls": 1,
    "elapsed_time": 5.390739917755127,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 2123,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_cli_args.zig",
    "exact_match": false,
    "similarity": 0.3505380327703215,
    "diff_lines": 1273,
    "predicted_output": "const std = @import(\"std\");\nconst mem = std.mem;\nconst assert = std.debug.assert;\nconst Allocator = mem.Allocator;\nconst ArenaAllocator = std.heap.ArenaAllocator;\nconst diags = @import(\"diagnostics.zig\");\nconst Diagnostic = diags.Diagnostic;\nconst DiagnosticList = diags.DiagnosticList;\n\n// TODO:\n//   - Only `--long=value` format is accepted. Do we want to allow\n//     `--long value`? Not currently allowed.\n\n// For trimming\npub const whitespace = \" \\t\";\n\n/// The base errors for arg parsing. Additional errors can be returned due\n/// to type-specific parsing but these are always possible.\npub const Error = error{\n    ValueRequired,\n    InvalidField,\n    InvalidValue,\n};\n\n/// Parse the command line arguments from iter into dst.\n///\n/// dst must be a struct. The fields and their types will be used to determine\n/// the valid CLI flags. See the tests in this file as an example. For field\n/// types that are structs, the struct can implement the `parseCLI` function\n/// to do custom parsing.\n///\n/// If the destination type has a field \"_diagnostics\", it must be of type\n/// \"DiagnosticList\" and any diagnostic messages will be added to that list.\n/// When diagnostics are present, only allocation errors will be returned.\n///\n/// If the destination type has a decl \"renamed\", it must be of type\n/// std.StaticStringMap([]const u8) and contains a mapping from the old\n/// field name to the new field name. This is used to allow renaming fields\n/// while still supporting the old name. If a renamed field is set, parsing\n/// will automatically set the new field name.\n///\n/// Note: If the arena is already non-null, then it will be used. In this\n/// case, in the case of an error some memory might be leaked into the arena.\npub fn parse(\n    comptime T: type,\n    alloc: Allocator,\n    dst: *T,\n    iter: anytype,\n) !void {\n    const info = @typeInfo(T);\n    assert(info == .@\"struct\");\n\n    comptime {\n        // Verify all renamed fields are valid (source does not exist,\n        // destination does exist).\n        if (@hasDecl(T, \"renamed\")) {\n            for (T.renamed.keys(), T.renamed.values()) |key, value| {\n                if (@hasField(T, key)) {\n                    @compileLog(key);\n                    @compileError(\"renamed field source exists\");\n                }\n\n                if (!@hasField(T, value)) {\n                    @compileLog(value);\n                    @compileError(\"renamed field destination does not exist\");\n                }\n            }\n        }\n    }\n\n    // Make an arena for all our allocations if we support it. Otherwise,\n    // use an allocator that always fails. If the arena is already set on\n    // the config, then we reuse that. See memory note in parse docs.\n    const arena_available = @hasField(T, \"_arena\");\n    var arena_owned: bool = false;\n    const arena_alloc = if (arena_available) arena: {\n        // If the arena is unset, we create it. We mark that we own it\n        // only so that we can clean it up on error.\n        if (dst._arena == null) {\n            dst._arena = ArenaAllocator.init(alloc);\n            arena_owned = true;\n        }\n\n        break :arena dst._arena.?.allocator();\n    } else fail: {\n        // Note: this is... not safe...\n        var fail = std.testing.FailingAllocator.init(alloc, .{});\n        break :fail fail.allocator();\n    };\n    errdefer if (arena_available and arena_owned) {\n        dst._arena.?.deinit();\n        dst._arena = null;\n    };\n\n    while (iter.next()) |arg| {\n        // If this doesn't start with \"--\" then it isn't a config\n        // flag. We don't support positional arguments or configuration\n        // values set with spaces so this is an error.\n        if (!mem.startsWith(u8, arg, \"--\")) {\n            if (comptime !canTrackDiags(T)) return Error.InvalidField;\n\n            // Add our diagnostic\n            try dst._diagnostics.append(arena_alloc, .{\n                .key = try arena_alloc.dupeZ(u8, arg),\n                .message = \"invalid field\",\n                .location = try diags.Location.fromIter(iter, arena_alloc),\n            });\n\n            continue;\n        }\n\n        var key: []const u8 = arg[2..];\n        const value: ?[]const u8 = value: {\n            // If the arg has \"=\" then the value is after the \"=\".\n            if (mem.indexOf(u8, key, \"=\")) |idx| {\n                defer key = key[0..idx];\n                break :value key[idx + 1 ..];\n            }\n\n            break :value null;\n        };\n\n        parseIntoField(T, arena_alloc, dst, key, value) catch |err| {\n            if (comptime !canTrackDiags(T)) return err;\n\n            // The error set is dependent on comptime T, so we always add\n            // an extra error so we can have the \"else\" below.\n            const ErrSet = @TypeOf(err) || error{ Unknown, OutOfMemory };\n            const message: [:0]const u8 = switch (@as(ErrSet, @errorCast(err))) {\n                // OOM is not recoverable since we need to allocate to\n                // track more error messages.\n                error.OutOfMemory => return err,\n                error.InvalidField => \"unknown field\",\n                error.ValueRequired => formatValueRequired(T, arena_alloc, key) catch \"value required\",\n                error.InvalidValue => formatInvalidValue(T, arena_alloc, key, value) catch \"invalid value\",\n                else => try std.fmt.allocPrintZ(\n                    arena_alloc,\n                    \"unknown error {}\",\n                    .{err},\n                ),\n            };\n\n            // Add our diagnostic\n            try dst._diagnostics.append(arena_alloc, .{\n                .key = try arena_alloc.dupeZ(u8, key),\n                .message = message,\n                .location = try diags.Location.fromIter(iter, arena_alloc),\n            });\n        };\n    }\n}\n\n/// Returns true if this type can track diagnostics.\nfn canTrackDiags(comptime T: type) bool {\n    return @hasField(T, \"_diagnostics\");\n}\n\n/// Parse a single key/value pair into the destination type T.\n///\n/// This may result in allocations. The allocations can only be freed by freeing\n/// all the memory associated with alloc. It is expected that alloc points to\n/// an arena.\npub fn parseIntoField(\n    comptime T: type,\n    alloc: Allocator,\n    dst: *T,\n    key: []const u8,\n    value: ?[]const u8,\n) !void {\n    const info = @typeInfo(T);\n    assert(info == .@\"struct\");\n\n    inline for (info.@\"struct\".fields) |field| {\n        if (field.name[0] != '_' and mem.eql(u8, field.name, key)) {\n            // For optional fields, we just treat it as the child type.\n            // This lets optional fields default to null but get set by\n            // the CLI.\n            const Field = switch (@typeInfo(field.type)) {\n                .optional => |opt| opt.child,\n                else => field.type,\n            };\n            const fieldInfo = @typeInfo(Field);\n            const canHaveDecls = fieldInfo == .@\"struct\" or\n                fieldInfo == .@\"union\" or\n                fieldInfo == .@\"enum\";\n\n            // If the value is empty string (set but empty string),\n            // then we reset the value to the default.\n            if (value) |v| default: {\n                if (v.len != 0) break :default;\n                // Set default value if possible.\n                if (canHaveDecls and @hasDecl(Field, \"init\")) {\n                    try @field(dst, field.name).init(alloc);\n                    return;\n                }\n                const raw = field.default_value_ptr orelse break :default;\n                const ptr: *const field.type = @alignCast(@ptrCast(raw));\n                @field(dst, field.name) = ptr.*;\n                return;\n            }\n\n            // If we are a type that can have decls and have a parseCLI decl,\n            // we call that and use that to set the value.\n            if (canHaveDecls) {\n                if (@hasDecl(Field, \"parseCLI\")) {\n                    const fnInfo = @typeInfo(@TypeOf(Field.parseCLI)).@\"fn\";\n                    switch (fnInfo.params.len) {\n                        // 1 arg = (input) => output\n                        1 => @field(dst, field.name) = try Field.parseCLI(value),\n\n                        // 2 arg = (self, input) => void\n                        2 => switch (@typeInfo(field.type)) {\n                            .@\"struct\",\n                            .@\"union\",\n                            .@\"enum\",\n                            => try @field(dst, field.name).parseCLI(value),\n\n                            // If the field is optional and set, then we use\n                            // the pointer value directly into it. If its not\n                            // set we need to create a new instance.\n                            .optional => if (@field(dst, field.name)) |*v| {\n                                try v.parseCLI(value);\n                            } else {\n                                // Note: you cannot do @field(dst, name) = undefined\n                                // because this causes the value to be \"null\"\n                                // in ReleaseFast modes.\n                                var tmp: Field = undefined;\n                                try tmp.parseCLI(value);\n                                @field(dst, field.name) = tmp;\n                            },\n                        },\n\n                        // 3 arg = (self, alloc, input) => void\n                        3 => switch (@typeInfo(field.type)) {\n                            .@\"struct\",\n                            .@\"union\",\n                            .@\"enum\",\n                            => try @field(dst, field.name).parseCLI(alloc, value),\n\n                            .optional => if (@field(dst, field.name)) |*v| {\n                                try v.parseCLI(alloc, value);\n                            } else {\n                                var tmp: Field = undefined;\n                                try tmp.parseCLI(alloc, value);\n                                @field(dst, field.name) = tmp;\n                            },\n\n                            else => @compileError(\"unexpected field type\"),\n                        },\n                        else => @compileError(\"parseCLI invalid argument count\"),\n                    }\n                }\n            }\n\n            switch (fieldInfo) {\n                .@\"enum\" => std.meta.stringToEnum(\n                    Field,\n                    value orelse return error.ValueRequired,\n                ) orelse return error.InvalidValue,\n\n                .@\"struct\" => try parseStruct(\n                    Field,\n                    alloc,\n                    value orelse return error.ValueRequired,\n                ),\n\n                .@\"union\" => try parseTaggedUnion(\n                    Field,\n                    alloc,\n                    value orelse return error.ValueRequired,\n                ),\n\n                else => unreachable,\n            };\n        }\n    }\n\n    return error.InvalidField;\n}\n\n// Other relevant functions and tests...",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.3505380327703215,
      "diff_lines": 1273,
      "added_lines": 21,
      "removed_lines": 1198,
      "diff_preview": "--- expected+++ predicted@@ -4,11 +4,8 @@ const Allocator = mem.Allocator;\n const ArenaAllocator = std.heap.ArenaAllocator;\n const diags = @import(\"diagnostics.zig\");\n-const internal_os = @import(\"../os/main.zig\");\n const Diagnostic = diags.Diagnostic;\n const DiagnosticList = diags.DiagnosticList;\n-\n-const log = std.log.scoped(.cli);\n \n // TODO:\n //   - Only `--long=value` format is accepted. Do we want to allow\n@@ -31,10 +28,6 @@ /// the valid CLI flags. See the tests in this file as an example. For field\n /// types that are structs, the struct can implement the `parseCLI` function\n /// to do custom parsing.\n-///\n-/// If the destination type has a field \"_arena\" of type `?ArenaAllocator`,\n-/// an arena allocator will be created (or reused if set already) for any\n-/// allocations. Allocations are necessary for certain types, like `[]const u8`.\n ///\n /// If the destination type has a field \"_diagnostics\", it must be of type\n /// \"DiagnosticList\" and any diagnostic messages will be added to that list.\n@@ -100,26 +93,6 @@     };\n \n     while (iter.next()) |arg| {\n-        // Do manual parsing if we have a hook for it.\n-        if (@hasDecl(T, \"parseManuallyHook\")) {\n-            if (!try dst.parseManuallyHook(\n-                arena_alloc,\n-                arg,\n-                iter,\n-            )) return;\n-        }\n-\n-        // If the destination supports help then we check for it, call\n-        // the help function and return.\n-        if (@hasDecl(T, \"help\")) {\n-            if (mem.eql(u8, arg, \"--help\") or\n-                mem.eql(u8, arg, \"-h\"))\n-            {\n-                try dst.help();\n-                return;\n-            }\n-        }\n-\n         // If this doesn't start with \"--\" then it isn't a config\n"
    },
    "llm_calls": 1,
    "elapsed_time": 78.52138018608093,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 33658,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_config.zig",
    "exact_match": false,
    "similarity": 0.005053057099545225,
    "diff_lines": 345,
    "predicted_output": "const std = @import(\"std\");\n\npub const Config = struct {\n    /// The font families to use.\n    @\"font-family\": ?[:0]const u8 = null,\n    @\"font-family-bold\": ?[:0]const u8 = null,\n    @\"font-family-italic\": ?[:0]const u8 = null,\n    @\"font-family-bold-italic\": ?[:0]const u8 = null,\n\n    /// Apply a font feature. This can be repeated multiple times to enable\n    /// multiple font features. You can NOT set multiple font features with\n    /// a single value (yet).\n    ///\n    /// The font feature will apply to all fonts rendered by Ghostty. A\n    /// future enhancement will allow targeting specific faces.\n    ///\n    /// A valid value is the name of a feature. Prefix the feature with a\n    /// \"-\" to explicitly disable it. Example: \"ss20\" or \"-ss20\".\n    @\"font-feature\": RepeatableString = .{},\n\n    /// Font size in points\n    @\"font-size\": u8 = 12,\n\n    /// A repeatable configuration to set one or more font variations values\n    /// for a variable font. A variable font is a single font, usually\n    /// with a filename ending in \"-VF.ttf\" or \"-VF.otf\" that contains\n    /// one or more configurable axes for things such as weight, slant,\n    /// etc. Not all fonts support variations; only fonts that explicitly\n    /// state they are variable fonts will work.\n    ///\n    /// The format of this is \"id=value\" where \"id\" is the axis identifier.\n    /// An axis identifier is always a 4 character string, such as \"wght\".\n    /// To get the list of supported axes, look at your font documentation\n    /// or use a font inspection tool.\n    ///\n    /// Common axes are: \"wght\" (weight), \"slnt\" (slant), \"ital\" (italic),\n    /// \"opsz\" (optical size), \"wdth\" (width), \"GRAD\" (gradient), etc.\n    @\"font-variation\": RepeatableFontVariation = .{},\n\n    /// Draw fonts with a thicker stroke, if supported. This is only supported\n    /// currently on macOS.\n    @\"font-thicken\": bool = false,\n\n    /// Background color for the window.\n    background: Color = .{ .r = 0x28, .g = 0x2C, .b = 0x34 },\n\n    /// Foreground color for the window.\n    foreground: Color = .{ .r = 0xFF, .g = 0xFF, .b = 0xFF },\n\n    /// The foreground and background color for selection. If this is not\n    /// set, then the selection color is just the inverted window background\n    /// and foreground (note: not to be confused with the cell bg/fg).\n    @\"selection-foreground\": ?Color = null,\n    @\"selection-background\": ?Color = null,\n\n    /// Color palette for the 256 color form that many terminal applications\n    /// use. The syntax of this configuration is \"N=HEXCODE\" where \"n\"\n    /// is 0 to 255 (for the 256 colors) and HEXCODE is a typical RGB\n    /// color code such as \"#AABBCC\". The 0 to 255 correspond to the\n    /// terminal color table.\n    ///\n    /// For definitions on all the codes:\n    /// https://www.ditig.com/256-colors-cheat-sheet\n    palette: Palette = .{},\n\n    /// The color of the cursor. If this is not set, a default will be chosen.\n    @\"cursor-color\": ?Color = null,\n\n    /// The style of the cursor. This sets the default style. A running\n    /// program can still request an explicit cursor style using escape\n    /// sequences (such as CSI q). Shell configurations will often request\n    /// specific cursor styles.\n    ///\n    /// Caveat: Shell integration currently defaults to always be a bar\n    /// In order to fix it, we probably would want to add something similar to Kitty's\n    /// shell integration options (no-cursor). For more information see:\n    /// https://sw.kovidgoyal.net/kitty/conf/#opt-kitty.shell_integration\n    @\"cursor-style\": terminal.CursorStyle = .bar,\n\n    /// Whether the cursor shall blink\n    @\"cursor-style-blink\": bool = true,\n\n    /// The color of the text under the cursor. If this is not set, a default\n    /// will be chosen.\n    @\"cursor-text\": ?Color = null,\n\n    /// The opacity level (opposite of transparency) of the background.\n    /// A value of 1 is fully opaque and a value of 0 is fully transparent.\n    /// A value less than 0 or greater than 1 will be clamped to the nearest\n    /// valid value.\n    ///\n    /// Changing this value at runtime (and reloading config) will only\n    /// affect new windows, tabs, and splits.\n    @\"background-opacity\": f64 = 1.0,\n\n    /// A positive value enables blurring of the background when\n    /// background-opacity is less than 1. The value is the blur radius to\n    /// apply. A value of 20 is reasonable for a good looking blur.\n    /// Higher values will cause strange rendering issues as well as\n    /// performance issues.\n    ///\n    /// This is only supported on macOS.\n    @\"background-blur-radius\": u8 = 0,\n\n    /// The command to run, usually a shell. If this is not an absolute path,\n    /// it'll be looked up in the PATH. If this is not set, a default will\n    /// be looked up from your system. The rules for the default lookup are:\n    ///\n    ///   - SHELL environment variable\n    ///   - passwd entry (user information)\n    ///\n    command: ?[]const u8 = null,\n\n    /// The directory to change to after starting the command.\n    ///\n    /// The default is \"inherit\" except in special scenarios listed next.\n    /// If ghostty can detect it is launched on macOS from launchd\n    /// (double-clicked), then it defaults to \"home\".\n    ///\n    /// The value of this must be an absolute value or one of the special\n    /// values below:\n    ///\n    ///   - \"home\" - The home directory of the executing user.\n    ///   - \"inherit\" - The working directory of the launching process.\n    ///\n    @\"working-directory\": ?[]const u8 = null,\n\n    /// Key bindings. The format is \"trigger=action\". Duplicate triggers\n    /// will overwrite previously set values.\n    ///\n    /// Trigger: \"+\"-separated list of keys and modifiers. Example:\n    /// \"ctrl+a\", \"ctrl+shift+b\", \"up\". Some notes:\n    ///\n    ///   - modifiers cannot repeat, \"ctrl+ctrl+a\" is invalid.\n    ///   - modifiers and key can be in any order, \"shift+a+ctrl\" is weird,\n    ///     but valid.\n    ///   - only a single key input is allowed, \"ctrl+a+b\" is invalid.\n    ///\n    /// Action is the action to take when the trigger is satisfied. It takes\n    /// the format \"action\" or \"action:param\". The latter form is only valid\n    /// if the action requires a parameter.\n    ///\n    ///   - \"ignore\" - Do nothing, ignore the key input. This can be used to\n    ///     black hole certain inputs to have no effect.\n    ///   - \"unbind\" - Remove the binding. This makes it so the previous action\n    ///     is removed, and the key will be sent through to the child command\n    ///     if it is printable.\n    ///   - \"csi:text\" - Send a CSI sequence. i.e. \"csi:A\" sends \"cursor up\".\n    ///\n    /// Some notes for the action:\n    ///\n    ///   - The parameter is taken as-is after the \":\". Double quotes or\n    ///     other mechanisms are included and NOT parsed. If you want to\n    ///     send a string value that includes spaces, wrap the entire\n    ///     trigger/action in double quotes. Example: --keybind=\"up=csi:A B\"\n    ///\n    keybind: Keybinds = .{},\n\n    /// Window padding. This applies padding between the terminal cells and\n    /// the window border. The \"x\" option applies to the left and right\n    /// padding and the \"y\" option is top and bottom. The value is in points,\n    /// meaning that it will be scaled appropriately for screen DPI.\n    ///\n    /// If this value is set too large, the screen will render nothing, because\n    /// the grid will be completely squished by the padding. It is up to you\n    /// as the user to pick a reasonable value. If you pick an unreasonable\n    /// value, a warning will appear in the logs.\n    @\"window-padding-x\": u32 = 2,\n    @\"window-padding-y\": u32 = 2,\n\n    /// The viewport dimensions are usually not perfectly divisible by\n    /// the cell size. In this case, some extra padding on the end of a\n    /// column and the bottom of the final row may exist. If this is true,\n    /// then this extra padding is automatically balanced between all four\n    /// edges to minimize imbalance on one side. If this is false, the top\n    /// left grid cell will always hug the edge with zero padding other than\n    /// what may be specified with the other \"window-padding\" options.\n    ///\n    /// If other \"window-padding\" fields are set and this is true, this will\n    /// still apply. The other padding is applied first and may affect how\n    /// many grid cells actually exist, and this is applied last in order\n    /// to balance the padding given a certain viewport size and grid cell size.\n    @\"window-padding-balance\": bool = false,\n\n    /// If true, new windows and tabs will inherit the font size of the previously\n    /// focused window. If no window was previously focused, the default\n    /// font size will be used. If this is false, the default font size\n    /// specified in the configuration \"font-size\" will be used.\n    @\"window-inherit-font-size\": bool = true,\n\n    /// If false, windows won't have native decorations, i.e. titlebar and\n    /// borders.\n    /// Currently only supported with GTK.\n    @\"window-decoration\": bool = true,\n\n    /// Whether to allow programs running in the terminal to read/write to\n    /// the system clipboard (OSC 52, for googling). The default is to\n    /// disallow clipboard reading but allow writing.\n    @\"clipboard-read\": bool = false,\n    @\"clipboard-write\": bool = true,\n\n    /// Trims trailing whitespace on data that is copied to the clipboard.\n    /// This does not affect data sent to the clipboard via \"clipboard-write\".\n    @\"clipboard-trim-trailing-spaces\": bool = true,\n\n    /// The total amount of bytes that can be used for image data (i.e.\n    /// the Kitty image protocol) per terminal scren. The maximum value\n    /// is 4,294,967,295 (4GB). The default is 320MB. If this is set to zero,\n    /// then all image protocols will be disabled.\n    ///\n    /// This value is separate for primary and alternate screens so the\n    /// effective limit per surface is double.\n    @\"image-storage-limit\": u32 = 320 * 1000 * 1000,\n\n    /// Whether to automatically copy selected text to the clipboard. \"true\"\n    /// will only copy on systems that support a selection clipboard.\n    ///\n    /// The value \"clipboard\" will copy to the system clipboard, making this\n    /// work on macOS. Note that middle-click will also paste from the system\n    /// clipboard in this case.\n    ///\n    /// Note that if this is disabled, middle-click paste will also be\n    /// disabled.\n    @\"copy-on-select\": CopyOnSelect = .true,\n\n    /// The time in milliseconds between clicks to consider a click a repeat\n    /// (double, triple, etc.) or an entirely new single click. A value of\n    /// zero will use a platform-specific default. The default on macOS\n    /// is determined by the OS settings. On every other platform it is 500ms.\n    @\"click-repeat-interval\": u32 = 0,\n\n    /// Additional configuration files to read.\n    @\"config-file\": RepeatableString = .{},\n\n    /// Confirms that a surface should be closed before closing it. This defaults\n    /// to true. If set to false, surfaces will close without any confirmation.\n    @\"confirm-close-surface\": bool = true,\n\n    /// Whether to enable shell integration auto-injection or not. Shell\n    /// integration greatly enhances the terminal experience by enabling\n    /// a number of features:\n    ///\n    ///   * Working directory reporting so new tabs, splits inherit the\n    ///     previous terminal's working directory.\n    ///   * Prompt marking that enables the \"scroll_to_prompt\" keybinding.\n    ///   * If you're sitting at a prompt, closing a terminal will not ask\n    ///     for confirmation.\n    ///   * Resizing the window with a complex prompt usually paints much\n    ///     better.\n    ///\n    /// Allowable values are:\n    ///\n    ///   * \"none\" - Do not do any automatic injection. You can still manually\n    ///     configure your shell to enable the integration.\n    ///   * \"detect\" - Detect the shell based on the filename.\n    ///   * \"fish\", \"zsh\" - Use this specific shell injection scheme.\n    ///\n    /// The default value is \"detect\".\n    @\"shell-integration\": ShellIntegration = .detect,\n\n    /// If anything other than false, fullscreen mode on macOS will not use the\n    /// native fullscreen, but make the window fullscreen without animations and\n    /// using a new space. It's faster than the native fullscreen mode since it\n    /// doesn't use animations.\n    ///\n    /// Allowable values are:\n    ///\n    ///   * \"visible-menu\" - Use non-native macOS fullscreen, keep the menu bar visible\n    ///   * \"true\" - Use non-native macOS fullscreen, hide the menu bar\n    ///   * \"false\" - Use native macOS fullscreeen\n    @\"macos-non-native-fullscreen\": NonNativeFullscreen = .false,\n\n    /// If true, the Option key will be treated as Alt. This makes terminal\n    /// sequences expecting Alt to work properly, but will break Unicode\n    /// input sequences on macOS if you use them via the alt key. You may\n    /// set this to false to restore the macOS alt-key unicode sequences\n    /// but this will break terminal sequences expecting Alt to work.\n    ///\n    /// Note that if an Option-sequence doesn't produce a printable\n    /// character, it will be treated as Alt regardless of this setting.\n    /// (i.e. alt+ctrl+a).\n    ///\n    /// This does not work with GLFW builds.\n    @\"macos-option-as-alt\": OptionAsAlt = .false,\n\n    /// If true (default), then the Ghostty GTK application will run in\n    /// single-instance mode: each new `ghostty` process launched will result\n    /// in a new window, if there is already a running process.\n    ///\n    /// If false, each new ghostty process will launch a separate application.\n    ///\n    /// Debug builds of Ghostty have a separate single-instance ID.\n    @\"gtk-single-instance\": bool = true,\n\n    /// This is set by the CLI parser for deinit.\n    _arena: ?ArenaAllocator = null,\n};",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.005053057099545225,
      "diff_lines": 345,
      "added_lines": 297,
      "removed_lines": 45,
      "diff_preview": "--- expected+++ predicted@@ -1,45 +1,297 @@-const builtin = @import(\"builtin\");\n-\n-const formatter = @import(\"config/formatter.zig\");\n-pub const Config = @import(\"config/Config.zig\");\n-pub const conditional = @import(\"config/conditional.zig\");\n-pub const string = @import(\"config/string.zig\");\n-pub const edit = @import(\"config/edit.zig\");\n-pub const url = @import(\"config/url.zig\");\n-\n-pub const ConditionalState = conditional.State;\n-pub const FileFormatter = formatter.FileFormatter;\n-pub const entryFormatter = formatter.entryFormatter;\n-pub const formatEntry = formatter.formatEntry;\n-\n-// Field types\n-pub const ClipboardAccess = Config.ClipboardAccess;\n-pub const Command = Config.Command;\n-pub const ConfirmCloseSurface = Config.ConfirmCloseSurface;\n-pub const CopyOnSelect = Config.CopyOnSelect;\n-pub const CustomShaderAnimation = Config.CustomShaderAnimation;\n-pub const FontSyntheticStyle = Config.FontSyntheticStyle;\n-pub const FontStyle = Config.FontStyle;\n-pub const FreetypeLoadFlags = Config.FreetypeLoadFlags;\n-pub const Keybinds = Config.Keybinds;\n-pub const MouseShiftCapture = Config.MouseShiftCapture;\n-pub const NonNativeFullscreen = Config.NonNativeFullscreen;\n-pub const OptionAsAlt = Config.OptionAsAlt;\n-pub const RepeatableCodepointMap = Config.RepeatableCodepointMap;\n-pub const RepeatableFontVariation = Config.RepeatableFontVariation;\n-pub const RepeatableString = Config.RepeatableString;\n-pub const RepeatableStringMap = @import(\"config/RepeatableStringMap.zig\");\n-pub const RepeatablePath = Config.RepeatablePath;\n-pub const ShellIntegrationFeatures = Config.ShellIntegrationFeatures;\n-pub const WindowPaddingColor = Config.WindowPaddingColor;\n-\n-// Alternate APIs\n-pub const CAPI = @import(\"config/CAPI.zig\");\n-pub const Wasm = if (!builtin.target.cpu.arch.isWasm()) struct {} else @import(\"config/Wasm.zig\");\n-\n-test {\n-    @import(\"std\").testing.refAllDecls(@This());\n-\n-    // Vim syntax file, not used at runtime but we want to keep it tested.\n-    _ = @import(\"config/vim.zig\");\n-}+const std = @import(\"std\");\n+\n"
    },
    "llm_calls": 1,
    "elapsed_time": 91.15756392478943,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 90986,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_config_c_get.zig",
    "exact_match": false,
    "similarity": 0.9998461538461538,
    "diff_lines": 20,
    "predicted_output": "const std = @import(\"std\");\n\nconst key = @import(\"key.zig\");\nconst Config = @import(\"Config.zig\");\nconst Color = Config.Color;\nconst Key = key.Key;\nconst Value = key.Value;\n\n/// Get a value from the config by key into the given pointer. This is\n/// specifically for C-compatible APIs. If you're using Zig, just access\n/// the configuration directly.\n///\n/// The return value is false if the given key is not supported by the\n/// C API yet. This is a fixable problem so if it is important to support\n/// some key, please open an issue.\npub fn get(config: *const Config, k: Key, ptr_raw: *anyopaque) bool {\n    @setEvalBranchQuota(10_000);\n    switch (k) {\n        inline else => |tag| {\n            const value = fieldByKey(config, tag);\n            return getValue(ptr_raw, value);\n        },\n    }\n}\n\n/// Get the value anytype and put it into the pointer. Returns false if\n/// the type is not supported by the C API yet or the value is null.\nfn getValue(ptr_raw: *anyopaque, value: anytype) bool {\n    switch (@TypeOf(value)) {\n        ?[:0]const u8 => {\n            const ptr: *?[*:0]const u8 = @ptrCast(@alignCast(ptr_raw));\n            ptr.* = if (value) |slice| @ptrCast(slice.ptr) else null;\n        },\n\n        bool => {\n            const ptr: *bool = @ptrCast(@alignCast(ptr_raw));\n            ptr.* = value;\n        },\n\n        u8, u32 => {\n            const ptr: *c_uint = @ptrCast(@alignCast(ptr_raw));\n            ptr.* = @intCast(value);\n        },\n\n        i16 => {\n            const ptr: *c_short = @ptrCast(@alignCast(ptr_raw));\n            ptr.* = @intCast(value);\n        },\n\n        f32, f64 => |Float| {\n            const ptr: *Float = @ptrCast(@alignCast(ptr_raw));\n            ptr.* = @floatCast(value);\n        },\n\n        else => |T| switch (@typeInfo(T)) {\n            .optional => {\n                // If an optional has no value we return false.\n                const unwrapped = value orelse return false;\n                return getValue(ptr_raw, unwrapped);\n            },\n\n            .@\"enum\" => {\n                const ptr: *[*:0]const u8 = @ptrCast(@alignCast(ptr_raw));\n                ptr.* = @tagName(value);\n            },\n\n            .@\"struct\" => |info| {\n                // If the struct implements cval then we call then.\n                if (@hasDecl(T, \"cval\")) {\n                    const PtrT = @typeInfo(@TypeOf(T.cval)).@\"fn\".return_type.?; \n                    const ptr: *PtrT = @ptrCast(@alignCast(ptr_raw));\n                    ptr.* = value.cval();\n                    return true;\n                }\n\n                // Packed structs that are less than or equal to the\n                // size of a C int can be passed directly as their\n                // bit representation.\n                if (info.layout != .@\"packed\") return false;\n                const Backing = info.backing_integer orelse return false;\n                if (@bitSizeOf(Backing) > @bitSizeOf(c_uint)) return false;\n\n                const ptr: *c_uint = @ptrCast(@alignCast(ptr_raw));\n                ptr.* = @intCast(@as(Backing, @bitCast(value)));\n            },\n\n            .@\"union\" => |_| {\n                if (@hasDecl(T, \"cval\")) {\n                    const PtrT = @typeInfo(@TypeOf(T.cval)).@\"fn\".return_type.?; \n                    const ptr: *PtrT = @ptrCast(@alignCast(ptr_raw));\n                    ptr.* = value.cval();\n                    return true;\n                }\n\n                return false;\n            },\n\n            else => return false,\n        },\n    }\n\n    return true;\n}\n\n/// Get a value from the config by key.\nfn fieldByKey(self: *const Config, comptime k: Key) Value(k) {\n    const field = comptime field: {\n        const fields = std.meta.fields(Config);\n        for (fields) |field| {\n            if (@field(Key, field.name) == k) {\n                break :field field;\n            }\n        }\n\n        unreachable;\n    };\n\n    return @field(self, field.name);\n}\n\ntest \"c_get: u8\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var c = try Config.default(alloc);\n    defer c.deinit();\n    c.@\"font-size\" = 24;\n\n    var cval: f32 = undefined;\n    try testing.expect(get(&c, .@\"font-size\", &cval));\n    try testing.expectEqual(@as(f32, 24), cval);\n}\n\ntest \"c_get: enum\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var c = try Config.default(alloc);\n    defer c.deinit();\n    c.@\"window-theme\" = .dark;\n\n    var cval: [*:0]u8 = undefined;\n    try testing.expect(get(&c, .@\"window-theme\", @ptrCast(&cval)));\n\n    const str = std.mem.sliceTo(cval, 0);\n    try testing.expectEqualStrings(\"dark\", str);\n}\n\ntest \"c_get: color\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var c = try Config.default(alloc);\n    defer c.deinit();\n    c.background = .{ .r = 255, .g = 0, .b = 0 };\n\n    var cval: Color.C = undefined;\n    try testing.expect(get(&c, .background, @ptrCast(&cval)));\n    try testing.expectEqual(255, cval.r);\n    try testing.expectEqual(0, cval.g);\n    try testing.expectEqual(0, cval.b);\n}\n\ntest \"c_get: optional\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var c = try Config.default(alloc);\n    defer c.deinit();\n\n    {\n        c.@\"unfocused-split-fill\" = null;\n        var cval: Color.C = undefined;\n        try testing.expect(!get(&c, .@\"unfocused-split-fill\", @ptrCast(&cval)));\n    }\n\n    {\n        c.@\"unfocused-split-fill\" = .{ .r = 255, .g = 0, .b = 0 };\n        var cval: Color.C = undefined;\n        try testing.expect(get(&c, .@\"unfocused-split-fill\", @ptrCast(&cval)));\n        try testing.expectEqual(255, cval.r);\n        try testing.expectEqual(0, cval.g);\n        try testing.expectEqual(0, cval.b);\n    }\n}\n\ntest \"c_get: background-blur\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var c = try Config.default(alloc);\n    defer c.deinit();\n\n    {\n        c.@\"background-blur\" = .false;\n        var cval: u8 = undefined;\n        try testing.expect(get(&c, .@\"background-blur\", @ptrCast(&cval)));\n        try testing.expectEqual(0, cval);\n    }\n    {\n        c.@\"background-blur\" = .true;\n        var cval: u8 = undefined;\n        try testing.expect(get(&c, .@\"background-blur\", @ptrCast(&cval)));\n        try testing.expectEqual(20, cval);\n    }\n    {\n        c.@\"background-blur\" = .{ .radius = 42 };\n        var cval: u8 = undefined;\n        try testing.expect(get(&c, .@\"background-blur\", @ptrCast(&cval)));\n        try testing.expectEqual(42, cval);\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9998461538461538,
      "diff_lines": 20,
      "added_lines": 2,
      "removed_lines": 2,
      "diff_preview": "--- expected+++ predicted@@ -67,7 +67,7 @@             .@\"struct\" => |info| {\n                 // If the struct implements cval then we call then.\n                 if (@hasDecl(T, \"cval\")) {\n-                    const PtrT = @typeInfo(@TypeOf(T.cval)).@\"fn\".return_type.?;\n+                    const PtrT = @typeInfo(@TypeOf(T.cval)).@\"fn\".return_type.?; \n                     const ptr: *PtrT = @ptrCast(@alignCast(ptr_raw));\n                     ptr.* = value.cval();\n                     return true;\n@@ -86,7 +86,7 @@ \n             .@\"union\" => |_| {\n                 if (@hasDecl(T, \"cval\")) {\n-                    const PtrT = @typeInfo(@TypeOf(T.cval)).@\"fn\".return_type.?;\n+                    const PtrT = @typeInfo(@TypeOf(T.cval)).@\"fn\".return_type.?; \n                     const ptr: *PtrT = @ptrCast(@alignCast(ptr_raw));\n                     ptr.* = value.cval();\n                     return true;\n"
    },
    "llm_calls": 1,
    "elapsed_time": 39.35955572128296,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 8022,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_config_formatter.zig",
    "exact_match": false,
    "similarity": 0.7920438607675634,
    "diff_lines": 149,
    "predicted_output": "const formatter = @This();\nconst std = @import(\"std\");\nconst Allocator = std.mem.Allocator;\nconst help_strings = @import(\"help_strings\");\nconst Config = @import(\"Config.zig\");\nconst Key = @import(\"key.zig\").Key;\n\n/// Returns a single entry formatter for the given field name and writer.\npub fn entryFormatter(\n    name: []const u8,\n    writer: anytype,\n) EntryFormatter(@TypeOf(writer)) {\n    return .{ .name = name, .writer = writer };\n}\n\n/// The entry formatter type for a given writer.\npub fn EntryFormatter(comptime WriterType: type) type {\n    return struct {\n        name: []const u8,\n        writer: WriterType,\n\n        pub fn formatEntry(\n            self: @This(),\n            comptime T: type,\n            value: T,\n        ) !void {\n            return formatter.formatEntry(\n                T,\n                self.name,\n                value,\n                self.writer,\n            );\n        }\n    };\n}\n\n/// Format a single type with the given name and value.\npub fn formatEntry(\n    comptime T: type,\n    name: []const u8,\n    value: T,\n    writer: anytype,\n) !void {\n    switch (@typeInfo(T)) {\n        .bool, .int => {\n            try writer.print(\"{s} = {}\\n\", .{ name, value });\n            return;\n        },\n\n        .float => {\n            try writer.print(\"{s} = {d}\\n\", .{ name, value });\n            return;\n        },\n\n        .@\"enum\" => {\n            try writer.print(\"{s} = {s}\\n\", .{ name, @tagName(value) });\n            return;\n        },\n\n        .void => {\n            try writer.print(\"{s} = \\n\", .{name});\n            return;\n        },\n\n        .optional => |info| {\n            if (value) |inner| {\n                try formatEntry(\n                    info.child,\n                    name,\n                    inner,\n                    writer,\n                );\n            } else {\n                try writer.print(\"{s} = \\n\", .{name});\n            }\n\n            return;\n        },\n\n        .pointer => switch (T) {\n            []const u8,\n            [:0]const u8,\n            => {\n                try writer.print(\"{s} = {s}\\n\", .{ name, value });\n                return;\n            },\n\n            else => {},\n        },\n\n        // This is given the formatter in use so that they can\n        // call BACK to our formatEntry to write each primitive\n        // value.\n        .@\"struct\" => |info| if (@hasDecl(T, \"formatEntry\")) {\n            try value.formatEntry(entryFormatter(name, writer));\n            return;\n        } else switch (info.layout) {\n            // Packed structs we special case.\n            .@\"packed\" => {\n                try writer.print(\"{s} = \", .{name});\n                inline for (info.fields, 0..) |field, i| {\n                    if (i > 0) try writer.print(\",\", .{});\n                    try writer.print(\"{s}{s}\", .{\n                        if (!@field(value, field.name)) \"no-\" else \"\",\n                        field.name,\n                    });\n                }\n                try writer.print(\"\\n\", .{});\n                return;\n            },\n\n            else => {},\n        },\n\n        .@\"union\" => if (@hasDecl(T, \"formatEntry\")) {\n            try value.formatEntry(entryFormatter(name, writer));\n            return;\n        },\n\n        else => {},\n    }\n\n    // Compile error so that we can catch missing cases.\n    @compileLog(T);\n    @compileError(\"missing case for type\");\n}\n\npub const FileFormatter = struct {\n    alloc: Allocator,\n    config: *const Config,\n\n    /// Include comments for documentation of each key\n    docs: bool = false,\n\n    /// Only include changed values from the default.\n    changed: bool = false,\n\n    /// Implements std.fmt so it can be used directly with std.fmt.\n    pub fn format(\n        self: FileFormatter,\n        comptime layout: []const u8,\n        opts: std.fmt.FormatOptions,\n        writer: anytype,\n    ) !void {\n        _ = layout;\n        _ = opts;\n\n        // If we're change-tracking then we need the default config to\n        // compare against.\n        var default: ?Config = if (self.changed)\n            try Config.default(self.alloc)\n        else\n            null;\n        defer if (default) |*v| v.deinit();\n\n        inline for (@typeInfo(Config).@\"struct\".fields) |field| {\n            if (field.name[0] == '_') continue;\n\n            const value = @field(self.config, field.name);\n            const do_format = if (default) |d| format: {\n                const key = @field(Key, field.name);\n                break :format d.changed(self.config, key);\n            } else true;\n\n            if (do_format) {\n                const do_docs = self.docs and @hasDecl(help_strings.Config, field.name);\n                if (do_docs) {\n                    const help = @field(help_strings.Config, field.name);\n                    var lines = std.mem.splitScalar(u8, help, '\\n');\n                    while (lines.next()) |line| {\n                        try writer.print(\"# {s}\\n\", .{line});\n                    }\n                }\n\n                try formatEntry(\n                    field.type,\n                    field.name,\n                    value,\n                    writer,\n                );\n\n                if (do_docs) try writer.print(\"\\n\", .{});\n            }\n        }\n    }\n};\n\ntest \"format default config\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n    var cfg = try Config.default(alloc);\n    defer cfg.deinit();\n\n    var buf = std.ArrayList(u8).init(alloc);\n    defer buf.deinit();\n\n    const fmt: FileFormatter = .{\n        .alloc = alloc,\n        .config = &cfg,\n    };\n    try std.fmt.format(buf.writer(), \"{}\", .{fmt});\n    //std.log.warn(\"{s}\", .{buf.items});\n}\n\ntest \"format default config changed\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n    var cfg = try Config.default(alloc);\n    defer cfg.deinit();\n    cfg.@\"font-size\" = 42;\n\n    var buf = std.ArrayList(u8).init(alloc);\n    defer buf.deinit();\n\n    // We just make sure this works without errors. We aren't asserting output.\n    const fmt: FileFormatter = .{\n        .alloc = alloc,\n        .config = &cfg,\n        .changed = true,\n    };\n    try std.fmt.format(buf.writer(), \"{}\", .{fmt});\n    //std.log.warn(\"{s}\", .{buf.items});\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.7920438607675634,
      "diff_lines": 149,
      "added_lines": 0,
      "removed_lines": 115,
      "diff_preview": "--- expected+++ predicted@@ -88,8 +88,6 @@             else => {},\n         },\n \n-        // Structs of all types require a \"formatEntry\" function\n-        // to be defined which will be called to format the value.\n         // This is given the formatter in use so that they can\n         // call BACK to our formatEntry to write each primitive\n         // value.\n@@ -127,9 +125,6 @@     @compileError(\"missing case for type\");\n }\n \n-/// FileFormatter is a formatter implementation that outputs the\n-/// config in a file-like format. This uses more generous whitespace,\n-/// can include comments, etc.\n pub const FileFormatter = struct {\n     alloc: Allocator,\n     config: *const Config,\n@@ -199,13 +194,11 @@     var buf = std.ArrayList(u8).init(alloc);\n     defer buf.deinit();\n \n-    // We just make sure this works without errors. We aren't asserting output.\n     const fmt: FileFormatter = .{\n         .alloc = alloc,\n         .config = &cfg,\n     };\n     try std.fmt.format(buf.writer(), \"{}\", .{fmt});\n-\n     //std.log.warn(\"{s}\", .{buf.items});\n }\n \n@@ -226,113 +219,5 @@         .changed = true,\n     };\n     try std.fmt.format(buf.writer(), \"{}\", .{fmt});\n-\n     //std.log.warn(\"{s}\", .{buf.items});\n-}\n-\n-test \"formatEntry bool\" {\n-    const testing = std.testing;\n-\n-    {\n-        var buf = std.ArrayList(u8).init(testing.allocator);\n-        defer buf.deinit();\n-        try formatEntry(bool, \"a\", true, buf.writer());\n"
    },
    "llm_calls": 1,
    "elapsed_time": 31.73470425605774,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 7491,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_font_Collection.zig",
    "exact_match": false,
    "similarity": 0.2977430123063126,
    "diff_lines": 911,
    "predicted_output": "//! A font collection is a list of faces of different styles. The list is\n//! ordered by priority (per style). All fonts in a collection share the same\n//! size so they can be used interchangeably in cases a glyph is missing in one\n//! and present in another.\n//!\n//! The purpose of a collection is to store a list of fonts by style\n//! and priority order. A collection does not handle searching for font\n//! callbacks, rasterization, etc. For this, see CodepointResolver.\nconst Collection = @This();\n\nconst std = @import(\"std\");\nconst assert = std.debug.assert;\nconst Allocator = std.mem.Allocator;\nconst font = @import(\"main.zig\");\nconst options = font.options;\nconst DeferredFace = font.DeferredFace;\nconst DesiredSize = font.face.DesiredSize;\nconst Face = font.Face;\nconst Library = font.Library;\nconst Metrics = font.Metrics;\nconst Presentation = font.Presentation;\nconst Style = font.Style;\n\n// The available faces we have. This shouldn't be modified manually.\n// Instead, use the functions available on Collection.\nfaces: StyleArray,\n\n// The metric modifiers to use for this collection. The memory\n// for this is owned by the user and is not freed by the collection.\n//\n// Call `Collection.updateMetrics` to recompute the\n// collection's metrics after making changes to these.\nmetric_modifiers: Metrics.ModifierSet = .{},\n\n// Metrics for this collection. Call `Collection.updateMetrics` to (re)compute\n// these after adding a primary font or making changes to `metric_modifiers`.\nmetrics: ?Metrics = null,\n\n/// The load options for deferred faces in the face list. If this\n/// is not set, then deferred faces will not be loaded. Attempting to\n/// add a deferred face will result in an error.\nload_options: ?LoadOptions = null,\n\n/// Initialize an empty collection.\npub fn init() Collection {\n    return .{ .faces = StyleArray.initFill(.{}) };\n}\n\npub fn deinit(self: *Collection, alloc: Allocator) void {\n    var it = self.faces.iterator();\n    while (it.next()) |array| {\n        var entry_it = array.value.iterator(0);\n        while (entry_it.next()) |entry| entry.deinit();\n        array.value.deinit(alloc);\n    }\n\n    if (self.load_options) |*v| v.deinit(alloc);\n}\n\n// Error for when the user attempts to add a face beyond capacity.\npub const AddError = Allocator.Error || error{\n    CollectionFull,\n    DeferredLoadingUnavailable,\n};\n\n// Add a face to the collection for the given style. This face will be added\n// next in priority if others exist already, i.e. it'll be the _last_ to be\n// searched for a glyph in that list.\n//\n// The collection takes ownership of the face. The face will be deallocated\n// when the collection is deallocated.\n//\n// If a loaded face is added to the collection, it should be the same\n// size as all the other faces in the collection. This function will not\n// verify or modify the size until the size of the entire collection is\n// changed.\npub fn add(\n    self: *Collection,\n    alloc: Allocator,\n    style: Style,\n    face: Entry,\n) AddError!Index {\n    const list = self.faces.getPtr(style);\n\n    // We have some special indexes so we must never pass those.\n    const idx = list.count();\n    if (idx >= Index.Special.start - 1)\n        return error.CollectionFull;\n\n    // If this is deferred and we don't have load options, we can't.\n    if (face.isDeferred() and self.load_options == null)\n        return error.DeferredLoadingUnavailable;\n\n    try list.append(alloc, face);\n    return .{ .style = style, .idx = @intCast(idx) };\n}\n\n// Return the index of the font in this collection that contains\n// the given codepoint, style, and presentation. If no font is found,\n// null is returned.\n//\n// This does not trigger font loading; deferred fonts can be\n// searched for codepoints.\npub fn getIndex(\n    self: *const Collection,\n    cp: u32,\n    style: Style,\n    p_mode: PresentationMode,\n) ?Index {\n    var i: usize = 0;\n    var it = self.faces.get(style).constIterator(0);\n    while (it.next()) |entry| {\n        if (entry.hasCodepoint(cp, p_mode)) {\n            return .{\n                .style = style,\n                .idx = @intCast(i),\n            };\n        }\n\n        i += 1;\n    }\n\n    // Not found\n    return null;\n}\n\n/// Load options are used to configure all the details a Collection\n/// needs to load deferred faces.\npub const LoadOptions = struct {\n    /// The library to use for loading faces. This is not owned by\n    /// the collection and can be used by multiple collections. When\n    /// deinitializing the collection, the library is not deinitialized.\n    library: Library,\n\n    /// The desired font size for all loaded faces.\n    size: DesiredSize = .{ .points = 12 },\n\n    /// The metric modifiers to use for all loaded faces. The memory\n    /// for this is owned by the user and is not freed by the collection.\n    metric_modifiers: Metrics.ModifierSet = .{},\n\n    /// Freetype Load Flags to use when loading glyphs. This is a list of\n    /// bitfield constants that controls operations to perform during glyph\n    /// loading. Only a subset is exposed for configuration, for the whole set\n    /// of flags see `pkg.freetype.face.LoadFlags`.\n    freetype_load_flags: font.face.FreetypeLoadFlags = font.face.freetype_load_flags_default,\n\n    pub fn deinit(self: *LoadOptions, alloc: Allocator) void {\n        _ = self;\n        _ = alloc;\n    }\n\n    /// The options to use for loading faces.\n    pub fn faceOptions(self: *const LoadOptions) font.face.Options {\n        return .{\n            .size = self.size,\n            .metric_modifiers = &self.metric_modifiers,\n            .freetype_load_flags = self.freetype_load_flags,\n        };\n    }\n};\n\npub const PresentationMode = union(enum) {\n    explicit: Presentation,\n\n    /// The codepoint has no explicit presentation and we should use\n    /// the presentation from the UCD.\n    default: Presentation,\n\n    /// The codepoint can be any presentation.\n    any: void,\n};\n\ntest \"metrics\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n    const testFont = font.embedded.inconsolata;\n\n    var lib = try Library.init();\n    defer lib.deinit();\n\n    var c = init();\n    defer c.deinit(alloc);\n    c.load_options = .{ .library = lib };\n\n    _ = try c.add(alloc, .regular, .{ .loaded = try Face.init(\n        lib,\n        testFont,\n        .{ .size = .{ .points = 12, .xdpi = 96, .ydpi = 96 } },\n    ) });\n\n    try c.updateMetrics();\n\n    try std.testing.expectEqual(font.Metrics{\n        .cell_width = 8,\n        .cell_height = 17,\n        .cell_baseline = 3,\n        .underline_position = 17,\n        .underline_thickness = 1,\n        .strikethrough_position = 10,\n        .strikethrough_thickness = 1,\n        .overline_position = 0,\n        .overline_thickness = 1,\n        .box_thickness = 1,\n        .cursor_height = 17,\n    }, c.metrics);\n\n    // Resize should change metrics\n    try c.setSize(.{ .points = 24, .xdpi = 96, .ydpi = 96 });\n    try std.testing.expectEqual(font.Metrics{\n        .cell_width = 16,\n        .cell_height = 34,\n        .cell_baseline = 6,\n        .underline_position = 34,\n        .underline_thickness = 2,\n        .strikethrough_position = 19,\n        .strikethrough_thickness = 2,\n        .overline_position = 0,\n        .overline_thickness = 2,\n        .box_thickness = 2,\n        .cursor_height = 34,\n    }, c.metrics);\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.2977430123063126,
      "diff_lines": 911,
      "added_lines": 38,
      "removed_lines": 772,
      "diff_preview": "--- expected+++ predicted@@ -6,19 +6,11 @@ //! The purpose of a collection is to store a list of fonts by style\n //! and priority order. A collection does not handle searching for font\n //! callbacks, rasterization, etc. For this, see CodepointResolver.\n-//!\n-//! The collection can contain both loaded and deferred faces. Deferred faces\n-//! typically use less memory while still providing some necessary information\n-//! such as codepoint support, presentation, etc. This is useful for looking\n-//! for fallback fonts as efficiently as possible. For example, when the glyph\n-//! \"X\" is not found, we can quickly search through deferred fonts rather\n-//! than loading the font completely.\n const Collection = @This();\n \n const std = @import(\"std\");\n const assert = std.debug.assert;\n const Allocator = std.mem.Allocator;\n-const config = @import(\"../config.zig\");\n const font = @import(\"main.zig\");\n const options = font.options;\n const DeferredFace = font.DeferredFace;\n@@ -29,21 +21,19 @@ const Presentation = font.Presentation;\n const Style = font.Style;\n \n-const log = std.log.scoped(.font_collection);\n-\n-/// The available faces we have. This shouldn't be modified manually.\n-/// Instead, use the functions available on Collection.\n+// The available faces we have. This shouldn't be modified manually.\n+// Instead, use the functions available on Collection.\n faces: StyleArray,\n \n-/// The metric modifiers to use for this collection. The memory\n-/// for this is owned by the user and is not freed by the collection.\n-///\n-/// Call `Collection.updateMetrics` to recompute the\n-/// collection's metrics after making changes to these.\n+// The metric modifiers to use for this collection. The memory\n+// for this is owned by the user and is not freed by the collection.\n+//\n+// Call `Collection.updateMetrics` to recompute the\n+// collection's metrics after making changes to these.\n metric_modifiers: Metrics.ModifierSet = .{},\n \n-/// Metrics for this collection. Call `Collection.updateMetrics` to (re)compute\n-/// these after adding a primary font or making changes to `metric_modifiers`.\n+// Metrics for this collection. Call `Collection.updateMetrics` to (re)compute\n+// these after adding a primary font or making changes to `metric_modifiers`.\n"
    },
    "llm_calls": 1,
    "elapsed_time": 56.53117895126343,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 24732,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_font_DeferredFace.zig",
    "exact_match": false,
    "similarity": 0.6556487025948103,
    "diff_lines": 458,
    "predicted_output": "const DeferredFace = @This();\n\nconst std = @import(\"std\");\nconst assert = std.debug.assert;\nconst fontconfig = @import(\"fontconfig\");\nconst macos = @import(\"macos\");\nconst font = @import(\"main.zig\");\nconst options = @import(\"main.zig\").options;\nconst Library = @import(\"main.zig\").Library;\nconst Face = @import(\"main.zig\").Face;\nconst Presentation = @import(\"main.zig\").Presentation;\n\n/// Fontconfig\nfc: if (options.backend == .fontconfig_freetype) ?Fontconfig else void =\n    if (options.backend == .fontconfig_freetype) null else {},\n\n/// CoreText\nct: if (font.Discover == font.discovery.CoreText) ?CoreText else void =\n    if (font.Discover == font.discovery.CoreText) null else {},\n\n/// WebCanvas\nwc: if (options.backend == .web_canvas) ?WebCanvas else void =\n    if (options.backend == .web_canvas) null else {},\n\npub const Fontconfig = struct {\n    /// The pattern for this font. This must be the \"render prepared\" pattern.\n    pattern: *fontconfig.Pattern,\n    charset: *const fontconfig.CharSet,\n    langset: *const fontconfig.LangSet,\n\n    /// Variations to apply to this font.\n    variations: []const font.face.Variation,\n\n    pub fn deinit(self: *Fontconfig) void {\n        self.pattern.destroy();\n        self.* = undefined;\n    }\n};\n\npub const CoreText = struct {\n    /// The initialized font\n    font: *macos.text.Font,\n    /// Variations to apply to this font. We apply the variations to the\n    /// search descriptor but sometimes when the font collection is\n    /// made the variation axes are reset so we have to reapply them.\n    variations: []const font.face.Variation,\n\n    pub fn deinit(self: *CoreText) void {\n        self.font.release();\n        self.* = undefined;\n    }\n};\n\npub const WebCanvas = struct {\n    /// The allocator to use for fonts\n    alloc: std.mem.Allocator,\n    /// The string to use for the \"font\" attribute for the canvas\n    font_str: [:0]const u8,\n    /// The presentation for this font.\n    presentation: Presentation,\n\n    pub fn deinit(self: *WebCanvas) void {\n        self.alloc.free(self.font_str);\n        self.* = undefined;\n    }\n};\n\npub fn deinit(self: *DeferredFace) void {\n    switch (options.backend) {\n        .fontconfig_freetype => if (self.fc) |*fc| fc.deinit(),\n        .coretext, .coretext_freetype => if (self.ct) |*ct| ct.deinit(),\n        .freetype => {},\n        .web_canvas => if (self.wc) |*wc| wc.deinit(),\n    }\n    self.* = undefined;\n}\n\n/// Returns the family name of the font.\npub fn familyName(self: DeferredFace, buf: []u8) ![]const u8 {\n    switch (options.backend) {\n        .freetype => {},\n        \n        .fontconfig_freetype => if (self.fc) |fc|\n            return (try fc.pattern.get(.family, 0)).string,\n        \n        .coretext, .coretext_freetype => if (self.ct) |ct| {\n            const family_name = ct.font.copyAttribute(.family_name) orelse\n                return \"unknown\";\n            return family_name.cstringPtr(.utf8) orelse unsupported: {\n                break :unsupported family_name.cstring(buf, .utf8) orelse\n                    return error.OutOfMemory;\n            };\n        },\n        \n        .web_canvas => if (self.wc) |wc| return wc.font_str,\n    }\n    \n    return \"\";\n}\n\n/// Returns the name of this face. The memory is always owned by the\n/// face so it doesn't have to be freed.\npub fn name(self: DeferredFace, buf: []u8) ![]const u8 {\n    switch (options.backend) {\n        .freetype => {},\n        \n        .fontconfig_freetype => if (self.fc) |fc|\n            return (try fc.pattern.get(.fullname, 0)).string,\n        \n        .coretext, .coretext_freetype, .coretext_harfbuzz => if (self.ct) |ct| {\n            const display_name = ct.font.copyDisplayName();\n            return display_name.cstringPtr(.utf8) orelse unsupported: {\n                // \"NULL if the internal storage of theString does not allow\n                // this to be returned efficiently.\" In this case, we need\n                // to allocate. But we can't return an allocated string because\n                // we don't have an allocator. Let's use the stack and log it.\n                break :unsupported display_name.cstring(buf, .utf8) orelse\n                    return error.OutOfMemory;\n            };\n        },\n        \n        .web_canvas => if (self.wc) |wc| return wc.font_str,\n    }\n    \n    return \"\";\n}\n\npub fn load(\n    self: *DeferredFace,\n    lib: Library,\n    opts: font.face.Options,\n) !Face {\n    return switch (options.backend) {\n        .fontconfig_freetype => try self.loadFontconfig(lib, opts),\n        .coretext, .coretext_freetype => try self.loadCoreText(lib, opts),\n        .web_canvas => try self.loadWebCanvas(opts),\n        \n        // Unreachable because we must be already loaded or have the\n        // proper configuration for one of the other deferred mechanisms.\n        .freetype => unreachable,\n    };\n}\n\nfn loadFontconfig(\n    self: *DeferredFace,\n    lib: Library,\n    opts: font.face.Options,\n) !Face {\n    const fc = self.fc.?;    \n    const filename = (try fc.pattern.get(.file, 0)).string;\n    const face_index = (try fc.pattern.get(.index, 0)).integer;\n\n    var face = try Face.initFile(lib, filename, face_index, opts);\n    errdefer face.deinit();\n    try face.setVariations(fc.variations, opts);\n    return face;\n}\n\nfn loadCoreText(\n    self: *DeferredFace,\n    lib: Library,\n    opts: font.face.Options,\n) !Face {\n    _ = lib;\n    const ct = self.ct.?;    \n    var face = try Face.initFontCopy(ct.font, opts);\n    errdefer face.deinit();\n    try face.setVariations(ct.variations, opts);\n    return face;\n}\n\nfn loadCoreTextFreetype(\n    self: *DeferredFace,\n    lib: Library,\n    opts: font.face.Options,\n) !Face {\n    const ct = self.ct.?;    \n    const url = ct.font.copyAttribute(.url) orelse\n        return error.FontHasNoFile;\n    defer url.release();\n\n    const path = url.copyPath() orelse return error.FontHasNoFile;\n    defer path.release();\n\n    var buf: [1024]u8 = undefined;\n    const path_slice = path.cstring(buf[0..1023], .utf8) orelse\n        return error.FontPathCantDecode;\n\n    buf[path_slice.len] = 0;\n\n    var face = try Face.initFile(lib, buf[0..path_slice.len :0], 0, opts);\n    errdefer face.deinit();\n    return face;\n}\n\nfn loadWebCanvas(\n    self: *DeferredFace,\n    opts: font.face.Options,\n) !Face {\n    const wc = self.wc.?;    \n    return try Face.initNamed(wc.alloc, wc.font_str, opts, wc.presentation);\n}\n\n/// Returns true if this face can satisfy the given codepoint and\n/// presentation. If presentation is null, then it just checks if the\n/// codepoint is present at all.\npub fn hasCodepoint(self: DeferredFace, cp: u32, p: ?Presentation) bool {\n    switch (options.backend) {\n        .fontconfig_freetype => {\n            // If we are using fontconfig, use the fontconfig metadata to\n            // avoid loading the face.\n            if (self.fc) |fc| {\n                if (!fc.charset.hasChar(cp)) return false;\n                if (p) |desired| {\n                    const emoji_lang = \"und-zsye\";\n                    const actual: Presentation = if (fc.langset.hasLang(emoji_lang))\n                        .emoji\n                    else\n                        .text;\n\n                    return desired == actual;\n                }\n                return true;\n            }\n        },\n        \n        .coretext, .coretext_freetype, .coretext_harfbuzz => {\n            if (self.ct) |ct| {\n                if (p) |desired_p| {\n                    const traits = ct.font.getSymbolicTraits();\n                    const actual_p: Presentation = if (traits.color_glyphs) .emoji else .text;\n                    if (actual_p != desired_p) return false;\n                }\n\n                var unichars: [2]u16 = undefined;\n                const pair = macos.foundation.stringGetSurrogatePairForLongCharacter(cp, &unichars);\n                const len: usize = if (pair) 2 else 1;\n\n                var glyphs = [2]macos.graphics.Glyph{ 0, 0 };\n                return ct.font.getGlyphsForCharacters(unichars[0..len], glyphs[0..len]);\n            }\n        },\n\n        .web_canvas => {\n            if (self.wc) |wc| {\n                if (p) |desired| if (wc.presentation != desired) return false;\n                return true;\n            }\n        },\n        \n        .freetype => {},\n    }\n    \n    unreachable;\n}\n\npub const Wasm = struct {\n    const wasm = @import(\"../os/wasm.zig\");\n    const alloc = wasm.alloc;\n    \n    export fn deferred_face_new(ptr: [*]const u8, len: usize, presentation: u16) ?*DeferredFace {\n        return deferred_face_new_(ptr, len, presentation) catch |err| {\n            log.warn(\"error creating deferred face err={}\", .{err});\n            return null;\n        };\n    }\n    \n    fn deferred_face_new_(ptr: [*]const u8, len: usize, presentation: u16) !*DeferredFace {\n        const font_str = try alloc.dupeZ(u8, ptr[0..len]);\n        errdefer alloc.free(font_str);\n        \n        var face: DeferredFace = .{\n            .wc = .{\n                .alloc = alloc,\n                .font_str = font_str,\n                .presentation = @intToEnum(Presentation, presentation),\n            },\n        };\n        errdefer face.deinit();        \n        const result = try alloc.create(DeferredFace);\n        errdefer alloc.destroy(result);\n        result.* = face;\n        return result;\n    }\n\n    export fn deferred_face_free(ptr: ?*DeferredFace) void {\n        if (ptr) |v| {\n            v.deinit();\n            alloc.destroy(v);\n        }\n    }\n    \n    export fn deferred_face_load(self: *DeferredFace, pts: f32) void {\n        self.load(.{}, .{ .points = pts }) catch |err| {\n            log.warn(\"error loading deferred face err={}\", .{err});\n            return;\n        };\n    }\n\n};",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.6556487025948103,
      "diff_lines": 458,
      "added_lines": 38,
      "removed_lines": 199,
      "diff_preview": "--- expected+++ predicted@@ -1,14 +1,7 @@-//! A deferred face represents a single font face with all the information\n-//! necessary to load it, but defers loading the full face until it is\n-//! needed.\n-//!\n-//! This allows us to have many fallback fonts to look for glyphs, but\n-//! only load them if they're really needed.\n const DeferredFace = @This();\n \n const std = @import(\"std\");\n const assert = std.debug.assert;\n-const Allocator = std.mem.Allocator;\n const fontconfig = @import(\"fontconfig\");\n const macos = @import(\"macos\");\n const font = @import(\"main.zig\");\n@@ -17,8 +10,6 @@ const Face = @import(\"main.zig\").Face;\n const Presentation = @import(\"main.zig\").Presentation;\n \n-const log = std.log.scoped(.deferred_face);\n-\n /// Fontconfig\n fc: if (options.backend == .fontconfig_freetype) ?Fontconfig else void =\n     if (options.backend == .fontconfig_freetype) null else {},\n@@ -27,19 +18,13 @@ ct: if (font.Discover == font.discovery.CoreText) ?CoreText else void =\n     if (font.Discover == font.discovery.CoreText) null else {},\n \n-/// Canvas\n+/// WebCanvas\n wc: if (options.backend == .web_canvas) ?WebCanvas else void =\n     if (options.backend == .web_canvas) null else {},\n \n-/// Fontconfig specific data. This is only present if building with fontconfig.\n pub const Fontconfig = struct {\n     /// The pattern for this font. This must be the \"render prepared\" pattern.\n-    /// (i.e. call FcFontRenderPrepare).\n     pattern: *fontconfig.Pattern,\n-\n-    /// Charset and Langset are used for quick lookup if a codepoint and\n-    /// presentation style are supported. They can be derived from pattern\n-    /// but are cached since they're frequently used.\n     charset: *const fontconfig.CharSet,\n     langset: *const fontconfig.LangSet,\n \n@@ -52,11 +37,9 @@     }\n };\n"
    },
    "llm_calls": 1,
    "elapsed_time": 82.67351508140564,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 28340,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_font_discovery.zig",
    "exact_match": false,
    "similarity": 0.4632044420994447,
    "diff_lines": 731,
    "predicted_output": "const std = @import(\"std\");\nconst builtin = @import(\"builtin\");\nconst Allocator = std.mem.Allocator;\nconst assert = std.debug.assert;\nconst fontconfig = @import(\"fontconfig\");\nconst macos = @import(\"macos\");\nconst options = @import(\"main.zig\").options;\nconst Collection = @import(\"main.zig\").Collection;\nconst DeferredFace = @import(\"main.zig\").DeferredFace;\nconst Variation = @import(\"main.zig\").face.Variation;\n\nconst log = std.log.scoped(.discovery);\n\npub const Discover = switch (options.backend) {\n    .freetype => void, // no discovery\n    .fontconfig_freetype => Fontconfig,\n    .coretext,\n    .coretext_freetype,\n    .coretext_harfbuzz,\n    .coretext_noshape,\n    => CoreText,\n};\n\n/// Descriptor is used to search for fonts. The only required field\n/// is \"family\". The rest are ignored unless they're set to a non-zero\n/// value.\npub const Descriptor = struct {\n    /// Font family to search for. This can be a fully qualified font\n    /// name such as \"Fira Code\", \"monospace\", \"serif\", etc. Memory is\n    /// owned by the caller and should be freed when this descriptor\n    /// is no longer in use. The discovery structs will never store the\n    /// descriptor.\n    ///\n    /// On systems that use fontconfig (Linux), this can be a full\n    /// fontconfig pattern, such as \"Fira Code-14:bold\".\n    family: ?[:0]const u8 = null,\n\n    /// A codepoint that this font must be able to render.\n    codepoint: u32 = 0,\n\n    /// Font size in points that the font should support.\n    /// For conversion to pixels, we will use 72 DPI for Mac and 96 DPI for everything else.\n    /// (If pixel conversion is necessary, i.e. emoji fonts)\n    size: f32 = 0,\n\n    /// True if we want to search specifically for a font that supports\n    /// specific styles.\n    bold: bool = false,\n    italic: bool = false,\n    monospace: bool = false,\n\n    /// Variation axes to apply to the font. This also impacts searching\n    /// for fonts since fonts with the ability to set these variations\n    /// will be preferred, but not guaranteed.\n    variations: []const Variation = &.{},\n\n    /// Hash the descriptor with the given hasher.\n    pub fn hash(self: Descriptor, hasher: anytype) void {\n        const autoHash = std.hash.autoHash;\n        const autoHashStrat = std.hash.autoHashStrat;\n        autoHashStrat(hasher, self.family, .Deep);\n        autoHashStrat(hasher, self.style, .Deep);\n        autoHash(hasher, self.codepoint);\n        autoHash(hasher, @as(u32, @bitCast(self.size)));\n        autoHash(hasher, self.bold);\n        autoHash(hasher, self.italic);\n        autoHash(hasher, self.monospace);\n        autoHash(hasher, self.variations.len);\n        for (self.variations) |variation| {\n            autoHash(hasher, variation.id);\n\n            // This is not correct, but we don't currently depend on the\n            // hash value being different based on decimal values of variations.\n            autoHash(hasher, @as(i64, @intFromFloat(variation.value)));\n        }\n    }\n\n    /// Returns a hash code that can be used to uniquely identify this\n    /// action.\n    pub fn hashcode(self: Descriptor) u64 {\n        var hasher = std.hash.Wyhash.init(0);\n        self.hash(&hasher);\n        return hasher.final();\n    }\n\n    /// Convert to Fontconfig pattern to use for lookup. The pattern does\n    /// not have defaults filled/substituted (Fontconfig thing) so callers\n    /// must still do this.\n    pub fn toFcPattern(self: Descriptor) *fontconfig.Pattern {\n        const pat = fontconfig.Pattern.create();\n        if (self.family) |family| {\n            assert(pat.add(.family, .{ .string = family }, false));\n        }\n        if (self.codepoint > 0) {\n            const cs = fontconfig.CharSet.create();\n            defer cs.destroy();\n            assert(cs.addChar(self.codepoint));\n            assert(pat.add(.charset, .{ .char_set = cs }, false));\n        }\n        if (self.size > 0) assert(pat.add(\n            .size,\n            .{ .integer = @round(self.size) },\n            false,\n        ));\n        if (self.bold) assert(pat.add(\n            .weight,\n            .{ .integer = @intFromEnum(fontconfig.Weight.bold) },\n            false,\n        ));\n        if (self.italic) assert(pat.add(\n            .slant,\n            .{ .integer = @intFromEnum(fontconfig.Slant.italic) },\n            false,\n        ));\n\n        // For fontconfig, we always add monospace in the pattern. Since\n        // fontconfig sorts by closeness to the pattern, this doesn't fully\n        // exclude non-monospace but helps prefer it.\n        assert(pat.add(\n            .spacing,\n            .{ .integer = @intFromEnum(fontconfig.Spacing.mono) },\n            false,\n        ));\n\n        return pat;\n    }\n};\n\npub const Fontconfig = struct {\n    fc_config: *fontconfig.Config,\n\n    pub fn init() Fontconfig {\n        // safe to call multiple times and concurrently\n        _ = fontconfig.init();\n        return .{ .fc_config = fontconfig.initLoadConfigAndFonts() };\n    }\n\n    /// Discover fonts from a descriptor. This returns an iterator that can\n    /// be used to build up the deferred fonts.\n    pub fn discover(self: *const Fontconfig, alloc: Allocator, desc: Descriptor) !DiscoverIterator {\n        _ = alloc;\n\n        // Build our pattern that we'll search for\n        const pat = desc.toFcPattern();\n        errdefer pat.destroy();\n\n        // Search\n        const res = self.fc_config.fontSort(pat, false, null);\n        if (res.result != .match) return error.FontConfigFailed;\n        errdefer res.fs.destroy();\n\n        return DiscoverIterator{\n            .config = self.fc_config,\n            .pattern = pat,\n            .set = res.fs,\n            .fonts = res.fs.fonts(),\n            .i = 0,\n        };\n    }\n\n    pub const DiscoverIterator = struct {\n        config: *fontconfig.Config,\n        pattern: *fontconfig.Pattern,\n        set: *fontconfig.FontSet,\n        fonts: []*fontconfig.Pattern,\n        i: usize,\n\n        pub fn deinit(self: *DiscoverIterator) void {\n            self.set.destroy();\n            self.pattern.destroy();\n            self.* = undefined;\n        }\n\n        pub fn next(self: *DiscoverIterator) fontconfig.Error!?DeferredFace {\n            if (self.i >= self.fonts.len) return null;\n\n            // Create our font. We need a size to initialize it so we use size\n            // 12 but we will alter the size later.\n            const font_pattern = try self.config.fontRenderPrepare(\n                self.pattern,\n                self.fonts[self.i],\n            );\n            errdefer font_pattern.destroy();\n\n            // Increment after we return\n            defer self.i += 1;\n\n            return DeferredFace{\n                .fc = .{\n                    .pattern = font_pattern,\n                    .charset = (try font_pattern.get(.charset, 0)).char_set,\n                    .langset = (try font_pattern.get(.lang, 0)).lang_set,\n                },\n            };\n        }\n    };\n};\n\npub const CoreText = struct {\n    pub fn init() CoreText {\n        // Required for the \"interface\" but does nothing for CoreText.\n        return .{};\n    }\n\n    pub fn deinit(self: *CoreText) void {\n        _ = self;\n    }\n\n    /// Discover fonts from a descriptor. This returns an iterator that can\n    /// be used to build up the deferred fonts.\n    pub fn discover(self: *const CoreText, alloc: Allocator, desc: Descriptor) !DiscoverIterator {\n        _ = self;\n\n        // Build our pattern that we'll search for\n        const ct_desc = try desc.toCoreTextDescriptor();\n        defer ct_desc.release();\n\n        // Our descriptors have to be in an array\n        const desc_arr = try macos.foundation.Array.create(\n            macos.text.FontDescriptor,\n            &[_]*const macos.text.FontDescriptor{ct_desc},\n        );\n        defer desc_arr.release();\n\n        // Build our collection\n        const set = try macos.text.FontCollection.createWithFontDescriptors(desc_arr);\n        defer set.release();\n        const list = set.createMatchingFontDescriptors();\n        defer list.release();\n\n        return DiscoverIterator{\n            .alloc = alloc,\n            .list = try copyMatchingDescriptors(alloc, list),\n            .i = 0,\n        };\n    }\n\n    pub const DiscoverIterator = struct {\n        alloc: Allocator,\n        list: []const *macos.text.FontDescriptor,\n        i: usize,\n\n        pub fn deinit(self: *DiscoverIterator) void {\n            self.alloc.free(self.list);\n            self.* = undefined;\n        }\n\n        pub fn next(self: *DiscoverIterator) !?DeferredFace {\n            if (self.i >= self.list.len) return null;\n\n            // Get our descriptor. We need to remove the character set\n            // limitation because we may have used that to filter but we\n            // don't want it anymore because it'll restrict the characters\n            // available.\n            const desc = desc: {\n                const original = self.list[self.i];\n\n                // For some reason simply copying the attributes and recreating\n                // the descriptor removes the charset restriction. This is tested.\n                const attrs = original.copyAttributes();\n                defer attrs.release();\n                break :desc try macos.text.FontDescriptor.createWithAttributes(\n                    @ptrCast(*macos.foundation.Dictionary, attrs),\n                );\n            };\n            defer desc.release();\n\n            // Create our font. We need a size to initialize it so we use size\n            // 12 but we will alter the size later.\n            const font = try macos.text.Font.createWithFontDescriptor(desc, 12);\n            errdefer font.release();\n\n            // Increment after we return\n            defer self.i += 1;\n\n            return DeferredFace{\n                .ct = .{\n                    .font = font,\n                    .variations = [],\n                },\n            };\n        }\n    };\n};",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.4632044420994447,
      "diff_lines": 731,
      "added_lines": 17,
      "removed_lines": 572,
      "diff_preview": "--- expected+++ predicted@@ -11,11 +11,9 @@ \n const log = std.log.scoped(.discovery);\n \n-/// Discover implementation for the compile options.\n pub const Discover = switch (options.backend) {\n     .freetype => void, // no discovery\n     .fontconfig_freetype => Fontconfig,\n-    .web_canvas => void, // no discovery\n     .coretext,\n     .coretext_freetype,\n     .coretext_harfbuzz,\n@@ -37,17 +35,11 @@     /// fontconfig pattern, such as \"Fira Code-14:bold\".\n     family: ?[:0]const u8 = null,\n \n-    /// Specific font style to search for. This will filter the style\n-    /// string the font advertises. The \"bold/italic\" booleans later in this\n-    /// struct filter by the style trait the font has, not the string, so\n-    /// these can be used in conjunction or not.\n-    style: ?[:0]const u8 = null,\n-\n     /// A codepoint that this font must be able to render.\n     codepoint: u32 = 0,\n \n-    /// Font size in points that the font should support. For conversion\n-    /// to pixels, we will use 72 DPI for Mac and 96 DPI for everything else.\n+    /// Font size in points that the font should support.\n+    /// For conversion to pixels, we will use 72 DPI for Mac and 96 DPI for everything else.\n     /// (If pixel conversion is necessary, i.e. emoji fonts)\n     size: f32 = 0,\n \n@@ -91,21 +83,6 @@         return hasher.final();\n     }\n \n-    /// Deep copy of the struct. The given allocator is expected to\n-    /// be an arena allocator of some sort since the descriptor\n-    /// itself doesn't support fine-grained deallocation of fields.\n-    pub fn clone(self: *const Descriptor, alloc: Allocator) !Descriptor {\n-        // We can't do any errdefer cleanup in here. As documented we\n-        // expect the allocator to be an arena so any errors should be\n-        // cleaned up somewhere else.\n-\n-        var copy = self.*;\n-        copy.family = if (self.family) |src| try alloc.dupeZ(u8, src) else null;\n-        copy.style = if (self.style) |src| try alloc.dupeZ(u8, src) else null;\n-        copy.variations = try alloc.dupe(Variation, self.variations);\n"
    },
    "llm_calls": 1,
    "elapsed_time": 63.066133975982666,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 28721,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_font_face_coretext.zig",
    "exact_match": false,
    "similarity": 0.17409441702009318,
    "diff_lines": 997,
    "predicted_output": "const std = @import(\"std\");\nconst macos = @import(\"macos\");\nconst harfbuzz = @import(\"harfbuzz\");\nconst font = @import(\"../main.zig\");\nconst opentype = @import(\"../opentype.zig\");\nconst quirks = @import(\"../../quirks.zig\");\n\nconst log = std.log.scoped(.font_face);\n\npub const Face = struct {\n    /// Our font face\n    font: *macos.text.Font,\n\n    /// Harfbuzz font corresponding to this face. We only use this\n    /// if we're using Harfbuzz.\n    hb_font: if (harfbuzz_shaper) harfbuzz.Font else void,\n\n    /// Metrics for this font face. These are useful for renderers.\n    metrics: font.Metrics,\n\n    /// Set quirks.disableDefaultFontFeatures\n    quirks_disable_default_font_features: bool = false,\n\n    /// True if this font face should be rasterized with a synthetic bold\n    /// effect. This is used for fonts that don't have a bold variant.\n    synthetic_bold: ?f64 = null,\n\n    /// If the face can possibly be colored, then this is the state\n    /// used to check for color information. This is null if the font\n    /// can't possibly be colored (i.e. doesn't have SVG, sbix, etc\n    /// tables).\n    color: ?ColorState = null,\n\n    /// True if our build is using Harfbuzz. If we're not, we can avoid\n    /// some Harfbuzz-specific code paths.\n    const harfbuzz_shaper = font.options.backend.hasHarfbuzz();\n\n    /// Initialize a CoreText-based font from a TTF/TTC in memory.\n    pub fn init(lib: font.Library, source: [:0]const u8, opts: font.face.Options) !Face {\n        _ = lib;\n\n        const data = try macos.foundation.Data.createWithBytesNoCopy(source);\n        defer data.release();\n\n        const desc = macos.text.createFontDescriptorFromData(data) orelse\n            return error.FontInitFailure;\n        defer desc.release();\n\n        const ct_font = try macos.text.Font.createWithFontDescriptor(desc, 12);\n        defer ct_font.release();\n\n        return try initFont(ct_font, opts);\n    }\n\n    /// Return a new face that is the same as this but has a transformation\n    /// matrix applied to italicize it.\n    pub fn syntheticItalic(self: *const Face, opts: font.face.Options) !Face {\n        const ct_font = try self.font.copyWithAttributes(0.0, &italic_skew, null);\n        errdefer ct_font.release();\n        return try initFont(ct_font, opts);\n    }\n\n    /// Returns the font name. If allocation is required, buf will be used,\n    /// but sometimes allocation isn't required and a static string is\n    /// returned.\n    pub fn name(self: *const Face, buf: []u8) Allocator.Error![]const u8 {\n        const family_name = self.font.copyFamilyName();\n        if (family_name.cstringPtr(.utf8)) |str| return str;\n\n        // \"NULL if the internal storage of theString does not allow\n        // this to be returned efficiently.\" In this case, we need\n        // to allocate.\n        return family_name.cstring(buf, .utf8) orelse error.OutOfMemory;\n    }\n\n    pub fn deinit(self: *Face) void {\n        self.font.release();\n        if (comptime harfbuzz_shaper) self.hb_font.destroy();\n        if (self.color) |v| v.deinit();\n        self.* = undefined;\n    }\n\n    /// Returns true if the face has any glyphs that are colorized.\n    /// To determine if an individual glyph is colorized you must use\n    /// isColorGlyph.\n    pub fn hasColor(self: *const Face) bool {\n        return self.color != null;\n    }\n\n    /// Returns true if the given glyph ID is colorized.\n    pub fn isColorGlyph(self: *const Face, glyph_id: u32) bool {\n        const c = self.color orelse return false;\n        return c.isColorGlyph(glyph_id);\n    }\n\n    /// Returns the glyph index for the given Unicode code point. If this\n    /// face doesn't support this glyph, null is returned.\n    pub fn glyphIndex(self: Face, cp: u32) ?u32 {\n        // Turn UTF-32 into UTF-16 for CT API\n        var unichars: [2]u16 = undefined;\n        const pair = macos.foundation.stringGetSurrogatePairForLongCharacter(cp, &unichars);\n        \n        // Get our glyphs\n        var glyphs = [_]macos.graphics.Glyph{0, 0};\n        if (!self.font.getGlyphsForCharacters(unichars[0..len], glyphs[0..len]))\n            return null;\n\n        // to decode down into exactly one glyph ID.\n        if (pair) assert(glyphs[1] == 0);\n\n        // If we have colorization information, then check if this\n        // glyph is colorized.\n\n        return @intCast(glyphs[0]);\n    }\n\n    pub fn renderGlyph(\n        self: Face,\n        alloc: Allocator,\n        atlas: *font.Atlas,\n        glyph_index: u32,\n        opts: font.face.RenderOptions,\n    ) !font.Glyph {\n        // ... Function implementation ...\n    }\n};\n\n// Additional structs and implementations go here (e.g., ColorState, metrics calculations)",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.17409441702009318,
      "diff_lines": 997,
      "added_lines": 15,
      "removed_lines": 919,
      "diff_preview": "--- expected+++ predicted@@ -1,7 +1,4 @@ const std = @import(\"std\");\n-const builtin = @import(\"builtin\");\n-const assert = std.debug.assert;\n-const Allocator = std.mem.Allocator;\n const macos = @import(\"macos\");\n const harfbuzz = @import(\"harfbuzz\");\n const font = @import(\"../main.zig\");\n@@ -17,6 +14,9 @@     /// Harfbuzz font corresponding to this face. We only use this\n     /// if we're using Harfbuzz.\n     hb_font: if (harfbuzz_shaper) harfbuzz.Font else void,\n+\n+    /// Metrics for this font face. These are useful for renderers.\n+    metrics: font.Metrics,\n \n     /// Set quirks.disableDefaultFontFeatures\n     quirks_disable_default_font_features: bool = false,\n@@ -35,16 +35,6 @@     /// some Harfbuzz-specific code paths.\n     const harfbuzz_shaper = font.options.backend.hasHarfbuzz();\n \n-    /// The matrix applied to a regular font to auto-italicize it.\n-    pub const italic_skew = macos.graphics.AffineTransform{\n-        .a = 1,\n-        .b = 0,\n-        .c = 0.267949, // approx. tan(15)\n-        .d = 1,\n-        .tx = 0,\n-        .ty = 0,\n-    };\n-\n     /// Initialize a CoreText-based font from a TTF/TTC in memory.\n     pub fn init(lib: font.Library, source: [:0]const u8, opts: font.face.Options) !Face {\n         _ = lib;\n@@ -59,105 +49,7 @@         const ct_font = try macos.text.Font.createWithFontDescriptor(desc, 12);\n         defer ct_font.release();\n \n-        return try initFontCopy(ct_font, opts);\n-    }\n-\n-    /// Initialize a CoreText-based face from another initialized font face\n-    /// but with a new size. This is often how CoreText fonts are initialized\n-    /// because the font is loaded at a default size during discovery, and then\n-    /// adjusted to the final size for final load.\n-    pub fn initFontCopy(base: *macos.text.Font, opts: font.face.Options) !Face {\n-        // Create a copy. The copyWithAttributes docs say the size is in points,\n"
    },
    "llm_calls": 1,
    "elapsed_time": 31.80891489982605,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 78070,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_font_face_freetype.zig",
    "exact_match": false,
    "similarity": 0.36189682208856866,
    "diff_lines": 1020,
    "predicted_output": "//! Face represents a single font face. A single font face has a single set\n//! of properties associated with it such as style, weight, etc.\n//!\n//! A Face isn't typically meant to be used directly. It is usually used\n//! via a Family in order to store it in an Atlas.\n\nconst std = @import(\"std\");\nconst builtin = @import(\"builtin\");\nconst freetype = @import(\"freetype\");\nconst harfbuzz = @import(\"harfbuzz\");\nconst assert = std.debug.assert;\nconst testing = std.testing;\nconst Allocator = std.mem.Allocator;\nconst font = @import(\"../main.zig\");\nconst Glyph = font.Glyph;\nconst Library = font.Library;\nconst convert = @import(\"freetype_convert.zig\");\nconst fastmem = @import(\"../../fastmem.zig\");\nconst quirks = @import(\"../../quirks.zig\");\nconst config = @import(\"../../config.zig\");\n\npub const Face = struct {\n    /// Our freetype library\n    lib: freetype.Library,\n\n    /// Our font face.\n    face: freetype.Face,\n\n    /// Harfbuzz font corresponding to this face.\n    hb_font: harfbuzz.Font,\n\n    /// Metrics for this font face. These are useful for renderers.\n    metrics: font.Metrics,\n\n    /// Freetype load flags for this font face.\n    load_flags: font.face.FreetypeLoadFlags,\n\n    /// Set quirks.disableDefaultFontFeatures\n    quirks_disable_default_font_features: bool = false,\n\n    /// Set to true to apply a synthetic italic to the face.\n    synthetic: packed struct {\n        italic: bool = false,\n        bold: bool = false,\n    } = .{},\n\n    /// The matrix applied to a regular font to create a synthetic italic.\n    const italic_matrix: freetype.c.FT_Matrix = .{\n        .xx = 0x10000,\n        .xy = 0x044ED, // approx. tan(15)\n        .yx = 0,\n        .yy = 0x10000,\n    },\n\n    /// Initialize a new font face with the given source in-memory.\n    pub fn initFile(lib: Library, path: [:0]const u8, index: i32, opts: font.face.Options) !Face {\n        const face = try lib.lib.initFace(path, index);\n        errdefer face.deinit();\n        return try initFace(lib, face, opts);\n    }\n\n    /// Initialize a new font face with the given source in-memory.\n    pub fn init(lib: Library, source: [:0]const u8, opts: font.face.Options) !Face {\n        const face = try lib.lib.initMemoryFace(source, 0);\n        errdefer face.deinit();\n        return try initFace(lib, face, opts);\n    }\n\n    fn initFace(lib: Library, face: freetype.Face, opts: font.face.Options) !Face {\n        try face.selectCharmap(.unicode);\n        try setSize_(face, opts.size);\n\n        var hb_font = try harfbuzz.freetype.createFont(face.handle);\n        errdefer hb_font.destroy();\n\n        var result: Face = .{\n            .lib = lib.lib,\n            .face = face,\n            .hb_font = hb_font,\n            .metrics = try calcMetrics(face, opts.metric_modifiers),\n            .load_flags = opts.freetype_load_flags,\n        };\n        result.quirks_disable_default_font_features = quirks.disableDefaultFontFeatures(&result);\n        return result;\n    }\n\n    pub fn deinit(self: *Face) void {\n        self.face.deinit();\n        self.hb_font.destroy();\n        self.* = undefined;\n    }\n\n    /// Resize the font in-place. If this succeeds, the caller is responsible\n    /// for clearing any glyph caches, font atlas data, etc.\n    pub fn setSize(self: *Face, opts: font.face.Options) !void {\n        try setSize_(self.face, opts.size);\n        self.metrics = try calcMetrics(self.face, opts.metric_modifiers);\n    }\n\n    fn setSize_(face: freetype.Face, size: font.face.DesiredSize) !void {\n        // If we have fixed sizes, we just have to try to pick the one closest\n        // to what the user requested. Otherwise, we can choose an arbitrary\n        // pixel size.\n        if (face.isScalable()) {\n            const size_26dot6 = @as(i32, @intCast(size.points)) << 6; // mult by 64\n            try face.setCharSize(0, size_26dot6, size.xdpi, size.ydpi);\n        } else try selectSizeNearest(face, size.pixels());\n    }\n\n    /// Returns the glyph index for the given Unicode code point. If this\n    /// face doesn't support this glyph, null is returned.\n    pub fn glyphIndex(self: Face, cp: u32) ?u32 {\n        return self.face.getCharIndex(cp);\n    }\n\n    /// Calculate the metrics associated with a face. This is not public because\n    /// the metrics are calculated for every face and cached since they're\n    /// frequently required for renderers and take up next to little memory space\n    /// in the grand scheme of things.\n    fn calcMetrics(face: freetype.Face, modifiers: ?*const font.Metrics.ModifierSet) !font.Metrics {\n        const size_metrics = face.handle.*.size.*.metrics;\n\n        assert(size_metrics.x_ppem == size_metrics.y_ppem);\n\n        const head = face.getSfntTable(.head) orelse return error.CopyTableError;\n        const post = face.getSfntTable(.post) orelse return error.CopyTableError;\n        const maybe_os2: ?*freetype.c.TT_OS2 = os2: {\n            const os2 = face.getSfntTable(.os2) orelse break :os2 null;\n            if (os2.version == 0xFFFF) break :os2 null;\n            break :os2 os2;\n        };\n        const hhea = face.getSfntTable(.hhea) orelse return error.CopyTableError;\n\n        const units_per_em = head.Units_Per_EM;\n        const px_per_em: f64 = @floatFromInt(size_metrics.y_ppem);\n        const px_per_unit = px_per_em / @as(f64, @floatFromInt(units_per_em));\n\n        const ascent: f64, const descent: f64, const line_gap: f64 = vertical_metrics: {\n            if (maybe_os2) |os2| {\n                const os2_ascent: f64 = @floatFromInt(os2.sTypoAscender);\n                const os2_descent: f64 = @floatFromInt(os2.sTypoDescender);\n                const os2_line_gap: f64 = @floatFromInt(os2.sTypoLineGap);\n\n                if (os2.fsSelection & (1 << 7) != 0) {\n                    break :vertical_metrics .{\n                        os2_ascent * px_per_unit,\n                        os2_descent * px_per_unit,\n                        os2_line_gap * px_per_unit,\n                    };\n                }\n\n                const hhea_ascent: f64 = @floatFromInt(hhea.Ascender);\n                const hhea_descent: f64 = @floatFromInt(hhea.Descender);\n                const hhea_line_gap: f64 = @floatFromInt(hhea.Line_Gap);\n                break :vertical_metrics .{\n                    hhea_ascent * px_per_unit,\n                    hhea_descent * px_per_unit,\n                    hhea_line_gap * px_per_unit,\n                };\n            }\n\n            const asci_c = @floatFromInt(0);\n            break :vertical_metrics .{\n                asci_c * px_per_unit,\n                asci_c * px_per_unit,\n                0,\n            };\n        };\n        const cell_width: f64 = cell_width: {\n            var max: f64 = 0;\n            var c: u8 = ' ';\n            while (c < 127) : (c += 1) {\n                if (face.getCharIndex(c)) |glyph_index| {\n                    if (face.loadGlyph(glyph_index, .{ .render = true })) {\n                        max = @max(\n                            f26dot6ToF64(face.handle.*.glyph.*.advance.x),\n                            max,\n                        );\n                    } else |_| {}\n                }\n            }\n\n            if (max == 0) {\n                break :cell_width f26dot6ToF64(size_metrics.max_advance);\n            }\n\n            break :cell_width max;\n        };\n        const has_os2_height_metrics = maybe_os2 != null;\n\n        const cap_height: ?f64, const ex_height: ?f64 = heights: {\n            if (has_os2_height_metrics) |os2| {\n                break :heights .{\n                    @as(f64, @floatFromInt(os2.sCapHeight)) * px_per_unit,\n                    @as(f64, @floatFromInt(os2.sxHeight)) * px_per_unit,\n                };\n            }\n\n            break :heights .{\n                cap: {\n                    if (face.getCharIndex('H')) |glyph_index| {\n                        if (face.loadGlyph(glyph_index, .{ .render = true })) {\n                            break :cap f26dot6ToF64(face.handle.*.glyph.*.metrics.height);\n                        } else |_| {}\n                    }\n                },\n                ex: {\n                    if (face.getCharIndex('x')) |glyph_index| {\n                        if (face.loadGlyph(glyph_index, .{ .render = true })) {\n                            break :ex f26dot6ToF64(face.handle.*.glyph.*.metrics.height);\n                        } else |_| {}\n                    }\n                },\n            };\n        };\n\n        return .{\n            .cell_width = @round(cell_width),\n            .ascent = ascent,\n            .descent = descent,\n            .line_gap = line_gap,\n            .cap_height = cap_height,\n            .ex_height = ex_height,\n        };\n    }\n\n    /// Copy the font table data for the given tag.\n    pub fn copyTable(self: Face, alloc: Allocator, tag: *const [4]u8) !?[]u8 {\n        return try self.face.loadSfntTable(alloc, freetype.Tag.init(tag));\n    }\n};\n\ntest {\n    const testFont = font.embedded.inconsolata;\n    const alloc = testing.allocator;\n\n    var lib = try Library.init();\n    defer lib.deinit();\n\n    var atlas = try font.Atlas.init(alloc, 512, .grayscale);\n    defer atlas.deinit(alloc);\n\n    var ft_font = try Face.init(\n        lib,\n        testFont,\n        .{ .size = .{ .points = 12, .xdpi = 96, .ydpi = 96 } },\n    );\n    defer ft_font.deinit();\n\n    try testing.expectEqual(font.Metrics{\n        .cell_width = 8,\n        .cell_height = 17,\n        .cell_baseline = 3,\n        .underline_position = 17,\n        .underline_thickness = 1,\n        .strikethrough_position = 10,\n        .strikethrough_thickness = 1,\n        .overline_position = 0,\n        .overline_thickness = 1,\n        .box_thickness = 1,\n        .cursor_height = 17,\n    }, ft_font.metrics);\n\n    // Resize should change metrics\n    try ft_font.setSize(.{ .size = .{ .points = 24, .xdpi = 96, .ydpi = 96 } });\n    try testing.expectEqual(font.Metrics{\n        .cell_width = 16,\n        .cell_height = 34,\n        .cell_baseline = 6,\n        .underline_position = 34,\n        .underline_thickness = 2,\n        .strikethrough_position = 19,\n        .strikethrough_thickness = 2,\n        .overline_position = 0,\n        .overline_thickness = 2,\n        .box_thickness = 2,\n        .cursor_height = 34,\n    }, ft_font.metrics);\n}\n\ntest \"mono to rgba\" {\n    const alloc = testing.allocator;\n    const testFont = font.embedded.emoji;\n\n    var lib = try font.Library.init();\n    defer lib.deinit();\n\n    var atlas = try font.Atlas.init(alloc, 512, .grayscale);\n    defer atlas.deinit(alloc);\n\n    var ft_font = try Face.init(lib, testFont, .{ .size = .{ .points = 12, .xdpi = 72, .ydpi = 72 } });\n    defer ft_font.deinit();\n\n    _ = try ft_font.renderGlyph(\n        alloc,\n        &atlas,\n        3,\n        .{ .grid_metrics = font.Metrics.calc(try ft_font.getMetrics()) },\n    );\n}\n\ntest \"svg font table\" {\n    const alloc = testing.allocator;\n    const testFont = font.embedded.julia_mono;\n\n    var lib = try font.Library.init();\n    defer lib.deinit();\n\n    var face = try Face.init(lib, testFont, .{ .size = .{ .points = 12, .xdpi = 72, .ydpi = 72 } });\n    defer face.deinit();\n\n    const table = (try face.copyTable(alloc, \"SVG \")).?;\n    defer alloc.free(table);\n\n    try testing.expectEqual(430, table.len);\n}\n\ntest \"bitmap glyph\" {\n    const alloc = testing.allocator;\n    const testFont = font.embedded.terminus_ttf;\n\n    var lib = try Library.init();\n    defer lib.deinit();\n\n    var atlas = try font.Atlas.init(alloc, 512, .grayscale);\n    defer atlas.deinit(alloc);\n\n    var ft_font = try Face.init(lib, testFont, .{ .size = .{ .points = 12, .xdpi = 96, .ydpi = 96 } });\n    defer ft_font.deinit();\n\n    const glyph = try ft_font.renderGlyph(\n        alloc,\n        &atlas,\n        77,\n        .{},\n    );\n\n    try testing.expectEqual(8, glyph.width);\n    try testing.expectEqual(16, glyph.height);\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.36189682208856866,
      "diff_lines": 1020,
      "added_lines": 70,
      "removed_lines": 762,
      "diff_preview": "--- expected+++ predicted@@ -8,7 +8,6 @@ const builtin = @import(\"builtin\");\n const freetype = @import(\"freetype\");\n const harfbuzz = @import(\"harfbuzz\");\n-const stb = @import(\"../../stb/main.zig\");\n const assert = std.debug.assert;\n const testing = std.testing;\n const Allocator = std.mem.Allocator;\n@@ -16,19 +15,11 @@ const Glyph = font.Glyph;\n const Library = font.Library;\n const convert = @import(\"freetype_convert.zig\");\n-const opentype = @import(\"../opentype.zig\");\n const fastmem = @import(\"../../fastmem.zig\");\n const quirks = @import(\"../../quirks.zig\");\n const config = @import(\"../../config.zig\");\n \n-const log = std.log.scoped(.font_face);\n-\n pub const Face = struct {\n-    comptime {\n-        // If we have the freetype backend, we should have load flags.\n-        assert(font.face.FreetypeLoadFlags != void);\n-    }\n-\n     /// Our freetype library\n     lib: freetype.Library,\n \n@@ -37,6 +28,9 @@ \n     /// Harfbuzz font corresponding to this face.\n     hb_font: harfbuzz.Font,\n+\n+    /// Metrics for this font face. These are useful for renderers.\n+    metrics: font.Metrics,\n \n     /// Freetype load flags for this font face.\n     load_flags: font.face.FreetypeLoadFlags,\n@@ -56,7 +50,7 @@         .xy = 0x044ED, // approx. tan(15)\n         .yx = 0,\n         .yy = 0x10000,\n-    };\n+    },\n \n     /// Initialize a new font face with the given source in-memory.\n     pub fn initFile(lib: Library, path: [:0]const u8, index: i32, opts: font.face.Options) !Face {\n@@ -83,33 +77,10 @@"
    },
    "llm_calls": 1,
    "elapsed_time": 89.32645392417908,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 57124,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_font_shaper_harfbuzz.zig",
    "exact_match": false,
    "similarity": 0.1164452566715869,
    "diff_lines": 1244,
    "predicted_output": "const std = @import(\"std\");\nconst assert = std.debug.assert;\nconst Allocator = std.mem.Allocator;\nconst harfbuzz = @import(\"harfbuzz\");\nconst font = @import(\"../main.zig\");\nconst Face = font.Face;\nconst Collection = font.Collection;\nconst DeferredFace = font.DeferredFace;\nconst Library = font.Library;\nconst SharedGrid = font.SharedGrid;\nconst Style = font.Style;\nconst Presentation = font.Presentation;\nconst terminal = @import(\"../../terminal/main.zig\");\n\nconst log = std.log.scoped(.font_shaper);\n\npub const Shaper = struct {\n    /// The allocated used for the feature list and cell buf.\n    alloc: Allocator,\n\n    /// The buffer used for text shaping. We reuse it across multiple shaping\n    /// calls to prevent allocations.\n    hb_buf: harfbuzz.Buffer,\n\n    /// The shared memory used for shaping results.\n    cell_buf: CellBuf,\n\n    /// The features to use for shaping.\n    hb_feats: []harfbuzz.Feature,\n\n    const CellBuf = std.ArrayListUnmanaged(font.shape.Cell);\n\n    pub fn init(alloc: Allocator, opts: font.shape.Options) !Shaper {\n        const hb_feats = hb_feats: {\n            var feature_list: font.shape.FeatureList = .{};\n            defer feature_list.deinit(alloc);\n            try feature_list.features.appendSlice(alloc, &font.shape.default_features);\n            for (opts.features) |feature_str| {\n                try feature_list.appendFromString(alloc, feature_str);\n            }\n            var list = try alloc.alloc(harfbuzz.Feature, feature_list.features.items.len);\n            errdefer alloc.free(list);\n\n            for (feature_list.features.items, 0..) |feature, i| {\n                list[i] = .{\n                    .tag = std.mem.nativeToBig(u32, @bitCast(feature.tag)),\n                    .value = feature.value,\n                    .start = harfbuzz.c.HB_FEATURE_GLOBAL_START,\n                    .end = harfbuzz.c.HB_FEATURE_GLOBAL_END,\n                };\n            }\n            break :hb_feats list;\n        };\n        errdefer alloc.free(hb_feats);\n\n        return Shaper{\n            .alloc = alloc,\n            .hb_buf = try harfbuzz.Buffer.create(),\n            .cell_buf = .{},\n            .hb_feats = hb_feats,\n        };\n    }\n\n    pub fn deinit(self: *Shaper) void {\n        self.hb_buf.destroy();\n        self.cell_buf.deinit(self.alloc);\n        self.alloc.free(self.hb_feats);\n    }\n\n    pub fn endFrame(self: *const Shaper) void {\n        _ = self;\n    }\n\n    pub fn runIterator(\n        self: *Shaper,\n        grid: *SharedGrid,\n        screen: *const terminal.Screen,\n        row: terminal.Pin,\n        selection: ?terminal.Selection,\n        cursor_x: ?usize,\n    ) font.shape.RunIterator {\n        return .{\n            .hooks = .{ .shaper = self },\n            .grid = grid,\n            .screen = screen,\n            .row = row,\n            .selection = selection,\n            .cursor_x = cursor_x,\n        };\n    }\n\n    // More code here defining functions related to the shaping logic...\n};\n\n// Remaining contents of the harfbuzz.zig file...",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.1164452566715869,
      "diff_lines": 1244,
      "added_lines": 5,
      "removed_lines": 1187,
      "diff_preview": "--- expected+++ predicted@@ -3,10 +3,6 @@ const Allocator = std.mem.Allocator;\n const harfbuzz = @import(\"harfbuzz\");\n const font = @import(\"../main.zig\");\n-const terminal = @import(\"../../terminal/main.zig\");\n-const Feature = font.shape.Feature;\n-const FeatureList = font.shape.FeatureList;\n-const default_features = font.shape.default_features;\n const Face = font.Face;\n const Collection = font.Collection;\n const DeferredFace = font.DeferredFace;\n@@ -14,10 +10,10 @@ const SharedGrid = font.SharedGrid;\n const Style = font.Style;\n const Presentation = font.Presentation;\n+const terminal = @import(\"../../terminal/main.zig\");\n \n const log = std.log.scoped(.font_shaper);\n \n-/// Shaper that uses Harfbuzz.\n pub const Shaper = struct {\n     /// The allocated used for the feature list and cell buf.\n     alloc: Allocator,\n@@ -34,18 +30,14 @@ \n     const CellBuf = std.ArrayListUnmanaged(font.shape.Cell);\n \n-    /// The cell_buf argument is the buffer to use for storing shaped results.\n-    /// This should be at least the number of columns in the terminal.\n     pub fn init(alloc: Allocator, opts: font.shape.Options) !Shaper {\n-        // Parse all the features we want to use.\n         const hb_feats = hb_feats: {\n-            var feature_list: FeatureList = .{};\n+            var feature_list: font.shape.FeatureList = .{};\n             defer feature_list.deinit(alloc);\n-            try feature_list.features.appendSlice(alloc, &default_features);\n+            try feature_list.features.appendSlice(alloc, &font.shape.default_features);\n             for (opts.features) |feature_str| {\n                 try feature_list.appendFromString(alloc, feature_str);\n             }\n-\n             var list = try alloc.alloc(harfbuzz.Feature, feature_list.features.items.len);\n             errdefer alloc.free(list);\n \n@@ -57,7 +49,6 @@                     .end = harfbuzz.c.HB_FEATURE_GLOBAL_END,\n                 };\n             }\n"
    },
    "llm_calls": 1,
    "elapsed_time": 22.627207279205322,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 66334,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_font_shaper_run.zig",
    "exact_match": false,
    "similarity": 0.7550967164846633,
    "diff_lines": 217,
    "predicted_output": "const std = @import(\"std\");\nconst assert = std.debug.assert;\nconst Allocator = std.mem.Allocator;\nconst font = @import(\"../main.zig\");\nconst shape = @import(\"../shape.zig\");\nconst terminal = @import(\"../../terminal/main.zig\");\n\n/// A single text run. A text run is only valid for one Shaper instance and\n/// until the next run is created. A text run never goes across multiple\n/// rows in a terminal, so it is guaranteed to always be one line.\npub const TextRun = struct {\n    /// A unique hash for this run. This can be used to cache the shaping\n    /// results. We don't provide a means to compare actual values if the\n    /// hash is the same, so we should continue to improve this hash to\n    /// lower the chance of hash collisions if they become a problem. If\n    /// there are hash collisions, it would result in rendering issues but\n    /// the core data would be correct.\n    hash: u64,\n\n    /// The offset in the row where this run started\n    offset: u16,\n\n    /// The total number of cells produced by this run.\n    cells: u16,\n\n    /// The font grid that built this run.\n    grid: *font.SharedGrid,\n\n    /// The font index to use for the glyphs of this run.\n    font_index: font.Collection.Index,\n};\n\n/// RunIterator is an iterator that yields text runs.\npub const RunIterator = struct {\n    hooks: font.Shaper.RunIteratorHook,\n    grid: *font.SharedGrid,\n    screen: *const terminal.Screen,\n    row: terminal.Pin,\n    selection: ?terminal.Selection = null,\n    cursor_x: ?usize = null,\n    i: usize = 0,\n\n    pub fn next(self: *RunIterator, alloc: Allocator) !?TextRun {\n        const cells = self.row.cells(.all);\n\n        // Trim the right side of a row that might be empty\n        const max: usize = max: {\n            var j: usize = self.row.lenCells();\n            while (j > 0) : (j -= 1) if (!self.row.getCell(j - 1).empty()) break;\n            break :max j;\n        };\n\n        // Invisible cells don't have any glyphs rendered,\n        // so we explicitly skip them in the shaping process.\n        while (self.i < max and\n            self.row.style(&cells[self.i]).flags.invisible)\n        {\n            self.i += 1;\n        }\n\n        // We're over at the max\n        if (self.i >= max) return null;\n\n        // Track the font for our current run\n        var current_font: font.Collection.Index = .{};\n\n        // Allow the hook to prepare\n        try self.hooks.prepare();\n\n        // Initialize our hash for this run.\n        var hasher = Hasher.init(0);\n\n        // Let's get our style that we'll expect for the run.\n        const style = self.row.style(&cells[self.i]);\n\n        // Go through cell by cell and accumulate while we build our run.\n        var j: usize = self.i;\n        while (j < max) : (j += 1) {\n            const cluster = j;\n            const cell = &cells[j];\n\n            // If our cell attributes are changing, then we split the run.\n            // This prevents a single glyph for \">=\" to be rendered with\n            // one color when the two components have different styling.\n            if (j > self.i) style: {\n                const prev_cell = cells[j - 1];\n\n                // If the prev cell and this cell are both plain\n                // codepoints then we check if they are commonly \"bad\"\n                // ligatures and spit the run if they are.\n                if (prev_cell.content_tag == .codepoint and\n                    cell.content_tag == .codepoint)\n                {\n                    const prev_cp = prev_cell.codepoint();\n                    switch (prev_cp) {\n                        // fl, fi\n                        'f' => {\n                            const cp = cell.codepoint();\n                            if (cp == 'l' or cp == 'i') break;\n                        },\n\n                        // st\n                        's' => {\n                            const cp = cell.codepoint();\n                            if (cp == 't') break;\n                        },\n\n                        else => {},\n                    }\n                }\n\n                // If the style is exactly the change then fast path out.\n                if (prev_cell.style_id == cell.style_id) break :style;\n\n                // The style is different. We allow differing background\n                // styles but any other change results in a new run.\n                const c1 = comparableStyle(style);\n                const c2 = comparableStyle(self.row.style(&cells[j]));\n                if (!c1.eql(c2)) break;\n            }\n\n            // Text runs break when font styles change so we need to get\n            // the proper style.\n            const font_style: font.Style = style: {\n                if (style.flags.bold) {\n                    if (style.flags.italic) break :style .bold_italic;\n                    break :style .bold;\n                }\n\n                if (style.flags.italic) break :style .italic;\n                break :style .regular;\n            };\n\n            // Determine the presentation format for this glyph.\n            const presentation: ?font.Presentation = if (cell.hasGrapheme()) p: {\n                // We only check the FIRST codepoint because I believe the\n                // presentation format must be directly adjacent to the codepoint.\n                const cps = self.row.grapheme(cell) orelse break :p null;\n                assert(cps.len > 0);\n                if (cps[0] == 0xFE0E) break :p .text;\n                if (cps[0] == 0xFE0F) break :p .emoji;\n                break :p null;\n            } else emoji: {\n                // The font grid indexForCodepoint we use below will do this\n                // automatically.\n                break :emoji null;\n            };\n\n            // Find a font that supports this codepoint. If none support this\n            // then the whole grapheme can't be rendered so we return null.\n            //\n            // We explicitly do not require the additional grapheme components\n            // to support the base presentation, since it is common for emoji\n            // fonts to support the base emoji with emoji presentation but not\n            // certain ZWJ-combined characters like the male and female signs.\n            const idx = try self.grid.getIndex(\n                alloc,\n                cell.codepoint(),\n                font_style,\n                presentation,\n            ) orelse return null;\n\n            // If our fonts are not equal, then we're done with our run.\n            if (idx.int() != current_font.int()) break;\n\n            // If we're a fallback character, add that and continue; we\n            // don't want to add the entire grapheme.\n            if (font_info.fallback) |cp| {\n                try self.addCodepoint(&hasher, cp, @intCast(cluster));\n                continue;\n            }\n\n            // Add all the codepoints for our grapheme\n            try self.addCodepoint(\n                &hasher,\n                if (cell.codepoint() == 0) ' ' else cell.codepoint(),\n                @intCast(cluster),\n            );\n            if (cell.hasGrapheme()) {\n                const cps = self.row.grapheme(cell).?;\n                for (cps) |cp| {\n                    // Do not send presentation modifiers\n                    if (cp == 0xFE0E or cp == 0xFE0F) continue;\n                    try self.addCodepoint(&hasher, cp, @intCast(cluster));\n                }\n            }\n        }\n\n        // Finalize our buffer\n        try self.hooks.finalize();\n\n        // Add our length to the hash as an additional mechanism to avoid collisions\n        autoHash(&hasher, j - self.i);\n\n        // Add our font index\n        autoHash(&hasher, current_font);\n\n        // Move our cursor. Must defer since we use self.i below.\n        defer self.i = j;\n\n        return TextRun{\n            .hash = hasher.final(),\n            .offset = @intCast(self.i),\n            .cells = @intCast(j - self.i),\n            .grid = self.grid,\n            .font_index = current_font,\n        };\n    }\n\n    fn addCodepoint(self: *RunIterator, hasher: anytype, cp: u32, cluster: u32) !void {\n        autoHash(hasher, cp);\n        autoHash(hasher, cluster);\n        try self.hooks.addCodepoint(cp, cluster);\n    }\n\n    /// Find a font index that supports the grapheme for the given cell,\n    /// or null if no such font exists.\n    ///\n    /// This is used to find a font that supports the entire grapheme.\n    /// We look for fonts that support each individual codepoint and then\n    /// find the common font amongst all candidates.\n    fn indexForCell(\n        self: *RunIterator,\n        alloc: Allocator,\n        cell: *terminal.Cell,\n        style: font.Style,\n        presentation: ?font.Presentation,\n    ) !?font.Collection.Index {\n        if (cell.isEmpty() or\n            cell.codepoint() == 0 or\n            cell.codepoint() == terminal.kitty.graphics.unicode.placeholder)\n        {\n            return try self.grid.getIndex(\n                alloc,\n                ' ',\n                style,\n                presentation,\n            );\n        }\n\n        // Get the font index for the primary codepoint.\n        const primary_cp: u32 = cell.codepoint();\n        const primary = try self.grid.getIndex(\n            alloc,\n            primary_cp,\n            style,\n            presentation,\n        ) orelse return null;\n\n        // Easy, and common: we aren't a multi-codepoint grapheme, so\n        // we just return whatever index for the cell codepoint.\n        if (!cell.hasGrapheme()) return primary;\n\n        // If this is a grapheme, we need to find a font that supports\n        // all of the codepoints in the grapheme.\n        const cps = self.row.grapheme(cell) orelse return primary;\n        var candidates = try std.ArrayList(font.Collection.Index).initCapacity(alloc, cps.len + 1);\n        defer candidates.deinit();\n        candidates.appendAssumeCapacity(primary);\n\n        for (cps) |cp| {\n            // Ignore Emoji ZWJs\n            if (cp == 0xFE0E or cp == 0xFE0F or cp == 0x200D) continue;\n\n            // Find a font that supports this codepoint. If none support this\n            // then the whole grapheme can't be rendered so we return null.\n            const idx = try self.grid.getIndex(\n                alloc,\n                cp,\n                style,\n                presentation,\n            ) orelse return null;\n            candidates.appendAssumeCapacity(idx);\n        }\n\n        // We need to find a candidate that has ALL of our codepoints\n        for (candidates.items) |idx| {\n            if (!self.grid.hasCodepoint(idx, primary_cp, presentation)) continue;\n            for (cps) |cp| {\n                // Ignore Emoji ZWJs\n                if (cp == 0xFE0E or cp == 0xFE0F or cp == 0x200D) continue;\n                if (!self.grid.hasCodepoint(idx, cp, presentation)) break;\n            } else {\n                // If the while completed, then we have a candidate that\n                // supports all of our codepoints.\n                return idx;\n            }\n        }\n\n        return null;\n    }\n};",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.7550967164846633,
      "diff_lines": 217,
      "added_lines": 19,
      "removed_lines": 133,
      "diff_preview": "--- expected+++ predicted@@ -4,8 +4,6 @@ const font = @import(\"../main.zig\");\n const shape = @import(\"../shape.zig\");\n const terminal = @import(\"../../terminal/main.zig\");\n-const autoHash = std.hash.autoHash;\n-const Hasher = std.hash.Wyhash;\n \n /// A single text run. A text run is only valid for one Shaper instance and\n /// until the next run is created. A text run never goes across multiple\n@@ -47,12 +45,9 @@ \n         // Trim the right side of a row that might be empty\n         const max: usize = max: {\n-            for (0..cells.len) |i| {\n-                const rev_i = cells.len - i - 1;\n-                if (!cells[rev_i].isEmpty()) break :max rev_i + 1;\n-            }\n-\n-            break :max 0;\n+            var j: usize = self.row.lenCells();\n+            while (j > 0) : (j -= 1) if (!self.row.getCell(j - 1).empty()) break;\n+            break :max j;\n         };\n \n         // Invisible cells don't have any glyphs rendered,\n@@ -83,28 +78,6 @@         while (j < max) : (j += 1) {\n             const cluster = j;\n             const cell = &cells[j];\n-\n-            // If we have a selection and we're at a boundary point, then\n-            // we break the run here.\n-            if (self.selection) |unordered_sel| {\n-                if (j > self.i) {\n-                    const sel = unordered_sel.ordered(self.screen, .forward);\n-                    const start_x = sel.start().x;\n-                    const end_x = sel.end().x;\n-\n-                    if (start_x > 0 and\n-                        j == start_x) break;\n-\n-                    if (end_x > 0 and\n-                        j == end_x + 1) break;\n-                }\n-            }\n-\n-            // If we're a spacer, then we ignore it\n-            switch (cell.wide) {\n"
    },
    "llm_calls": 1,
    "elapsed_time": 61.77132534980774,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 18593,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_font_sprite_canvas.zig",
    "exact_match": false,
    "similarity": 0.9727596741344195,
    "diff_lines": 64,
    "predicted_output": "const std = @import(\"std\");\nconst Allocator = std.mem.Allocator;\nconst z2d = @import(\"z2d\");\nconst font = @import(\"../main.zig\");\n\npub fn Point(comptime T: type) type {\n    return struct {\n        x: T,\n        y: T,\n    };\n}\n\npub fn Line(comptime T: type) type {\n    return struct {\n        p0: Point(T),\n        p1: Point(T),\n    };\n}\n\npub fn Box(comptime T: type) type {\n    return struct {\n        p0: Point(T),\n        p1: Point(T),\n\n        pub fn rect(self: Box(T)) Rect(T) {\n            const tl_x = @min(self.p0.x, self.p1.x);\n            const tl_y = @min(self.p0.y, self.p1.y);\n            const br_x = @max(self.p0.x, self.p1.x);\n            const br_y = @max(self.p0.y, self.p1.y);\n\n            return .{\n                .x = tl_x,\n                .y = tl_y,\n                .width = br_x - tl_x,\n                .height = br_y - tl_y,\n            };\n        }\n    };\n}\n\npub fn Rect(comptime T: type) type {\n    return struct {\n        x: T,\n        y: T,\n        width: T,\n        height: T,\n    };\n}\n\npub fn Triangle(comptime T: type) type {\n    return struct {\n        p0: Point(T),\n        p1: Point(T),\n        p2: Point(T),\n    };\n}\n\npub fn Quad(comptime T: type) type {\n    return struct {\n        p0: Point(T),\n        p1: Point(T),\n        p2: Point(T),\n        p3: Point(T),\n    };\n}\n\n/// We only use alpha-channel so a pixel can only be \"on\" or \"off\".\npub const Color = enum(u8) {\n    on = 255,\n    off = 0,\n    _,\n};\n\n/// This is a managed struct, it keeps a reference to the allocator that is\n/// used to initialize it, and the same allocator is used for any further\n/// necessary allocations when drawing.\npub const Canvas = struct {\n    /// The underlying z2d surface.\n    sfc: z2d.Surface,\n    alloc: Allocator,\n\n    pub fn init(alloc: Allocator, width: u32, height: u32) !Canvas {\n        const sfc = try z2d.Surface.initPixel(\n            .{ .alpha8 = .{ .a = 0 } },\n            alloc,\n            @intCast(width),\n            @intCast(height),\n        );\n        errdefer sfc.deinit(alloc);\n\n        return .{ .sfc = sfc, .alloc = alloc };\n    }\n\n    pub fn deinit(self: *Canvas) void {\n        self.sfc.deinit(self.alloc);\n        self.* = undefined;\n    }\n\n    /// Write the data in this drawing to the atlas.\n    pub fn writeAtlas(self: *Canvas, alloc: Allocator, atlas: *font.Atlas) !font.Atlas.Region {\n        assert(atlas.format == .grayscale);\n\n        const width = @as(u32, @intCast(self.sfc.getWidth()));\n        const height = @as(u32, @intCast(self.sfc.getHeight()));\n\n        // Allocate our texture atlas region\n        const region = region: {\n            // We need to add a 1px padding to the font so that we don't\n            // get fuzzy issues when blending textures.\n            const padding = 1;\n\n            // Get the full padded region\n            var region = try atlas.reserve(\n                alloc,\n                width + (padding * 2), // * 2 because left+right\n                height + (padding * 2), // * 2 because top+bottom\n            );\n\n            // Modify the region so that we remove the padding so that\n            // we write to the non-zero location. The data in an Atlas\n            // is always initialized to zero (Atlas.clear) so we don't\n            // need to worry about zero-ing that.\n            region.x += padding;\n            region.y += padding;\n            region.width -= padding * 2;\n            region.height -= padding * 2;\n            break :region region;\n        };\n\n        if (region.width > 0 and region.height > 0) {\n            const buffer: []u8 = @ptrCast(self.sfc.image_surface_alpha8.buf);\n            // Write the glyph information into the atlas\n            assert(region.width == width);\n            assert(region.height == height);\n            atlas.set(region, buffer);\n        }\n\n        return region;\n    }\n\n    /// Draw and fill a single pixel\n    pub fn pixel(self: *Canvas, x: u32, y: u32, color: Color) void {\n        self.sfc.putPixel(\n            @intCast(x),\n            @intCast(y),\n            .{ .alpha8 = .{ .a = @intFromEnum(color) } },\n        );\n    }\n\n    /// Draw and fill a rectangle. This is the main primitive for drawing\n    /// lines as well (which are just generally skinny rectangles...)\n    pub fn rect(self: *Canvas, v: Rect(u32), color: Color) void {\n        const x0 = v.x;\n        const x1 = v.x + v.width;\n        const y0 = v.y;\n        const y1 = v.y + v.height;\n\n        for (y0..y1) |y| {\n            for (x0..x1) |x| {\n                self.pixel(\n                    @intCast(x),\n                    @intCast(y),\n                    color,\n                );\n            }\n        }\n    }\n\n    /// Draw and fill a quad.\n    pub fn quad(self: *Canvas, q: Quad(f64), color: Color) !void {\n        var path: z2d.StaticPath(6) = .{};\n        path.init(); // nodes.len = 0\n\n        path.moveTo(q.p0.x, q.p0.y); // +1, nodes.len = 1\n        path.lineTo(q.p1.x, q.p1.y); // +1, nodes.len = 2\n        path.lineTo(q.p2.x, q.p2.y); // +1, nodes.len = 3\n        path.lineTo(q.p3.x, q.p3.y); // +1, nodes.len = 4\n        path.close(); // +2, nodes.len = 6\n\n        try z2d.painter.fill(\n            self.alloc,\n            &self.sfc,\n            &.{ .opaque_pattern = .{\n                .pixel = .{ .alpha8 = .{ .a = @intFromEnum(color) } },\n            } },\n            path.wrapped_path.nodes.items,\n            .{},\n        );\n    }\n\n    /// Draw and fill a triangle.\n    pub fn triangle(self: *Canvas, t: Triangle(f64), color: Color) !void {\n        var path: z2d.StaticPath(5) = .{};\n        path.init(); // nodes.len = 0\n\n        path.moveTo(t.p0.x, t.p0.y); // +1, nodes.len = 1\n        path.lineTo(t.p1.x, t.p1.y); // +1, nodes.len = 2\n        path.lineTo(t.p2.x, t.p2.y); // +1, nodes.len = 3\n        path.close(); // +2, nodes.len = 5\n\n        try z2d.painter.fill(\n            self.alloc,\n            &self.sfc,\n            &.{ .opaque_pattern = .{\n                .pixel = .{ .alpha8 = .{ .a = @intFromEnum(color) } },\n            } },\n            path.wrapped_path.nodes.items,\n            .{},\n        );\n    }\n\n    pub fn triangle_outline(self: *Canvas, t: Triangle(f64), thickness: f64, color: Color) !void {\n        var path: z2d.StaticPath(3) = .{};\n        path.init(); // nodes.len = 0\n\n        path.moveTo(t.p0.x, t.p0.y); // +1, nodes.len = 1\n        path.lineTo(t.p1.x, t.p1.y); // +1, nodes.len = 2\n        path.lineTo(t.p2.x, t.p2.y); // +1, nodes.len = 3\n\n        try z2d.painter.stroke(\n            self.alloc,\n            &self.sfc,\n            &.{ .opaque_pattern = .{\n                .pixel = .{ .alpha8 = .{ .a = @intFromEnum(color) } },\n            } },\n            path.wrapped_path.nodes.items,\n            .{\n                .line_cap_mode = .round,\n                .line_width = thickness,\n            },\n        );\n    }\n\n    /// Stroke a line.\n    pub fn line(self: *Canvas, l: Line(f64), thickness: f64, color: Color) !void {\n        var path: z2d.StaticPath(2) = .{};\n        path.init(); // nodes.len = 0\n\n        path.moveTo(l.p0.x, l.p0.y); // +1, nodes.len = 1\n        path.lineTo(l.p1.x, l.p1.y); // +1, nodes.len = 2\n\n        try z2d.painter.stroke(\n            self.alloc,\n            &self.sfc,\n            &.{ .opaque_pattern = .{\n                .pixel = .{ .alpha8 = .{ .a = @intFromEnum(color) } },\n            } },\n            path.wrapped_path.nodes.items,\n            .{\n                .line_cap_mode = .round,\n                .line_width = thickness,\n            },\n        );\n    }\n\n    pub fn invert(self: *Canvas) void {\n        for (std.mem.sliceAsBytes(self.sfc.image_surface_alpha8.buf)) |*v| {\n            v.* = 255 - v.*;\n        }\n    }\n};",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9727596741344195,
      "diff_lines": 64,
      "added_lines": 2,
      "removed_lines": 17,
      "diff_preview": "--- expected+++ predicted@@ -1,7 +1,4 @@-//! This exposes primitives to draw 2D graphics and export the graphic to\n-//! a font atlas.\n const std = @import(\"std\");\n-const assert = std.debug.assert;\n const Allocator = std.mem.Allocator;\n const z2d = @import(\"z2d\");\n const font = @import(\"../main.zig\");\n@@ -80,11 +77,9 @@ pub const Canvas = struct {\n     /// The underlying z2d surface.\n     sfc: z2d.Surface,\n-\n     alloc: Allocator,\n \n     pub fn init(alloc: Allocator, width: u32, height: u32) !Canvas {\n-        // Create the surface we'll be using.\n         const sfc = try z2d.Surface.initPixel(\n             .{ .alpha8 = .{ .a = 0 } },\n             alloc,\n@@ -102,11 +97,7 @@     }\n \n     /// Write the data in this drawing to the atlas.\n-    pub fn writeAtlas(\n-        self: *Canvas,\n-        alloc: Allocator,\n-        atlas: *font.Atlas,\n-    ) (Allocator.Error || font.Atlas.Error)!font.Atlas.Region {\n+    pub fn writeAtlas(self: *Canvas, alloc: Allocator, atlas: *font.Atlas) !font.Atlas.Region {\n         assert(atlas.format == .grayscale);\n \n         const width = @as(u32, @intCast(self.sfc.getWidth()));\n@@ -126,7 +117,7 @@             );\n \n             // Modify the region so that we remove the padding so that\n-            // we write to the non-zero location. The data in an Altlas\n+            // we write to the non-zero location. The data in an Atlas\n             // is always initialized to zero (Atlas.clear) so we don't\n             // need to worry about zero-ing that.\n             region.x += padding;\n@@ -138,7 +129,6 @@ \n         if (region.width > 0 and region.height > 0) {\n             const buffer: []u8 = @ptrCast(self.sfc.image_surface_alpha8.buf);\n-\n             // Write the glyph information into the atlas\n"
    },
    "llm_calls": 1,
    "elapsed_time": 60.548935890197754,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 25637,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_font_sprite_underline.zig",
    "exact_match": false,
    "similarity": 0.6717907394780884,
    "diff_lines": 281,
    "predicted_output": "//! This file renders underline sprites. To draw underlines, we render the\n//! full cell-width as a sprite and then draw it as a separate pass to the\n//! text.\n//!\n//! We used to render the underlines directly in the GPU shaders but its\n//! annoying to support multiple types of underlines and its also annoying\n//! to maintain and debug another set of shaders for each renderer instead of\n//! just relying on the glyph system we already need to support for text\n//! anyways.\nconst std = @import(\"std\");\nconst builtin = @import(\"builtin\");\nconst assert = std.debug.assert;\nconst Allocator = std.mem.Allocator;\nconst font = @import(\"../main.zig\");\nconst Sprite = font.sprite.Sprite;\n\npub fn renderGlyph(\n    alloc: Allocator,\n    atlas: *font.Atlas,\n    sprite: Sprite,\n    width: u32,\n    height: u32,\n    line_pos: u32,\n    line_thickness: u32,\n) !font.Glyph {\n    var canvas: font.sprite.Canvas, const offset_y: i32 = switch (sprite) {\n        .underline => try drawSingle(alloc, width, line_thickness),\n        .underline_double => try drawDouble(alloc, width, line_thickness),\n        .underline_dotted => try drawDotted(alloc, width, line_thickness),\n        .underline_dashed => try drawDashed(alloc, width, line_thickness),\n        .underline_curly => try drawCurly(alloc, width, line_thickness),\n        .overline => try drawSingle(alloc, width, line_thickness),\n        .strikethrough => try drawSingle(alloc, width, line_thickness),\n        else => unreachable,\n    };\n    \n    defer canvas.deinit();\n\n    const region = try canvas.writeAtlas(alloc, atlas);\n\n    .offset_y = @as(i32, @intCast(height -| line_pos)) - offset_y,\n    .atlas_x = region.x,\n    .atlas_y = region.y,\n    .advance_x = @floatFromInt(width),\n};\n\nfn drawSingle(alloc: Allocator, width: u32, thickness: u32) !CanvasAndOffset {\n    const height: u32 = thickness;\n    var canvas = try font.sprite.Canvas.init(alloc, width, height);\n\n    canvas.rect(.{\n        .x = 0,\n        .y = 0,\n        .width = width,\n        .height = thickness,\n    }, .on);\n\n    const offset_y: i32 = 0;\n\n    return .{ canvas, offset_y };\n}\n\nfn drawDouble(alloc: Allocator, width: u32, thickness: u32) !CanvasAndOffset {\n    const gap = @max(2, thickness);\n    const height: u32 = thickness * 2 * gap;\n    var canvas = try font.sprite.Canvas.init(alloc, width, height);\n\n    canvas.rect(.{\n        .x = 0,\n        .y = 0,\n        .width = width,\n        .height = thickness,\n    }, .on);\n\n    canvas.rect(.{\n        .x = 0,\n        .y = @intCast(thickness + gap),\n        .width = width,\n        .height = thickness,\n    }, .on);\n\n    const offset_y: i32 = -@as(i32, @intCast(thickness));\n\n    return .{ canvas, offset_y };\n}\n\nfn drawDotted(alloc: Allocator, width: u32, thickness: u32) !CanvasAndOffset {\n    const height: u32 = thickness;\n    var canvas = try font.sprite.Canvas.init(alloc, width, height);\n\n    const dot_width = @max(thickness, 3);\n    const dot_count = @max((width / dot_width) / 2, 1);\n    const gap_width = try std.math.divCeil(u32, width -| (dot_count * dot_width), dot_count);\n    var i: u32 = 0;\n    while (i < dot_count) : (i += 1) {\n        const x = @min(i * (dot_width + gap_width), width - 1);\n        const rect_width = @min(width - x, dot_width);\n        canvas.rect(.{\n            .x = @intCast(x),\n            .y = 0,\n            .width = rect_width,\n            .height = thickness,\n        }, .on);\n    }\n\n    const offset_y: i32 = 0;\n\n    return .{ canvas, offset_y };\n}\n\nfn drawDashed(alloc: Allocator, width: u32, thickness: u32) !CanvasAndOffset {\n    const height: u32 = thickness;\n    var canvas = try font.sprite.Canvas.init(alloc, width, height);\n\n    const dash_width = width / 3 + 1;\n    const dash_count = (width / dash_width) + 1;\n    var i: u32 = 0;\n    while (i < dash_count) : (i += 2) {\n        const x = @min(i * dash_width, width - 1);\n        const rect_width = @min(width - x, dash_width);\n        canvas.rect(.{\n            .x = @intCast(x),\n            .y = 0,\n            .width = rect_width,\n            .height = thickness,\n        }, .on);\n    }\n\n    const offset_y: i32 = 0;\n\n    return .{ canvas, offset_y };\n}\n\nfn drawCurly(alloc: Allocator, width: u32, thickness: u32) !CanvasAndOffset {\n    const float_width: f64 = @floatFromInt(width);\n    const float_thick: f64 = @max(0.414, @floatFromInt(thickness -| 1));\n\n    const wave_period = 2 * std.math.pi / float_width;\n\n    const half_amplitude = 1.0 / wave_period;\n    const y_mid: f64 = half_amplitude + float_thick * 0.5 + 1;\n    const height: u32 = @intFromFloat(@ceil(half_amplitude + float_thick + 1) * 2);\n\n    var canvas = try font.sprite.Canvas.init(alloc, width, height);\n\n    var x: u32 = 0;\n    while (x < width) : (x += 1) {\n        const t: f64 = (@as(f64, @floatFromInt(x)) + 0.5) * wave_period;\n        const cosx: f64 = @cos(t);\n        const y: f64 = y_mid + half_amplitude * cosx;\n        const y_upper: u32 = @intFromFloat(@floor(y));\n        const y_lower: u32 = @intFromFloat(@ceil(y + float_thick));\n        const alpha_u: u8 = @intFromFloat(255 * @abs(y - @floor(y)));\n        const alpha_l: u8 = @intFromFloat(255 * @abs(y + float_thick - @ceil(y)));\n\n        canvas.pixel(x, @min(y_upper, height - 1), @enumFromInt(alpha_u));\n        canvas.pixel(x, @min(y_lower, height - 1), @enumFromInt(alpha_l));\n\n        var y_fill: u32 = y_upper + 1;\n        while (y_fill < y_lower) : (y_fill += 1) {\n            canvas.pixel(x, @min(y_fill, height - 1), .on);\n        }\n    }\n\n    const offset_y: i32 = @intFromFloat(-@round(half_amplitude));\n\n    return .{ canvas, offset_y };\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.6717907394780884,
      "diff_lines": 281,
      "added_lines": 14,
      "removed_lines": 158,
      "diff_preview": "--- expected+++ predicted@@ -7,9 +7,6 @@ //! to maintain and debug another set of shaders for each renderer instead of\n //! just relying on the glyph system we already need to support for text\n //! anyways.\n-//!\n-//! This also renders strikethrough, so its really more generally a\n-//! \"horizontal line\" renderer.\n const std = @import(\"std\");\n const builtin = @import(\"builtin\");\n const assert = std.debug.assert;\n@@ -17,7 +14,6 @@ const font = @import(\"../main.zig\");\n const Sprite = font.sprite.Sprite;\n \n-/// Draw an underline.\n pub fn renderGlyph(\n     alloc: Allocator,\n     atlas: *font.Atlas,\n@@ -27,7 +23,6 @@     line_pos: u32,\n     line_thickness: u32,\n ) !font.Glyph {\n-    // Draw the appropriate sprite\n     var canvas: font.sprite.Canvas, const offset_y: i32 = switch (sprite) {\n         .underline => try drawSingle(alloc, width, line_thickness),\n         .underline_double => try drawDouble(alloc, width, line_thickness),\n@@ -38,32 +33,17 @@         .strikethrough => try drawSingle(alloc, width, line_thickness),\n         else => unreachable,\n     };\n+    \n     defer canvas.deinit();\n \n-    // Write the drawing to the atlas\n     const region = try canvas.writeAtlas(alloc, atlas);\n \n-    return font.Glyph{\n-        .width = width,\n-        .height = @intCast(region.height),\n-        .offset_x = 0,\n-        // Glyph.offset_y is the distance between the top of the glyph and the\n-        // bottom of the cell. We want the top of the glyph to be at line_pos\n-        // from the TOP of the cell, and then offset by the offset_y from the\n-        // draw function.\n-        .offset_y = @as(i32, @intCast(height -| line_pos)) - offset_y,\n-        .atlas_x = region.x,\n-        .atlas_y = region.y,\n-        .advance_x = @floatFromInt(width),\n"
    },
    "llm_calls": 1,
    "elapsed_time": 46.778831005096436,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 27262,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_inspector_Inspector.zig",
    "exact_match": false,
    "similarity": 0.2990099389111243,
    "diff_lines": 1168,
    "predicted_output": "//! The Inspector is a development tool to debug the terminal. This is\n//! useful for terminal application developers as well as people potentially\n//! debugging issues in Ghostty itself.\nconst Inspector = @This();\n\nconst std = @import(\"std\");\nconst Allocator = std.mem.Allocator;\nconst builtin = @import(\"builtin\");\nconst cimgui = @import(\"cimgui\");\nconst Surface = @import(\"../Surface.zig\");\nconst input = @import(\"../input.zig\");\nconst terminal = @import(\"../terminal/main.zig\");\nconst inspector = @import(\"main.zig\");\n\n/// The window names. These are used with docking so we need to have access.\nconst window_cell = \"Cell\";\nconst window_modes = \"Modes\";\nconst window_keyboard = \"Keyboard\";\nconst window_termio = \"Terminal IO\";\nconst window_screen = \"Screen\";\nconst window_size = \"Surface Info\";\nconst window_imgui_demo = \"Dear ImGui Demo\";\n\n/// The surface that we're inspecting.\nsurface: *Surface,\n\n/// This is used to track whether we're rendering for the first time. This\n/// is used to set up the initial window positions.\nfirst_render: bool = true,\n\n/// Mouse state that we track in addition to normal mouse states that\n/// Ghostty always knows about.\nmouse: struct {\n    last_xpos: f64 = 0,\n    last_ypos: f64 = 0,\n    last_point: ?terminal.Pin = null,\n} = .{},\n\n/// A selected cell.\ncell: CellInspect = .{ .idle = {} },\n\n/// The list of keyboard events\nkey_events: inspector.key.EventRing,\n\n/// The VT stream\nvt_events: inspector.termio.VTEventRing,\nvt_stream: inspector.termio.Stream,\n\n/// The currently selected event sequence number for keyboard navigation\nselected_event_seq: ?u32 = null,\n\n/// Flag indicating whether we need to scroll to the selected item\nneed_scroll_to_selected: bool = false,\n\n/// Flag indicating whether the selection was made by keyboard\nis_keyboard_selection: bool = false,\n\n/// Enum representing keyboard navigation actions\nconst KeyAction = enum {\n    down,\n    none,\n    up,\n};\n\npub fn setup() void {\n    const io: *cimgui.c.ImGuiIO = cimgui.c.igGetIO();\n\n    // Enable docking, which we use heavily for the UI.\n    io.ConfigFlags |= cimgui.c.ImGuiConfigFlags_DockingEnable;\n\n    // Our colorspace is sRGB.\n    io.ConfigFlags |= cimgui.c.ImGuiConfigFlags_IsSRGB;\n\n    // Disable the ini file to save layout\n    io.IniFilename = null;\n    io.LogFilename = null;\n\n    // Use our own embedded font\n    {\n        const font_size = 16 * 2;\n\n        const font_config: *cimgui.c.ImFontConfig = cimgui.c.ImFontConfig_ImFontConfig();\n        defer cimgui.c.ImFontConfig_destroy(font_config);\n        font_config.FontDataOwnedByAtlas = false;\n        _ = cimgui.c.ImFontAtlas_AddFontFromMemoryTTF(\n            io.Fonts,\n            @constCast(@ptrCast(Surface.face_ttf)),\n            Surface.face_ttf.len,\n            font_size,\n            font_config,\n            null,\n        );\n    }\n}\n\npub fn init(surface: *Surface) !Inspector {\n    var key_buf = try inspector.key.EventRing.init(surface.alloc, 2);\n    errdefer key_buf.deinit(surface.alloc);\n\n    var vt_events = try inspector.termio.VTEventRing.init(surface.alloc, 2);\n    errdefer vt_events.deinit(surface.alloc);\n\n    var vt_handler = inspector.termio.VTHandler.init(surface);\n    errdefer vt_handler.deinit();\n\n    return .{\n        .surface = surface,\n        .key_events = key_buf,\n        .vt_events = vt_events,\n        .vt_stream = .{\n            .handler = vt_handler,\n            .parser = .{\n                .osc_parser = .{\n                    .alloc = surface.alloc,\n                },\n            },\n        },\n    };\n}\n\npub fn deinit(self: *Inspector) void {\n    self.cell.deinit();\n\n    {\n        var it = self.key_events.iterator(.forward);\n        while (it.next()) |v| v.deinit(self.surface.alloc);\n        self.key_events.deinit(self.surface.alloc);\n    }\n\n    {\n        var it = self.vt_events.iterator(.forward);\n        while (it.next()) |v| v.deinit(self.surface.alloc);\n        self.vt_events.deinit(self.surface.alloc);\n        self.vt_stream.handler.deinit();\n        self.vt_stream.deinit();\n    }\n}\n\n/// Record data read from the pty.\npub fn recordPtyRead(self: *Inspector, data: []const u8) !void {\n    try self.vt_stream.nextSlice(data);\n}\n\n/// Render the frame.\npub fn render(self: *Inspector) void {\n    const dock_id = cimgui.c.igDockSpaceOverViewport(\n        cimgui.c.igGetMainViewport(),\n        cimgui.c.ImGuiDockNodeFlags_None,\n        null,\n    );\n\n    // Render all of our data. We hold the mutex for this duration. This is\n    // expensive but this is an initial implementation until it doesn't work\n    // anymore.\n    {\n        self.surface.renderer_state.mutex.lock();\n        defer self.surface.renderer_state.mutex.unlock();\n        self.renderScreenWindow();\n        self.renderModesWindow();\n        self.renderKeyboardWindow();\n        self.renderTermioWindow();\n        self.renderCellWindow();\n        self.renderSizeWindow();\n    }\n\n    // In debug we show the ImGui demo window so we can easily view available\n    // widgets and such.\n    if (builtin.mode == .Debug) {\n        var show: bool = true;\n        cimgui.c.igShowDemoWindow(&show);\n    }\n\n    // On first render we set up the layout. We can actually do this at\n    // the end of the frame, allowing the individual rendering to also\n    // observe the first render flag.\n    if (self.first_render) {\n        self.first_render = false;\n        self.setupLayout(dock_id);\n    }\n}\n\nfn setupLayout(self: *Inspector, dock_id_main: cimgui.c.ImGuiID) void {\n    _ = self;\n\n    // Our initial focus\n    cimgui.c.igSetWindowFocus_Str(window_screen);\n\n    // Setup our initial layout.\n    const dock_id: struct {\n        left: cimgui.c.ImGuiID,\n        right: cimgui.c.ImGuiID,\n    } = dock_id: {\n        var dock_id_left: cimgui.c.ImGuiID = undefined;\n        var dock_id_right: cimgui.c.ImGuiID = undefined;\n        _ = cimgui.c.igDockBuilderSplitNode(\n            dock_id_main,\n            cimgui.c.ImGuiDir_Left,\n            0.7,\n            &dock_id_left,\n            &dock_id_right,\n        );\n\n        break :dock_id .{\n            .left = dock_id_left,\n            .right = dock_id_right,\n        };\n    };\n\n    cimgui.c.igDockBuilderDockWindow(window_cell, dock_id.left);\n    cimgui.c.igDockBuilderDockWindow(window_modes, dock_id.left);\n    cimgui.c.igDockBuilderDockWindow(window_keyboard, dock_id.left);\n    cimgui.c.igDockBuilderDockWindow(window_termio, dock_id.left);\n    cimgui.c.igDockBuilderDockWindow(window_screen, dock_id.left);\n    cimgui.c.igDockBuilderDockWindow(window_imgui_demo, dock_id.left);\n    cimgui.c.igDockBuilderDockWindow(window_size, dock_id.right);\n}\n\nfn renderScreenWindow(self: *Inspector) void {\n    // Start our window. If we're collapsed we do nothing.\n    defer cimgui.c.igEnd();\n    if (!cimgui.c.igBegin(\n        window_screen,\n        null,\n        cimgui.c.ImGuiWindowFlags_NoFocusOnAppearing,\n    )) return;\n\n    // More rendering code...\n\n    if (cimgui.c.igCollapsingHeader_TreeNodeFlags(\n        \"Internal Terminal State\",\n        cimgui.c.ImGuiTreeNodeFlags_DefaultOpen,\n    )) {\n        const pages = &self.surface.renderer_state.terminal.pages;\n\n        {\n            _ = cimgui.c.igBeginTable(\n                \"##terminal_state\",\n                2,\n                cimgui.c.ImGuiTableFlags_None,\n                .{ .x = 0, .y = 0 },\n                0,\n            );\n            defer cimgui.c.igEndTable();\n\n            {\n                cimgui.c.igTableNextRow(cimgui.c.ImGuiTableRowFlags_None, 0);\n                {\n                    _ = cimgui.c.igTableSetColumnIndex(0);\n                    cimgui.c.igText(\"Memory Usage\");\n                }\n                {\n                    _ = cimgui.c.igTableSetColumnIndex(1);\n                    cimgui.c.igText(\"%d bytes (%d KiB)\", pages.page_size, units.toKibiBytes(pages.page_size));\n                }\n            }\n        } // table\n\n        if (cimgui.c.igCollapsingHeader_TreeNodeFlags(\n            \"Active Page\",\n            cimgui.c.ImGuiTreeNodeFlags_DefaultOpen,\n        )) {\n            inspector.page.render(&pages.pages.last.?.data);\n        }\n    } // terminal state\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.2990099389111243,
      "diff_lines": 1168,
      "added_lines": 5,
      "removed_lines": 1075,
      "diff_preview": "--- expected+++ predicted@@ -4,17 +4,13 @@ const Inspector = @This();\n \n const std = @import(\"std\");\n-const assert = std.debug.assert;\n const Allocator = std.mem.Allocator;\n const builtin = @import(\"builtin\");\n const cimgui = @import(\"cimgui\");\n const Surface = @import(\"../Surface.zig\");\n-const font = @import(\"../font/main.zig\");\n const input = @import(\"../input.zig\");\n-const renderer = @import(\"../renderer.zig\");\n const terminal = @import(\"../terminal/main.zig\");\n const inspector = @import(\"main.zig\");\n-const units = @import(\"units.zig\");\n \n /// The window names. These are used with docking so we need to have access.\n const window_cell = \"Cell\";\n@@ -35,11 +31,8 @@ /// Mouse state that we track in addition to normal mouse states that\n /// Ghostty always knows about.\n mouse: struct {\n-    /// Last hovered x/y\n     last_xpos: f64 = 0,\n     last_ypos: f64 = 0,\n-\n-    // Last hovered screen point\n     last_point: ?terminal.Pin = null,\n } = .{},\n \n@@ -69,62 +62,6 @@     up,\n };\n \n-const CellInspect = union(enum) {\n-    /// Idle, no cell inspection is requested\n-    idle: void,\n-\n-    /// Requested, a cell is being picked.\n-    requested: void,\n-\n-    /// The cell has been picked and set to this. This is a copy so that\n-    /// if the cell contents change we still have the original cell.\n-    selected: Selected,\n-\n-    const Selected = struct {\n-        alloc: Allocator,\n-        row: usize,\n"
    },
    "llm_calls": 1,
    "elapsed_time": 53.0739221572876,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 34432,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_main.zig",
    "exact_match": false,
    "similarity": 0.088415325323056,
    "diff_lines": 130,
    "predicted_output": "const std = @import(\"std\");\nconst builtin = @import(\"builtin\");\nconst build_config = @import(\"build_config.zig\");\nconst glfw = @import(\"glfw\");\nconst macos = @import(\"macos\");\nconst oni = @import(\"oniguruma\");\nconst cli = @import(\"cli.zig\");\nconst internal_os = @import(\"os/main.zig\");\nconst xev = @import(\"xev\");\nconst fontconfig = @import(\"fontconfig\");\nconst harfbuzz = @import(\"harfbuzz\");\nconst renderer = @import(\"renderer.zig\");\nconst apprt = @import(\"apprt.zig\");\n\npub var state: GlobalState = undefined;\n\npub const main = switch (build_config.exe_entrypoint) {\n    .ghostty => @import(\"main_ghostty.zig\"),\n    .helpgen => @import(\"helpgen.zig\"),\n    .mdgen_ghostty_1 => @import(\"build/mdgen/main_ghostty_1.zig\"),\n    .mdgen_ghostty_5 => @import(\"build/mdgen/main_ghostty_5.zig\"),\n    .webgen_config => @import(\"build/webgen/main_config.zig\"),\n    .webgen_actions => @import(\"build/webgen/main_actions.zig\"),\n    .webgen_commands => @import(\"build/webgen/main_commands.zig\"),\n};\n\npub const GlobalState = struct {\n    const GPA = std.heap.GeneralPurposeAllocator(.{});\n\n    gpa: ?GPA,\n    alloc: std.mem.Allocator,\n    action: ?cli.Action,\n    logging: Logging,\n\n    pub const Logging = union(enum) {\n        disabled: void,\n        stderr: void,\n    };\n\n    pub fn init(self: *GlobalState) !void {\n        self.* = .{\n            .gpa = null,\n            .alloc = undefined,\n            .action = null,\n            .logging = .{ .stderr = {} },\n        };\n        errdefer self.deinit();\n\n        self.gpa = gpa: {\n            if (builtin.link_libc) {\n                if (switch (builtin.mode) {\n                    .ReleaseSafe, .ReleaseFast => true,\n                    else => std.valgrind.runningOnValgrind() > 0,\n                }) break :gpa null;\n            }\n\n            break :gpa GPA{};\n        };\n\n        self.alloc = if (self.gpa) |*value|\n            value.allocator()\n        else if (builtin.link_libc)\n            std.heap.c_allocator\n        else\n            unreachable;\n\n        self.action = try cli.Action.detectCLI(self.alloc);\n\n        if (self.action != null) self.logging = .{ .disabled = {} };\n\n        if (comptime build_config.app_runtime == .none) {\n            self.logging = .{ .disabled = {} };\n        }\n\n        if ((try internal_os.getenv(self.alloc, \"GHOSTTY_LOG\"))) |v| {\n            defer v.deinit(self.alloc);\n            if (v.value.len > 0) {\n                self.logging = .{ .stderr = {} };\n            }\n        }\n\n        try internal_os.ensureLocale(self.alloc);\n    }\n\n    pub fn deinit(self: *GlobalState) void {\n        if (self.gpa) |*value| {\n            _ = value.deinit();\n        }\n    }\n};\n\ntest {\n    _ = @import(\"circ_buf.zig\");\n    _ = @import(\"pty.zig\");\n    _ = @import(\"Command.zig\");\n    _ = @import(\"font/main.zig\");\n    _ = @import(\"apprt.zig\");\n    _ = @import(\"renderer.zig\");\n    _ = @import(\"termio.zig\");\n    _ = @import(\"input.zig\");\n    _ = @import(\"cli.zig\");\n    _ = @import(\"surface_mouse.zig\");\n\n    _ = @import(\"segmented_pool.zig\");\n    _ = @import(\"inspector/main.zig\");\n    _ = @import(\"terminal/main.zig\");\n    _ = @import(\"terminfo/main.zig\");\n    \n    _ = @import(\"blocking_queue.zig\");\n    _ = @import(\"config.zig\");\n    _ = @import(\"lru.zig\");\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.088415325323056,
      "diff_lines": 130,
      "added_lines": 96,
      "removed_lines": 15,
      "diff_preview": "--- expected+++ predicted@@ -1,8 +1,20 @@ const std = @import(\"std\");\n+const builtin = @import(\"builtin\");\n const build_config = @import(\"build_config.zig\");\n+const glfw = @import(\"glfw\");\n+const macos = @import(\"macos\");\n+const oni = @import(\"oniguruma\");\n+const cli = @import(\"cli.zig\");\n+const internal_os = @import(\"os/main.zig\");\n+const xev = @import(\"xev\");\n+const fontconfig = @import(\"fontconfig\");\n+const harfbuzz = @import(\"harfbuzz\");\n+const renderer = @import(\"renderer.zig\");\n+const apprt = @import(\"apprt.zig\");\n \n-/// See build_config.ExeEntrypoint for why we do this.\n-const entrypoint = switch (build_config.exe_entrypoint) {\n+pub var state: GlobalState = undefined;\n+\n+pub const main = switch (build_config.exe_entrypoint) {\n     .ghostty => @import(\"main_ghostty.zig\"),\n     .helpgen => @import(\"helpgen.zig\"),\n     .mdgen_ghostty_1 => @import(\"build/mdgen/main_ghostty_1.zig\"),\n@@ -10,22 +22,91 @@     .webgen_config => @import(\"build/webgen/main_config.zig\"),\n     .webgen_actions => @import(\"build/webgen/main_actions.zig\"),\n     .webgen_commands => @import(\"build/webgen/main_commands.zig\"),\n-    .bench_parser => @import(\"bench/parser.zig\"),\n-    .bench_stream => @import(\"bench/stream.zig\"),\n-    .bench_codepoint_width => @import(\"bench/codepoint-width.zig\"),\n-    .bench_grapheme_break => @import(\"bench/grapheme-break.zig\"),\n-    .bench_page_init => @import(\"bench/page-init.zig\"),\n };\n \n-/// The main entrypoint for the program.\n-pub const main = entrypoint.main;\n+pub const GlobalState = struct {\n+    const GPA = std.heap.GeneralPurposeAllocator(.{});\n \n-/// Standard options such as logger overrides.\n-pub const std_options: std.Options = if (@hasDecl(entrypoint, \"std_options\"))\n-    entrypoint.std_options\n-else\n-    .{};\n+    gpa: ?GPA,\n+    alloc: std.mem.Allocator,\n+    action: ?cli.Action,\n+    logging: Logging,\n"
    },
    "llm_calls": 1,
    "elapsed_time": 29.249746799468994,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 57778,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_os_flatpak.zig",
    "exact_match": false,
    "similarity": 0.7037777696978511,
    "diff_lines": 281,
    "predicted_output": "const std = @import(\"std\");\nconst assert = std.debug.assert;\nconst Allocator = std.mem.Allocator;\nconst posix = std.posix;\nconst xev = @import(\"../global.zig\").xev;\n\nconst log = std.log.scoped(.flatpak);\n\npub fn isFlatpak() bool {\n    // If we're not on Linux then we'll make this comptime false.\n    if (comptime builtin.os.tag != .linux) return false;\n    return if (std.fs.accessAbsolute(\"/.flatpak-info\", .{})) true else |_| false;\n}\n\n/// A struct to help execute commands on the host via the\n/// org.freedesktop.Flatpak.Development DBus module. This uses GIO/GLib\n/// under the hood.\n///\n/// This always spawns its own thread and maintains its own GLib event loop.\n/// This makes it easy for the command to behave synchronously similar to\n/// std.process.Child.\n///\n/// There are lots of chances for low-hanging improvements here (automatic\n/// pipes, /dev/null, etc.) but this was purpose built for my needs so\n/// it doesn't have all of those.\n///\n/// Requires GIO, GLib to be available and linked.\npub const FlatpakHostCommand = struct {\n    const fd_t = posix.fd_t;\n    const EnvMap = std.process.EnvMap;\n    const c = @cImport({\n        @cInclude(\"gio/gio.h\");\n        @cInclude(\"gio/gunixfdlist.h\");\n    });\n\n    /// Argv are the arguments to call on the host with argv[0] being\n    /// the command to execute.\n    argv: []const []const u8,\n\n    /// The cwd for the new process. If this is not set then it will use\n    /// the current cwd of the calling process.\n    cwd: ?[:0]const u8 = null,\n\n    /// Environment variables for the child process. If this is null, this\n    /// does not send any environment variables.\n    stdin: fd_t,\n    stdout: fd_t,\n    stderr: fd_t,\n\n    /// State of the process. This is updated by the dedicated thread it\n    /// runs in and is protected by the given lock and condition variable.\n    state: State = .{ .init = {} },\n    state_mutex: std.Thread.Mutex = .{},\n    state_cv: std.Thread.Condition = .{},\n\n    /// State the process is in. This can't be inspected directly, you\n    /// must use getters on the struct to get access.\n    const State = union(enum) {\n        /// Initial state\n        init: void,\n\n        /// Error starting. The error message is only available via logs.\n        /// (This isn't a fundamental limitation, just didn't need the\n        /// error message yet)\n        err: void,\n\n        /// Process started with the given pid on the host.\n        started: struct {\n            pid: u32,\n            loop_xev: ?*xev.Loop,\n            completion: ?*Completion,\n            subscription: c.guint,\n            loop: *c.GMainLoop,\n        },\n\n        /// Process exited\n        exited: struct {\n            pid: u32,\n            status: u8,\n        },\n    };\n\n    pub const Completion = struct {\n        callback: *const fn (ud: ?*anyopaque, l: *xev.Loop, c: *Completion, r: WaitError!u8) void = noopCallback,\n        c_xev: xev.Completion = .{},\n        userdata: ?*anyopaque = null,\n        timer: ?xev.Timer = null,\n        result: ?WaitError!u8 = null,\n    };\n\n    /// Errors that are possible from us.\n    pub const Error = error{\n        FlatpakMustBeStarted,\n        FlatpakSpawnFail,\n        FlatpakSetupFail,\n        FlatpakRPCFail,\n    };\n\n    pub const WaitError = xev.Timer.RunError || Error;\n\n    /// Spawn the command. This will start the host command. On return,\n    /// the pid will be available. This must only be called with the\n    /// state in \"init\".\n    ///\n    /// Precondition: The self pointer MUST be stable.\n    pub fn spawn(self: *FlatpakHostCommand, alloc: Allocator) !u32 {\n        const thread = try std.Thread.spawn(.{}, threadMain, .{ self, alloc });\n        thread.setName(\"flatpak-host-command\") catch {};\n\n        // Wait for the process to start or error.\n        self.state_mutex.lock();\n        defer self.state_mutex.unlock();\n        while (self.state == .init) self.state_cv.wait(&self.state_mutex);\n\n        return switch (self.state) {\n            .init => unreachable,\n            .err => Error.FlatpakSpawnFail,\n            .started => |v| v.pid,\n            .exited => |v| v.pid,\n        };\n    }\n\n    /// Wait for the process to end asynchronously via libxev. This\n    /// can only be called ONCE.\n    pub fn waitXev(\n        self: *FlatpakHostCommand,\n        loop: *xev.Loop,\n        completion: *Completion,\n        comptime Userdata: type,\n        userdata: ?*Userdata,\n        comptime cb: *const fn (\n            ud: ?*Userdata,\n            l: *xev.Loop,\n            c: *Completion,\n            r: WaitError!u8,\n        ) void,\n    ) void {\n        self.state_mutex.lock();\n        defer self.state_mutex.unlock();\n\n        completion.* = .{\n            .callback = (struct {\n                fn callback(\n                    ud_: ?*anyopaque,\n                    l_inner: *xev.Loop,\n                    c_inner: *Completion,\n                    r: WaitError!u8,\n                ) void {\n                    const ud = @as(?*Userdata, if (Userdata == void) null else @ptrCast(@alignCast(ud_)));\n                    @call(.always_inline, cb, .{ ud, l_inner, c_inner, r });\n                }\n            }).callback,\n            .userdata = userdata,\n            .timer = xev.Timer.init() catch unreachable, // not great, but xev timer can't fail atm\n        };\n\n        switch (self.state) {\n            .init => completion.result = Error.FlatpakMustBeStarted,\n            .err => completion.result = Error.FlatpakSpawnFail,\n            .started => |*v| {\n                v.loop_xev = loop;\n                v.completion = completion;\n                return;\n            },\n            .exited => |v| {\n                completion.result = v.status;\n            },\n        }\n\n        completion.timer.?.run(\n            loop,\n            &completion.c_xev,\n            0,\n            anyopaque,\n            completion.userdata,\n            (struct {\n                fn callback(\n                    ud: ?*anyopaque,\n                    l_inner: *xev.Loop,\n                    c_inner: *xev.Completion,\n                    r: xev.Timer.RunError!void,\n                ) xev.CallbackAction {\n                    const c_outer: *Completion = @fieldParentPtr(\"c_xev\", c_inner);\n                    defer if (c_outer.timer) |*t| t.deinit();\n\n                    const result = if (r) |_| c_outer.result.? else |err| err;\n                    c_outer.callback(ud, l_inner, c_outer, result);\n                    return .disarm;\n                }\n            }).callback,\n        );\n    }\n\n    /// Send a signal to the started command. This does nothing if the\n    /// command is not in the started state.\n    pub fn signal(self: *FlatpakHostCommand, sig: u8, pg: bool) !void {\n        const pid = pid: {\n            self.state_mutex.lock();\n            defer self.state_mutex.unlock();\n            switch (self.state) {\n                .started => |v| break :pid v.pid,\n                else => return,\n            }\n        };\n\n        // Get our bus connection.\n        var g_err: [*c]c.GError = null;\n        const bus = c.g_bus_get_sync(c.G_BUS_TYPE_SESSION, null, &g_err) orelse {\n            log.warn(\"signal error getting bus: {s}\", .{g_err.*.message});\n            return Error.FlatpakSetupFail;\n        };\n        defer c.g_object_unref(bus);\n\n        const reply = c.g_dbus_connection_call_sync(\n            bus,\n            \"org.freedesktop.Flatpak\",\n            \"/org/freedesktop/Flatpak/Development\",\n            \"org.freedesktop.Flatpak.Development\",\n            \"HostCommandSignal\",\n            c.g_variant_new(\n                \"(uub)\",\n                pid,\n                sig,\n                @intCast(c_int, @boolToInt(pg)),\n            ),\n            c.G_VARIANT_TYPE(\"()\"),\n            c.G_DBUS_CALL_FLAGS_NONE,\n            c.G_MAXINT,\n            null,\n            &g_err,\n        );\n        if (g_err != null) {\n            log.warn(\"signal send error: {s}\", .{g_err.*.message});\n            return;\n        }\n        defer c.g_variant_unref(reply);\n    }\n    \n    fn threadMain(self: *FlatpakHostCommand, alloc: Allocator) void {\n        // Create a new thread-local context so that all our sources go\n        // to this context and we can run our loop correctly.\n        const ctx = c.g_main_context_new();\n        defer c.g_main_context_unref(ctx);\n        c.g_main_context_push_thread_default(ctx);\n        defer c.g_main_context_pop_thread_default(ctx);\n\n        // Get our loop for the current thread\n        const loop = c.g_main_loop_new(ctx, 1).?;\n        defer c.g_main_loop_unref(loop);\n\n        // Get our bus connection. This has to remain active until we exit\n        // the thread otherwise our signals won't be called.\n        var g_err: [*c]c.GError = null;\n        const bus = c.g_bus_get_sync(c.G_BUS_TYPE_SESSION, null, &g_err) orelse {\n            log.warn(\"spawn error getting bus: {s}\", .{g_err.*.message});\n            self.updateState(.{ .err = {} });\n            return;\n        };\n        defer c.g_object_unref(bus);\n\n        // Spawn the command first. This will setup all our IO.\n        self.start(alloc, bus, loop) catch |err| {\n            log.warn(\"error starting host command: {}\", .{err});\n            self.updateState(.{ .err = {} });\n            return;\n        };\n\n        // Run the event loop. It quits in the exit callback.\n        c.g_main_loop_run(loop);\n    }\n\n    /// Helper to update the state and notify waiters via the cv.\n    fn updateState(self: *FlatpakHostCommand, state: State) void {\n        self.state_mutex.lock();\n        defer self.state_mutex.unlock();\n        defer self.state_cv.broadcast();\n        self.state = state;\n    }\n\n    fn noopCallback(_: ?*anyopaque, _: *xev.Loop, _: *Completion, _: WaitError!u8) void {}\n};",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.7037777696978511,
      "diff_lines": 281,
      "added_lines": 2,
      "removed_lines": 224,
      "diff_preview": "--- expected+++ predicted@@ -1,13 +1,11 @@ const std = @import(\"std\");\n const assert = std.debug.assert;\n const Allocator = std.mem.Allocator;\n-const builtin = @import(\"builtin\");\n const posix = std.posix;\n const xev = @import(\"../global.zig\").xev;\n \n const log = std.log.scoped(.flatpak);\n \n-/// Returns true if we're running in a Flatpak environment.\n pub fn isFlatpak() bool {\n     // If we're not on Linux then we'll make this comptime false.\n     if (comptime builtin.os.tag != .linux) return false;\n@@ -45,10 +43,6 @@ \n     /// Environment variables for the child process. If this is null, this\n     /// does not send any environment variables.\n-    env: ?*const EnvMap = null,\n-\n-    /// File descriptors to send to the child process. It is up to the\n-    /// caller to create the file descriptors and set them up.\n     stdin: fd_t,\n     stdout: fd_t,\n     stderr: fd_t,\n@@ -124,28 +118,6 @@             .started => |v| v.pid,\n             .exited => |v| v.pid,\n         };\n-    }\n-\n-    /// Wait for the process to end and return the exit status. This\n-    /// can only be called ONCE. Once this returns, the state is reset.\n-    pub fn wait(self: *FlatpakHostCommand) !u8 {\n-        self.state_mutex.lock();\n-        defer self.state_mutex.unlock();\n-\n-        while (true) {\n-            switch (self.state) {\n-                .init => return Error.FlatpakMustBeStarted,\n-                .err => return Error.FlatpakSpawnFail,\n-                .started => {},\n-                .exited => |v| {\n-                    self.state = .{ .init = {} };\n-                    self.state_cv.broadcast();\n-                    return v.status;\n-                },\n-            }\n"
    },
    "llm_calls": 1,
    "elapsed_time": 69.11536002159119,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 11975,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_renderer_Thread.zig",
    "exact_match": false,
    "similarity": 0.37359611622346206,
    "diff_lines": 639,
    "predicted_output": "pub const Thread = @This();\n\nconst std = @import(\"std\");\nconst builtin = @import(\"builtin\");\nconst assert = std.debug.assert;\nconst xev = @import(\"../global.zig\").xev;\nconst crash = @import(\"../crash/main.zig\");\nconst internal_os = @import(\"../os/main.zig\");\nconst renderer = @import(\"../renderer.zig\");\nconst apprt = @import(\"../apprt.zig\");\nconst configpkg = @import(\"../config.zig\");\nconst BlockingQueue = @import(\"../datastruct/main.zig\").BlockingQueue;\nconst App = @import(\"../App.zig\");\n\nconst Allocator = std.mem.Allocator;\n\nconst CURSOR_BLINK_INTERVAL = 600;\n\n/// The type used for sending messages to the IO thread. For now this is\n/// hardcoded with a capacity. We can make this a comptime parameter in\n/// the future if we want it configurable.\npub const Mailbox = BlockingQueue(renderer.Message, 64);\n\n/// Allocator used for some state\nalloc: std.mem.Allocator,\n\n/// The main event loop for the application. The user data of this loop\n/// is always the allocator used to create the loop. This is a convenience\n/// so that users of the loop always have an allocator.\nloop: xev.Loop,\n\n/// This can be used to wake up the renderer and force a render safely from\n/// any thread.\nwakeup: xev.Async,\nwakeup_c: xev.Completion = .{},\n\n/// This can be used to stop the renderer on the next loop iteration.\nstop: xev.Async,\nstop_c: xev.Completion = .{},\n\n/// The timer used for rendering\nrender_h: xev.Timer,\nrender_c: xev.Completion = .{},\n\n/// The timer used for cursor blinking\ncursor_h: xev.Timer,\ncursor_c: xev.Completion = .{},\n\n/// The timer used for draw calls. Draw calls don't update from the\n/// terminal state so they're much cheaper. They're used for animation\n/// and are paused when the terminal is not focused.\ndraw_h: xev.Timer,\ndraw_c: xev.Completion = .{},\ndraw_active: bool = false,\n\n/// The surface we're rendering to.\nsurface: *apprt.Surface,\n\n/// The underlying renderer implementation.\nrenderer: *renderer.Renderer,\n\n/// Pointer to the shared state that is used to generate the final render.\nstate: *renderer.State,\n\n/// The mailbox that can be used to send this thread messages. Note\n/// this is a blocking queue so if it is full you will get errors (or block).\nmailbox: *Mailbox,\n\n/// Mailbox to send messages to the app thread\napp_mailbox: App.Mailbox,\n\nflags: packed struct {\n    /// This is true when a blinking cursor should be visible and false\n    /// when it should not be visible. This is toggled on a timer by the\n    /// thread automatically.\n    cursor_blink_visible: bool = false,\n\n    /// This is true when the inspector is active.\n    has_inspector: bool = false,\n\n    /// This is true when the view is visible. This is used to determine\n    /// if we should be rendering or not.\n    visible: bool = true,\n\n    /// This is true when the view is focused. This defaults to true\n    /// and it is up to the apprt to set the correct value.\n    focused: bool = true,\n} = .{},\n\npub const DerivedConfig = struct {\n    custom_shader_animation: configpkg.CustomShaderAnimation,\n\n    pub fn init(config: *const configpkg.Config) DerivedConfig {\n        return .{\n            .custom_shader_animation = config.@\"custom-shader-animation\",\n        };\n    }\n};\n\n/// Initialize the thread. This does not START the thread. This only sets\n/// up all the internal state necessary prior to starting the thread. It\n/// is up to the caller to start the thread with the threadMain entrypoint.\npub fn init(\n    alloc: Allocator,\n    config: *const configpkg.Config,\n    surface: *apprt.Surface,\n    renderer_impl: *renderer.Renderer,\n    state: *renderer.State,\n    app_mailbox: App.Mailbox,\n) !Thread {\n    // Create our event loop.\n    var loop = try xev.Loop.init(.{});\n    errdefer loop.deinit();\n\n    // Create our mailbox\n    var mailbox = try Mailbox.create(alloc);\n    errdefer mailbox.destroy(alloc);\n\n    return Thread{\n        .alloc = alloc,\n        .config = DerivedConfig.init(config),\n        .loop = loop,\n        .wakeup = wakeup_h,\n        .stop = stop_h,\n        .render_h = render_h,\n        .cursor_h = cursor_timer,\n        .draw_h = draw_h,\n        .surface = surface,\n        .renderer = renderer_impl,\n        .state = state,\n        .mailbox = mailbox,\n        .app_mailbox = app_mailbox,\n        .flags = .{ .seen = false },\n    };\n}\n\n/// Clean up the thread. This is only safe to call once the thread\n/// completes executing; the caller must join prior to this.\npub fn deinit(self: *Thread) void {\n    self.stop.deinit();\n    self.wakeup.deinit();\n    self.render_h.deinit();\n    self.cursor_h.deinit();\n    self.loop.deinit();\n}\n\n/// The main entrypoint for the thread.\npub fn threadMain(self: *Thread) void {\n    var has_loop = @hasDecl(renderer.Renderer, \"loopEnter\");\n    if (has_loop) try self.renderer.loopEnter(self);\n    defer if (has_loop) self.renderer.loopExit();\n\n    // Send an initial wakeup message so that we render right away.\n    try self.wakeup.notify();\n\n    // Run our loop.\n    log.debug(\"starting renderer thread\", .{});\n    _ = try self.loop.run(.until_done);\n}\n\n/// Drain the mailbox.\nfn drainMailbox(self: *Thread) !void {\n    while (self.mailbox.pop()) |message| {\n        switch (message) {\n            .crash => @panic(\"crash request, crashing intentionally\"),\n\n            .visible => |v| {\n                if (self.flags.visible == v) continue; // Only care about state changes\n                self.flags.visible = v;\n\n                // If we became visible then we immediately trigger a draw.\n                if (v) self.drawFrame(false);\n            },\n\n            .focus => |v| {\n                self.flags.focused = v;\n                try self.renderer.setFocus(v);\n            },\n        }\n    }\n}\n\n/// Trigger a draw. This will not update frame data or anything, it will\n/// just trigger a draw/paint.\nfn drawFrame(self: *Thread, now: bool) void {\n    // If we're invisible, we do not draw.\n    if (!self.flags.visible) return;\n\n    // If the renderer is managing a vsync on its own, we only draw\n    // when we're forced to via now.\n    if (!now and self.renderer.hasVsync()) return;\n\n    // If we're doing single-threaded GPU calls then we just wake up the\n    // app thread to redraw at this point.\n    if (renderer.Renderer == renderer.OpenGL and\n        renderer.OpenGL.single_threaded_draw)\n    {\n        _ = self.app_mailbox.push(\n            .{ .redraw_surface = self.surface },\n            .{ .instant = {} },\n        );\n    } else {\n        self.renderer.drawFrame(self.surface) catch |err| log.warn(\"error drawing err={}\", .{err});\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.37359611622346206,
      "diff_lines": 639,
      "added_lines": 23,
      "removed_lines": 485,
      "diff_preview": "--- expected+++ predicted@@ -1,5 +1,3 @@-//! Represents the renderer thread logic. The renderer thread is able to\n-//! be woken up to render.\n pub const Thread = @This();\n \n const std = @import(\"std\");\n@@ -8,22 +6,20 @@ const xev = @import(\"../global.zig\").xev;\n const crash = @import(\"../crash/main.zig\");\n const internal_os = @import(\"../os/main.zig\");\n-const rendererpkg = @import(\"../renderer.zig\");\n+const renderer = @import(\"../renderer.zig\");\n const apprt = @import(\"../apprt.zig\");\n const configpkg = @import(\"../config.zig\");\n const BlockingQueue = @import(\"../datastruct/main.zig\").BlockingQueue;\n const App = @import(\"../App.zig\");\n \n const Allocator = std.mem.Allocator;\n-const log = std.log.scoped(.renderer_thread);\n-\n-const DRAW_INTERVAL = 8; // 120 FPS\n+\n const CURSOR_BLINK_INTERVAL = 600;\n \n /// The type used for sending messages to the IO thread. For now this is\n /// hardcoded with a capacity. We can make this a comptime parameter in\n /// the future if we want it configurable.\n-pub const Mailbox = BlockingQueue(rendererpkg.Message, 64);\n+pub const Mailbox = BlockingQueue(renderer.Message, 64);\n \n /// Allocator used for some state\n alloc: std.mem.Allocator,\n@@ -45,6 +41,10 @@ /// The timer used for rendering\n render_h: xev.Timer,\n render_c: xev.Completion = .{},\n+\n+/// The timer used for cursor blinking\n+cursor_h: xev.Timer,\n+cursor_c: xev.Completion = .{},\n \n /// The timer used for draw calls. Draw calls don't update from the\n /// terminal state so they're much cheaper. They're used for animation\n@@ -53,24 +53,14 @@ draw_c: xev.Completion = .{},\n draw_active: bool = false,\n \n-/// This async is used to force a draw immediately. This does not\n"
    },
    "llm_calls": 1,
    "elapsed_time": 57.48223805427551,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 34445,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_renderer_metal_shaders.zig",
    "exact_match": false,
    "similarity": 0.3325478367423885,
    "diff_lines": 619,
    "predicted_output": "const std = @import(\"std\");\nconst Allocator = std.mem.Allocator;\nconst assert = std.debug.assert;\nconst macos = @import(\"macos\");\nconst objc = @import(\"objc\");\nconst math = @import(\"../../math.zig\");\n\nconst mtl = @import(\"api.zig\");\n\nconst log = std.log.scoped(.metal);\n\n/// This contains the state for the shaders used by the Metal renderer.\npub const Shaders = struct {\n    library: objc.Object,\n\n    /// Renders cell foreground elements (text, decorations).\n    cell_text_pipeline: objc.Object,\n\n    /// The cell background shader is the shader used to render the\n    /// background of terminal cells.\n    cell_bg_pipeline: objc.Object,\n\n    /// The image shader is the shader used to render images for things\n    /// like the Kitty image protocol.\n    image_pipeline: objc.Object,\n\n    /// Initialize our shader set.\n    ///\n    /// \"post_shaders\" is an optional list of postprocess shaders to run\n    /// against the final drawable texture. This is an array of shader source\n    /// code, not file paths.\n    pub fn init(\n        alloc: Allocator,\n        device: objc.Object,\n        post_shaders: []const [:0]const u8,\n        pixel_format: mtl.MTLPixelFormat,\n    ) !Shaders {\n        const library = try initLibrary(device);\n        errdefer library.msgSend(void, objc.sel(\"release\"), .{});\n\n        const cell_text_pipeline = try initCellTextPipeline(device, library, pixel_format);\n        errdefer cell_text_pipeline.msgSend(void, objc.sel(\"release\"), .{});\n\n        const cell_bg_pipeline = try initCellBgPipeline(device, library, pixel_format);\n        errdefer cell_bg_pipeline.msgSend(void, objc.sel(\"release\"), .{});\n\n        const image_pipeline = try initImagePipeline(device, library, pixel_format);\n        errdefer image_pipeline.msgSend(void, objc.sel(\"release\"), .{});\n\n        const post_pipelines: []const objc.Object = initPostPipelines(\n            alloc,\n            device,\n            library,\n            post_shaders,\n            pixel_format,\n        ) catch |err| err: {\n            // If an error happens while building postprocess shaders we\n            // want to just not use any postprocess shaders since we don't\n            // want to block Ghostty from working.\n            log.warn(\"error initializing postprocess shaders err={}\", .{err});\n            break :err &.{};\n        };\n        errdefer if (post_pipelines.len > 0) {\n            for (post_pipelines) |pipeline| pipeline.msgSend(void, objc.sel(\"release\"), .{});\n            alloc.free(post_pipelines);\n        };\n\n        return .{\n            .library = library,\n            .cell_text_pipeline = cell_text_pipeline,\n            .cell_bg_pipeline = cell_bg_pipeline,\n            .image_pipeline = image_pipeline,\n            .post_pipelines = post_pipelines,\n        };\n    }\n\n    pub fn deinit(self: *Shaders, alloc: Allocator) void {\n        // Release our primary shaders\n        self.cell_text_pipeline.msgSend(void, objc.sel(\"release\"), .{});\n        self.cell_bg_pipeline.msgSend(void, objc.sel(\"release\"), .{});\n        self.image_pipeline.msgSend(void, objc.sel(\"release\"), .{});\n        self.library.msgSend(void, objc.sel(\"release\"), .{});\n\n        // Release our postprocess shaders\n        if (self.post_pipelines.len > 0) {\n            for (self.post_pipelines) |pipeline| {\n                pipeline.msgSend(void, objc.sel(\"release\"), .{});\n            }\n            alloc.free(self.post_pipelines);\n        }\n    }\n};\n\n/// The uniforms that are passed to the terminal cell shader.\npub const Uniforms = extern struct {\n    /// The background color for the whole surface.\n    bg_color: [4]u8 align(4),\n\n    /// The cursor position and color.\n    cursor_pos: [2]u16 align(4),\n    cursor_color: [4]u8 align(4),\n\n    /// Whether the cursor is 2 cells wide.\n    cursor_wide: bool align(1),\n\n    /// Indicates that colors provided to the shader are already in\n    /// the P3 color space, so they don't need to be converted from\n    /// sRGB.\n    use_display_p3: bool align(1),\n\n    /// Indicates that the color attachments for the shaders have\n    /// an `*_srgb` pixel format, which means the shaders need to\n    /// output linear RGB colors rather than gamma encoded colors,\n    /// since blending will be performed in linear space and then\n    /// Metal itself will re-encode the colors for storage.\n    use_linear_blending: bool align(1),\n\n    /// Enables a weight correction step that makes text rendered\n    /// with linear alpha blending have a similar apparent weight\n    /// (thickness) to gamma-incorrect blending.\n    use_linear_correction: bool align(1) = false,\n\n    const PaddingExtend = packed struct(u8) {\n        left: bool = false,\n        right: bool = false,\n        up: bool = false,\n        down: bool = false,\n        _padding: u4 = 0,\n    };\n};\n\n/// Initialize the MTLLibrary. A MTLLibrary is a collection of shaders.\nfn initLibrary(device: objc.Object) !objc.Object {\n    const start = try std.time.Instant.now();\n\n    const data = try macos.dispatch.Data.create(\n        @embedFile(\"ghostty_metallib\"),\n        macos.dispatch.queue.getMain(),\n        macos.dispatch.Data.DESTRUCTOR_DEFAULT,\n    );\n    defer data.release();\n\n    var err: ?*anyopaque = null;\n    const library = device.msgSend(\n        objc.Object,\n        objc.sel(\"newLibraryWithData:error:\"),\n        .{ data, &err },\n    );\n    try checkError(err);\n\n    const end = try std.time.Instant.now();\n    log.debug(\"shader library loaded time={}us\", .{end.since(start) / std.time.ns_per_us});\n\n    return library;\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.3325478367423885,
      "diff_lines": 619,
      "added_lines": 3,
      "removed_lines": 575,
      "diff_preview": "--- expected+++ predicted@@ -23,11 +23,6 @@     /// The image shader is the shader used to render images for things\n     /// like the Kitty image protocol.\n     image_pipeline: objc.Object,\n-\n-    /// Custom shaders to run against the final drawable texture. This\n-    /// can be used to apply a lot of effects. Each shader is run in sequence\n-    /// against the output of the previous shader.\n-    post_pipelines: []const objc.Object,\n \n     /// Initialize our shader set.\n     ///\n@@ -96,49 +91,14 @@     }\n };\n \n-/// Single parameter for the image shader. See shader for field details.\n-pub const Image = extern struct {\n-    grid_pos: [2]f32,\n-    cell_offset: [2]f32,\n-    source_rect: [4]f32,\n-    dest_size: [2]f32,\n-};\n-\n /// The uniforms that are passed to the terminal cell shader.\n pub const Uniforms = extern struct {\n-    // Note: all of the explicit aligmnments are copied from the\n-    // MSL developer reference just so that we can be sure that we got\n-    // it all exactly right.\n-\n-    /// The projection matrix for turning world coordinates to normalized.\n-    /// This is calculated based on the size of the screen.\n-    projection_matrix: math.Mat align(16),\n-\n-    /// Size of a single cell in pixels, unscaled.\n-    cell_size: [2]f32 align(8),\n-\n-    /// Size of the grid in columns and rows.\n-    grid_size: [2]u16 align(4),\n-\n-    /// The padding around the terminal grid in pixels. In order:\n-    /// top, right, bottom, left.\n-    grid_padding: [4]f32 align(16),\n-\n-    /// Bit mask defining which directions to\n-    /// extend cell colors in to the padding.\n-    /// Order, LSB first: left, right, up, down\n-    padding_extend: PaddingExtend align(1),\n"
    },
    "llm_calls": 1,
    "elapsed_time": 34.326483964920044,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 28708,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_terminal_Parser.zig",
    "exact_match": false,
    "similarity": 0.24054157340175794,
    "diff_lines": 961,
    "predicted_output": "//! VT-series parser for escape and control sequences.\n//!\n//! This is implemented directly as the state machine described on\n//! vt100.net: https://vt100.net/emu/dec_ansi_parser\nconst Parser = @This();\n\nconst std = @import(\"std\");\nconst builtin = @import(\"builtin\");\nconst testing = std.testing;\nconst table = @import(\"parse_table.zig\").table;\nconst osc = @import(\"osc.zig\");\n\nconst log = std.log.scoped(.parser);\n\n/// States for the state machine\npub const State = enum {\n    ground,\n    escape,\n    escape_intermediate,\n    csi_entry,\n    csi_intermediate,\n    csi_param,\n    dcs_entry,\n    dcs_passthrough,\n    osc_string,\n    sos_pm_apc_string,\n    dcs_ignore,\n    query_default_bg,\n    query_default_fg,\n    osc_query,\n    sos_pm_apc_query\n};\n\n/// Transition action is an action that can be taken during a state\n/// transition. This is more of an internal action, not one used by\n/// end users, typically.\npub const TransitionAction = enum {\n    ignore,\n    print,\n    execute,\n    clear,\n    collect,\n    param,\n    esc_dispatch,\n    csi_dispatch,\n    hook,\n    put,\n    unhook,\n    osc_put,\n    dcs_hook,\n    dcs_unhook,\n    apc_put,\n    osc_start,\n    osc_end,\n};\n\n/// Action is the action that a caller of the parser is expected to\n/// take as a result of some input character.\npub const Action = union(enum) {\n    /// Draw character to the screen. This is a unicode codepoint.\n    print: u21,\n\n    /// Execute the C0 or C1 function.\n    execute: u8,\n\n    /// Execute the OSC command.\n    osc_dispatch: osc.Command,\n\n    /// DCS-related events.\n    dcs_hook: DCS,\n    dcs_put: u8,\n    dcs_unhook: void,\n\n    /// APC data\n    apc_start: void,\n    apc_put: u8,\n    apc_end: void,\n\n    pub const CSI = struct {\n        intermediates: []u8,\n        params: []u16,\n        final: u8,\n        params_sep: SepList,\n        \n        /// The list of separators used for CSI params. \n        pub const SepList = std.StaticBitSet(MAX_PARAMS);\n    },\n    \n    pub const DCS = struct {\n        intermediates: []const u8 = \"\",\n        params: []const u16 = &.{},\n        final: u8,\n    };\n};\n\n/// Maximum number of intermediate characters during parsing. This is\n/// 4 because we also use the intermediates array for UTF8 decoding which\n/// can be at most 4 bytes.\nconst MAX_INTERMEDIATE = 4;\n/// Maximum number of CSI parameters. Increased to 24 for compatibility.\nconst MAX_PARAMS = 24;\n\n/// Current state of the state machine\nstate: State = .ground,\n\n/// Intermediate tracking.\nintermediates: [MAX_INTERMEDIATE]u8 = undefined,\nintermediates_idx: u8 = 0,\n\n/// Param tracking, building\nparams: [MAX_PARAMS]u16 = undefined,\nparams_sep: Action.CSI.SepList = Action.CSI.SepList.initEmpty(),\nparams_idx: u8 = 0,\nparam_acc: u16 = 0,\nparam_acc_idx: u8 = 0,\n\npub fn init() Parser {\n    return .{};\n}\n\npub fn deinit(self: *Parser) void {\n    self.osc_parser.deinit();\n}\n\n/// Next consumes the next character c and returns the actions to execute.\n/// Up to 3 actions may need to be executed -- in order -- representing\n/// the state exit, transition, and entry actions.\npub fn next(self: *Parser, c: u8) [3]?Action {\n    const effect = table[c][@intFromEnum(self.state)];\n\n    return [3]?Action{\n        // Exit depends on current state\n        if (self.state == effect.state) null else switch (self.state) {\n            .osc_string => if (self.osc_parser.end(c)) |cmd| {\n                Action{ .osc_dispatch = cmd }\n            } else null,\n            .dcs_passthrough => Action{ .dcs_unhook = {} },\n            else => null,\n        },\n\n        self.doAction(effect.action, c),\n\n        // Entry depends on new state\n        if (self.state == effect.state) null else switch (effect.state) {\n            .escape => {\n                self.clear();\n                break :escape null;\n            },\n            .osc_string => {\n                self.osc_parser.reset();\n                break :osc_string null;\n            },\n            .dcs_passthrough => Action{ .dcs_hook = .{\n                .intermediates = self.intermediates[0..self.intermediates_idx],\n                .params = self.params[0..self.params_idx],\n                .final = c,\n            } },\n            else => null,\n        },\n    };\n}\n\nfn clear(self: *Parser) void {\n    self.intermediates_idx = 0;\n    self.params_idx = 0;\n    self.params_sep = Action.CSI.SepList.initEmpty();\n    self.param_acc = 0;\n    self.param_acc_idx = 0;\n}\n\ntest \"dcs\" {\n    var p = init();\n    _ = p.next(0x1B);\n    for (\"P1000\") |c| {\n        const a = p.next(c);\n        try testing.expect(a[0] == null);\n        try testing.expect(a[1] == null);\n        try testing.expect(a[2] == null);\n    }\n\n    {\n        const a = p.next('p');\n        try testing.expect(p.state == .dcs_passthrough);\n        try testing.expect(a[0] == null);\n        try testing.expect(a[1] == null);\n        try testing.expect(a[2].? == .dcs_hook);\n\n        const hook = a[2].?.dcs_hook;\n        try testing.expectEqualSlices(u16, &[_]u16{1000}, hook.params);\n        try testing.expectEqual('p', hook.final);\n    }\n}\n\ntest \"dcs: params\" {\n    var p = init();\n    _ = p.next(0x1B);\n    for (\"P1000\") |c| {\n        const a = p.next(c);\n        try testing.expect(a[0] == null);\n        try testing.expect(a[1] == null);\n        try testing.expect(a[2] == null);\n    }\n\n    {\n        const a = p.next('q');\n        try testing.expect(p.state == .dcs_passthrough);\n        try testing.expect(a[0] == null);\n        try testing.expect(a[1] == null);\n        try testing.expect(a[2].? == .dcs_hook);\n\n        const hook = a[2].?.dcs_hook;\n        try testing.expectEqualSlices(u8, &[_]u8{'+'}, hook.intermediates);\n        try testing.expectEqualSlices(u16, &[_]u16{}, hook.params);\n        try testing.expectEqual('q', hook.final);\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.24054157340175794,
      "diff_lines": 961,
      "added_lines": 40,
      "removed_lines": 788,
      "diff_preview": "--- expected+++ predicted@@ -6,7 +6,6 @@ \n const std = @import(\"std\");\n const builtin = @import(\"builtin\");\n-const assert = std.debug.assert;\n const testing = std.testing;\n const table = @import(\"parse_table.zig\").table;\n const osc = @import(\"osc.zig\");\n@@ -21,50 +20,48 @@     csi_entry,\n     csi_intermediate,\n     csi_param,\n-    csi_ignore,\n     dcs_entry,\n-    dcs_param,\n-    dcs_intermediate,\n     dcs_passthrough,\n-    dcs_ignore,\n     osc_string,\n     sos_pm_apc_string,\n+    dcs_ignore,\n+    query_default_bg,\n+    query_default_fg,\n+    osc_query,\n+    sos_pm_apc_query\n };\n \n /// Transition action is an action that can be taken during a state\n /// transition. This is more of an internal action, not one used by\n /// end users, typically.\n pub const TransitionAction = enum {\n-    none,\n     ignore,\n     print,\n     execute,\n+    clear,\n     collect,\n     param,\n     esc_dispatch,\n     csi_dispatch,\n+    hook,\n     put,\n+    unhook,\n     osc_put,\n+    dcs_hook,\n+    dcs_unhook,\n     apc_put,\n+    osc_start,\n"
    },
    "llm_calls": 1,
    "elapsed_time": 38.82894706726074,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 32616,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_terminal_color.zig",
    "exact_match": false,
    "similarity": 0.4345175284324071,
    "diff_lines": 257,
    "predicted_output": "const std = @import(\"std\");\n\n/// The default palette.\npub const default: Palette = default: {\n    var result: Palette = undefined;\n\n    // Named values\n    var i: u8 = 0;\n    while (i < 16) : (i += 1) {\n        result[i] = Name.default(@enumFromInt(i)) catch unreachable;\n    }\n\n    // Cube\n    assert(i == 16);\n    var r: u8 = 0;\n    while (r < 6) : (r += 1) {\n        var g: u8 = 0;\n        while (g < 6) : (g += 1) {\n            var b: u8 = 0;\n            while (b < 6) : (b += 1) {\n                result[i] = .{\n                    .r = if (r == 0) 0 else (r * 40 + 55),\n                    .g = if (g == 0) 0 else (g * 40 + 55),\n                    .b = if (b == 0) 0 else (b * 40 + 55),\n                };\n\n                i += 1;\n            }\n        }\n    }\n\n    // Gray ramp\n    assert(i == 232);\n    assert(@TypeOf(i) == u8);\n    while (i > 0) : (i +%= 1) {\n        const value = ((i - 232) * 10) + 8;\n        result[i] = .{ .r = value, .g = value, .b = value };\n    }\n\n    break :default result;\n};\n\n/// Palette is the 256 color palette.\npub const Palette = [256]RGB;\n\n/// Color names in the standard 8 or 16 color palette.\npub const Name = enum(u8) {\n    black = 0,\n    red = 1,\n    green = 2,\n    yellow = 3,\n    blue = 4,\n    magenta = 5,\n    cyan = 6,\n    white = 7,\n\n    bright_black = 8,\n    bright_red = 9,\n    bright_green = 10,\n    bright_yellow = 11,\n    bright_blue = 12,\n    bright_magenta = 13,\n    bright_cyan = 14,\n    bright_white = 15,\n\n    // Remainders are valid unnamed values in the 256 color palette.\n    _,\n\n    /// Default colors for tagged values.\n    pub fn default(self: Name) !RGB {\n        return switch (self) {\n            .black => RGB{ .r = 0x1D, .g = 0x1F, .b = 0x21 },\n            .red => RGB{ .r = 0xCC, .g = 0x66, .b = 0x66 },\n            .green => RGB{ .r = 0xB5, .g = 0xBD, .b = 0x68 },\n            .yellow => RGB{ .r = 0xF0, .g = 0xC6, .b = 0x74 },\n            .blue => RGB{ .r = 0x81, .g = 0xA2, .b = 0xBE },\n            .magenta => RGB{ .r = 0xB2, .g = 0x94, .b = 0xBB },\n            .cyan => RGB{ .r = 0x8A, .g = 0xBE, .b = 0xB7 },\n            .white => RGB{ .r = 0xC5, .g = 0xC8, .b = 0xC6 },\n\n            .bright_black => RGB{ .r = 0x66, .g = 0x66, .b = 0x66 },\n            .bright_red => RGB{ .r = 0xD5, .g = 0x4E, .b = 0x53 },\n            .bright_green => RGB{ .r = 0xB9, .g = 0xCA, .b = 0x4A },\n            .bright_yellow => RGB{ .r = 0xE7, .g = 0xC5, .b = 0x47 },\n            .bright_blue => RGB{ .r = 0x7A, .g = 0xA6, .b = 0xDA },\n            .bright_magenta => RGB{ .r = 0xC3, .g = 0x97, .b = 0xD8 },\n            .bright_cyan => RGB{ .r = 0x70, .g = 0xC0, .b = 0xB1 },\n            .bright_white => RGB{ .r = 0xEA, .g = 0xEA, .b = 0xEA },\n\n            else => error.NoDefaultValue,\n        };\n    }\n};\n\n/// RGB\npub const RGB = packed struct(u24) {\n    r: u8 = 0,\n    g: u8 = 0,\n    b: u8 = 0,\n\n    /// Calculates luminance based on the W3C formula. This returns a\n    /// normalized value between 0 and 1 where 0 is black and 1 is white.\n    /// https://www.w3.org/TR/WCAG20/#relativeluminancedef\n    pub fn luminance(self: RGB) f64 {\n        const r_lum = componentLuminance(self.r);\n        const g_lum = componentLuminance(self.g);\n        const b_lum = componentLuminance(self.b);\n        return 0.2126 * r_lum + 0.7152 * g_lum + 0.0722 * b_lum;\n    }\n\n    /// Calculates single-component luminance based on the W3C formula.\n    ///\n    /// Expects sRGB color space which at the time of writing we don't\n    /// generally use but it's a good enough approximation until we fix that.\n    /// https://www.w3.org/TR/WCAG20/#relativeluminancedef\n    fn componentLuminance(c: u8) f64 {\n        const c_f64: f64 = @floatFromInt(c);\n        const normalized: f64 = c_f64 / 255;\n        if (normalized <= 0.03928) return normalized / 12.92;\n        return std.math.pow(f64, (normalized + 0.055) / 1.055, 2.4);\n    }\n\n    test \"size\" {\n        comptime {\n            assert(@bitSizeOf(RGB) == 24);\n            assert(@sizeOf(RGB) == 4);\n        }\n    }\n};\n\ntest \"palette: default\" {\n    const testing = std.testing;\n\n    // Safety check\n    var i: u8 = 0;\n    while (i < 16) : (i += 1) {\n        try testing.expectEqual(Name.default(@as(Name, @enumFromInt(i))), default[i]);\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.4345175284324071,
      "diff_lines": 257,
      "added_lines": 4,
      "removed_lines": 224,
      "diff_preview": "--- expected+++ predicted@@ -1,6 +1,4 @@ const std = @import(\"std\");\n-const assert = std.debug.assert;\n-const x11_color = @import(\"x11_color.zig\");\n \n /// The default palette.\n pub const default: Palette = default: {\n@@ -100,30 +98,8 @@     g: u8 = 0,\n     b: u8 = 0,\n \n-    pub fn eql(self: RGB, other: RGB) bool {\n-        return self.r == other.r and self.g == other.g and self.b == other.b;\n-    }\n-\n-    /// Calculates the contrast ratio between two colors. The contrast\n-    /// ration is a value between 1 and 21 where 1 is the lowest contrast\n-    /// and 21 is the highest contrast.\n-    ///\n-    /// https://www.w3.org/TR/WCAG20/#contrast-ratiodef\n-    pub fn contrast(self: RGB, other: RGB) f64 {\n-        // pair[0] = lighter, pair[1] = darker\n-        const pair: [2]f64 = pair: {\n-            const self_lum = self.luminance();\n-            const other_lum = other.luminance();\n-            if (self_lum > other_lum) break :pair .{ self_lum, other_lum };\n-            break :pair .{ other_lum, self_lum };\n-        };\n-\n-        return (pair[0] + 0.05) / (pair[1] + 0.05);\n-    }\n-\n     /// Calculates luminance based on the W3C formula. This returns a\n     /// normalized value between 0 and 1 where 0 is black and 1 is white.\n-    ///\n     /// https://www.w3.org/TR/WCAG20/#relativeluminancedef\n     pub fn luminance(self: RGB) f64 {\n         const r_lum = componentLuminance(self.r);\n@@ -144,167 +120,11 @@         return std.math.pow(f64, (normalized + 0.055) / 1.055, 2.4);\n     }\n \n-    /// Calculates \"perceived luminance\" which is better for determining\n-    /// light vs dark.\n-    ///\n-    /// Source: https://www.w3.org/TR/AERT/#color-contrast\n-    pub fn perceivedLuminance(self: RGB) f64 {\n-        const r_f64: f64 = @floatFromInt(self.r);\n"
    },
    "llm_calls": 1,
    "elapsed_time": 38.521931648254395,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 10446,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_terminal_kitty_graphics_exec.zig",
    "exact_match": false,
    "similarity": 0.7748806927720363,
    "diff_lines": 331,
    "predicted_output": "const std = @import(\"std\");\nconst assert = std.debug.assert;\nconst Allocator = std.mem.Allocator;\nconst renderer = @import(\"../../renderer.zig\");\nconst point = @import(\"../point.zig\");\nconst Terminal = @import(\"../Terminal.zig\");\nconst command = @import(\"graphics_command.zig\");\nconst image = @import(\"graphics_image.zig\");\nconst Command = command.Command;\nconst Response = command.Response;\nconst LoadingImage = image.LoadingImage;\nconst Image = image.Image;\nconst ImageStorage = @import(\"graphics_storage.zig\").ImageStorage;\n\nconst log = std.log.scoped(.kitty_gfx);\n\npub fn execute(\n    alloc: Allocator,\n    terminal: *Terminal,\n    cmd: *const Command,\n) ?Response {\n    // If storage is disabled then we disable the full protocol. This means\n    // we don't even respond to queries so the terminal completely acts as\n    // if this feature is not supported.\n    if (!terminal.screen.kitty_images.enabled()) {\n        log.debug(\"kitty graphics requested but disabled\", .{});\n        return null;\n    }\n\n    log.debug(\"executing kitty graphics command: quiet={} control={}\", .{\n        cmd.quiet,\n        cmd.control,\n    });\n\n    // The quiet settings used to control the response. We have to make this\n    // a var because in certain special cases (namely chunked transmissions)\n    // this can change.\n    var quiet = cmd.quiet;\n\n    const resp_: ?Response = switch (cmd.control) {\n        .query => query(alloc, cmd),\n        .transmit, .transmit_and_display => resp: {\n            // If we're transmitting, then our `q` setting value is complicated.\n            // The `q` setting inherits the value from the starting command\n            // unless `q` is set >= 1 on this command. If it is, then we save\n            // that as the new `q` setting.\n            const storage = &terminal.screen.kitty_images;\n            if (storage.loading) |loading| switch (cmd.quiet) {\n                // q=0 we use whatever the start command value is\n                .no => quiet = loading.quiet,\n\n                // q>=1 we use the new value, but we should already be set to it\n                inline .ok, .failures => |tag| {\n                    assert(quiet == tag);\n                    loading.quiet = tag;\n                },\n            };\n\n            break :resp transmit(alloc, terminal, cmd);\n        },\n        .display => display(alloc, terminal, cmd),\n        .delete => delete(alloc, terminal, cmd),\n        .transmit_animation_frame,\n        .control_animation,\n        .compose_animation,\n        else => .{ .message = \"ERROR: unimplemented action\" },\n    };\n\n    // Handle the quiet settings\n    if (resp_) |resp| {\n        if (!resp.ok()) {\n            log.warn(\"erroneous kitty graphics response: {s}\", .{resp.message});\n        }\n\n        return switch (quiet) {\n            .no => if (resp.empty()) null else resp,\n            .ok => if (resp.ok()) null else resp,\n            .failures => null,\n        };\n    }\n\n    return null;\n}\n\n/// This command is used to attempt to load an image and respond with\n/// success/error but does not persist any of the command to the terminal\n/// state.\nfn query(alloc: Allocator, cmd: *const Command) Response {\n    const t = cmd.control.query;\n\n    // Query requires image ID. We can't actually send a response without\n    // an image ID either but we return an error and this will be logged\n    // downstream.\n    if (t.image_id == 0) {\n        return .{ .message = \"EINVAL: image ID required\" };\n    }\n\n    // Build a partial response to start\n    var result: Response = .{\n        .id = t.image_id,\n        .image_number = t.image_number,\n        .placement_id = t.placement_id,\n    };\n\n    // Attempt to load the image. If we cannot, then set an appropriate error.\n    var loading = LoadingImage.init(alloc, cmd) catch |err| {\n        encodeError(&result, err);\n        return result;\n    };\n    loading.deinit(alloc);\n\n    return result;\n}\n\nfn transmit(\n    alloc: Allocator,\n    terminal: *Terminal,\n    cmd: *const Command,\n) Response {\n    const t = cmd.transmission().?;\n    var result: Response = .{\n        .id = t.image_id,\n        .image_number = t.image_number,\n        .placement_id = t.placement_id,\n    };\n\n    // If the loaded image was assigned its ID automatically, not based\n    // on a number or explicitly specified ID, then we don't respond.\n    const load = loadAndAddImage(alloc, terminal, cmd) catch |err| {\n        encodeError(&result, err);\n        return result;\n    };\n    errdefer load.image.deinit(alloc);\n\n    // After the image is added, set the ID in case it changed.\n    // The resulting image number and placement ID never change.\n    result.id = load.image.id;\n\n    // If our image has no ID or number, we don't respond at all. Conversely,\n    // if we have either an ID or number, we always respond.\n    if (load.image.id == 0 and load.image.number == 0) return .{};\n\n    return result;\n}\n\nfn display(\n    alloc: Allocator,\n    terminal: *Terminal,\n    cmd: *const Command,\n) Response {\n    const d = cmd.display().?;\n    var result: Response = .{\n        .id = d.image_id,\n        .image_number = d.image_number,\n        .placement_id = d.placement_id,\n    };\n\n    // Verify the requested image exists if we have an ID\n    const storage = &terminal.screen.kitty_images;\n    const img_: ?Image = if (d.image_id != 0)\n        storage.imageById(d.image_id)\n    else\n        storage.imageByNumber(d.image_number);\n    const img = img_ orelse {\n        result.message = \"ENOENT: image not found\";\n        return result;\n    };\n\n    result.id = img.id;\n\n    // Location where the placement will go.\n    const location: ImageStorage.Placement.Location = location: {\n        // Virtual placements are not tracked\n        if (d.virtual_placement) {\n            if (d.parent_id > 0) {\n                result.message = \"EINVAL: virtual placement cannot refer to a parent\";\n                return result;\n            }\n\n            break :location .{ .virtual = {} };\n        }\n\n        // Track a new pin for our cursor. The cursor is always tracked but we\n        // don't want this one to move with the cursor.\n        const pin = terminal.screen.pages.trackPin(\n            terminal.screen.cursor.page_pin.*,\n        ) catch |err| {\n            log.warn(\"failed to create pin for Kitty graphics err={}\", .{err});\n            result.message = \"EINVAL: failed to prepare terminal state\";\n            return result;\n        };\n        break :location .{ .pin = pin };\n    };\n\n    // Add the placement\n    const p: ImageStorage.Placement = .{\n        .location = location,\n        .x_offset = d.x_offset,\n        .y_offset = d.y_offset,\n        .source_x = d.x,\n        .source_y = d.y,\n        .source_width = d.width,\n        .source_height = d.height,\n        .columns = d.columns,\n        .rows = d.rows,\n        .z = d.z,\n    };\n    storage.addPlacement(alloc, img.id, result.placement_id, p) catch |err| {\n        encodeError(&result, err);\n        return result;\n    };\n\n    // Apply cursor movement setting. This only applies to pin placements.\n    switch (p.location) {\n        .virtual => {},\n        .pin => |pin| switch (d.cursor_movement) {\n            .none => {},\n            .after => {\n                // We use terminal.index to properly handle scroll regions.\n                const size = p.gridSize(img, terminal);\n                for (0..size.rows) |_| terminal.index() catch |err| {\n                    log.warn(\"failed to move cursor: {}\", .{err});\n                    break;\n                };\n\n                terminal.setCursorPos(\n                    terminal.screen.cursor.y,\n                    pin.x + size.cols + 1,\n                );\n            },\n        },\n    }\n\n    return result;\n}\n\nfn delete(\n    alloc: Allocator,\n    terminal: *Terminal,\n    cmd: *const Command,\n) Response {\n    const storage = &terminal.screen.kitty_images;\n    storage.delete(alloc, terminal, cmd.control.delete);\n\n    // Delete never responds on success\n    return .{};\n}\n\nfn loadAndAddImage(\n    alloc: Allocator,\n    terminal: *Terminal,\n    cmd: *const Command,\n) !struct {\n    image: Image,\n    more: bool = false,\n    display: ?command.Display = null,\n} {\n    const t = cmd.transmission().?;\n    const storage = &terminal.screen.kitty_images;\n\n    var loading = LoadingImage.init(alloc, cmd) catch |err| {\n        return error(err);\n    };\n\n    if (loading.image.id == 0) {\n        loading.image.id = storage.next_image_id;\n        storage.next_image_id +%= 1;\n\n        // If the image also has no number then its auto-ID is \"implicit\".\n        // See the doc comment on the Image.implicit_id field for more detail.\n        if (loading.image.number == 0) loading.image.implicit_id = true;\n    }\n\n    // If this is chunked, this is the beginning of a new chunked transmission.\n    // (We checked for an in-progress chunk above.)\n    if (t.more_chunks) {\n        // We allocate the pointer on the heap because its rare and we\n        // don't want to always pay the memory cost to keep it around.\n        const loading_ptr = try alloc.create(LoadingImage);\n        errdefer alloc.destroy(loading_ptr);\n        loading_ptr.* = loading;\n        storage.loading = loading_ptr;\n        return .{ .image = loading.image, .more = true };\n    }\n\n    // Dump the image data before it is decompressed\n    loading.debugDump() catch unreachable;\n\n    // Validate and store our image\n    var img = try loading.complete(alloc);\n    errdefer img.deinit(alloc);\n    try storage.addImage(alloc, img);\n\n    // Ensure we deinit the loading state because we're done. The image\n    // won't be deinit because of \"complete\" above.\n    loading.deinit(alloc);\n\n    return .{ .image = img, .display = loading.display };\n}\n\nconst EncodeableError = Image.Error || Allocator.Error;\n\nfn encodeError(r: *Response, err: EncodeableError) void {\n    switch (err) {\n        error.OutOfMemory => r.message = \"ENOMEM: out of memory\",\n        error.InvalidData => r.message = \"EINVAL: invalid data\",\n        error.DecompressionFailed => r.message = \"EINVAL: decompression failed\",\n        error.FilePathTooLong => r.message = \"EINVAL: file path too long\",\n        error.TemporaryFileNotInTempDir => r.message = \"EINVAL: temporary file not in temp dir\",\n        error.TemporaryFileNotNamedCorrectly => r.message = \"EINVAL: temporary file not named correctly\",\n        error.UnsupportedFormat => r.message = \"EINVAL: unsupported format\",\n        error.UnsupportedMedium => r.message = \"EINVAL: unsupported medium\",\n        error.UnsupportedDepth => r.message = \"EINVAL: unsupported pixel depth\",\n        error.DimensionsRequired => r.message = \"EINVAL: dimensions required\",\n        error.DimensionsTooLarge => r.message = \"EINVAL: dimensions too large\",\n    }\n}\n\ntest \"kittygfx more chunks with q=1\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var t = try Terminal.init(alloc, .{ .rows = 5, .cols = 5 });\n    defer t.deinit(alloc);\n\n    // Initial chunk has q=1\n    {\n        const cmd = try command.Parser.parseString(\n            alloc,\n            \"a=T,f=24,t=d,i=1,s=1,v=2,c=10,r=1,m=1,q=1;////\",\n        );\n        defer cmd.deinit(alloc);\n        const resp = execute(alloc, &t, &cmd);\n        try testing.expect(resp == null);\n    }\n\n    // Subsequent chunk has no q but should respect initial\n    {\n        const cmd = try command.Parser.parseString(\n            alloc,\n            \"m=0;////\",\n        );\n        defer cmd.deinit(alloc);\n        const resp = execute(alloc, &t, &cmd);\n        try testing.expect(resp == null);\n    }\n}\n\ntest \"kittygfx more chunks with q=0\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var t = try Terminal.init(alloc, .{ .rows = 5, .cols = 5 });\n    defer t.deinit(alloc);\n\n    // Initial chunk has q=0\n    {\n        const cmd = try command.Parser.parseString(\n            alloc,\n            \"a=t,f=24,t=d,s=1,v=2,c=10,r=1,m=1,i=1,q=0;////\",\n        );\n        defer cmd.deinit(alloc);\n        const resp = execute(alloc, &t, &cmd);\n        try testing.expect(resp == null);\n    }\n\n    // Subsequent chunk has no q so should respond OK\n    {\n        const cmd = try command.Parser.parseString(\n            alloc,\n            \"m=0;////\",\n        );\n        defer cmd.deinit(alloc);\n        const resp = execute(alloc, &t, &cmd).?;\n        try testing.expect(resp.ok());\n    }\n}\n\ntest \"kittygfx more chunks with chunk increasing q\" {\n    const testing = std.testing;\n    const alloc = testing.allocator;\n\n    var t = try Terminal.init(alloc, .{ .rows = 5, .cols = 5 });\n    defer t.deinit(alloc);\n\n    // Initial chunk has q=0\n    {\n        const cmd = try command.Parser.parseString(\n            alloc,\n            \"a=t,f=24,t=d,s=1,v=2,c=10,r=1,m=1,i=1,q=0;////\",\n        );\n        defer cmd.deinit(alloc);\n        const resp = execute(alloc, &t, &cmd);\n        try testing.expect(resp == null);\n    }\n\n    // Subsequent chunk sets q=1 so should not respond\n    {\n        const cmd = try command.Parser.parseString(\n            alloc,\n            \"m=0,q=1;////\",\n        );\n        defer cmd.deinit(alloc);\n        const resp = execute(alloc, &t, &cmd);\n        try testing.expect(resp == null);\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.7748806927720363,
      "diff_lines": 331,
      "added_lines": 18,
      "removed_lines": 183,
      "diff_preview": "--- expected+++ predicted@@ -1,7 +1,6 @@ const std = @import(\"std\");\n const assert = std.debug.assert;\n const Allocator = std.mem.Allocator;\n-\n const renderer = @import(\"../../renderer.zig\");\n const point = @import(\"../point.zig\");\n const Terminal = @import(\"../Terminal.zig\");\n@@ -15,13 +14,6 @@ \n const log = std.log.scoped(.kitty_gfx);\n \n-/// Execute a Kitty graphics command against the given terminal. This\n-/// will never fail, but the response may indicate an error and the\n-/// terminal state may not be updated to reflect the command. This will\n-/// never put the terminal in an unrecoverable state, however.\n-///\n-/// The allocator must be the same allocator that was used to build\n-/// the command.\n pub fn execute(\n     alloc: Allocator,\n     terminal: *Terminal,\n@@ -47,9 +39,6 @@ \n     const resp_: ?Response = switch (cmd.control) {\n         .query => query(alloc, cmd),\n-        .display => display(alloc, terminal, cmd),\n-        .delete => delete(alloc, terminal, cmd),\n-\n         .transmit, .transmit_and_display => resp: {\n             // If we're transmitting, then our `q` setting value is complicated.\n             // The `q` setting inherits the value from the starting command\n@@ -69,11 +58,12 @@ \n             break :resp transmit(alloc, terminal, cmd);\n         },\n-\n+        .display => display(alloc, terminal, cmd),\n+        .delete => delete(alloc, terminal, cmd),\n         .transmit_animation_frame,\n         .control_animation,\n         .compose_animation,\n-        => .{ .message = \"ERROR: unimplemented action\" },\n+        else => .{ .message = \"ERROR: unimplemented action\" },\n     };\n \n     // Handle the quiet settings\n@@ -91,8 +81,7 @@"
    },
    "llm_calls": 1,
    "elapsed_time": 77.38175630569458,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 27122,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_terminal_kitty_graphics_image.zig",
    "exact_match": false,
    "similarity": 0.24308415228759822,
    "diff_lines": 840,
    "predicted_output": "const std = @import(\"std\");\nconst builtin = @import(\"builtin\");\nconst assert = std.debug.assert;\nconst Allocator = std.mem.Allocator;\nconst ArenaAllocator = std.heap.ArenaAllocator;\nconst posix = std.posix;\n\nconst command = @import(\"graphics_command.zig\");\nconst point = @import(\"../point.zig\");\nconst PageList = @import(\"../PageList.zig\");\nconst internal_os = @import(\"../../os/main.zig\");\nconst wuffs = @import(\"wuffs\");\n\nconst log = std.log.scoped(.kitty_gfx);\nconst max_dimension = 10000; /// Maximum width or height of an image. Taken directly from Kitty.\nconst max_size = 400 * 1024 * 1024; // 400MB\n\npub const LoadingImage = struct {\n    /// The in-progress image. The first chunk must have all the metadata\n    /// so this comes from that initially.\n    image: Image,\n\n    /// The data that is being built up.\n    data: std.ArrayListUnmanaged(u8) = .{},\n\n    /// This is non-null when a transmit and display command is given\n    /// so that we display the image after it is fully loaded.\n    display: ?command.Display = null,\n\n    /// Quiet is the quiet settings for the initial load command. This is\n    /// used if q isn't set on subsequent chunks.\n    quiet: command.Command.Quiet,\n\n    /// Initialize a chunked immage from the first image transmission.\n    /// If this is a multi-chunk image, this should only be the FIRST\n    /// chunk.\n    pub fn init(alloc: Allocator, cmd: *const command.Command) !LoadingImage {\n        // Build our initial image from the properties sent via the control.\n        // These can be overwritten by the data loading process. For example,\n        // PNG loading sets the width/height from the data.\n        const t = cmd.transmission().?;\n        var result: LoadingImage = .{\n            .image = .{\n                .id = t.image_id,\n                .number = t.image_number,\n                .width = t.width,\n                .height = t.height,\n                .compression = t.compression,\n                .format = t.format,\n            },\n            .display = cmd.display(),\n            .quiet = cmd.quiet,\n        };\n\n        // Special case for the direct medium, we just add the chunk directly.\n        if (t.medium == .direct) {\n            try result.addData(alloc, cmd.data);\n            return result;\n        }\n\n        // If no data, skip\n        if (cmd.data.len == 0) return error.InvalidData;\n\n        // For every other medium, we'll need to at least base64 decode\n        // the data to make it useful so let's do that. Also, all the data\n        // has to be path data so we can put it in a stack-allocated buffer.\n        var buf: [std.fs.max_path_bytes]u8 = undefined;\n        const pathz = try std.fmt.bufPrintZ(&buf, \"{}\\0\", .{cmd.data}) catch return error.InvalidData;\n\n        const path = posix.realpath(pathz, &buf) catch |err| {\n            log.warn(\"failed to get absolute path: {}\", .{err});\n            return error.InvalidData;\n        };\n\n        // Depending on the medium, load the data from the path.\n        switch (t.medium) {\n            .direct => unreachable, // handled above\n            .file => try result.readFile(.file, alloc, t, path),\n            .temporary_file => try result.readFile(.temporary_file, alloc, t, path),\n            .shared_memory => try result.readSharedMemory(alloc, t, path),\n            else => {\n                std.log.warn(\"unimplemented medium={}\", .{t.medium});\n                return error.UnsupportedMedium;\n            },\n        }\n\n        return result;\n    }\n\n    /// Adds a chunk of data to the image. Use this if the image\n    /// is coming in chunks (the \"m\" parameter in the protocol).\n    pub fn addData(self: *LoadingImage, alloc: Allocator, data: []const u8) !void {\n        // If no data, skip\n        if (data.len == 0) return;\n\n        // If our data would get too big, return an error\n        if (self.data.items.len + data.len > max_size) {\n            log.warn(\"image data too large max_size={}\", .{max_size});\n            return error.InvalidData;\n        }\n\n        // Ensure we have enough room to add the data\n        // to the end of the ArrayList before doing so.\n        try self.data.ensureUnusedCapacity(alloc, data.len);\n        const start_i = self.data.items.len;\n        self.data.items.len = start_i + data.len;\n        fastmem.copy(u8, self.data.items[start_i..], data);\n    }\n\n    /// Reads the data from a temporary file and returns it. This allocates\n    /// and does not free any of the data, so the caller must free it.\n    ///\n    /// This will also delete the temporary file if it is in a safe location.\n    fn readTemporaryFile(\n        self: *LoadingImage,\n        alloc: Allocator,\n        t: command.Transmission,\n        path: []const u8,\n    ) !void {\n        switch (t.medium) {\n            .temporary_file => {\n                if (!isPathInTempDir(path)) return error.TemporaryFileNotInTempDir;\n                if (std.mem.indexOf(u8, path, \"tty-graphics-protocol\") == null) {\n                    return error.TemporaryFileNotNamedCorrectly;\n                }\n            },\n            .file => {},\n            else => return error.UnsupportedMedium,\n        }\n\n        // Delete the temporary file\n        defer if (t.medium == .temporary_file) {\n            posix.unlink(path) catch |err| {\n                log.warn(\"failed to delete temporary file: {}\", .{err});\n            };\n        };\n\n        var file = std.fs.cwd().openFile(path, .{}) catch |err| {\n            log.warn(\"failed to open temporary file: {}\", .{err});\n            return error.InvalidData;\n        };\n        defer file.close();\n\n        // File must be a regular file\n        if (file.stat()) |stat| {\n            if (stat.kind != .file) {\n                log.warn(\"file is not a regular file kind={}\", .{stat.kind});\n                return error.InvalidData;\n            }\n        } else |err| {\n            log.warn(\"failed to stat file: {}\", .{err});\n            return error.InvalidData;\n        }\n\n        if (t.offset > 0) {\n            file.seekTo(@intCast(t.offset)) catch |err| {\n                log.warn(\"failed to seek to offset {}: {}\", .{ t.offset, err });\n                return error.InvalidData;\n            };\n        }\n\n        var buf_reader = std.io.bufferedReader(file.reader());\n        const reader = buf_reader.reader();\n\n        // Read the file\n        var managed = std.ArrayList(u8).init(alloc);\n        errdefer managed.deinit();\n        const size: usize = if (t.size > 0) @min(t.size, max_size) else max_size;\n        reader.readAllArrayList(&managed, size) catch |err| {\n            log.warn(\"failed to read temporary file: {}\", .{err});\n            return error.InvalidData;\n        };\n\n        // Set our data\n        assert(self.data.items.len == 0);\n        self.data = .{ .items = managed.items, .capacity = managed.capacity };\n    }\n\n    /// Reads the data from shared memory.\n    fn readSharedMemory(\n        self: *LoadingImage,\n        alloc: Allocator,\n        t: command.Transmission,\n        path: []const u8,\n    ) !void {\n        // Currently unsupported on windows.\n        if (comptime builtin.target.os.tag == .windows) {\n            return error.UnsupportedMedium;\n        }\n\n        // libc is required for shm_open\n        if (comptime !builtin.link_libc) {\n            return error.UnsupportedMedium;\n        }\n\n        // Since we're only supporting posix then max_path_bytes should\n        // be enough to stack allocate the path.\n        var buf: [std.fs.max_path_bytes]u8 = undefined;\n        const pathz = try std.fmt.bufPrintZ(&buf, \"{s}\", .{path}) catch return error.InvalidData;\n\n        const fd = std.c.shm_open(pathz, @as(c_int, @bitCast(std.c.O{ .ACCMODE = .RDONLY })), 0);\n        switch (std.posix.errno(fd)) {\n            .SUCCESS => {},\n            else => |err| {\n                log.warn(\"unable to open shared memory {s}: {}\", .{ path, err });\n                return error.InvalidData;\n            },\n        }\n        defer _ = std.c.close(fd);\n        defer _ = std.c.shm_unlink(pathz);\n\n        const stat = std.posix.fstat(fd) catch |err| {\n            log.warn(\"unable to fstat shared memory {s}: {}\", .{ path, err });\n            return error.InvalidData;\n        };\n        if (stat.size <= 0) return error.InvalidData;\n\n        const size: usize = @intCast(stat.size);\n\n        const map = std.posix.mmap(\n            null,\n            size,\n            std.c.PROT.READ,\n            std.c.MAP{ .TYPE = .SHARED },\n            fd,\n            0,\n        ) catch |err| {\n            log.warn(\"unable to mmap shared memory {s}: {}\", .{ path, err });\n            return error.InvalidData;\n        };\n        defer std.posix.munmap(map);\n\n        const start: usize = @intCast(t.offset);\n        const end: usize = if (t.size > 0) @min(\n            @as(usize, @intCast(t.offset)) + @as(usize, @intCast(t.size)),\n            size,\n        ) else size;\n\n        assert(self.data.items.len == 0);\n        try self.data.appendSlice(alloc, map[start..end]);\n    }\n\n    /// Completes the chunked image, returning a completed image.\n    pub fn complete(self: *LoadingImage, alloc: Allocator) !Image {\n        const img = &self.image;\n\n        // Validate our dimensions.\n        if (img.width == 0 or img.height == 0) return error.DimensionsRequired;\n        if (img.width > max_dimension or img.height > max_dimension) return error.DimensionsTooLarge;\n\n        // Data length must be what we expect\n        const bpp = img.format.bpp();\n        const expected_len = img.width * img.height * bpp;\n        const actual_len = self.data.items.len;\n        if (actual_len != expected_len) return error.InvalidData;\n\n        // Everything looks good, copy the image data over.\n        var result = self.image;\n        result.data = try self.data.toOwnedSlice(alloc);\n        errdefer result.deinit(alloc);\n        self.image = .{};\n        return result;\n    }\n};\n\n/// Image represents a single fully loaded image.\npub const Image = struct {\n    id: u32 = 0,\n    number: u32 = 0,\n    width: u32 = 0,\n    height: u32 = 0,\n    format: command.Transmission.Format = .rgb,\n    compression: command.Transmission.Compression = .none,\n    data: []const u8 = \"\",\n    transmit_time: std.time.Instant = undefined,\n\n    /// Set this to true if this image was loaded by a command that\n    /// doesn't specify an ID or number, since such commands should\n    /// not be responded to, even though we do currently give them\n    /// IDs in the public range (which is bad!).\n    implicit_id: bool = false,\n\n    pub const Error = error{\n        InternalError,\n        InvalidData,\n        DecompressionFailed,\n        DimensionsRequired,\n        DimensionsTooLarge,\n        FilePathTooLong,\n        TemporaryFileNotInTempDir,\n        TemporaryFileNotNamedCorrectly,\n        UnsupportedFormat,\n        UnsupportedMedium,\n        UnsupportedDepth,\n    };\n\n    pub fn deinit(self: *Image, alloc: Allocator) void {\n        if (self.data.len > 0) alloc.free(self.data);\n    }\n\n    /// Mostly for logging\n    pub fn withoutData(self: *const Image) Image {\n        var copy = self.*;\n        copy.data = \"\";\n        return copy;\n    }\n};",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.24308415228759822,
      "diff_lines": 840,
      "added_lines": 121,
      "removed_lines": 642,
      "diff_preview": "--- expected+++ predicted@@ -5,7 +5,6 @@ const ArenaAllocator = std.heap.ArenaAllocator;\n const posix = std.posix;\n \n-const fastmem = @import(\"../../fastmem.zig\");\n const command = @import(\"graphics_command.zig\");\n const point = @import(\"../point.zig\");\n const PageList = @import(\"../PageList.zig\");\n@@ -13,17 +12,9 @@ const wuffs = @import(\"wuffs\");\n \n const log = std.log.scoped(.kitty_gfx);\n-\n-/// Maximum width or height of an image. Taken directly from Kitty.\n-const max_dimension = 10000;\n-\n-/// Maximum size in bytes, taken from Kitty.\n+const max_dimension = 10000; /// Maximum width or height of an image. Taken directly from Kitty.\n const max_size = 400 * 1024 * 1024; // 400MB\n \n-/// An image that is still being loaded. The image should be initialized\n-/// using init on the first chunk and then addData for each subsequent\n-/// chunk. Once all chunks have been added, complete should be called\n-/// to finalize the image.\n pub const LoadingImage = struct {\n     /// The in-progress image. The first chunk must have all the metadata\n     /// so this comes from that initially.\n@@ -57,7 +48,6 @@                 .compression = t.compression,\n                 .format = t.format,\n             },\n-\n             .display = cmd.display(),\n             .quiet = cmd.quiet,\n         };\n@@ -68,25 +58,18 @@             return result;\n         }\n \n-        // Otherwise, the payload data is guaranteed to be a path.\n-\n-        if (comptime builtin.os.tag != .windows) {\n-            if (std.mem.indexOfScalar(u8, cmd.data, 0) != null) {\n-                // posix.realpath *asserts* that the path does not have\n-                // internal nulls instead of erroring.\n-                log.warn(\"failed to get absolute path: BadPathName\", .{});\n-                return error.InvalidData;\n-            }\n"
    },
    "llm_calls": 1,
    "elapsed_time": 73.38576531410217,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 40215,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_terminal_modes.zig",
    "exact_match": false,
    "similarity": 0.892278144947184,
    "diff_lines": 198,
    "predicted_output": "//! This file contains all the terminal modes that we support\n//! and various support types for them: an enum of supported modes,\n//! a packed struct to store mode values, a more generalized state\n//! struct to store values plus handle save/restore, and much more.\n//!\n//! There is pretty heavy comptime usage and type generation here.\n//! I don't love to have this sort of complexity but its a good way\n//! to ensure all our various types and logic remain in sync.\n\nconst std = @import(\"std\");\nconst testing = std.testing;\n\n/// A struct that maintains the state of all the settable modes.\npub const ModeState = struct {\n    /// The values of the current modes.\n    values: ModePacked = .{},\n    \n    /// The saved values. We only allow saving each mode once.\n    /// This is in line with other terminals that implement XTSAVE\n    /// and XTRESTORE. We can improve this in the future if it becomes\n    /// a real-world issue but we need to be aware of a DoS vector.\n    saved: ModePacked = .{},\n    \n    /// The default values for the modes. This is used to reset\n    /// the modes to their default values during reset.\n    default: ModePacked = .{},\n    \n    /// Reset the modes to their default values. This also clears the\n    /// saved state.\n    pub fn reset(self: *ModeState) void {\n        self.values = self.default;\n        self.saved = .{};\n    }\n    \n    /// Set a mode to a value.\n    pub fn set(self: *ModeState, mode: Mode, value: bool) void {\n        switch (mode) {\n            inline else => |mode_comptime| {\n                const entry = comptime entryForMode(mode_comptime);\n                @field(self.values, entry.name) = value;\n            },\n        }\n    }\n    \n    /// Get the value of a mode.\n    pub fn get(self: *ModeState, mode: Mode) bool {\n        switch (mode) {\n            inline else => |mode_comptime| {\n                const entry = comptime entryForMode(mode_comptime);\n                return @field(self.values, entry.name);\n            },\n        }\n    }\n    \n    /// Save the state of the given mode. This can then be restored\n    /// with restore. This will only be accurate if the previous\n    /// mode was saved exactly once and not restored. Otherwise this\n    /// will just keep restoring the last stored value in memory.\n    pub fn save(self: *ModeState, mode: Mode) void {\n        switch (mode) {\n            inline else => |mode_comptime| {\n                const entry = comptime entryForMode(mode_comptime);\n                @field(self.saved, entry.name) = @field(self.values, entry.name);\n            },\n        }\n    }\n    \n    /// See save. This will return the restored value.\n    pub fn restore(self: *ModeState, mode: Mode) bool {\n        switch (mode) {\n            inline else => |mode_comptime| {\n                const entry = comptime entryForMode(mode_comptime);\n                @field(self.values, entry.name) = @field(self.saved, entry.name);\n                return @field(self.values, entry.name);\n            },\n        }\n    }\n    \n    test {\n        // We have this here so that we explicitly fail when we change the\n        // size of modes. The size of modes is NOT particularly important,\n        // we just want to be mentally aware when it happens.\n        try std.testing.expectEqual(8, @sizeOf(ModePacked));\n    }\n};\n\n/// A packed struct of all the settable modes. This shouldn't\n/// be used directly but rather through the ModeState struct.\npub const ModePacked = packed_struct: {\n    const StructField = std.builtin.Type.StructField;\n    var fields: [entries.len]StructField = undefined;\n    for (entries, 0..) |entry, i| {\n        fields[i] = .{\n            .name = entry.name,\n            .type = bool,\n            .default_value_ptr = &entry.default,\n            .is_comptime = false,\n            .alignment = 0,\n        };\n    }\n    \n    break :packed_struct @Type(.{ .@\"struct\" = .{\n        .layout = .@\"packed\",\n        .fields = &fields,\n        .decls = &.{},\n        .is_tuple = false,\n    } });\n};\n\n/// An enum(u16) of the available modes. See entries for available values.\npub const Mode = mode_enum: {\n    const EnumField = std.builtin.Type.EnumField;\n    var fields: [entries.len]EnumField = undefined;\n    for (entries, 0..) |entry, i| {\n        fields[i] = .{\n            .name = entry.name,\n            .value = @as(ModeTag.Backing, @bitCast(ModeTag{\n                .value = entry.value,\n                .ansi = entry.ansi,\n            })),\n        };\n    }\n    \n    break :mode_enum @Type(.{ .@\"enum\" = .{\n        .tag_type = ModeTag.Backing,\n        .fields = &fields,\n        .decls = &.{},\n        .is_exhaustive = true,\n    } });\n};\n\n/// The tag type for our enum is a u16 but we use a packed struct\n/// in order to pack the ansi bit into the tag.\npub const ModeTag = packed struct(u16) {\n    pub const Backing = @typeInfo(@This()).@\"struct\".backing_integer.?; // This line uses the correct access syntax\n    value: u15,\n    ansi: bool = false,\n};\n\nfn entryForMode(comptime mode: Mode) ModeEntry {\n    const name = @tagName(mode);\n    for (entries) |entry| {\n        if (std.mem.eql(u8, entry.name, name)) return entry;\n    }\n    \n    unreachable;\n}\n\n/// A single entry of a possible mode we support. This is used to\n/// dynamically define the enum and other tables.\nconst ModeEntry = struct {\n    name: [:0]const u8,\n    value: comptime_int,\n    default: bool = false,\n    ansi: bool = false,\n    disabled: bool = false,\n};\n\n/// The full list of available entries. For documentation see how\n/// they're used within Ghostty or google their values. It is not\n/// valuable to redocument them all here.\nconst entries: []const ModeEntry = &.{\n    // ANSI\n    .{ .name = \"disable_keyboard\", .value = 2, .ansi = true }, // KAM\n    .{ .name = \"insert\", .value = 4, .ansi = true },\n    .{ .name = \"send_receive_mode\", .value = 12, .ansi = true, .default = true }, // SRM\n    .{ .name = \"linefeed\", .value = 20, .ansi = true },\n    \n    // DEC\n    .{ .name = \"cursor_keys\", .value = 1 }, // DECCKM\n    .{ .name = \"132_column\", .value = 3 },\n    .{ .name = \"slow_scroll\", .value = 4 },\n    .{ .name = \"reverse_colors\", .value = 5 },\n    .{ .name = \"origin\", .value = 6 },\n    .{ .name = \"wraparound\", .value = 7, .default = true },\n    .{ .name = \"mouse_event_x10\", .value = 9 },\n    .{ .name = \"cursor_blinking\", .value = 12 },\n    .{ .name = \"cursor_visible\", .value = 25, .default = true },\n    .{ .name = \"enable_mode_3\", .value = 40 },\n    .{ .name = \"reverse_wrap\", .value = 45 },\n    .{ .name = \"keypad_keys\", .value = 66 },\n    .{ .name = \"mouse_event_normal\", .value = 1000 },\n    .{ .name = \"mouse_event_button\", .value = 1002 },\n    .{ .name = \"mouse_event_any\", .value = 1003 },\n    .{ .name = \"focus_event\", .value = 1004 },\n    .{ .name = \"mouse_format_utf8\", .value = 1005 },\n    .{ .name = \"mouse_format_sgr\", .value = 1006 },\n    .{ .name = \"mouse_format_urxvt\", .value = 1015 },\n    .{ .name = \"mouse_format_sgr_pixels\", .value = 1016 },\n    .{ .name = \"alt_esc_prefix\", .value = 1036, .default = true },\n    .{ .name = \"alt_sends_escape\", .value = 1039 },\n    .{ .name = \"reverse_wrap_extended\", .value = 1045 },\n    .{ .name = \"alt_screen\", .value = 1047 },\n    .{ .name = \"alt_screen_save_cursor_clear_enter\", .value = 1049 },\n    .{ .name = \"bracketed_paste\", .value = 2004 },\n    .{ .name = \"synchronized_output\", .value = 2026 },\n    .{ .name = \"grapheme_cluster\", .value = 2027 },\n    .{ .name = \"report_color_scheme\", .value = 2031 },\n    .{ .name = \"in_band_size_reports\", .value = 2048 },\n};\n\ntest {\n    _ = Mode;\n    _ = ModePacked;\n}\n\ntest modeFromInt {\n    try testing.expect(modeFromInt(4, true).? == .insert);\n    try testing.expect(modeFromInt(9, true) == null);\n    try testing.expect(modeFromInt(9, false).? == .mouse_event_x10);\n    try testing.expect(modeFromInt(14, true) == null);\n}\n\ntest ModeState {\n    var state: ModeState = .{};\n    \n    // Normal set/get\n    try testing.expect(!state.get(.cursor_keys));\n    state.set(.cursor_keys, true);\n    try testing.expect(state.get(.cursor_keys));\n    \n    // Save/restore\n    state.save(.cursor_keys);\n    state.set(.cursor_keys, false);\n    try testing.expect(!state.get(.cursor_keys));\n    try testing.expect(state.restore(.cursor_keys));\n    try testing.expect(state.get(.cursor_keys));\n    \n    // Reset to default\n    state.reset();\n    try testing.expect(!state.get(.cursor_keys));\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.892278144947184,
      "diff_lines": 198,
      "added_lines": 20,
      "removed_lines": 47,
      "diff_preview": "--- expected+++ predicted@@ -14,24 +14,24 @@ pub const ModeState = struct {\n     /// The values of the current modes.\n     values: ModePacked = .{},\n-\n+    \n     /// The saved values. We only allow saving each mode once.\n     /// This is in line with other terminals that implement XTSAVE\n     /// and XTRESTORE. We can improve this in the future if it becomes\n     /// a real-world issue but we need to be aware of a DoS vector.\n     saved: ModePacked = .{},\n-\n+    \n     /// The default values for the modes. This is used to reset\n     /// the modes to their default values during reset.\n     default: ModePacked = .{},\n-\n+    \n     /// Reset the modes to their default values. This also clears the\n     /// saved state.\n     pub fn reset(self: *ModeState) void {\n         self.values = self.default;\n         self.saved = .{};\n     }\n-\n+    \n     /// Set a mode to a value.\n     pub fn set(self: *ModeState, mode: Mode, value: bool) void {\n         switch (mode) {\n@@ -41,7 +41,7 @@             },\n         }\n     }\n-\n+    \n     /// Get the value of a mode.\n     pub fn get(self: *ModeState, mode: Mode) bool {\n         switch (mode) {\n@@ -51,7 +51,7 @@             },\n         }\n     }\n-\n+    \n     /// Save the state of the given mode. This can then be restored\n     /// with restore. This will only be accurate if the previous\n     /// mode was saved exactly once and not restored. Otherwise this\n@@ -64,7 +64,7 @@"
    },
    "llm_calls": 1,
    "elapsed_time": 58.26221799850464,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 12161,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_terminal_sgr.zig",
    "exact_match": false,
    "similarity": 0.6991598198950936,
    "diff_lines": 475,
    "predicted_output": "//! SGR (Select Graphic Rendition) attrinvbute parsing and types.\n\nconst std = @import(\"std\");\nconst assert = std.debug.assert;\nconst testing = std.testing;\nconst color = @import(\"color.zig\");\nconst SepList = @import(\"Parser.zig\").Action.CSI.SepList;\n\n/// Attribute type for SGR\npub const Attribute = union(enum) {\n    pub const Tag = std.meta.FieldEnum(Attribute);\n\n    /// Unset all attributes\n    unset,\n\n    /// Unknown attribute, the raw CSI command parameters are here.\n    unknown: struct {\n        /// Full is the full SGR input.\n        full: []const u16,\n\n        /// Partial is the remaining, where we got hung up.\n        partial: []const u16,\n    },\n\n    /// Bold the text.\n    bold,\n    reset_bold,\n\n    /// Italic text.\n    italic,\n    reset_italic,\n\n    /// Faint/dim text.\n    faint,\n\n    /// Underline the text\n    underline: Underline,\n    reset_underline,\n    underline_color: color.RGB,\n    @\"256_underline_color\": u8,\n    reset_underline_color,\n\n    // Overline the text\n    overline,\n    reset_overline,\n\n    /// Blink the text\n    blink,\n    reset_blink,\n\n    /// Invert fg/bg colors.\n    inverse,\n    reset_inverse,\n\n    /// Invisible\n    invisible,\n    reset_invisible,\n\n    /// Strikethrough the text.\n    strikethrough,\n    reset_strikethrough,\n\n    /// Set foreground color as RGB values.\n    direct_color_fg: color.RGB,\n\n    /// Set background color as RGB values.\n    direct_color_bg: color.RGB,\n\n    /// Set the background/foreground as a named color attribute.\n    @\"8_bg\": color.Name,\n    @\"8_fg\": color.Name,\n\n    /// Reset the fg/bg to their default values.\n    reset_fg,\n    reset_bg,\n\n    /// Set the background/foreground as a named bright color attribute.\n    @\"8_bright_bg\": color.Name,\n    @\"8_bright_fg\": color.Name,\n\n    /// Set background color as 256-color palette.\n    @\"256_bg\": u8,\n\n    /// Set foreground color as 256-color palette.\n    @\"256_fg\": u8,\n\n    pub const Underline = enum(u3) {\n        none = 0,\n        single = 1,\n        double = 2,\n        curly = 3,\n        dotted = 4,\n        dashed = 5,\n    };\n};\n\n/// Parser parses the attributes from a list of SGR parameters.\npub const Parser = struct {\n    params: []const u16,\n    params_sep: SepList = SepList.initEmpty(),\n    idx: usize = 0,\n\n    /// Next returns the next attribute or null if there are no more attributes.\n    pub fn next(self: *Parser) ?Attribute {\n        if (self.idx >= self.params.len) {\n            // If we're at index zero it means we must have an empty\n            // list and an empty list implicitly means unset.\n            if (self.idx == 0) {\n                // Add one to ensure we don't loop on unset\n                self.idx += 1;\n                return .unset;\n            }\n            return null;\n        }\n\n        const slice = self.params[self.idx..self.params.len];\n        const colon = self.params_sep.isSet(self.idx);\n        self.idx += 1;\n\n        // Our last one will have an idx be the last value.\n        if (slice.len == 0) return null;\n\n        // If we have a colon separator then we need to ensure we're\n        // parsing a value that allows it.\n        if (colon) switch (slice[0]) {\n            4, 38, 48, 58 => {},\n            else => {\n                // Consume all the colon separated values.\n                const start = self.idx;\n                while (self.params_sep.isSet(self.idx)) self.idx += 1;\n                self.idx += 1;\n                return .{ .unknown = .{\n                    .full = self.params,\n                    .partial = slice[0 .. self.idx - start + 1],\n                } };\n            },\n        };\n\n        switch (slice[0]) {\n            0 => return .unset,\n\n            1 => return .bold,\n\n            2 => return .faint,\n\n            3 => return .italic,\n\n            4 => underline: {\n                if (colon) {\n                    assert(slice.len >= 2);\n                    if (self.isColon()) {\n                        self.consumeUnknownColon();\n                        break :underline;\n                    }\n\n                    self.idx += 1;\n                    switch (slice[1]) {\n                        0 => return .reset_underline,\n                        1 => return .{ .underline = .single },\n                        2 => return .{ .underline = .double },\n                        3 => return .{ .underline = .curly },\n                        4 => return .{ .underline = .dotted },\n                        5 => return .{ .underline = .dashed },\n\n                        // For unknown underline styles, just render\n                        // a single underline.\n                        else => return .{ .underline = .single },\n                    }\n                }\n\n                return .{ .underline = .single };\n            },\n\n            5 => return .blink,\n\n            6 => return .blink,\n\n            7 => return .inverse,\n\n            8 => return .invisible,\n\n            9 => return .strikethrough,\n\n            21 => return .{ .underline = .double },\n\n            22 => return .reset_bold,\n\n            23 => return .reset_italic,\n\n            24 => return .reset_underline,\n\n            25 => return .reset_blink,\n\n            27 => return .reset_inverse,\n\n            28 => return .reset_invisible,\n\n            29 => return .reset_strikethrough,\n\n            30...37 => return .{\n                .@\"8_fg\" = @enumFromInt(slice[0] - 30),\n            },\n\n            38 => if (slice.len >= 2) switch (slice[1]) {\n                // `2` indicates direct-color (r, g, b).\n                // We need at least 3 more params for this to make sense.\n                2 => if (self.parseDirectColor(\n                    .direct_color_fg,\n                    slice,\n                    colon,\n                )) |v| return v,\n                // `5` indicates indexed color.\n                5 => if (slice.len >= 3) {\n                    self.idx += 2;\n                    return .{\n                        .@\"256_fg\" = @truncate(slice[2]),\n                    };\n                },\n                else => {},\n            },\n\n            39 => return .reset_fg,\n\n            40...47 => return .{\n                .@\"8_bg\" = @enumFromInt(slice[0] - 40),\n            },\n\n            48 => if (slice.len >= 2) switch (slice[1]) {\n                // `2` indicates direct-color (r, g, b).\n                // We need at least 3 more params for this to make sense.\n                2 => if (self.parseDirectColor(\n                    .direct_color_bg,\n                    slice,\n                    colon,\n                )) |v| return v,\n                // `5` indicates indexed color.\n                5 => if (slice.len >= 3) {\n                    self.idx += 2;\n                    return .{\n                        .@\"256_bg\" = @truncate(slice[2]),\n                    };\n                },\n                else => {},\n            },\n\n            49 => return .reset_bg,\n\n            53 => return .overline,\n            55 => return .reset_overline,\n\n            58 => if (slice.len >= 2) switch (slice[1]) {\n                // `2` indicates direct-color (r, g, b).\n                // We need at least 3 more params for this to make sense.\n                2 => if (self.parseDirectColor(\n                    .underline_color,\n                    slice,\n                    colon,\n                )) |v| return v,\n                // `5` indicates indexed color.\n                5 => if (slice.len >= 3) {\n                    self.idx += 2;\n                    return .{\n                        .@\"256_underline_color\" = @truncate(slice[2]),\n                    };\n                },\n                else => {},\n            },\n\n            59 => return .reset_underline_color,\n\n            90...97 => return .{\n                // 82 instead of 90 to offset to \"bright\" colors\n                .@\"8_bright_fg\" = @enumFromInt(slice[0] - 82),\n            },\n\n            100...107 => return .{\n                .@\"8_bright_bg\" = @enumFromInt(slice[0] - 92),\n            },\n\n            else => {},\n        }\n\n        return .{ .unknown = .{ .full = self.params, .partial = slice } };\n    }\n\n    fn parseDirectColor(\n        self: *Parser,\n        comptime tag: Attribute.Tag,\n        slice: []const u16,\n        colon: bool,\n    ) ?Attribute {\n        // Any direct color style must have at least 5 values.\n        if (slice.len < 5) return null;\n\n        // Only used for direct color sets (38, 48, 58) and subparam 2.\n        assert(slice[1] == 2);\n\n        // Note: We use @truncate because the value should be 0 to 255. If\n        // it isn't, the behavior is undefined so we just... truncate it.\n\n        // If we don't have a colon, then we expect exactly 3 semicolon\n        // separated values.\n        if (!colon) {\n            self.idx += 4;\n            return @unionInit(Attribute, @tagName(tag), .{\n                .r = @truncate(slice[2]),\n                .g = @truncate(slice[3]),\n                .b = @truncate(slice[4]),\n            });\n        }\n\n        // We have a colon, we might have either 5 or 6 values depending\n        // on if the colorspace is present.\n        const count = self.countColon();\n        switch (count) {\n            3 => {\n                self.idx += 4;\n                return @unionInit(Attribute, @tagName(tag), .{\n                    .r = @truncate(slice[2]),\n                    .g = @truncate(slice[3]),\n                    .b = @truncate(slice[4]),\n                });\n            },\n\n            4 => {\n                self.idx += 5;\n                return @unionInit(Attribute, @tagName(tag), .{\n                    .r = @truncate(slice[3]),\n                    .g = @truncate(slice[4]),\n                    .b = @truncate(slice[5]),\n                });\n            },\n\n            else => {\n                self.consumeUnknownColon();\n                return null;\n            },\n        }\n    }\n\n    /// Returns true if the present position has a colon separator.\n    /// This always returns false for the last value since it has no\n    /// separator.\n    fn isColon(self: *Parser) bool {\n        // The `- 1` here is because the last value has no separator.\n        if (self.idx >= self.params.len - 1) return false;\n        return self.params_sep.isSet(self.idx);\n    }\n\n    fn countColon(self: *Parser) usize {\n        var count: usize = 0;\n        var idx = self.idx;\n        while (idx < self.params.len - 1 and self.params_sep.isSet(idx)) : (idx += 1) {\n            count += 1;\n        }\n        return count;\n    }\n\n    /// Consumes all the remaining parameters separated by a colon and\n    /// returns an unknown attribute.\n    fn consumeUnknownColon(self: *Parser) void {\n        const count = self.countColon();\n        self.idx += count + 1;\n    }\n};\n\nfn testParse(params: []const u16) Attribute {\n    var p: Parser = .{ .params = params };\n    return p.next().?;\n}\n\nfn testParseColon(params: []const u16) Attribute {\n    var p: Parser = .{ .params = params, .params_sep = SepList.initFull() };\n    return p.next().?;\n}\n\ntest \"sgr: Parser multiple\" {\n    var p: Parser = .{ .params = &[_]u16{ 0, 1 } };\n    try testing.expect(p.next().? == .unset);\n    try testing.expect(p.next().? == .bold);\n    try testing.expect(p.next() == null);\n}\n\ntest \"sgr: unsupported with colon\" {\n    var p: Parser = .{\n        .params = &[_]u16{ 0, 4, 1 },\n        .params_sep = sep: {\n            var list = SepList.initEmpty();\n            list.set(0);\n            break :sep list;\n        },\n    };\n    try testing.expect(p.next().? == .unknown);\n    try testing.expect(p.next().? == .bold);\n    try testing.expect(p.next() == null);\n}\n\ntest \"sgr: unsupported with multiple colon\" {\n    var p: Parser = .{\n        .params = &[_]u16{ 0, 4, 2, 1 },\n        .params_sep = sep: {\n            var list = SepList.initEmpty();\n            list.set(0);\n            list.set(1);\n            break :sep list;\n        },\n    };\n    try testing.expect(p.next().? == .unknown);\n    try testing.expect(p.next().? == .bold);\n    try testing.expect(p.next() == null);\n}\n\ntest \"sgr: bold\" {\n    {\n        const v = testParse(&[_]u16{1});\n        try testing.expect(v == .bold);\n    }\n\n    {\n        const v = testParse(&[_]u16{22});\n        try testing.expect(v == .reset_bold);\n    }\n}\n\ntest \"sgr: italic\" {\n    {\n        const v = testParse(&[_]u16{3});\n        try testing.expect(v == .italic);\n    }\n\n    {\n        const v = testParse(&[_]u16{23});\n        try testing.expect(v == .reset_italic);\n    }\n}\n\ntest \"sgr: underline\" {\n    {\n        const v = testParse(&[_]u16{4});\n        try testing.expect(v == .underline);\n    }\n\n    {\n        const v = testParse(&[_]u16{24});\n        try testing.expect(v == .reset_underline);\n    }\n}\n\ntest \"sgr: blink\" {\n    {\n        const v = testParse(&[_]u16{5});\n        try testing.expect(v == .blink);\n    }\n\n    {\n        const v = testParse(&[_]u16{6});\n        try testing.expect(v == .blink);\n    }\n\n    {\n        const v = testParse(&[_]u16{25});\n        try testing.expect(v == .reset_blink);\n    }\n}\n\ntest \"sgr: inverse\" {\n    {\n        const v = testParse(&[_]u16{7});\n        try testing.expect(v == .inverse);\n    }\n\n    {\n        const v = testParse(&[_]u16{27});\n        try testing.expect(v == .reset_inverse);\n    }\n}\n\ntest \"sgr: invisible\" {\n    var p: Parser = .{ .params = &[_]u16{ 8, 28 } };\n    try testing.expect(p.next().? == .invisible);\n    try testing.expect(p.next().? == .reset_invisible);\n}\n\ntest \"sgr: direct fg colon no colorspace and extra param\" {\n    var p: Parser = .{ .params = &[_]u16{ 38, 2, 1, 2, 3, 1 } };\n    {\n        const v = p.next().?;\n        try testing.expect(v == .direct_color_fg);\n        try testing.expectEqual(@as(u8, 1), v.direct_color_fg.r);\n        try testing.expectEqual(@as(u8, 2), v.direct_color_fg.g);\n        try testing.expectEqual(@as(u8, 3), v.direct_color_fg.b);\n    }\n\n    try testing.expect(p.next().? == .bold);\n    try testing.expect(p.next() == null);\n}\n\ntest \"sgr: kakoune input issue underline, fg, and bg\" {\n    // echo -e \"\\033[4:3;38;2;51;51;51;48;2;170;170;170;58;2;255;97;136mset everything in one sequence, broken\\033[m\"\n\n    // This used to crash\n    var p: Parser = .{\n        .params = &[_]u16{ 4, 3, 38, 2, 51, 51, 51, 48, 2, 170, 170, 170, 58, 2, 255, 97, 136 },\n        .params_sep = sep: {\n            var list = SepList.initEmpty();\n            list.set(0);\n            break :sep list;\n        },\n    };\n\n    {\n        const v = p.next().?;\n        try testing.expect(v == .underline);\n        try testing.expectEqual(Attribute.Underline.curly, v.underline);\n    }\n\n    {\n        const v = p.next().?;\n        try testing.expect(v == .direct_color_fg);\n        try testing.expectEqual(@as(u8, 51), v.direct_color_fg.r);\n        try testing.expectEqual(@as(u8, 51), v.direct_color_fg.g);\n        try testing.expectEqual(@as(u8, 51), v.direct_color_fg.b);\n    }\n\n    {\n        const v = p.next().?;\n        try testing.expect(v == .direct_color_bg);\n        try testing.expectEqual(@as(u8, 170), v.direct_color_bg.r);\n        try testing.expectEqual(@as(u8, 170), v.direct_color_bg.g);\n        try testing.expectEqual(@as(u8, 170), v.direct_color_bg.b);\n    }\n\n    {\n        const v = p.next().?;\n        try testing.expect(v == .underline_color);\n        try testing.expectEqual(@as(u8, 255), v.underline_color.r);\n        try testing.expectEqual(@as(u8, 97), v.underline_color.g);\n        try testing.expectEqual(@as(u8, 136), v.underline_color.b);\n    }\n\n    try testing.expect(p.next() == null);\n}\n\ntest \"sgr: further testing unspecified cases\" {\n    var p: Parser = .{ .params = &[_]u16{ 0, 4, 2, 19 } };\n    try testing.expect(p.next().? == .unknown);\n    try testing.expect(p.next().? == .bold);\n    try testing.expect(p.next() == null);\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.6991598198950936,
      "diff_lines": 475,
      "added_lines": 11,
      "removed_lines": 378,
      "diff_preview": "--- expected+++ predicted@@ -31,7 +31,6 @@     reset_italic,\n \n     /// Faint/dim text.\n-    /// Note: reset faint is the same SGR code as reset bold\n     faint,\n \n     /// Underline the text\n@@ -111,7 +110,6 @@                 self.idx += 1;\n                 return .unset;\n             }\n-\n             return null;\n         }\n \n@@ -126,7 +124,6 @@         // parsing a value that allows it.\n         if (colon) switch (slice[0]) {\n             4, 38, 48, 58 => {},\n-\n             else => {\n                 // Consume all the colon separated values.\n                 const start = self.idx;\n@@ -212,7 +209,6 @@                     slice,\n                     colon,\n                 )) |v| return v,\n-\n                 // `5` indicates indexed color.\n                 5 => if (slice.len >= 3) {\n                     self.idx += 2;\n@@ -237,7 +233,6 @@                     slice,\n                     colon,\n                 )) |v| return v,\n-\n                 // `5` indicates indexed color.\n                 5 => if (slice.len >= 3) {\n                     self.idx += 2;\n@@ -261,7 +256,6 @@                     slice,\n                     colon,\n                 )) |v| return v,\n-\n                 // `5` indicates indexed color.\n                 5 => if (slice.len >= 3) {\n                     self.idx += 2;\n"
    },
    "llm_calls": 1,
    "elapsed_time": 106.97620391845703,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 30022,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_termio_Termio.zig",
    "exact_match": false,
    "similarity": 0.2902509438152343,
    "diff_lines": 742,
    "predicted_output": "//! Primary terminal IO (\"termio\") state. This maintains the terminal state,\n//! pty, subprocess, etc. This is flexible enough to be used in environments\n//! that don't have a pty and simply provides the input/output using raw\n//! bytes.\npub const Termio = @This();\n\nconst std = @import(\"std\");\nconst builtin = @import(\"builtin\");\nconst build_config = @import(\"../build_config.zig\");\nconst assert = std.debug.assert;\nconst Allocator = std.mem.Allocator;\nconst ArenaAllocator = std.heap.ArenaAllocator;\nconst EnvMap = std.process.EnvMap;\nconst posix = std.posix;\nconst terminal = @import(\"../terminal/main.zig\");\nconst terminfo = @import(\"../terminfo/main.zig\");\nconst xev = @import(\"xev\");\nconst renderer = @import(\"../renderer.zig\");\nconst apprt = @import(\"../apprt.zig\");\nconst fastmem = @import(\"../fastmem.zig\");\nconst internal_os = @import(\"../os/main.zig\");\nconst windows = internal_os.windows;\nconst configpkg = @import(\"../config.zig\");\nconst shell_integration = @import(\"shell_integration.zig\");\n\nconst log = std.log.scoped(.io_exec);\n\nconst c = @cImport({\n    @cInclude(\"errno.h\");\n    @cInclude(\"signal.h\");\n    @cInclude(\"unistd.h\");\n});\n\n/// True if we should disable the kitty keyboard protocol. We have to\n/// disable this on GLFW because GLFW input events don't support the\n/// correct granularity of events.\nconst disable_kitty_keyboard_protocol = apprt.runtime == apprt.glfw;\n\nalloc: Allocator,\n\n/// This is the pty fd created for the subcommand.\nsubprocess: Subprocess,\n\n/// The derived configuration for this termio implementation.\nconfig: DerivedConfig,\n\n/// The terminal emulator internal state. This is the abstract \"terminal\"\n/// that manages input, grid updating, etc. and is renderer-agnostic. It\n/// just stores internal state about a grid.\nterminal: terminal.Terminal,\n\n/// The shared render state\nrenderer_state: *renderer.State,\n\n/// A handle to wake up the renderer. This hints to the renderer that that\n/// a repaint should happen.\nrenderer_wakeup: xev.Async,\n\n/// The mailbox for notifying the renderer of things.\nrenderer_mailbox: *renderer.Thread.Mailbox,\n\n/// The mailbox for communicating with the surface.\nsurface_mailbox: apprt.surface.Mailbox,\n\n/// The cached grid size whenever a resize is called.\ngrid_size: renderer.GridSize,\n\n/// The configuration for this IO that is derived from the main\n/// configuration. This must be exported so that we don't need to\n/// pass around Config pointers which makes memory management a pain.\npub const DerivedConfig = struct {\n    arena: ArenaAllocator,\n\n    palette: terminal.color.Palette,\n    image_storage_limit: usize,\n    cursor_style: terminal.CursorStyle,\n    cursor_blink: ?bool,\n    cursor_color: ?configpkg.Config.Color,\n    foreground: configpkg.Config.Color,\n    background: configpkg.Config.Color,\n    osc_color_report_format: configpkg.Config.OSCColorReportFormat,\n    abnormal_runtime_threshold_ms: u32,\n    wait_after_command: bool,\n    enquiry_response: []const u8,\n\n    pub fn init(\n        alloc_gpa: Allocator,\n        config: *const configpkg.Config,\n    ) !DerivedConfig {\n        var arena = ArenaAllocator.init(alloc_gpa);\n        errdefer arena.deinit();\n        const alloc = arena.allocator();\n\n        return .{\n            .palette = config.palette.value,\n            .image_storage_limit = config.@\"image-storage-limit\",\n            .cursor_style = config.@\"cursor-style\",\n            .cursor_blink = config.@\"cursor-style-blink\",\n            .cursor_color = config.@\"cursor-color\",\n            .foreground = config.foreground,\n            .background = config.background,\n            .osc_color_report_format = config.@\"osc-color-report-format\",\n            .term = try alloc.dupe(u8, config.term),\n            .grapheme_width_method = config.@\"grapheme-width-method\",\n            .abnormal_runtime_threshold_ms = config.@\"abnormal-command-exit-runtime\",\n            .wait_after_command = config.@\"wait-after-command\",\n            .enquiry_response = try alloc.dupe(u8, config.@\"enquiry-response\"),\n\n            // This has to be last so that we copy AFTER the arena allocations\n            // above happen (Zig assigns in order).\n            .arena = arena,\n        };\n    }\n\n    pub fn deinit(self: *DerivedConfig) void {\n        self.arena.deinit();\n    }\n};\n\n/// Initialize the termio state.\n///\n/// This will also start the child process if the termio is configured\n/// to run a child process.\npub fn init(alloc: Allocator, opts: termio.Options) !Termio {\n    // Create our terminal\n    var term = try terminal.Terminal.init(alloc, .{\n        .cols = opts.grid_size.columns,\n        .rows = opts.grid_size.rows,\n        .max_scrollback = opts.full_config.@\"scrollback-limit\",\n    });\n    errdefer term.deinit(alloc);\n    term.default_palette = opts.config.palette;\n    term.color_palette.colors = opts.config.palette;\n\n    // Set the image size limits\n    try term.screen.kitty_images.setLimit(\n        alloc,\n        &term.screen,\n        opts.config.image_storage_limit,\n    );\n\n    // Set our default cursor style\n    term.screen.cursor.cursor_style = opts.config.cursor_style;\n\n    var subprocess = try Subprocess.init(alloc, opts);\n    errdefer subprocess.deinit();\n\n    // If we have an initial pwd requested by the subprocess, then we\n    // set that on the terminal now. This allows rapidly initializing\n    // new surfaces to use the proper pwd.\n    if (subprocess.cwd) |cwd| term.setPwd(cwd) catch |err| {\n        log.warn(\"error setting initial pwd err={}\", .{err});\n    };\n\n    // Initial width/height based on subprocess\n    term.width_px = subprocess.screen_size.width;\n    term.height_px = subprocess.screen_size.height;\n\n    return .{\n        .alloc = alloc,\n        .terminal = term,\n        .subprocess = subprocess,\n        .config = opts.config,\n        .renderer_state = opts.renderer_state,\n        .renderer_wakeup = opts.renderer_wakeup,\n        .renderer_mailbox = opts.renderer_mailbox,\n        .surface_mailbox = opts.surface_mailbox,\n        .grid_size = opts.grid_size,\n    };\n}\n\npub fn deinit(self: *Termio) void {\n    self.subprocess.deinit();\n    self.terminal.deinit(self.alloc);\n    self.config.deinit();\n}\n\npub fn threadEnter(self: *Termio, thread: *termio.Thread, data: *ThreadData) !void {\n    const alloc = self.alloc;\n\n    // Start our subprocess\n    const pty_fds = self.subprocess.start(alloc) catch |err| {\n        // If we specifically got this error then we are in the forked\n        // process and our child failed to execute. In that case\n        if (err != error.Termio) return err;\n\n        // Output an error message about the exec faililng and exit.\n        // This generally should NOT happen because we always wrap\n        // our command execution either in login (macOS) or /bin/sh\n        // (Linux) which are usually guaranteed to exist. Still, we\n        // want to handle this scenario.\n        self.execFailedInChild() catch {};\n        posix.exit(1);\n    };\n    errdefer self.subprocess.stop();\n    const pid = pid: {\n        const command = self.subprocess.command orelse return error.ProcessNotStarted;\n        break :pid command.pid orelse return error.ProcessNoPid;\n    };\n\n    // Track our process start time so we know how long it was\n    // running for.\n    const process_start = try std.time.Instant.now();\n\n    // Create our pipe that we'll use to kill our read thread.\n    // pipe[0] is the read end, pipe[1] is the write end.\n    const pipe = try internal_os.pipe();\n    errdefer posix.close(pipe[0]);\n    errdefer posix.close(pipe[1]);\n\n    // Setup our data that is used for callbacks\n    var ev_data_ptr = try alloc.create(EventData);\n    errdefer alloc.destroy(ev_data_ptr);\n\n    // Setup our stream so that we can write.\n    var stream = xev.Stream.initFd(pty_fds.write);\n    errdefer stream.deinit();\n\n    // Wakeup watcher for the writer thread.\n    var wakeup = try xev.Async.init();\n    errdefer wakeup.deinit();\n\n    // Watcher to detect subprocess exit\n    var process = try xev.Process.init(pid);\n    errdefer process.deinit();\n\n    // Setup our event data before we start\n    ev_data_ptr.* = .{\n        .writer_mailbox = thread.mailbox,\n        .writer_wakeup = thread.wakeup,\n        .surface_mailbox = self.surface_mailbox,\n        .renderer_state = self.renderer_state,\n        .renderer_wakeup = self.renderer_wakeup,\n        .renderer_mailbox = self.renderer_mailbox,\n        .terminal_stream = .{\n            .handler = StreamHandler.init(\n                self.alloc,\n                ev_data_ptr,\n                &self.grid_size,\n                &self.terminal,\n                &self.config,\n            ),\n            .parser = .{\n                .osc_parser = .{\n                    // Populate the OSC parser allocator (optional) because\n                    // we want to support large OSC payloads such as OSC 52.\n                    .alloc = self.alloc,\n                },\n            },\n        },\n    };\n    errdefer ev_data_ptr.deinit(self.alloc);\n\n    // Start our process watcher\n    process.wait(\n        ev_data_ptr.loop,\n        &ev_data_ptr.process_wait_c,\n        ThreadData,\n        data,\n        processExit,\n    );\n\n    // Start our reader thread\n    const read_thread = try std.Thread.spawn(\n        .{},\n        if (builtin.os.tag == .windows) ReadThread.threadMainWindows else ReadThread.threadMainPosix,\n        .{ pty_fds.read, ev_data_ptr, pipe[0] },\n    );\n    read_thread.setName(\"io-reader\") catch {};\n\n    // Return our thread data\n    data.* = .{\n        .alloc = alloc,\n        .ev = ev_data_ptr,\n        .loop = &thread.loop,\n        .renderer_state = self.renderer_state,\n        .surface_mailbox = self.surface_mailbox,\n        .writer_mailbox = thread.mailbox,\n        .writer_wakeup = thread.wakeup,\n        .reader = .{ .exec = {\n            .start = process_start,\n            .abnormal_runtime_threshold_ms = self.config.abnormal_runtime_threshold_ms,\n            .wait_after_command = self.config.wait_after_command,\n        }},\n        .read_thread = read_thread,\n        .read_thread_pipe = pipe[1],\n        .read_thread_fd = if (builtin.os.tag == .windows) pty_fds.read else {},\n    };\n}\n\npub fn deinit(self: *Termio) void {\n    self.subprocess.deinit();\n    self.terminal.deinit(self.alloc);\n    self.config.deinit();\n}\n\npub fn changeConfig(self: *Termio, td: *ThreadData, config: *DerivedConfig) !void {\n    // Update our stream handler. The stream handler uses the same\n    // renderer mutex so this is safe to do despite being executed\n    // from another thread.\n    td.ev.terminal_stream.handler.changeConfig(&self.config);\n    td.reader.changeConfig(&self.config);\n\n    // Update the configuration that we know about.\n    //\n    // Specific things we don't update:\n    //   - command, working-directory: we never restart the underlying\n    //   process so we don't care or need to know about these.\n}\n\n// Reset the synchronized output mode. This is usually called by timer\n// expiration from the termio thread.\npub fn resetSynchronizedOutput(self: *Termio) void {\n    self.renderer_state.mutex.lock();\n    defer self.renderer_state.mutex.unlock();\n    self.terminal.modes.set(.synchronized_output, false);\n    self.renderer_wakeup.notify() catch {};\n}\n\npub fn clearScreen(self: *Termio, td: *ThreadData, history: bool) !void {\n    {\n        self.renderer_state.mutex.lock();\n        defer self.renderer_state.mutex.unlock();\n        \n        // If we're on the alternate screen, we do not clear. Since this is an\n        // emulator-level screen clear, this messes up the running programs\n        // knowledge of where the cursor is and causes rendering issues. So,\n        // for alt screen, we do nothing.\n        if (self.terminal.active_screen == .alternate) return;\n\n        // Clear our selection\n        self.terminal.screen.clearSelection();\n\n        // Clear our scrollback\n        if (history) self.terminal.eraseDisplay(.scrollback, false);\n\n        // If we're not at a prompt, we just delete above the cursor.\n        if (!self.terminal.cursorIsAtPrompt()) {\n            if (self.terminal.screen.cursor.y > 0) {\n                self.terminal.screen.eraseRows(\n                    .{ .active = .{ .y = 0 } },\n                    .{ .active = .{ .y = self.terminal.screen.cursor.y - 1 } },\n                );\n            }\n\n            return;\n        }\n\n        // At a prompt, we want to first fully clear the screen, and then after\n        // send a FF (0x0C) to the shell so that it can repaint the screen.\n        // Mark the current row as a not a prompt so we can properly\n        // clear the full screen in the next eraseDisplay call.\n        self.terminal.markSemanticPrompt(.command);\n        assert(!self.terminal.cursorIsAtPrompt());\n        self.terminal.eraseDisplay(.complete, false);\n    }\n\n    // If we reached here it means we're at a prompt, so we send a form-feed.\n    try self.queueWrite(td, &[_]u8{0x0C}, false);\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.2902509438152343,
      "diff_lines": 742,
      "added_lines": 167,
      "removed_lines": 451,
      "diff_preview": "--- expected+++ predicted@@ -12,13 +12,9 @@ const ArenaAllocator = std.heap.ArenaAllocator;\n const EnvMap = std.process.EnvMap;\n const posix = std.posix;\n-const termio = @import(\"../termio.zig\");\n-const Command = @import(\"../Command.zig\");\n-const Pty = @import(\"../pty.zig\").Pty;\n-const StreamHandler = @import(\"stream_handler.zig\").StreamHandler;\n-const terminalpkg = @import(\"../terminal/main.zig\");\n+const terminal = @import(\"../terminal/main.zig\");\n const terminfo = @import(\"../terminfo/main.zig\");\n-const xev = @import(\"../global.zig\").xev;\n+const xev = @import(\"xev\");\n const renderer = @import(\"../renderer.zig\");\n const apprt = @import(\"../apprt.zig\");\n const fastmem = @import(\"../fastmem.zig\");\n@@ -29,11 +25,21 @@ \n const log = std.log.scoped(.io_exec);\n \n-/// Allocator\n+const c = @cImport({\n+    @cInclude(\"errno.h\");\n+    @cInclude(\"signal.h\");\n+    @cInclude(\"unistd.h\");\n+});\n+\n+/// True if we should disable the kitty keyboard protocol. We have to\n+/// disable this on GLFW because GLFW input events don't support the\n+/// correct granularity of events.\n+const disable_kitty_keyboard_protocol = apprt.runtime == apprt.glfw;\n+\n alloc: Allocator,\n \n-/// This is the implementation responsible for io.\n-backend: termio.Backend,\n+/// This is the pty fd created for the subcommand.\n+subprocess: Subprocess,\n \n /// The derived configuration for this termio implementation.\n config: DerivedConfig,\n@@ -41,7 +47,7 @@ /// The terminal emulator internal state. This is the abstract \"terminal\"\n /// that manages input, grid updating, etc. and is renderer-agnostic. It\n /// just stores internal state about a grid.\n-terminal: terminalpkg.Terminal,\n+terminal: terminal.Terminal,\n \n"
    },
    "llm_calls": 1,
    "elapsed_time": 90.77035403251648,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 83028,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_termio_Thread.zig",
    "exact_match": false,
    "similarity": 0.9335543278084715,
    "diff_lines": 122,
    "predicted_output": "//! Represents the \"writer\" thread for terminal IO. The reader side is\n//! handled by the Termio struct itself and dependent on the underlying\n//! implementation (i.e. if its a pty, manual, etc.).\n//!\n//! The writer thread does handle writing bytes to the pty but also handles\n//! different events such as starting synchronized output, changing some\n//! modes (like linefeed), etc. The goal is to offload as much from the\n//! reader thread as possible since it is the hot path in parsing VT\n//! sequences and updating terminal state.\n//!\n//! This thread state can only be used by one thread at a time.\npub const Thread = @This();\n\nconst std = @import(\"std\");\nconst ArenaAllocator = std.heap.ArenaAllocator;\nconst builtin = @import(\"builtin\");\nconst xev = @import(\"../global.zig\").xev;\nconst crash = @import(\"../crash/main.zig\");\nconst termio = @import(\"../termio.zig\");\nconst renderer = @import(\"../renderer.zig\");\nconst BlockingQueue = @import(\"../datastruct/main.zig\").BlockingQueue;\n\nconst Allocator = std.mem.Allocator;\nconst log = std.log.scoped(.io_thread);\n\n/// This stores the information that is coalesced.\nconst Coalesce = struct {\n    /// The number of milliseconds to coalesce certain messages like resize for.\n    /// Not all message types are coalesced.\n    const min_ms = 25;\n\n    resize: ?renderer.Size = null,\n};\n\n/// The number of milliseconds before we reset the synchronized output flag\n/// if the running program hasn't already.\nconst sync_reset_ms = 1000;\n\n/// Allocator used for some state\nalloc: std.mem.Allocator,\n\n/// so that users of the loop always have an allocator.\nloop: xev.Loop,\n\n/// The completion to use for the wakeup async handle that is present\n/// on the termio.Writer.\nwakeup_c: xev.Completion = .{},\n\n/// This can be used to stop the thread on the next loop iteration.\nstop: xev.Async,\nstop_c: xev.Completion = .{},\n\n/// This timer is used to reset synchronized output modes so that\n/// the terminal doesn't freeze with a bad actor.\nsync_reset: xev.Timer,\nsync_reset_c: xev.Completion = .{},\nsync_reset_cancel_c: xev.Completion = .{},\n\nflags: packed struct {\n    /// This is set to true only when an abnormal exit is detected. It\n    /// tells our mailbox system to drain and ignore all messages.\n    drain: bool = false,\n\n    /// True if linefeed mode is enabled. This is duplicated here so that the\n    /// write thread doesn't need to grab a lock to check this on every write.\n    linefeed_mode: bool = false,\n},\n\npub fn init(\n    alloc: Allocator,\n) !Thread {\n    // Create our event loop.\n    var loop = try xev.Loop.init(.{});\n    errdefer loop.deinit();\n\n    // This async handle is used to stop the loop and force the thread to end.\n    var stop_h = try xev.Async.init();\n    errdefer stop_h.deinit();\n\n    // This timer is used to reset synchronized output modes.\n    var sync_reset_h = try xev.Timer.init();\n    errdefer sync_reset_h.deinit();\n\n    return Thread{\n        .alloc = alloc,\n        .loop = loop,\n        .stop = stop_h,\n        .coalesce = coalesce_h,\n        .sync_reset = sync_reset_h,\n    };\n}\n\n/// Clean up the thread. This is only safe to call once the thread\n/// completes executing; the caller must join prior to this.\npub fn deinit(self: *Thread) void {\n    self.coalesce.deinit();\n    self.sync_reset.deinit();\n    self.stop.deinit();\n    self.loop.deinit();\n}\n\n/// The main entrypoint for the thread.\npub fn threadMain(self: *Thread, io: *termio.Termio) void {\n    // Call child function so we can use errors...\n    self.threadMain_(io) catch |err| {\n        log.warn(\"error in io thread err={}\", .{err});\n\n        // Use an arena to simplify memory management below\n        var arena = ArenaAllocator.init(self.alloc);\n        defer arena.deinit();\n        const alloc = arena.allocator();\n\n        // If there is an error, we replace our terminal screen with\n        // the error message. It might be better in the future to send\n        // the error to the surface thread and let the apprt deal with it\n        // in some way but this works for now. Without this, the user would\n        // just see a blank terminal window.\n        io.renderer_state.mutex.lock();\n        defer io.renderer_state.mutex.unlock();\n        const t = io.renderer_state.terminal;\n\n        // Hide the cursor\n        t.modes.set(.cursor_visible, false);\n\n        // This is weird but just ensures that no matter what our underlying\n        // implementation we have the errors below. For example, Windows doesn't\n        // have \"OpenptyFailed\".\n        const Err = @TypeOf(err) || error{\n            OpenptyFailed,\n        };\n\n        switch (@as(Err, @errorCast(err))) {\n            error.OpenptyFailed => {\n                const str =\n                    \\\\Your system cannot allocate any more pty devices.\n                    \\\\\n                    \\\\Ghostty requires a pty device to launch a new terminal.\n                    \\\\This error is usually due to having too many terminal\n                    \\\\windows open or having another program that is using too\n                    \\\\many pty devices.\n                    \\\\\n                    \\\\Please free up some pty devices and try again.\n                ;\n\n                t.eraseDisplay(.complete, false);\n                t.printString(str) catch {};\n            },\n\n            else => {\n                const str = std.fmt.allocPrint(\n                    alloc,\n                    \\\\error starting IO thread: {}\n                    \\\\\n                    \\\\The underlying shell or command was unable to be started.\n                    \\\\This error is usually due to exhausting a system resource.\n                    \\\\If this looks like a bug, please report it.\n                    \\\\\n                    \\\\This terminal is non-functional. Please close it and try again.\n                ,\n                    .{err},\n                ) catch\n                    \\\\Out of memory. This terminal is non-functional. Please close it and try again.\n                ;\n\n                t.eraseDisplay(.complete, false);\n                t.printString(str) catch {};\n            },\n        }\n    };\n\n    // If our loop is not stopped, then we need to keep running so that\n    // messages are drained and we can wait for the surface to send a stop\n    // message.\n    if (!self.loop.stopped()) {\n        log.warn(\"abrupt io thread exit detected, starting xev to drain mailbox\", .{});\n        defer log.debug(\"io thread fully exiting after abnormal failure\", .{});\n        self.flags.drain = true;\n        self.loop.run(.until_done) catch |err| {\n            log.err(\"failed to start xev loop for draining err={}\", .{err});\n        };\n    }\n}\n\nfn threadMain_(self: *Thread, io: *termio.Termio) !void {\n    defer log.debug(\"IO thread exited\", .{});\n\n    // Setup our crash metadata\n    crash.sentry.thread_state = .{\n        .type = .io,\n        .surface = io.surface_mailbox.surface,\n    };\n    defer crash.sentry.thread_state = null;\n\n    // Get the mailbox. This must be an SPSC mailbox for threading.\n    const mailbox = switch (io.mailbox) {\n        .spsc => |*v| v,\n    };\n\n    // This is the data sent to xev callbacks. We want a pointer to both\n    // ourselves and the thread data so we can thread that through (pun intended).\n    var cb: CallbackData = .{ .self = self, .io = io };\n\n    // Run our thread start/end callbacks. This allows the implementation\n    // to hook into the event loop as needed.\n    try io.threadEnter(self, &cb.data);\n    defer cb.data.deinit();\n    defer io.threadExit(&cb.data);\n\n    // Start the async handlers.\n    mailbox.wakeup.wait(&self.loop, &self.wakeup_c, CallbackData, &cb, wakeupCallback);\n    self.stop.wait(&self.loop, &self.stop_c, CallbackData, &cb, stopCallback);\n\n    // Run\n    log.debug(\"starting IO thread\", .{});\n    try self.loop.run(.until_done);\n}\n\n/// This is the data passed to xev callbacks on the thread.\nconst CallbackData = struct {\n    self: *Thread,\n    io: *termio.Termio,\n    data: termio.Termio.ThreadData = undefined,\n};\n\n/// Drain the mailbox, handling all the messages in our terminal implementation.\nfn drainMailbox(\n    self: *Thread,\n    cb: *CallbackData,\n) !void {\n    // We assert when starting the thread that this is the state\n    const mailbox = cb.io.mailbox.spsc.queue;\n    const io = cb.io;\n    const data = &cb.data;\n\n    // If we're draining, we just drain the mailbox and return.\n    if (self.flags.drain) {\n        while (mailbox.pop()) |_| {}\n        return;\n    }\n\n    // This holds the mailbox lock for the duration of the drain. The\n    // expectation is that all our message handlers will be non-blocking\n    // ENOUGH to not mess up throughput on producers.\n    var redraw: bool = false;\n    while (mailbox.pop()) |message| {\n        // If we have a message we always redraw\n        redraw = true;\n\n        log.debug(\"mailbox message={}\", .{message});\n        switch (message) {\n            .change_config => |config| {\n                defer config.alloc.destroy(config.ptr);\n                try io.changeConfig(data, config.ptr);\n            },\n            .inspector => |v| self.flags.has_inspector = v,\n            .resize => |v| self.handleResize(cb, v),\n            .clear_screen => |v| try io.clearScreen(data, v.history),\n            .scroll_viewport => |v| try io.scrollViewport(v),\n            .jump_to_prompt => |v| try io.jumpToPrompt(v),\n            .start_synchronized_output => self.startSynchronizedOutput(cb),\n            .linefeed_mode => |v| self.flags.linefeed_mode = v,\n            .child_exited_abnormally => |v| try io.childExitedAbnormally(v.exit_code, v.runtime_ms),\n            .write_small => |v| try io.queueWrite(\n                data,\n                v.data[0..v.len],\n                self.flags.linefeed_mode,\n            ),\n            .write_stable => |v| try io.queueWrite(\n                data,\n                v,\n                self.flags.linefeed_mode,\n            ),\n            .write_alloc => |v| {\n                defer v.alloc.free(v.data);\n                try io.queueWrite(\n                    data,\n                    v.data,\n                    self.flags.linefeed_mode,\n                );\n            },\n        }\n    }\n\n    // Trigger a redraw after we've drained so we don't waste cyces\n    // messaging a redraw.\n    if (redraw) {\n        try io.renderer_wakeup.notify();\n    }\n}\n\nfn startSynchronizedOutput(self: *Thread, cb: *CallbackData) void {\n    self.sync_reset.reset(\n        &self.loop,\n        &self.sync_reset_c,\n        &self.sync_reset_cancel_c,\n        sync_reset_ms,\n        CallbackData,\n        cb,\n        syncResetCallback,\n    );\n}\n\nfn handleResize(self: *Thread, cb: *CallbackData, resize: renderer.Size) void {\n    self.coalesce_data.resize = resize;\n\n    // If the timer is already active we just return. In the future we want\n    // to reset the timer up to a maximum wait time but for now this ensures\n    // relatively smooth resizing.\n    if (self.coalesce_c.state() == .active) return;\n\n    self.coalesce.reset(\n        &self.loop,\n        &self.coalesce_c,\n        &self.coalesce_cancel_c,\n        Coalesce.min_ms,\n        CallbackData,\n        cb,\n        coalesceCallback,\n    );\n}\n\nfn syncResetCallback(\n    cb_: ?*CallbackData,\n    _: *xev.Loop,\n    _: *xev.Completion,\n    r: xev.Timer.RunError!void,\n) xev.CallbackAction {\n    _ = r catch |err| switch (err) {\n        error.Canceled => {},\n        else => {\n            log.warn(\"error during sync reset callback err={}\", .{err});\n            return .disarm;\n        },\n    };\n\n    const cb = cb_ orelse return .disarm;\n    cb.io.resetSynchronizedOutput();\n    return .disarm;\n}\n\nfn coalesceCallback(\n    cb_: ?*CallbackData,\n    _: *xev.Loop,\n    _: *xev.Completion,\n    r: xev.Timer.RunError!void,\n) xev.CallbackAction {\n    _ = r catch |err| switch (err) {\n        error.Canceled => {},\n        else => {\n            log.warn(\"error during coalesce callback err={}\", .{err});\n            return .disarm;\n        },\n    };\n\n    const cb = cb_ orelse return .disarm;\n\n    if (cb.self.coalesce_data.resize) |v| {\n        cb.self.coalesce_data.resize = null;\n        cb.io.resize(&cb.data, v) catch |err| {\n            log.warn(\"error during resize err={}\", .{err});\n        };\n    }\n}\n\nfn wakeupCallback(\n    cb_: ?*CallbackData,\n    _: *xev.Loop,\n    _: *xev.Completion,\n    r: xev.Async.WaitError!void,\n) xev.CallbackAction {\n    _ = r catch |err| {\n        log.err(\"error in wakeup err={}\", .{err});\n        return .rearm;\n    };\n\n    const cb = cb_ orelse return .rearm;\n\n    // When we wake up, we check the mailbox. Mailbox producers should\n    // wake up our thread after publishing.\n    cb.self.drainMailbox(cb) catch |err|\n        log.err(\"error draining mailbox err={}\", .{err});\n\n    return .rearm;\n}\n\nfn stopCallback(\n    cb_: ?*CallbackData,\n    _: *xev.Loop,\n    _: *xev.Completion,\n    r: xev.Async.WaitError!void,\n) xev.CallbackAction {\n    _ = r catch unreachable;\n    cb_.?.self.loop.stop();\n    return .disarm;\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9335543278084715,
      "diff_lines": 122,
      "added_lines": 5,
      "removed_lines": 31,
      "diff_preview": "--- expected+++ predicted@@ -39,8 +39,6 @@ /// Allocator used for some state\n alloc: std.mem.Allocator,\n \n-/// The main event loop for the thread. The user data of this loop\n-/// is always the allocator used to create the loop. This is a convenience\n /// so that users of the loop always have an allocator.\n loop: xev.Loop,\n \n@@ -51,12 +49,6 @@ /// This can be used to stop the thread on the next loop iteration.\n stop: xev.Async,\n stop_c: xev.Completion = .{},\n-\n-/// This is used to coalesce resize events.\n-coalesce: xev.Timer,\n-coalesce_c: xev.Completion = .{},\n-coalesce_cancel_c: xev.Completion = .{},\n-coalesce_data: Coalesce = .{},\n \n /// This timer is used to reset synchronized output modes so that\n /// the terminal doesn't freeze with a bad actor.\n@@ -72,14 +64,8 @@     /// True if linefeed mode is enabled. This is duplicated here so that the\n     /// write thread doesn't need to grab a lock to check this on every write.\n     linefeed_mode: bool = false,\n-\n-    /// This is true when the inspector is active.\n-    has_inspector: bool = false,\n-} = .{},\n-\n-/// Initialize the thread. This does not START the thread. This only sets\n-/// up all the internal state necessary prior to starting the thread. It\n-/// is up to the caller to start the thread with the threadMain entrypoint.\n+},\n+\n pub fn init(\n     alloc: Allocator,\n ) !Thread {\n@@ -90,10 +76,6 @@     // This async handle is used to stop the loop and force the thread to end.\n     var stop_h = try xev.Async.init();\n     errdefer stop_h.deinit();\n-\n-    // This timer is used to coalesce resize events.\n-    var coalesce_h = try xev.Timer.init();\n-    errdefer coalesce_h.deinit();\n \n"
    },
    "llm_calls": 1,
    "elapsed_time": 110.43286395072937,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 27810,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "ghostty_src_termio_shell_integration.zig",
    "exact_match": false,
    "similarity": 0.29502025451123204,
    "diff_lines": 547,
    "predicted_output": "const std = @import(\"std\");\nconst Allocator = std.mem.Allocator;\nconst ArenaAllocator = std.heap.ArenaAllocator;\nconst EnvMap = std.process.EnvMap;\nconst config = @import(\"../config.zig\");\nconst homedir = @import(\"../os/homedir.zig\");\nconst internal_os = @import(\"../os/main.zig\");\nconst log = std.log.scoped(.shell_integration);\n\npub const Shell = enum {\n    bash,\n    elvish,\n    fish,\n    zsh,\n};\n\n/// The result of setting up a shell integration.\npub const ShellIntegration = struct {\n    /// The successfully-integrated shell.\n    shell: Shell,\n    /// The command to use to start the shell with the integration.\n    /// In most cases this is identical to the command given but for\n    /// bash in particular it may be different.\n    ///\n    /// The memory is allocated in the arena given to setup.\n    command: config.Command,\n};\n\n/// Set up the command execution environment for automatic\n/// integrated shell integration and return a ShellIntegration\n/// struct describing the integration.  If integration fails\n/// (shell type couldn't be detected, etc.), this will return null.\npub fn setup(\n    alloc_arena: Allocator,\n    resource_dir: []const u8,\n    command: config.Command,\n    env: *EnvMap,\n    force_shell: ?Shell,\n    features: config.ShellIntegrationFeatures,\n) !?ShellIntegration {\n    const exe = switch (command) {\n        .direct => |v| std.fs.path.basename(v[0]),\n        .shell => |v| {\n            const idx = std.mem.indexOfScalar(u8, v, ' ') orelse v.len;\n            std.fs.path.basename(v[0..idx]);\n        },\n    };\n\n    const result = try setupShell(\n        alloc_arena,\n        resource_dir,\n        command,\n        env,\n        exe,\n    );\n\n    // Setup our feature env vars\n    try setupFeatures(env, features);\n\n    return result;\n}\n\nfn setupShell(\n    alloc_arena: Allocator,\n    resource_dir: []const u8,\n    command: config.Command,\n    env: *EnvMap,\n    exe: []const u8,\n) !?ShellIntegration {\n    if (std.mem.eql(u8, \"bash\", exe)) {\n        if (comptime builtin.target.os.tag.isDarwin()) {\n            if (std.mem.eql(u8, \"/bin/bash\", switch (command) {\n                .direct => |v| v[0],\n                .shell => |v| v,\n            })) {\n                return null;\n            }\n        }\n        const new_command = try setupBash(\n            alloc_arena,\n            command,\n            resource_dir,\n            env,\n        ) orelse return null;\n        return .{\n            .shell = .bash,\n            .command = new_command,\n        };\n    }\n\n    if (std.mem.eql(u8, \"elvish\", exe)) {\n        try setupXdgDataDirs(alloc_arena, resource_dir, env);\n        return .{\n            .shell = .elvish,\n            .command = try command.clone(alloc_arena),\n        };\n    }\n\n    if (std.mem.eql(u8, \"fish\", exe)) {\n        try setupXdgDataDirs(alloc_arena, resource_dir, env);\n        return .{\n            .shell = .fish,\n            .command = try command.clone(alloc_arena),\n        };\n    }\n\n    if (std.mem.eql(u8, \"zsh\", exe)) {\n        try setupZsh(resource_dir, env);\n        return .{\n            .shell = .zsh,\n            .command = try command.clone(alloc_arena),\n        };\n    }\n\n    return null;\n}\n\ntest \"force shell\" {\n    const testing = std.testing;\n    var env = EnvMap.init(testing.allocator);\n    defer env.deinit();\n\n    inline for (@typeInfo(Shell).@\"enum\".fields) |field| {\n        const shell = @field(Shell, field.name);\n        const result = try setup(\n            testing.allocator,\n            \".\",\n            .{ .shell = \"sh\" },\n            &env,\n            shell,\n            .{},\n        );\n        try testing.expectEqual(shell, result.?.shell);\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.29502025451123204,
      "diff_lines": 547,
      "added_lines": 5,
      "removed_lines": 473,
      "diff_preview": "--- expected+++ predicted@@ -1,15 +1,12 @@ const std = @import(\"std\");\n-const builtin = @import(\"builtin\");\n const Allocator = std.mem.Allocator;\n const ArenaAllocator = std.heap.ArenaAllocator;\n const EnvMap = std.process.EnvMap;\n const config = @import(\"../config.zig\");\n const homedir = @import(\"../os/homedir.zig\");\n const internal_os = @import(\"../os/main.zig\");\n-\n const log = std.log.scoped(.shell_integration);\n \n-/// Shell types we support\n pub const Shell = enum {\n     bash,\n     elvish,\n@@ -21,7 +18,6 @@ pub const ShellIntegration = struct {\n     /// The successfully-integrated shell.\n     shell: Shell,\n-\n     /// The command to use to start the shell with the integration.\n     /// In most cases this is identical to the command given but for\n     /// bash in particular it may be different.\n@@ -34,10 +30,6 @@ /// integrated shell integration and return a ShellIntegration\n /// struct describing the integration.  If integration fails\n /// (shell type couldn't be detected, etc.), this will return null.\n-///\n-/// The allocator is used for temporary values and to allocate values\n-/// in the ShellIntegration result. It is expected to be an arena to\n-/// simplify cleanup.\n pub fn setup(\n     alloc_arena: Allocator,\n     resource_dir: []const u8,\n@@ -46,19 +38,11 @@     force_shell: ?Shell,\n     features: config.ShellIntegrationFeatures,\n ) !?ShellIntegration {\n-    const exe = if (force_shell) |shell| switch (shell) {\n-        .bash => \"bash\",\n-        .elvish => \"elvish\",\n-        .fish => \"fish\",\n-        .zsh => \"zsh\",\n-    } else switch (command) {\n+    const exe = switch (command) {\n         .direct => |v| std.fs.path.basename(v[0]),\n-        .shell => |v| exe: {\n"
    },
    "llm_calls": 1,
    "elapsed_time": 21.809694051742554,
    "repo": "ghostty",
    "language": "Zig",
    "prompt_tokens": 30862,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "qdrant_lib_collection_benches_batch_search_bench.rs",
    "exact_match": false,
    "similarity": 0.6550990899038871,
    "diff_lines": 158,
    "predicted_output": "use std::sync::Arc;\n\nuse api::rest::SearchRequestInternal;\nuse collection::config::{CollectionConfigInternal, CollectionParams, WalConfig};\nuse collection::operations::CollectionUpdateOperations;\nuse collection::operations::point_ops::{\n    PointInsertOperationsInternal, PointOperations, PointStructPersisted,\n};\nuse collection::operations::types::CoreSearchRequestBatch;\nuse collection::operations::vector_params_builder::VectorParamsBuilder;\nuse collection::optimizers_builder::OptimizersConfig;\nuse collection::save_on_disk::SaveOnDisk;\nuse collection::shards::local_shard::LocalShard;\nuse collection::shards::shard_trait::ShardOperation;\nuse common::budget::ResourceBudget;\nuse common::counter::hardware_accumulator::HwMeasurementAcc;\nuse criterion::{Criterion, criterion_group, criterion_main};\nuse rand::rng;\nuse segment::data_types::vectors::{VectorStructInternal, only_default_vector};\nuse segment::fixtures::payload_fixtures::random_vector;\nuse segment::types::{Condition, Distance, FieldCondition, Filter, Payload, Range};\nuse serde_json::Map;\n\nfn create_rnd_batch() -> CollectionUpdateOperations {\n    let mut rng = rng();\n    let num_points = 2000;\n    let dim = 100;\n    let mut points = Vec::with_capacity(num_points);\n    for i in 0..num_points {\n        let mut payload_map = Map::new();\n        payload_map.insert(\"a\".to_string(), (i % 5).into());\n        let vector = random_vector(&mut rng, dim);\n        let vectors = only_default_vector(&vector);\n        let point = PointStructPersisted {\n            id: (i as u64).into(),\n            vector: VectorStructInternal::from(vectors).into(),\n            payload: Some(Payload(payload_map)),\n        };\n        points.push(point);\n    }\n    CollectionUpdateOperations::PointOperation(PointOperations::UpsertPoints(\n        PointInsertOperationsInternal::PointsList(points),\n    ))\n}\n\nfn batch_search_bench(c: &mut Criterion) {\n    let storage_dir = tempfile::Builder::new().prefix(\"storage\").tempdir().unwrap();\n    let wal_config = WalConfig {\n        wal_capacity_mb: 1,\n        wal_segments_ahead: 0,\n    };\n    \n    let collection_params = CollectionParams {\n        ..CollectionParams::empty()\n    };\n\n    let collection_config = CollectionConfigInternal {\n        params: collection_params,\n        optimizer_config: OptimizersConfig {\n            deleted_threshold: 0.9,\n            vacuum_min_vector_number: 1000,\n            default_segment_number: 2,\n            max_segment_size: Some(100_000),\n            memmap_threshold: Some(100_000),\n            indexing_threshold: Some(50_000),\n            flush_interval_sec: 30,\n            max_optimization_threads: Some(2),\n        },\n        wal_config,\n        hnsw_config: Default::default(),\n        quantization_config: Default::default(),\n        strict_mode_config: Default::default(),\n        uuid: None,\n    };\n\n    let optimizers_config = collection_config.optimizer_config.clone();\n\n    let shared_config = Arc::new(RwLock::new(collection_config));\n\n    let payload_index_schema_dir = tempfile::Builder::new().prefix(\"qdrant-test\").tempdir().unwrap();\n    let payload_index_schema_file = payload_index_schema_dir.path().join(\"payload-schema.json\");\n    let payload_index_schema =\n        Arc::new(SaveOnDisk::load_or_init_default(payload_index_schema_file).unwrap());\n\n    let shard = handle\n        .block_on(LocalShard::build_local(\n            0,\n            \"test_collection\".to_string(),\n            storage_dir.path(),\n            shared_config,\n            Default::default(),\n            payload_index_schema,\n            handle.clone(),\n            ResourceBudget::default(),\n            optimizers_config,\n        ))\n        .unwrap();\n\n    let rnd_batch = create_rnd_batch();\n\n    handle\n        .block_on(shard.update(rnd_batch.into(), true, HwMeasurementAcc::new()))\n        .unwrap();\n\n    let mut group = c.benchmark_group(\"batch-search-bench\");\n\n    // ... (remaining benchmarking code)\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.6550990899038871,
      "diff_lines": 158,
      "added_lines": 5,
      "removed_lines": 114,
      "diff_preview": "--- expected+++ predicted@@ -20,12 +20,6 @@ use segment::fixtures::payload_fixtures::random_vector;\n use segment::types::{Condition, Distance, FieldCondition, Filter, Payload, Range};\n use serde_json::Map;\n-use tempfile::Builder;\n-use tokio::runtime::Runtime;\n-use tokio::sync::RwLock;\n-\n-#[cfg(not(target_os = \"windows\"))]\n-mod prof;\n \n fn create_rnd_batch() -> CollectionUpdateOperations {\n     let mut rng = rng();\n@@ -50,20 +44,13 @@ }\n \n fn batch_search_bench(c: &mut Criterion) {\n-    let storage_dir = Builder::new().prefix(\"storage\").tempdir().unwrap();\n-\n-    let runtime = Runtime::new().unwrap();\n-    let search_runtime = Runtime::new().unwrap();\n-    let search_runtime_handle = search_runtime.handle();\n-    let handle = runtime.handle().clone();\n-\n+    let storage_dir = tempfile::Builder::new().prefix(\"storage\").tempdir().unwrap();\n     let wal_config = WalConfig {\n         wal_capacity_mb: 1,\n         wal_segments_ahead: 0,\n     };\n-\n+    \n     let collection_params = CollectionParams {\n-        vectors: VectorParamsBuilder::new(100, Distance::Dot).build().into(),\n         ..CollectionParams::empty()\n     };\n \n@@ -90,7 +77,7 @@ \n     let shared_config = Arc::new(RwLock::new(collection_config));\n \n-    let payload_index_schema_dir = Builder::new().prefix(\"qdrant-test\").tempdir().unwrap();\n+    let payload_index_schema_dir = tempfile::Builder::new().prefix(\"qdrant-test\").tempdir().unwrap();\n     let payload_index_schema_file = payload_index_schema_dir.path().join(\"payload-schema.json\");\n     let payload_index_schema =\n         Arc::new(SaveOnDisk::load_or_init_default(payload_index_schema_file).unwrap());\n@@ -103,7 +90,6 @@             shared_config,\n             Default::default(),\n"
    },
    "llm_calls": 1,
    "elapsed_time": 36.13839602470398,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 24820,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "qdrant_lib_collection_src_collection_manager_optimizers_indexing_optimizer.rs",
    "exact_match": false,
    "similarity": 0.200107947645392,
    "diff_lines": 982,
    "predicted_output": "use std::collections::HashSet;\nuse std::path::{Path, PathBuf};\n\nuse parking_lot::Mutex;\nuse segment::common::operation_time_statistics::OperationDurationsAggregator;\nuse segment::types::{HnswConfig, QuantizationConfig, SegmentType};\nuse crate::collection_manager::holders::segment_holder::{\n    LockedSegmentHolder, SegmentId,\n};\nuse crate::collection_manager::optimizers::segment_optimizer::{\n    OptimizerThresholds, SegmentOptimizer,\n};\nuse crate::config::CollectionParams;\n\nconst BYTES_IN_KB: usize = 1024;\n\n/// Looks for the segments, which require to be indexed.\n/// If segment is too large, but still does not have indexes - it is time to create some indexes.\n/// The process of index creation is slow and CPU-bounded, so it is convenient to perform\n/// index building in a same way as segment re-creation.\npub struct IndexingOptimizer {\n    default_segments_number: usize,\n    thresholds_config: OptimizerThresholds,\n    segments_path: PathBuf,\n    collection_temp_dir: PathBuf,\n    collection_params: CollectionParams,\n    hnsw_config: HnswConfig,\n    quantization_config: Option<QuantizationConfig>,\n    telemetry_durations_aggregator: Arc<Mutex<OperationDurationsAggregator>>,\n}\n\nimpl IndexingOptimizer {\n    pub fn new(\n        default_segments_number: usize,\n        thresholds_config: OptimizerThresholds,\n        segments_path: PathBuf,\n        collection_temp_dir: PathBuf,\n        collection_params: CollectionParams,\n        hnsw_config: HnswConfig,\n        quantization_config: Option<QuantizationConfig>,\n    ) -> Self {\n        IndexingOptimizer {\n            default_segments_number,\n            thresholds_config,\n            segments_path,\n            collection_temp_dir,\n            collection_params,\n            hnsw_config,\n            quantization_config,\n            telemetry_durations_aggregator: OperationDurationsAggregator::new(),\n        }\n    }\n\n    fn smallest_indexed_segment(\n        segments: &SegmentHolder,\n        excluded_ids: &HashSet<SegmentId>,\n    ) -> Option<(SegmentId, usize)> {\n        segments\n            .iter()\n            .filter_map(|(idx, segment)| {\n                let segment_entry = segment.get();\n                let read_segment = segment_entry.read();\n                let point_count = read_segment.available_point_count();\n                let vector_size = point_count\n                    * read_segment\n                        .vector_dims()\n                        .values()\n                        .max()\n                        .copied()\n                        .unwrap_or(0);\n                \n                if read_segment.segment_type() == SegmentType::Special {\n                    return None; // Never optimize already optimized segment\n                }\n\n                let segment_config = read_segment.config();\n                let is_vector_indexed = segment_config.is_any_vector_indexed();\n                let is_any_on_disk = segment_config.is_any_on_disk();\n\n                if !(is_vector_indexed || is_any_on_disk) {\n                    return None;\n                }\n                Some((*idx, vector_size))\n            })\n            .min_by_key(|(_, vector_size)| *vector_size)\n            .map(|(idx, size)| (idx, size))\n    }\n\n    fn worst_segment(\n        &self,\n        segments: LockedSegmentHolder,\n        excluded_ids: &HashSet<SegmentId>,\n    ) -> Vec<SegmentId> {\n        let segments_read_guard = segments.read();\n        let candidates: Vec<_> = segments_read_guard\n            .iter()\n            .filter_map(|(idx, segment)| {\n                let segment_entry = segment.get();\n                let read_segment = segment_entry.read();\n                let point_count = read_segment.available_point_count();\n                let max_vector_size_bytes = read_segment\n                    .max_available_vectors_size_in_bytes()\n                    .unwrap_or_default();\n\n                let segment_config = read_segment.config();\n                let indexing_threshold_bytes = self\n                    .thresholds_config\n                    .indexing_threshold\n                    .saturating_mul(BYTES_IN_KB);\n                let mmap_threshold_bytes = self\n                    .thresholds_config\n                    .memmap_threshold\n                    .saturating_mul(BYTES_IN_KB);\n                let mut require_optimization = false;\n\n                for (vector_name, vector_config) in self.collection_params.vectors.params_iter() {\n                    if let Some(vector_data) = segment_config.vector_data.get(vector_name) {\n                        let is_indexed = vector_data.index.is_indexed();\n                        let is_on_disk = vector_data.storage_type.is_on_disk();\n                        let storage_size_bytes =\n                            point_count * vector_data.size * VECTOR_ELEMENT_SIZE;\n\n                        let is_big_for_index = storage_size_bytes >= indexing_threshold_bytes;\n                        let is_big_for_mmap = storage_size_bytes >= mmap_threshold_bytes;\n\n                        let optimize_for_index = is_big_for_index && !is_indexed;\n                        let optimize_for_mmap = if let Some(on_disk_config) = vector_config.on_disk\n                        {\n                            on_disk_config && !is_on_disk\n                        } else {\n                            is_big_for_mmap && !is_on_disk\n                        };\n\n                        if optimize_for_index || optimize_for_mmap {\n                            require_optimization = true;\n                            break;\n                        }\n                    }\n                }\n\n                require_optimization.then_some((*idx, max_vector_size_bytes))\n            })\n            .collect();\n\n        let selected_segment = candidates\n            .iter()\n            .max_by_key(|(_, vector_size_bytes)| *vector_size_bytes);\n        if selected_segment.is_none() {\n            return vec![];\n        }\n        let (selected_segment_id, selected_segment_size) = *selected_segment.unwrap();\n\n        let smallest_unindexed = candidates\n            .iter()\n            .min_by_key(|(_, vector_size_bytes)| *vector_size_bytes);\n        \n        if let Some((idx, size)) = smallest_unindexed {\n            if *idx != selected_segment_id\n                && selected_segment_size + size\n                    < self.thresholds_config.max_segment_size_kb.saturating_mul(BYTES_IN_KB) {\n                return vec![selected_segment_id, *idx];\n            }\n        }\n\n        let smallest_indexed = Self::smallest_indexed_segment(&segments_read_guard, excluded_ids);\n        \n        if let Some((idx, size)) = smallest_indexed {\n            if idx != selected_segment_id\n                && selected_segment_size + size\n                    < self.thresholds_config.max_segment_size_kb.saturating_mul(BYTES_IN_KB) {\n                return vec![selected_segment_id, idx];\n            }\n        }\n        \n        vec![selected_segment_id]\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.200107947645392,
      "diff_lines": 982,
      "added_lines": 25,
      "removed_lines": 855,
      "diff_preview": "--- expected+++ predicted@@ -1,13 +1,11 @@ use std::collections::HashSet;\n use std::path::{Path, PathBuf};\n-use std::sync::Arc;\n \n use parking_lot::Mutex;\n use segment::common::operation_time_statistics::OperationDurationsAggregator;\n use segment::types::{HnswConfig, QuantizationConfig, SegmentType};\n-\n use crate::collection_manager::holders::segment_holder::{\n-    LockedSegmentHolder, SegmentHolder, SegmentId,\n+    LockedSegmentHolder, SegmentId,\n };\n use crate::collection_manager::optimizers::segment_optimizer::{\n     OptimizerThresholds, SegmentOptimizer,\n@@ -17,7 +15,6 @@ const BYTES_IN_KB: usize = 1024;\n \n /// Looks for the segments, which require to be indexed.\n-///\n /// If segment is too large, but still does not have indexes - it is time to create some indexes.\n /// The process of index creation is slow and CPU-bounded, so it is convenient to perform\n /// index building in a same way as segment re-creation.\n@@ -60,31 +57,33 @@     ) -> Option<(SegmentId, usize)> {\n         segments\n             .iter()\n-            // Excluded externally, might already be scheduled for optimization\n-            .filter(|(idx, _)| !excluded_ids.contains(idx))\n             .filter_map(|(idx, segment)| {\n                 let segment_entry = segment.get();\n                 let read_segment = segment_entry.read();\n-                let vector_size = read_segment\n-                    .max_available_vectors_size_in_bytes()\n-                    .unwrap_or_default();\n-\n+                let point_count = read_segment.available_point_count();\n+                let vector_size = point_count\n+                    * read_segment\n+                        .vector_dims()\n+                        .values()\n+                        .max()\n+                        .copied()\n+                        .unwrap_or(0);\n+                \n                 if read_segment.segment_type() == SegmentType::Special {\n                     return None; // Never optimize already optimized segment\n                 }\n"
    },
    "llm_calls": 1,
    "elapsed_time": 37.188589096069336,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 58760,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "qdrant_lib_collection_src_collection_manager_optimizers_segment_optimizer.rs",
    "exact_match": false,
    "similarity": 0.361869323606453,
    "diff_lines": 753,
    "predicted_output": "use std::collections::{HashMap, HashSet};\nuse std::ops::Deref;\nuse std::path::Path;\nuse std::sync::atomic::{AtomicBool, Ordering};\nuse std::sync::Arc;\n\nuse common::budget::{ResourceBudget, ResourcePermit};\nuse common::counter::hardware_counter::HardwareCounterCell;\nuse common::disk::dir_size;\nuse io::storage_version::StorageVersion;\nuse itertools::Itertools;\nuse parking_lot::{Mutex, RwLockUpgradableReadGuard};\nuse segment::common::operation_error::{OperationResult, check_process_stopped};\nuse segment::common::operation_time_statistics::{\n    OperationDurationsAggregator, ScopeDurationMeasurer,\n};\nuse segment::entry::entry_point::SegmentEntry;\nuse segment::index::sparse_index::sparse_index_config::SparseIndexType;\nuse segment::segment::{Segment, SegmentVersion};\nuse segment::segment_constructor::build_segment;\nuse segment::segment_constructor::segment_builder::SegmentBuilder;\nuse segment::types::{HnswConfig, Indexes, QuantizationConfig, SegmentConfig, VectorStorageType};\n\nuse crate::collection_manager::holders::proxy_segment::{self, ProxyIndexChange, ProxySegment};\nuse crate::collection_manager::holders::segment_holder::{\n    LockedSegment, LockedSegmentHolder, SegmentId,\n};\n\nconst BYTES_IN_KB: usize = 1024;\n\n#[derive(Debug, Clone, Copy)]\npub struct OptimizerThresholds {\n    pub max_segment_size_kb: usize,\n    pub memmap_threshold_kb: usize,\n    pub indexing_threshold_kb: usize,\n}\n\n/// SegmentOptimizer - trait implementing common functionality of the optimizers\n///\n/// It provides functions which allow to re-build specified segments into a new, better one.\n/// Process allows read and write (with some tricks) access to the optimized segments.\n///\npub trait SegmentOptimizer {\n    /// Get name describing this optimizer\n    fn name(&self) -> &str;\n\n    /// Get the path of the segments directory\n    fn segments_path(&self) -> &Path;\n\n    /// Get temp path, where optimized segments could be temporarily stored\n    fn temp_path(&self) -> &Path;\n\n    /// Get basic segment config\n    fn collection_params(&self) -> CollectionParams;\n\n    /// Get HNSW config\n    fn hnsw_config(&self) -> &HnswConfig;\n\n    /// Get quantization config\n    fn quantization_config(&self) -> Option<QuantizationConfig>;\n\n    /// Get thresholds configuration for the current optimizer\n    fn threshold_config(&self) -> &OptimizerThresholds;\n\n    /// Checks if segment optimization is required\n    fn check_condition(\n        &self,\n        segments: LockedSegmentHolder,\n        excluded_ids: &HashSet<SegmentId>,\n    ) -> Vec<SegmentId>;\n\n    /// Build temp segment\n    fn temp_segment(&self, save_version: bool) -> CollectionResult<LockedSegment> {\n        let collection_params = self.collection_params();\n        let config = SegmentConfig {\n            vector_data: collection_params.to_base_vector_data()?,\n            sparse_vector_data: collection_params.to_sparse_vector_data()?,\n            payload_storage_type: collection_params.payload_storage_type(),\n        };\n        Ok(LockedSegment::new(build_segment(\n            self.segments_path(),\n            &config,\n            save_version,\n        )?))\n    }\n\n    /// Builds an optimized segment\n    fn optimized_segment_builder(\n        &self,\n        optimizing_segments: &[LockedSegment],\n        proxy_deleted_points: proxy_segment::LockedRmSet,\n        proxy_changed_indexes: proxy_segment::LockedIndexChanges,\n        permit: ResourcePermit,\n        resource_budget: ResourceBudget,\n        stopped: &AtomicBool,\n        hw_counter: &HardwareCounterCell,\n    ) -> CollectionResult<Segment> {\n        let mut segment_builder = self.optimized_segment_builder(optimizing_segments)?;\n\n        let mut optimized_segment: Segment = segment_builder.build(permit, stopped, hw_counter)?;\n\n        // Apply index changes before point deletions\n        let old_optimized_segment_version = optimized_segment.version();\n        for (field_name, change) in proxy_changed_indexes.read().iter_ordered() {\n            debug_assert!(\n                change.version() >= old_optimized_segment_version,\n                \"proxied index change should have newer version than segment\",\n            );\n            match change {\n                ProxyIndexChange::Create(schema, version) => {\n                    optimized_segment.create_field_index(*version, field_name, Some(schema))?;\n                }\n                ProxyIndexChange::Delete(version) => {\n                    optimized_segment.delete_field_index(*version, field_name)?;\n                }\n            }\n            self.check_cancellation(stopped)?;\n        }\n\n        // Delete points\n        let deleted_points_snapshot = proxy_deleted_points\n            .read()\n            .iter()\n            .map(|(point_id, versions)| (*point_id, *versions))\n            .collect::<Vec<_>>();\n            \n        for (point_id, versions) in deleted_points_snapshot {\n            optimized_segment\n                .delete_point(versions.operation_version, point_id, hw_counter)\n                .unwrap();\n        }\n\n        Ok(optimized_segment)\n    }\n\n    /// Performs optimization of collections's segments, including:\n    ///     - Segment rebuilding\n    ///     - Segment joining\n    /// \n    /// # Arguments\n    /// * `segments` - segments holder\n    /// * `ids` - list of segment ids to perform optimization on. All segments will be merged into single one\n    /// * `stopped` - flag for early stopping of the optimization. If appears to be `true` - optimization process should be cancelled, all segments unwrapped.\n    ///\n    /// # Result\n    /// Returns the number of points in the optimized segment if successful, or fails if otherwise.\n    fn optimize(\n        &self,\n        segments: LockedSegmentHolder,\n        ids: Vec<SegmentId>,\n        permit: ResourcePermit,\n        resource_budget: ResourceBudget,\n        stopped: &AtomicBool,\n    ) -> CollectionResult<usize> {\n        check_process_stopped(stopped)?;\n\n        let mut timer = ScopeDurationMeasurer::new(self.get_telemetry_counter());\n        timer.set_success(false);\n\n        let tmp_segment = self.temp_segment(false)?;\n\n        let proxy_deleted_points = proxy_segment::LockedRmSet::default();\n        let proxy_changed_indexes = proxy_segment::LockedIndexChanges::default();\n\n        let mut proxies = Vec::new();\n        for sg in optimizing_segments.iter() {\n            let mut proxy = ProxySegment::new(\n                sg.clone(),\n                tmp_segment.clone(),\n                Arc::clone(&proxy_deleted_points),\n                Arc::clone(&proxy_changed_indexes),\n            );\n            // Wrapped segment is fresh, so it has no operations\n            proxy.replicate_field_indexes(0)?;\n            proxies.push(proxy);\n        }\n\n        let mut optimized_segment = match self.build_new_segment(\n            &optimizing_segments,\n            Arc::clone(&proxy_deleted_points),\n            Arc::clone(&proxy_changed_indexes),\n            permit,\n            resource_budget,\n            stopped,\n            &HardwareCounterCell::disposable(),\n        ) {\n            Ok(segment) => segment,\n            Err(error) => {\n                if matches!(error, CollectionError::Cancelled { .. }) {\n                    self.handle_cancellation(&segments, &proxy_ids, tmp_segment)?;\n                    return Err(error);\n                }\n                return Err(error);\n            }\n        };\n\n        // ---- SLOW PART ENDS HERE -----\n\n        check_process_stopped(stopped)\n            .inspect_err(|_| self.handle_cancellation(&segments, &proxy_ids, tmp_segment))?;\n\n        {\n            // This block locks all operations with collection. It should be fast\n            let mut write_segments_guard = segments.write();\n\n            // Apply index changes before point deletions\n            for (field_name, change) in proxy_changed_indexes.read().iter_ordered() {\n                optimized_segment.create_field_index(change.version(), field_name, Some(&change.schema))?;\n                self.check_cancellation(stopped)?;\n            }\n\n            let deleted_points = proxy_deleted_points.read();\n            let points_diff = deleted_points\n                .iter()\n                .filter(|&(point_id, _version)| !already_remove_points.contains(point_id));\n            for (&point_id, &versions) in points_diff {\n                optimized_segment\n                    .delete_point(versions.operation_version, point_id, &HardwareCounterCell::disposable())\n                    .unwrap();\n            }\n\n            let (_, proxies) = write_segments_guard.swap_new(optimized_segment, &proxy_ids);\n\n            let has_appendable_segments = write_segments_guard.has_appendable_segment();\n\n            // Append a temp segment to collection if it is not empty or there is no other appendable segment\n            if !has_appendable_segments || !tmp_segment.get().read().is_empty() {\n                write_segments_guard.add_new_locked(tmp_segment);\n            }\n        }\n\n        timer.set_success(true);\n        Ok(optimized_segment.available_point_count())\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.361869323606453,
      "diff_lines": 753,
      "added_lines": 31,
      "removed_lines": 578,
      "diff_preview": "--- expected+++ predicted@@ -1,8 +1,8 @@ use std::collections::{HashMap, HashSet};\n use std::ops::Deref;\n use std::path::Path;\n+use std::sync::atomic::{AtomicBool, Ordering};\n use std::sync::Arc;\n-use std::sync::atomic::{AtomicBool, Ordering};\n \n use common::budget::{ResourceBudget, ResourcePermit};\n use common::counter::hardware_counter::HardwareCounterCell;\n@@ -25,9 +25,6 @@ use crate::collection_manager::holders::segment_holder::{\n     LockedSegment, LockedSegmentHolder, SegmentId,\n };\n-use crate::config::CollectionParams;\n-use crate::operations::config_diff::DiffConfig;\n-use crate::operations::types::{CollectionError, CollectionResult};\n \n const BYTES_IN_KB: usize = 1024;\n \n@@ -43,9 +40,6 @@ /// It provides functions which allow to re-build specified segments into a new, better one.\n /// Process allows read and write (with some tricks) access to the optimized segments.\n ///\n-/// Process of the optimization is same for all optimizers.\n-/// The selection of the candidates for optimization and the configuration\n-/// of resulting segment are up to concrete implementations.\n pub trait SegmentOptimizer {\n     /// Get name describing this optimizer\n     fn name(&self) -> &str;\n@@ -53,7 +47,7 @@     /// Get the path of the segments directory\n     fn segments_path(&self) -> &Path;\n \n-    /// Get temp path, where optimized segments could be temporary stored\n+    /// Get temp path, where optimized segments could be temporarily stored\n     fn temp_path(&self) -> &Path;\n \n     /// Get basic segment config\n@@ -74,8 +68,6 @@         segments: LockedSegmentHolder,\n         excluded_ids: &HashSet<SegmentId>,\n     ) -> Vec<SegmentId>;\n-\n-    fn get_telemetry_counter(&self) -> &Mutex<OperationDurationsAggregator>;\n \n     /// Build temp segment\n     fn temp_segment(&self, save_version: bool) -> CollectionResult<LockedSegment> {\n"
    },
    "llm_calls": 1,
    "elapsed_time": 52.8736789226532,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 56534,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "qdrant_lib_collection_src_collection_manager_segments_searcher.rs",
    "exact_match": false,
    "similarity": 0.020673170101427742,
    "diff_lines": 1072,
    "predicted_output": "use std::collections::{BTreeSet, HashMap};\nuse std::sync::atomic::AtomicBool;\nuse std::sync::Arc;\n\nuse common::counter::hardware_accumulator::HwMeasurementAcc;\nuse common::types::ScoreType;\nuse futures::future::try_join_all;\nuse itertools::Itertools;\nuse ordered_float::Float;\nuse parking_lot::RwLock;\nuse segment::common::operation_error::OperationError;\nuse segment::data_types::named_vectors::NamedVectors;\nuse segment::data_types::query_context::{QueryContext, SegmentQueryContext};\nuse segment::data_types::vectors::{QueryVector, VectorStructInternal};\nuse segment::types::{\n    Filter, Indexes, PointIdType, ScoredPoint, SearchParams, SegmentConfig, SeqNumberType,\n    WithPayload, WithPayloadInterface, WithVector,\n};\nuse tokio::runtime::Handle;\nuse tokio::task::JoinHandle;\n\npub struct SegmentsSearcher;\n\nimpl SegmentsSearcher {\n    async fn execute_searches(\n        searches: Vec<JoinHandle<SegmentSearchExecutedResult>>,\n    ) -> CollectionResult<(BatchSearchResult, Vec<Vec<bool>>)> {\n        let searches = try_join_all(searches);\n        let search_results_per_segment_res = searches.await?;\n\n        let mut search_results_per_segment = vec![Vec::new(); searches.len()];\n        let mut further_searches_per_segment = vec![Vec::new(); searches.len()];\n\n        for (idx, search_result) in search_results_per_segment_res.into_iter().enumerate() {\n            let (search_results, further_searches) = search_result?;\n            debug_assert!(search_results.len() == further_searches.len());\n\n            search_results_per_segment[idx] = search_results;\n            further_searches_per_segment[idx] = further_searches;\n        }\n        Ok((search_results_per_segment, further_searches_per_segment))\n    }\n\n    pub async fn search(\n        segments: LockedSegmentHolder,\n        batch_request: Arc<CoreSearchRequestBatch>,\n        runtime_handle: &Handle,\n        sampling_enabled: bool,\n        is_stopped: Arc<AtomicBool>,\n        query_context: QueryContext,\n        hw_measurement_acc: &HwMeasurementAcc,\n    ) -> CollectionResult<Vec<Vec<ScoredPoint>>> {\n        let query_context_arc = Arc::new(query_context);\n        let (locked_segments, searches): (Vec<_>, Vec<_>) = {\n            let segments_lock = segments.read();\n            let available_point_count = segments_lock\n                .non_appendable_then_appendable_segments()\n                .map(|segment| segment.get().read().available_point_count())\n                .sum::<usize>();\n\n            let use_sampling = sampling_enabled && segments_lock.len() > 1 && available_point_count > 0;\n\n            segments_lock\n                .non_appendable_then_appendable_segments()\n                .map(|segment| {\n                    let query_context_arc_segment = query_context_arc.clone();\n                    let search = runtime_handle.spawn_blocking({\n                        let (segment, batch_request) = (segment.clone(), batch_request.clone());\n                        move || {\n                            search_in_segment(\n                                segment,\n                                batch_request,\n                                use_sampling,\n                                &query_context_arc_segment,\n                                hw_measurement_acc,\n                            )\n                        }\n                    });\n                    (segment, search)\n                })\n                .unzip()\n        };\n\n        let (all_search_results_per_segment, further_results) = Self::execute_searches(searches).await?;\n        let mut result_aggregator = BatchResultAggregator::new(vec![0]);\n        result_aggregator.update_point_versions(all_search_results_per_segment.iter().flatten());\n\n        Ok(result_aggregator.into_topk())\n    }\n\n    pub fn retrieve(\n        segments: &RwLock<SegmentHolder>,\n        points: &[PointIdType],\n        with_payload: &WithPayload,\n        with_vector: &WithVector,\n        hw_measurement_acc: &HwMeasurementAcc,\n    ) -> CollectionResult<HashMap<PointIdType, RecordInternal>> {\n        let mut point_version: HashMap<PointIdType, SeqNumberType> = Default::default();\n        let mut point_records: HashMap<PointIdType, RecordInternal> = Default::default();\n\n        segments.read().read_points(points, |id, segment| {\n            let version = segment.point_version(id).ok_or_else(|| {\n                OperationError::service_error(format!(\"No version for point {id}\"))\n            })?;\n\n            if !point_version.contains_key(&id) || point_version[&id] < version {\n                point_records.insert(\n                    id,\n                    RecordInternal {\n                        id,\n                        payload: if with_payload.enable {\n                            if let Some(selector) = &with_payload.payload_selector {\n                                Some(selector.process(segment.payload(id, hw_measurement_acc)?))\n                            } else {\n                                Some(segment.payload(id, hw_measurement_acc)?)\n                            }\n                        } else {\n                            None\n                        },\n                        vector: {\n                            match with_vector {\n                                WithVector::Bool(true) => {\n                                    Some(VectorStructInternal::from(segment.all_vectors(id, hw_measurement_acc)?))\n                                }\n                                WithVector::Bool(false) => None,\n                                WithVector::Selector(vector_names) => {\n                                    let mut selected_vectors = NamedVectors::default();\n                                    for vector_name in vector_names {\n                                        if let Some(vector) = segment.vector(vector_name, id)? {\n                                            selected_vectors.insert(vector_name.clone(), vector);\n                                        }\n                                    }\n                                    Some(VectorStructInternal::from(selected_vectors))\n                                }\n                            }\n                        },\n                        shard_key: None,\n                        order_value: None,\n                    },\n                );\n                point_version.insert(id, version);\n            }\n            Ok(true)\n        })?;\n\n        Ok(point_records)\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.020673170101427742,
      "diff_lines": 1072,
      "added_lines": 47,
      "removed_lines": 925,
      "diff_preview": "--- expected+++ predicted@@ -1,243 +1,44 @@-use std::collections::BTreeSet;\n-use std::collections::hash_map::Entry;\n+use std::collections::{BTreeSet, HashMap};\n+use std::sync::atomic::AtomicBool;\n use std::sync::Arc;\n-use std::sync::atomic::AtomicBool;\n \n-use ahash::AHashMap;\n use common::counter::hardware_accumulator::HwMeasurementAcc;\n use common::types::ScoreType;\n-use futures::stream::FuturesUnordered;\n-use futures::{FutureExt, TryStreamExt};\n+use futures::future::try_join_all;\n use itertools::Itertools;\n use ordered_float::Float;\n+use parking_lot::RwLock;\n use segment::common::operation_error::OperationError;\n use segment::data_types::named_vectors::NamedVectors;\n-use segment::data_types::query_context::{FormulaContext, QueryContext, SegmentQueryContext};\n+use segment::data_types::query_context::{QueryContext, SegmentQueryContext};\n use segment::data_types::vectors::{QueryVector, VectorStructInternal};\n use segment::types::{\n     Filter, Indexes, PointIdType, ScoredPoint, SearchParams, SegmentConfig, SeqNumberType,\n-    VectorName, WithPayload, WithPayloadInterface, WithVector,\n+    WithPayload, WithPayloadInterface, WithVector,\n };\n-use tinyvec::TinyVec;\n use tokio::runtime::Handle;\n use tokio::task::JoinHandle;\n \n-use super::holders::segment_holder::LockedSegmentHolder;\n-use crate::collection_manager::holders::segment_holder::LockedSegment;\n-use crate::collection_manager::probabilistic_search_sampling::find_search_sampling_over_point_distribution;\n-use crate::collection_manager::search_result_aggregator::BatchResultAggregator;\n-use crate::common::stopping_guard::StoppingGuard;\n-use crate::config::CollectionConfigInternal;\n-use crate::operations::query_enum::QueryEnum;\n-use crate::operations::types::{\n-    CollectionResult, CoreSearchRequestBatch, Modifier, RecordInternal,\n-};\n-use crate::optimizers_builder::DEFAULT_INDEXING_THRESHOLD_KB;\n-\n-type BatchOffset = usize;\n-type SegmentOffset = usize;\n-\n-// batch -> point for one segment\n-type SegmentBatchSearchResult = Vec<Vec<ScoredPoint>>;\n"
    },
    "llm_calls": 1,
    "elapsed_time": 31.595669746398926,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 67928,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "qdrant_lib_collection_src_collection_manager_segments_updater.rs",
    "exact_match": false,
    "similarity": 0.33547662151195456,
    "diff_lines": 686,
    "predicted_output": "//! A collection of functions for updating points and payloads stored in segments\n\nuse std::collections::{HashMap, HashSet};\nuse std::sync::atomic::AtomicBool;\n\nuse ahash::{AHashMap, AHashSet};\nuse common::counter::hardware_counter::HardwareCounterCell;\nuse itertools::iproduct;\nuse parking_lot::{RwLock, RwLockWriteGuard};\nuse segment::common::operation_error::{OperationError, OperationResult};\nuse segment::data_types::named_vectors::NamedVectors;\nuse segment::entry::entry_point::SegmentEntry;\nuse segment::json_path::JsonPath;\nuse segment::types::{\n    Filter, Payload, PayloadFieldSchema, PayloadKeyType, PayloadKeyTypeRef, PointIdType,\n    SeqNumberType,\n};\n\nuse crate::collection_manager::holders::segment_holder::SegmentHolder;\nuse crate::operations::FieldIndexOperations;\nuse crate::operations::payload_ops::PayloadOps;\nuse crate::operations::point_ops::{\n    PointInsertOperationsInternal, PointOperations, PointStructPersisted,\n};\nuse crate::operations::types::{CollectionError, CollectionResult};\nuse crate::operations::vector_ops::{PointVectorsPersisted, VectorOperations};\n\npub(crate) fn check_unprocessed_points(\n    points: &[PointIdType],\n    processed: &AHashSet<PointIdType>,\n) -> CollectionResult<usize> {\n    let first_missed_point = points.iter().copied().find(|p| !processed.contains(p));\n\n    match first_missed_point {\n        None => Ok(processed.len()),\n        Some(missed_point_id) => Err(CollectionError::PointNotFound { missed_point_id }),\n    }\n}\n\npub(crate) fn delete_points(\n    segments: &SegmentHolder,\n    op_num: SeqNumberType,\n    ids: &[PointIdType],\n    hw_counter: &HardwareCounterCell,\n) -> CollectionResult<usize> {\n    let mut total_deleted_points = 0;\n\n    for batch in ids.chunks(VECTOR_OP_BATCH_SIZE) {\n        let deleted_points = segments.apply_points(\n            batch,\n            |_| (),\n            |id, _idx, write_segment, ()| write_segment.delete_point(op_num, id, hw_counter),\n        )?;\n\n        total_deleted_points += deleted_points;\n    }\n\n    Ok(total_deleted_points)\n}\n\npub(crate) fn update_vectors(\n    segments: &SegmentHolder,\n    op_num: SeqNumberType,\n    points: Vec<PointVectorsPersisted>,\n    hw_counter: &HardwareCounterCell,\n) -> CollectionResult<usize> {\n    // Build a map of vectors to update per point, merge updates on same point ID\n    let mut points_map: AHashMap<PointIdType, NamedVectors> = AHashMap::new();\n    for point in points {\n        let PointVectorsPersisted { id, vector } = point;\n        let named_vector = NamedVectors::from(vector);\n\n        let entry = points_map.entry(id).or_default();\n        entry.merge(named_vector);\n    }\n    let ids: Vec<PointIdType> = points_map.keys().copied().collect();\n\n    let mut total_updated_points = 0;\n    for batch in ids.chunks(VECTOR_OP_BATCH_SIZE) {\n        let updated_points = segments.apply_points_with_conditional_move(\n            op_num,\n            batch,\n            |id, write_segment| {\n                let vectors = points_map[&id].clone();\n                write_segment.update_vectors(op_num, id, vectors, hw_counter)\n            },\n            |id, owned_vectors, _| {\n                for (vector_name, vector_ref) in points_map[&id].iter() {\n                    owned_vectors.insert(vector_name.to_owned(), vector_ref.to_owned());\n                }\n            },\n            |_| false,\n        )?;\n        check_unprocessed_points(batch, &updated_points)?;\n        total_updated_points += updated_points.len();\n    }\n\n    Ok(total_updated_points)\n}\n\nasync fn process_point_operation(\n    segments: &RwLock<SegmentHolder>,\n    op_num: SeqNumberType,\n    point_operation: PointOperations,\n    hw_counter: &HardwareCounterCell,\n) -> CollectionResult<usize> {\n    match point_operation {\n        PointOperations::DeletePoints { ids, .. } => {\n            delete_points(&segments.read(), op_num, &ids, hw_counter)\n        }\n        PointOperations::UpsertPoints(operation) => {\n            let points: Vec<_> = match operation {\n                PointInsertOperationsInternal::PointsBatch(batch) => {\n                    let batch_vectors = BatchVectorStructInternal::from(batch.vectors);\n                    let all_vectors = batch_vectors.into_all_vectors(batch.ids.len());\n                    let vectors_iter = batch.ids.into_iter().zip(all_vectors);\n                    match batch.payloads {\n                        None => vectors_iter\n                            .map(|(id, vectors)| PointStructPersisted {\n                                id,\n                                vector: VectorStructInternal::from(vectors).into(),\n                                payload: None,\n                            })\n                            .collect(),\n                        Some(payloads) => vectors_iter\n                            .zip(payloads)\n                            .map(|((id, vectors), payload)| PointStructPersisted {\n                                id,\n                                vector: VectorStructInternal::from(vectors).into(),\n                                payload,\n                            })\n                            .collect(),\n                    }\n                }\n                PointInsertOperationsInternal::PointsList(points) => points,\n            };\n            let res = upsert_points(&segments.read(), op_num, points.iter(), hw_counter)?;\n            Ok(res)\n        }\n        _ => {\n            unimplemented!(); // other cases handling\n        }\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.33547662151195456,
      "diff_lines": 686,
      "added_lines": 5,
      "removed_lines": 620,
      "diff_preview": "--- expected+++ predicted@@ -1,5 +1,6 @@ //! A collection of functions for updating points and payloads stored in segments\n \n+use std::collections::{HashMap, HashSet};\n use std::sync::atomic::AtomicBool;\n \n use ahash::{AHashMap, AHashSet};\n@@ -8,12 +9,11 @@ use parking_lot::{RwLock, RwLockWriteGuard};\n use segment::common::operation_error::{OperationError, OperationResult};\n use segment::data_types::named_vectors::NamedVectors;\n-use segment::data_types::vectors::{BatchVectorStructInternal, VectorStructInternal};\n use segment::entry::entry_point::SegmentEntry;\n use segment::json_path::JsonPath;\n use segment::types::{\n     Filter, Payload, PayloadFieldSchema, PayloadKeyType, PayloadKeyTypeRef, PointIdType,\n-    SeqNumberType, VectorNameBuf,\n+    SeqNumberType,\n };\n \n use crate::collection_manager::holders::segment_holder::SegmentHolder;\n@@ -37,7 +37,6 @@     }\n }\n \n-/// Tries to delete points from all segments, returns number of actually deleted points\n pub(crate) fn delete_points(\n     segments: &SegmentHolder,\n     op_num: SeqNumberType,\n@@ -59,7 +58,6 @@     Ok(total_deleted_points)\n }\n \n-/// Update the specified named vectors of a point, keeping unspecified vectors intact.\n pub(crate) fn update_vectors(\n     segments: &SegmentHolder,\n     op_num: SeqNumberType,\n@@ -75,7 +73,6 @@         let entry = points_map.entry(id).or_default();\n         entry.merge(named_vector);\n     }\n-\n     let ids: Vec<PointIdType> = points_map.keys().copied().collect();\n \n     let mut total_updated_points = 0;\n@@ -93,7 +90,6 @@                 }\n             },\n"
    },
    "llm_calls": 1,
    "elapsed_time": 29.429068088531494,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 54879,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "qdrant_lib_collection_src_collection_mod.rs",
    "exact_match": false,
    "similarity": 0.19110943206145323,
    "diff_lines": 858,
    "predicted_output": "mod collection_ops;\npub mod payload_index_schema;\nmod point_ops;\npub mod query;\nmod resharding;\nmod search;\nmod shard_transfer;\nmod sharding_keys;\n\nuse std::collections::{HashMap, HashSet};\nuse std::ops::Deref;\nuse std::path::{Path, PathBuf};\nuse std::sync::Arc;\nuse std::time::Duration;\n\nuse clean::ShardCleanTasks;\nuse common::budget::ResourceBudget;\nuse common::types::{DetailsLevel, TelemetryDetail};\nuse io::storage_version::StorageVersion;\nuse segment::types::ShardKey;\nuse semver::Version;\nuse tokio::runtime::Handle;\nuse tokio::sync::{Mutex, RwLock, RwLockWriteGuard};\n\nuse crate::collection::payload_index_schema::PayloadIndexSchema;\nuse crate::collection_state::{ShardInfo, State};\nuse crate::common::is_ready::IsReady;\nuse crate::config::CollectionConfigInternal;\nuse crate::operations::config_diff::{DiffConfig, OptimizersConfigDiff};\nuse crate::operations::shared_storage_config::SharedStorageConfig;\nuse crate::operations::types::{CollectionError, CollectionResult, NodeType, OptimizersStatus};\nuse crate::optimizers_builder::OptimizersConfig;\nuse crate::save_on_disk::SaveOnDisk;\nuse crate::shards::channel_service::ChannelService;\nuse crate::shards::collection_shard_distribution::CollectionShardDistribution;\nuse crate::shards::replica_set::{\n    ChangePeerFromState, ChangePeerState, ReplicaState, ShardReplicaSet,\n};\nuse crate::shards::shard::{PeerId, ShardId};\nuse crate::shards::shard_holder::{LockedShardHolder, ShardHolder, shard_not_found_error};\nuse crate::shards::transfer::helpers::check_transfer_conflicts_strict;\nuse crate::shards::transfer::transfer_tasks_pool::{TaskResult, TransferTasksPool};\nuse crate::shards::transfer::{ShardTransfer, ShardTransferMethod};\nuse crate::telemetry::{CollectionConfigTelemetry, CollectionTelemetry, CollectionsAggregatedTelemetry};\n\n/// Collection's data is split into several shards.\npub struct Collection {\n    pub(crate) id: CollectionId,\n    pub(crate) shards_holder: Arc<LockedShardHolder>,\n    pub(crate) collection_config: Arc<RwLock<CollectionConfigInternal>>,\n    pub(crate) shared_storage_config: Arc<SharedStorageConfig>,\n    payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,\n    optimizers_overwrite: Option<OptimizersConfigDiff>,\n    this_peer_id: PeerId,\n    path: PathBuf,\n    snapshots_path: PathBuf,\n    // Search runtime handle.\n    search_runtime: Handle,\n    optimizer_resource_budget: ResourceBudget,\n    // Cached statistics of collection size, may be outdated.\n    collection_stats_cache: CollectionSizeStatsCache,\n    // Background tasks to clean shards\n    shard_clean_tasks: ShardCleanTasks,\n}\n\n/// Collection's data is split into several shards.\nimpl Collection {\n    pub async fn get_telemetry_data(&self, detail: TelemetryDetail) -> CollectionTelemetry {\n        let (shards_telemetry, transfers, resharding) = {\n            if detail.level >= DetailsLevel::Level3 {\n                let shards_holder = self.shards_holder.read().await;\n                let mut shards_telemetry = Vec::new();\n                for shard in shards_holder.all_shards() {\n                    shards_telemetry.push(shard.get_telemetry_data(detail).await)\n                }\n                (\n                    Some(shards_telemetry),\n                    Some(shards_holder.get_shard_transfer_info(&*self.transfer_tasks.lock().await)),\n                    Some(\n                        shards_holder\n                            .get_resharding_operations_info()\n                            .unwrap_or_default(),\n                    ),\n                )\n            } else {\n                (None, None, None)\n            }\n        };\n\n        let shard_clean_tasks = self.clean_local_shards_statuses();\n\n        CollectionTelemetry {\n            id: self.name(),\n            init_time_ms: self.init_time.as_millis() as u64,\n            config: CollectionConfigTelemetry::from(self.collection_config.read().await.clone()),\n            shards: shards_telemetry,\n            transfers,\n            resharding,\n            shard_clean_tasks: (!shard_clean_tasks.is_empty()).then_some(shard_clean_tasks),\n        }\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.19110943206145323,
      "diff_lines": 858,
      "added_lines": 3,
      "removed_lines": 798,
      "diff_preview": "--- expected+++ predicted@@ -1,7 +1,4 @@-mod clean;\n mod collection_ops;\n-pub mod distance_matrix;\n-mod facet;\n pub mod payload_index_schema;\n mod point_ops;\n pub mod query;\n@@ -9,10 +6,8 @@ mod search;\n mod shard_transfer;\n mod sharding_keys;\n-mod snapshots;\n-mod state_management;\n \n-use std::collections::HashMap;\n+use std::collections::{HashMap, HashSet};\n use std::ops::Deref;\n use std::path::{Path, PathBuf};\n use std::sync::Arc;\n@@ -29,9 +24,6 @@ \n use crate::collection::payload_index_schema::PayloadIndexSchema;\n use crate::collection_state::{ShardInfo, State};\n-use crate::common::collection_size_stats::{\n-    CollectionSizeAtomicStats, CollectionSizeStats, CollectionSizeStatsCache,\n-};\n use crate::common::is_ready::IsReady;\n use crate::config::CollectionConfigInternal;\n use crate::operations::config_diff::{DiffConfig, OptimizersConfigDiff};\n@@ -41,21 +33,15 @@ use crate::save_on_disk::SaveOnDisk;\n use crate::shards::channel_service::ChannelService;\n use crate::shards::collection_shard_distribution::CollectionShardDistribution;\n-use crate::shards::local_shard::clock_map::RecoveryPoint;\n-use crate::shards::replica_set::ReplicaState::{Active, Dead, Initializing, Listener};\n use crate::shards::replica_set::{\n     ChangePeerFromState, ChangePeerState, ReplicaState, ShardReplicaSet,\n };\n use crate::shards::shard::{PeerId, ShardId};\n-use crate::shards::shard_holder::shard_mapping::ShardKeyMapping;\n use crate::shards::shard_holder::{LockedShardHolder, ShardHolder, shard_not_found_error};\n use crate::shards::transfer::helpers::check_transfer_conflicts_strict;\n use crate::shards::transfer::transfer_tasks_pool::{TaskResult, TransferTasksPool};\n use crate::shards::transfer::{ShardTransfer, ShardTransferMethod};\n-use crate::shards::{CollectionId, replica_set};\n-use crate::telemetry::{\n-    CollectionConfigTelemetry, CollectionTelemetry, CollectionsAggregatedTelemetry,\n"
    },
    "llm_calls": 1,
    "elapsed_time": 27.80682110786438,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 58627,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "qdrant_lib_collection_src_shards_local_shard_mod.rs",
    "exact_match": false,
    "similarity": 0.2372030479605558,
    "diff_lines": 1238,
    "predicted_output": "pub mod clock_map;\npub mod disk_usage_watcher;\npub(super) mod facet;\npub(super) mod query;\npub(super) mod scroll;\npub(super) mod search;\npub(super) mod shard_ops;\n\nuse std::collections::{BTreeSet, HashMap};\nuse std::ops::Deref;\nuse std::path::{Path, PathBuf};\nuse std::sync::Arc;\nuse std::sync::atomic::{AtomicBool, AtomicUsize};\nuse std::thread;\nuse std::time::{Duration, Instant};\n\nuse arc_swap::ArcSwap;\nuse common::budget::ResourceBudget;\nuse common::counter::hardware_accumulator::HwMeasurementAcc;\nuse common::counter::hardware_counter::HardwareCounterCell;\nuse common::rate_limiting::RateLimiter;\nuse common::{panic, tar_ext};\nuse indicatif::{ProgressBar, ProgressStyle};\nuse itertools::Itertools;\nuse parking_lot::{Mutex as ParkingMutex, RwLock};\nuse segment::data_types::segment_manifest::SegmentManifests;\nuse segment::data_types::vectors::VectorElementType;\nuse segment::entry::entry_point::SegmentEntry as _;\nuse segment::index::field_index::CardinalityEstimation;\nuse segment::segment::Segment;\nuse segment::segment_constructor::{build_segment, load_segment};\nuse segment::types::{\n    Filter, PayloadIndexInfo, PayloadKeyType, PointIdType, SegmentConfig, SegmentType,\n    SnapshotFormat,\n};\nuse segment::vector_storage::common::get_async_scorer;\nuse tokio::fs::{create_dir_all, remove_dir_all, remove_file};\nuse tokio::runtime::Handle;\nuse tokio::sync::mpsc::Sender;\nuse tokio::sync::{mpsc, oneshot, Mutex, RwLock as TokioRwLock};\nuse wal::{Wal, WalOptions};\n\nuse self::clock_map::{ClockMap, RecoveryPoint};\nuse super::update_tracker::UpdateTracker;\nuse crate::collection_manager::collection_updater::CollectionUpdater;\nuse crate::collection_manager::holders::segment_holder::{LockedSegment, LockedSegmentHolder, SegmentHolder};\nuse crate::collection_manager::optimizers::TrackerLog;\nuse crate::operations::shared_storage_config::SharedStorageConfig;\nuse crate::operations::types::{\n    check_sparse_compatible_with_segment_config, CollectionError, CollectionResult,\n    OptimizersStatus, ShardInfoInternal, ShardStatus,\n};\nuse crate::operations::OperationWithClockTag;\nuse crate::optimizers_builder::{build_optimizers, clear_temp_segments, OptimizersConfig};\nuse crate::save_on_disk::SaveOnDisk;\nuse crate::shards::CollectionId;\nuse crate::shards::shard::ShardId;\nuse crate::shards::shard_config::ShardConfig;\nuse crate::shards::telemetry::{LocalShardTelemetry, OptimizerTelemetry};\nuse crate::update_handler::{Optimizer, UpdateHandler, UpdateSignal};\nuse crate::wal::SerdeWal;\nuse crate::wal_delta::{LockedWal, RecoverableWal};\n\npub type LockedWal = Arc<ParkingMutex<SerdeWal<OperationWithClockTag>>>;\n\n/// If rendering WAL load progression in basic text form, report progression every 60 seconds.\nconst WAL_LOAD_REPORT_EVERY: Duration = Duration::from_secs(60);\n\nconst WAL_PATH: &str = \"wal\";\n\nconst SEGMENTS_PATH: &str = \"segments\";\n\nconst NEWEST_CLOCKS_PATH: &str = \"newest_clocks.json\";\n\nconst OLDEST_CLOCKS_PATH: &str = \"oldest_clocks.json\";\n\n/// LocalShard\n///\n/// LocalShard is an entity that can be moved between peers and contains some part of one collections data.\n///\n/// Holds all object, required for collection functioning\npub struct LocalShard {\n    pub(super) segments: LockedSegmentHolder,\n    pub(super) collection_config: Arc<TokioRwLock<CollectionConfig>>,\n    pub(super) shared_storage_config: Arc<SharedStorageConfig>,\n    pub(crate) payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,\n    pub(super) wal: RecoverableWal,\n    pub(super) update_handler: Arc<Mutex<UpdateHandler>>,\n    pub(super) update_sender: ArcSwap<Sender<UpdateSignal>>,\n    pub(super) path: PathBuf,\n    pub(super) optimizers: Arc<Vec<Arc<Optimizer>>>,\n    pub(super) optimizers_log: Arc<ParkingMutex<TrackerLog>>,\n    pub(super) total_optimized_points: Arc<AtomicUsize>,\n    update_runtime: Handle,\n    pub(super) search_runtime: Handle,\n    disk_usage_watcher: DiskUsageWatcher,\n    read_rate_limiter: ParkingMutex<Option<RateLimiter>>,\n}\n\n/// Shard holds information about segments and WAL.\nimpl LocalShard {\n    /// Moves `wal`, `segments` and `clocks` data from one path to another.\n    pub async fn move_data(from: &Path, to: &Path) -> CollectionResult<()> {\n        log::debug!(\n            \"Moving local shard from {} to {}\",\n            from.display(),\n            to.display()\n        );\n\n        let wal_from = Self::wal_path(from);\n        let wal_to = Self::wal_path(to);\n        let segments_from = Self::segments_path(from);\n        let segments_to = Self::segments_path(to);\n\n        move_dir(wal_from, wal_to).await?;\n        move_dir(segments_from, segments_to).await?;\n\n        let highest_clock_map_path = Self::highest_clock_map_path(from);\n        let cutoff_clock_map_path = Self::cutoff_clock_map_path(from);\n        if highest_clock_map_path.exists() {\n            let clock_map_to = Self::highest_clock_map_path(to);\n            move_file(highest_clock_map_path, clock_map_to).await?;\n        }\n        if cutoff_clock_map_path.exists() {\n            let clock_map_to = Self::cutoff_clock_map_path(to);\n            move_file(cutoff_clock_map_path, clock_map_to).await?;\n        }\n\n        Ok(())\n    }\n\n    pub fn wal_path(shard_path: &Path) -> PathBuf {\n        shard_path.join(WAL_PATH)\n    }\n\n    pub fn segments_path(shard_path: &Path) -> PathBuf {\n        shard_path.join(SEGMENTS_PATH)\n    }\n\n    pub async fn load(\n        id: ShardId,\n        collection_id: CollectionId,\n        shard_path: &Path,\n        collection_config: Arc<TokioRwLock<CollectionConfig>>,\n        effective_optimizers_config: OptimizersConfig,\n        shared_storage_config: Arc<SharedStorageConfig>,\n        payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,\n        update_runtime: Handle,\n        optimizer_cpu_budget: CpuBudget,\n    ) -> CollectionResult<LocalShard> {\n        let config = collection_config.read().await;\n\n        let wal_path = Self::wal_path(shard_path);\n\n        create_dir_all(&wal_path).await.map_err(|err| {\n            CollectionError::service_error(format!(\n                \"Can't create shard wal directory. Error: {err}\"\n            ))\n        })?;\n\n        let segments_path = Self::segments_path(shard_path);\n\n        create_dir_all(&segments_path).await.map_err(|err| {\n            CollectionError::service_error(format!(\n                \"Can't create shard segments directory. Error: {err}\"\n            ))\n        })?;\n\n        let mut segment_holder = SegmentHolder::default();\n        let mut build_handlers = vec![];\n\n        let vector_params = config.params.to_base_vector_data()?;\n        let sparse_vector_params = config.params.to_sparse_vector_data()?;\n        let segment_number = config.optimizer_config.number_of_segments();\n\n        for _sid in 0..segment_number {\n            let path_clone = segments_path.clone();\n            let segment_config = SegmentConfig {\n                vector_data: vector_params.clone(),\n                sparse_vector_data: sparse_vector_params.clone(),\n                payload_storage_type: config.params.payload_storage_type(),\n            };\n            let segment = thread::Builder::new()\n                .name(format!(\"shard-build-{collection_id}-{id}\"))\n                .spawn(move || build_segment(&path_clone, &segment_config, true))\n                .unwrap();\n            build_handlers.push(segment);\n        }\n\n        let join_results = build_handlers\n            .into_iter()\n            .map(|handler| handler.join())\n            .collect_vec();\n\n        for join_result in join_results {\n            let segment = join_result.map_err(|err| {\n                let message = panic::downcast_str(&err).unwrap_or(\"\");\n                let separator = if !message.is_empty() { \"with:\\n\" } else { \"\" };\n\n                CollectionError::service_error(format!(\n                    \"Segment DB create panicked{separator}{message}\",\n                ))\n            })??;\n\n            segment_holder.add_new(segment);\n        }\n\n        let wal: SerdeWal<OperationWithClockTag> =\n            SerdeWal::new(wal_path.to_str().unwrap(), (&config.wal_config).into())?;\n\n        let clocks = LocalShardClocks::load(shard_path)?;\n\n        let local_shard = LocalShard::new(\n            segment_holder,\n            collection_config,\n            shared_storage_config,\n            payload_index_schema,\n            wal,\n            optimizers,\n            optimizer_cpu_budget,\n            shard_path,\n            clocks,\n            update_runtime,\n        )\n        .await;\n\n        local_shard.load_from_wal(collection_id).await?;\n\n        Ok(local_shard)\n    }\n\n    // Remaining functions...\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.2372030479605558,
      "diff_lines": 1238,
      "added_lines": 45,
      "removed_lines": 1086,
      "diff_preview": "--- expected+++ predicted@@ -1,12 +1,10 @@ pub mod clock_map;\n pub mod disk_usage_watcher;\n pub(super) mod facet;\n-pub(super) mod formula_rescore;\n pub(super) mod query;\n pub(super) mod scroll;\n pub(super) mod search;\n pub(super) mod shard_ops;\n-mod telemetry;\n \n use std::collections::{BTreeSet, HashMap};\n use std::ops::Deref;\n@@ -26,6 +24,7 @@ use itertools::Itertools;\n use parking_lot::{Mutex as ParkingMutex, RwLock};\n use segment::data_types::segment_manifest::SegmentManifests;\n+use segment::data_types::vectors::VectorElementType;\n use segment::entry::entry_point::SegmentEntry as _;\n use segment::index::field_index::CardinalityEstimation;\n use segment::segment::Segment;\n@@ -34,38 +33,35 @@     Filter, PayloadIndexInfo, PayloadKeyType, PointIdType, SegmentConfig, SegmentType,\n     SnapshotFormat,\n };\n+use segment::vector_storage::common::get_async_scorer;\n use tokio::fs::{create_dir_all, remove_dir_all, remove_file};\n use tokio::runtime::Handle;\n use tokio::sync::mpsc::Sender;\n-use tokio::sync::{Mutex, RwLock as TokioRwLock, mpsc, oneshot};\n+use tokio::sync::{mpsc, oneshot, Mutex, RwLock as TokioRwLock};\n use wal::{Wal, WalOptions};\n \n use self::clock_map::{ClockMap, RecoveryPoint};\n-use self::disk_usage_watcher::DiskUsageWatcher;\n use super::update_tracker::UpdateTracker;\n-use crate::collection::payload_index_schema::PayloadIndexSchema;\n use crate::collection_manager::collection_updater::CollectionUpdater;\n-use crate::collection_manager::holders::segment_holder::{\n-    LockedSegment, LockedSegmentHolder, SegmentHolder,\n-};\n+use crate::collection_manager::holders::segment_holder::{LockedSegment, LockedSegmentHolder, SegmentHolder};\n use crate::collection_manager::optimizers::TrackerLog;\n-use crate::collection_manager::segments_searcher::SegmentsSearcher;\n-use crate::common::file_utils::{move_dir, move_file};\n-use crate::config::CollectionConfigInternal;\n-use crate::operations::OperationWithClockTag;\n use crate::operations::shared_storage_config::SharedStorageConfig;\n"
    },
    "llm_calls": 1,
    "elapsed_time": 56.208357095718384,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 59828,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "qdrant_lib_collection_src_shards_replica_set_mod.rs",
    "exact_match": false,
    "similarity": 0.23068938836362263,
    "diff_lines": 1243,
    "predicted_output": "mod execute_read_operation;\nmod locally_disabled_peers;\nmod read_ops;\nmod shard_transfer;\npub mod snapshots;\nmod update;\n\nuse std::collections::{HashMap, HashSet};\nuse std::time::Duration;\n\nuse common::budget::ResourceBudget;\nuse common::counter::hardware_accumulator::HwMeasurementAcc;\nuse common::rate_limiting::RateLimiter;\nuse common::types::TelemetryDetail;\n\nuse schemars::JsonSchema;\nuse segment::types::{ExtendedPointId, Filter, ShardKey};\nuse serde::{Deserialize, Serialize};\nuse tokio::runtime::Handle;\nuse tokio::sync::{Mutex, RwLock};\n\nuse super::CollectionId;\nuse super::local_shard::LocalShard;\nuse super::local_shard::clock_map::RecoveryPoint;\nuse super::remote_shard::RemoteShard;\nuse super::transfer::ShardTransfer;\nuse crate::collection::payload_index_schema::PayloadIndexSchema;\nuse crate::common::collection_size_stats::CollectionSizeStats;\nuse crate::common::snapshots_manager::SnapshotStorageManager;\nuse crate::config::CollectionConfigInternal;\nuse crate::operations::point_ops::{self};\nuse crate::operations::shared_storage_config::SharedStorageConfig;\nuse crate::operations::types::{CollectionError, CollectionResult, UpdateResult, UpdateStatus};\nuse crate::operations::CollectionUpdateOperations;\nuse crate::optimizers_builder::OptimizersConfig;\nuse crate::save_on_disk::SaveOnDisk;\nuse crate::shards::channel_service::ChannelService;\n\npub struct ShardReplicaSet {\n    locally_disabled_peers: parking_lot::RwLock<locally_disabled_peers::Registry>,\n    pub(crate) shard_path: PathBuf,\n    pub(crate) shard_id: ShardId,\n    shard_key: Option<ShardKey>,\n    notify_peer_failure_cb: ChangePeerFromState,\n    abort_shard_transfer_cb: AbortShardTransfer,\n    channel_service: ChannelService,\n    collection_id: CollectionId,\n    collection_config: Arc<RwLock<CollectionConfigInternal>>,\n    optimizers_config: OptimizersConfig,\n    pub(crate) shared_storage_config: Arc<SharedStorageConfig>,\n    payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,\n    update_runtime: Handle,\n    search_runtime: Handle,\n    optimizer_resource_budget: ResourceBudget,\n    write_ordering_lock: Mutex<()>,\n    clock_set: Mutex<ClockSet>,\n    write_rate_limiter: Option<parking_lot::Mutex<RateLimiter>>,\n}\n\npub type AbortShardTransfer = Arc<dyn Fn(ShardTransfer, &str) + Send + Sync>;\npub type ChangePeerFromState = Arc<dyn Fn(PeerId, ShardId, Option<ReplicaState>) + Send + Sync>;\n\nconst REPLICA_STATE_FILE: &str = \"replica_state.json\";\n\nimpl ShardReplicaSet {\n    /// Create a new fresh replica set, no previous state is expected.\n    #[allow(clippy::too_many_arguments)]\n    pub async fn build(\n        shard_id: ShardId,\n        shard_key: Option<ShardKey>,\n        collection_id: CollectionId,\n        this_peer_id: PeerId,\n        local: bool,\n        remotes: HashSet<PeerId>,\n        on_peer_failure: ChangePeerFromState,\n        abort_shard_transfer: AbortShardTransfer,\n        collection_path: &Path,\n        collection_config: Arc<RwLock<CollectionConfigInternal>>,\n        effective_optimizers_config: OptimizersConfig,\n        shared_storage_config: Arc<SharedStorageConfig>,\n        payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,\n        update_runtime: Handle,\n        search_runtime: Handle,\n    ) -> CollectionResult<Self> {\n        let shard_path = super::create_shard_dir(collection_path, shard_id).await?;\n        let local = if local {\n            let shard = LocalShard::build(\n                shard_id,\n                collection_id.clone(),\n                &shard_path,\n                collection_config.clone(),\n                shared_storage_config.clone(),\n                update_runtime.clone(),\n                effective_optimizers_config.clone(),\n                payload_index_schema.clone(),\n            )\n            .await?;\n            Some(Shard::Local(shard))\n        } else {\n            None\n        };\n\n        let local_shard_res = Self {\n            shard_id,\n            shard_key,\n            local: RwLock::new(local),\n            remotes: RwLock::new(vec![]), // Initialize with an empty vector\n            replica_state: SaveOnDisk::load_or_init(shard_path.join(REPLICA_STATE_FILE)).await?,\n            locally_disabled_peers: parking_lot::RwLock::new(locally_disabled_peers::Registry::default()),\n            channel_service,\n            collection_id,\n            collection_config,\n            optimizers_config,\n            shared_storage_config,\n            update_runtime,\n            search_runtime,\n            optimizer_resource_budget,\n            write_ordering_lock: Mutex::new(()),\n            clock_set: Mutex::new(ClockSet::new()),\n            write_rate_limiter: None,\n        };\n\n        Ok(local_shard_res)\n    }\n\n    pub async fn delete_local_points(\n        &self,\n        filter: Filter,\n        hw_measurement_acc: HwMeasurementAcc,\n        force: bool,\n    ) -> CollectionResult<UpdateResult> {\n        let local_shard_guard = self.local.read().await;\n\n        let Some(local_shard) = local_shard_guard.deref() else {\n            return Err(CollectionError::NotFound {\n                what: format!(\"local shard {}:{}\", self.collection_id, self.shard_id),\n            });\n        };\n\n        let mut next_offset = Some(ExtendedPointId::NumId(0));\n        let mut ids = Vec::new();\n\n        while let Some(current_offset) = next_offset {\n            const BATCH_SIZE: usize = 1000;\n\n            let mut points = local_shard\n                .get()\n                .scroll_by(\n                    Some(current_offset),\n                    BATCH_SIZE + 1,\n                    &false.into(),\n                    &false.into(),\n                    Some(&filter),\n                    &self.search_runtime,\n                    None,\n                    None,\n                    hw_measurement_acc.clone(),\n                )\n                .await?;\n\n            if points.len() > BATCH_SIZE {\n                next_offset = points.pop().map(|points| points.id);\n            } else {\n                next_offset = None;\n            }\n\n            ids.extend(points.into_iter().map(|points| points.id));\n        }\n\n        if ids.is_empty() {\n            return Ok(UpdateResult {\n                operation_id: None,\n                status: UpdateStatus::Completed,\n                clock_tag: None,\n            });\n        }\n\n        drop(local_shard_guard);\n\n        let op = CollectionUpdateOperations::PointOperation(point_ops::PointOperations::DeletePoints {\n            ids,\n        });\n\n        // TODO(resharding): Assign clock tag to the operation!? \ud83e\udd14\n        let result = self.update_local(op.into(), true, hw_measurement_acc, force)\n            .await?\n            .ok_or_else(|| {\n                CollectionError::bad_request(format!(\n                    \"local shard {}:{} does not exist or is unavailable\",\n                    self.collection_id, self.shard_id,\n                ))\n            })?;\n\n        Ok(result)\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.23068938836362263,
      "diff_lines": 1243,
      "added_lines": 18,
      "removed_lines": 1127,
      "diff_preview": "--- expected+++ predicted@@ -1,23 +1,19 @@-pub mod clock_set;\n mod execute_read_operation;\n mod locally_disabled_peers;\n mod read_ops;\n mod shard_transfer;\n pub mod snapshots;\n-mod telemetry;\n mod update;\n \n use std::collections::{HashMap, HashSet};\n-use std::ops::Deref as _;\n-use std::path::{Path, PathBuf};\n-use std::sync::Arc;\n use std::time::Duration;\n \n use common::budget::ResourceBudget;\n use common::counter::hardware_accumulator::HwMeasurementAcc;\n use common::rate_limiting::RateLimiter;\n+use common::types::TelemetryDetail;\n+\n use schemars::JsonSchema;\n-use segment::common::anonymize::Anonymize;\n use segment::types::{ExtendedPointId, Filter, ShardKey};\n use serde::{Deserialize, Serialize};\n use tokio::runtime::Handle;\n@@ -32,67 +28,15 @@ use crate::common::collection_size_stats::CollectionSizeStats;\n use crate::common::snapshots_manager::SnapshotStorageManager;\n use crate::config::CollectionConfigInternal;\n+use crate::operations::point_ops::{self};\n use crate::operations::shared_storage_config::SharedStorageConfig;\n use crate::operations::types::{CollectionError, CollectionResult, UpdateResult, UpdateStatus};\n-use crate::operations::{CollectionUpdateOperations, point_ops};\n+use crate::operations::CollectionUpdateOperations;\n use crate::optimizers_builder::OptimizersConfig;\n use crate::save_on_disk::SaveOnDisk;\n use crate::shards::channel_service::ChannelService;\n-use crate::shards::dummy_shard::DummyShard;\n-use crate::shards::replica_set::clock_set::ClockSet;\n-use crate::shards::shard::{PeerId, Shard, ShardId};\n-use crate::shards::shard_config::ShardConfig;\n \n-//    \u2502    Collection Created\n-//    \u2502\n-//    \u25bc\n-//  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n-//  \u2502              \u2502\n"
    },
    "llm_calls": 1,
    "elapsed_time": 48.253456115722656,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 57397,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "qdrant_lib_collection_src_update_handler.rs",
    "exact_match": false,
    "similarity": 0.3236907629709103,
    "diff_lines": 906,
    "predicted_output": "use std::cmp::min;\nuse std::collections::HashSet;\nuse std::path::{Path, PathBuf};\nuse std::sync::atomic::{AtomicBool, AtomicU64, AtomicUsize, Ordering};\nuse std::sync::Arc;\n\nuse common::budget::ResourceBudget;\nuse common::counter::hardware_counter::HardwareCounterCell;\nuse common::panic;\nuse itertools::Itertools;\nuse log::{debug, error, info, trace, warn};\nuse segment::common::operation_error::OperationResult;\nuse segment::index::hnsw_index::num_rayon_threads;\nuse segment::types::SeqNumberType;\nuse tokio::runtime::Handle;\nuse tokio::sync::mpsc::{self, Receiver, Sender};\nuse tokio::sync::{Mutex as TokioMutex, oneshot};\nuse tokio::task::{self, JoinHandle};\nuse tokio::time::error::Elapsed;\nuse tokio::time::{timeout, Duration};\n\nuse crate::collection::payload_index_schema::PayloadIndexSchema;\nuse crate::collection_manager::collection_updater::CollectionUpdater;\nuse crate::collection_manager::holders::segment_holder::LockedSegmentHolder;\nuse crate::collection_manager::optimizers::segment_optimizer::{\n    OptimizerThresholds, SegmentOptimizer,\n};\nuse crate::collection_manager::optimizers::{Tracker, TrackerLog, TrackerStatus};\nuse crate::common::stoppable_task::{spawn_stoppable, StoppableTaskHandle};\nuse crate::config::CollectionParams;\nuse crate::operations::shared_storage_config::SharedStorageConfig;\nuse crate::operations::types::{CollectionError, CollectionResult};\nuse crate::operations::CollectionUpdateOperations;\nuse crate::shards::local_shard::LocalShardClocks;\nuse crate::wal::WalError;\nuse crate::wal_delta::LockedWal;\n\n/// Interval at which the optimizer worker cleans up old optimization handles\n///\n/// The longer the duration, the longer it  takes for panicked tasks to be reported.\nconst OPTIMIZER_CLEANUP_INTERVAL: Duration = Duration::from_secs(5);\n\npub type Optimizer = dyn SegmentOptimizer + Sync + Send;\n\n/// Information, required to perform operation and notify regarding the result\npub struct OperationData {\n    pub op_num: SeqNumberType,\n    /// Operation\n    pub operation: CollectionUpdateOperations,\n    /// If operation was requested to wait for result\n    pub wait: bool,\n    /// Callback notification channel\n    pub sender: Option<oneshot::Sender<CollectionResult<usize>>>,\n    pub hw_measurements: HardwareCounterCell,\n}\n\n/// Signal, used to inform Updater process\npub enum UpdateSignal {\n    /// Requested operation to perform\n    Operation(OperationData),\n    Stop,\n    /// Empty signal used to trigger optimizers\n    Nop,\n    /// Ensures that previous updates are applied\n    Plunger(oneshot::Sender<()>),\n}\n\n/// Signal, used to inform Optimization process\n#[derive(PartialEq, Eq, Clone, Copy)]\npub enum OptimizerSignal {\n    /// Sequential number of the operation\n    Operation(SeqNumberType),\n    /// Stop all optimizers and listening\n    Stop,\n    /// Empty signal used to trigger optimizers\n    Nop,\n}\n\n/// Structure, which holds object, required for processing updates of the collection\npub struct UpdateHandler {\n    shared_storage_config: Arc<SharedStorageConfig>,\n    /// List of used optimizers\n    pub optimizers: Arc<Vec<Arc<Optimizer>>>,\n    /// Log of optimizer statuses\n    optimizers_log: Arc<Mutex<TrackerLog>>,\n    /// Total number of optimized points since last start\n    total_optimized_points: Arc<AtomicUsize>,\n    /// Global CPU budget in number of cores for all optimization tasks.\n    /// Assigns CPU permits to tasks to limit overall resource utilization.\n    optimizer_resource_budget: ResourceBudget,\n    /// How frequent can we flush data\n    /// This parameter depends on the optimizer config and should be updated accordingly.\n    pub flush_interval_sec: u64,\n    segments: LockedSegmentHolder,\n    /// Process, that listens updates signals and perform updates\n    update_worker: Option<JoinHandle<()>>,\n    /// Process that performs optimizations\n    optimizer_worker: Option<JoinHandle<()>>,\n    /// Always keep this WAL version and later and prevent acknowledging/truncating from the WAL.\n    /// This is used when other bits of code still depend on information in the WAL, such as the\n    /// queue proxy shard.\n    pub(super) wal_keep_from: Arc<AtomicU64>,\n    optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,\n    /// Maximum number of concurrent optimization jobs in this update handler.\n    /// This parameter depends on the optimizer config and should be updated accordingly.\n    pub max_optimization_threads: Option<usize>,\n    /// Highest and cutoff clocks for the shard WAL.\n    clocks: LocalShardClocks,\n    shard_path: PathBuf,\n    /// Whether we have ever triggered optimizers since starting.\n    has_triggered_optimizers: Arc<AtomicBool>,\n}\n\nimpl UpdateHandler {\n    #[allow(clippy::too_many_arguments)]\n    pub fn new(\n        shared_storage_config: Arc<SharedStorageConfig>,\n        payload_index_schema: Arc<SaveOnDisk<PayloadIndexSchema>>,\n        optimizers: Arc<Vec<Arc<Optimizer>>>,\n        optimizers_log: Arc<Mutex<TrackerLog>>,\n        total_optimized_points: Arc<AtomicUsize>,\n        optimizer_resource_budget: ResourceBudget,\n        runtime_handle: Handle,\n        segments: LockedSegmentHolder,\n        wal: LockedWal,\n        flush_interval_sec: u64,\n        max_optimization_threads: Option<usize>,\n    ) -> UpdateHandler {\n        UpdateHandler {\n            shared_storage_config,\n            optimizers,\n            update_worker: None,\n            optimizer_worker: None,\n            optimizers_log,\n            total_optimized_points,\n            optimizer_resource_budget,\n            flush_worker: None,\n            flush_stop: None,\n            runtime_handle,\n            wal,\n            flush_interval_sec,\n            wal_keep_from: Arc::new(u64::MAX.into()),\n            optimization_handles: Arc::new(TokioMutex::new(vec![])),\n            max_optimization_threads,\n            has_triggered_optimizers: Default::default(),\n            clocks: Default::default(),\n            shard_path: Default::default(),\n        }\n    }\n\n    pub fn run_workers(&mut self, update_receiver: Receiver<UpdateSignal>) {\n        let (tx, rx) = mpsc::channel(self.shared_storage_config.update_queue_size);\n        self.optimizer_worker = Some(self.runtime_handle.spawn(Self::optimization_worker_fn(\n            self.optimizers.clone(),\n            tx.clone(),\n            rx,\n            self.segments.clone(),\n            self.wal.clone(),\n            self.optimization_handles.clone(),\n            self.optimizers_log.clone(),\n            self.total_optimized_points.clone(),\n            &self.optimizer_resource_budget,\n            self.max_optimization_threads,\n            self.has_triggered_optimizers.clone(),\n        )));\n        self.update_worker = Some(self.runtime_handle.spawn(Self::update_worker_fn(\n            update_receiver,\n            tx,\n            self.segments.clone(),\n        )));\n    }\n\n    // Other methods follow...\n\n    /// Checks conditions for all optimizers and returns whether any is satisfied\n    ///\n    /// In other words, if this returns true we have pending optimizations.\n    pub(crate) fn check_optimizer_conditions(&self) -> (bool, bool) {\n        // Check if Qdrant triggered any optimizations since starting at all\n        let has_triggered_any_optimizers = self.has_triggered_optimizers.load(Ordering::Relaxed);\n\n        let excluded_ids = HashSet::<_>::default();\n        let has_suboptimal_optimizers = self.optimizers.iter().any(|optimizer| {\n            let nonoptimal_segment_ids =\n                optimizer.check_condition(self.segments.clone(), &excluded_ids);\n            !nonoptimal_segment_ids.is_empty()\n        });\n\n        (has_triggered_any_optimizers, has_suboptimal_optimizers)\n    }\n  \n    #[allow(clippy::too_many_arguments)]\n    pub(crate) async fn process_optimization(\n        optimizers: Arc<Vec<Arc<Optimizer>>>,\n        segments: LockedSegmentHolder,\n        optimization_handles: Arc<TokioMutex<Vec<StoppableTaskHandle<bool>>>>,\n        optimizers_log: Arc<Mutex<TrackerLog>>,\n        total_optimized_points: Arc<AtomicUsize>,\n        optimizer_resource_budget: &ResourceBudget,\n        sender: Sender<OptimizerSignal>,\n        limit: usize,\n    ) -> Vec<StoppableTaskHandle<bool>> {\n        let mut scheduled_segment_ids: HashSet<_> = Default::default();\n        let mut handles = vec![];\n\n        'outer: for optimizer in optimizers.iter() {\n            loop {\n                // Return early if we reached the optimization job limit\n                if limit.map(|extra| handles.len() >= extra).unwrap_or(false) {\n                    log::trace!(\"Reached optimization job limit, postponing other optimizations\");\n                    break 'outer;\n                }\n\n                let nonoptimal_segment_ids =\n                    optimizer.check_condition(segments.clone(), &scheduled_segment_ids);\n                if nonoptimal_segment_ids.is_empty() {\n                    break;\n                }\n\n                // Determine how many Resources we prefer for optimization task, acquire permit for it\n                // And use same amount of IO threads as CPUs\n                let max_indexing_threads = optimizer.hnsw_config().max_indexing_threads;\n                let desired_io = num_rayon_threads(max_indexing_threads);\n                let Some(mut permit) = optimizer_resource_budget.try_acquire(0, desired_io) else {\n                    log::trace!(\n                        \"No available IO permit for {} optimizer, postponing\",\n                        optimizer.name(),\n                    );\n                    if handles.is_empty() {\n                        sender.send(OptimizerSignal::Nop).ok();\n                    }\n                    break 'outer;\n                };\n                log::trace!(\n                    \"Acquired {} IO permit for {} optimizer\",\n                    permit.num_io,\n                    optimizer.name(),\n                );\n\n                let optimizer = optimizer.clone();\n                let optimizers_log = optimizers_log.clone();\n                let total_optimized_points = total_optimized_points.clone();\n                let segments = segments.clone();\n                let nsi = nonoptimal_segment_ids.clone();\n                scheduled_segment_ids.extend(&nsi);\n                let callback = total_optimized_points.clone();\n\n                let handle = spawn_stoppable(\n                    // Stoppable task\n                    {\n                        let segments = segments.clone();\n                        move |stopped| {\n                            // Track optimizer status\n                            let tracker = Tracker::start(optimizer.as_ref().name(), nsi.clone());\n                            let tracker_handle = tracker.handle();\n                            optimizers_log.lock().register(tracker);\n\n                            // Optimize and handle result\n                            match optimizer.as_ref().optimize(segments.clone(), nsi, permit, stopped) {\n                                // Perform some actions when optimization if finished\n                                Ok(points) => {\n                                    tracker_handle.update(TrackerStatus::Done);\n                                    callback.fetch_add(points, Ordering::Relaxed);\n                                    true\n                                }\n                                // Handle and report errors\n                                Err(error) => match error {\n                                    CollectionError::Cancelled { description } => {\n                                        log::debug!(\"Optimization cancelled - {}\", description);\n                                        tracker_handle\n                                            .update(TrackerStatus::Cancelled(description));\n                                        false\n                                    }\n                                    _ => {\n                                        segments.write().report_optimizer_error(error.clone());\n                                        log::error!(\"Optimization error: {}\", error);\n                                        tracker_handle\n                                            .update(TrackerStatus::Error(error.to_string()));\n                                        panic!(\"Optimization error: {}\", error);\n                                    }\n                                },\n                            }\n                        }\n                    },\n                    // Panic handler\n                    Some(Box::new(move |panic_payload| {\n                        let message = panic::downcast_str(&panic_payload).unwrap_or(\"\");\n                        let separator = if !message.is_empty() { \": \" } else { \"\" };\n            \n                        warn!(\n                            \"Optimization task panicked, collection may be in unstable state\\\n                             {separator}{message}\"\n                        );\n                        segments.write().report_optimizer_error(CollectionError::service_error(format!(\n                            \"Optimization task panicked{separator}{message}\"\n                        )));\n                    })),\n                );\n                handles.push(handle);\n            }\n        }\n        handles\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.3236907629709103,
      "diff_lines": 906,
      "added_lines": 110,
      "removed_lines": 665,
      "diff_preview": "--- expected+++ predicted@@ -1,16 +1,14 @@ use std::cmp::min;\n use std::collections::HashSet;\n use std::path::{Path, PathBuf};\n+use std::sync::atomic::{AtomicBool, AtomicU64, AtomicUsize, Ordering};\n use std::sync::Arc;\n-use std::sync::atomic::{AtomicBool, AtomicU64, AtomicUsize, Ordering};\n \n use common::budget::ResourceBudget;\n-use common::counter::hardware_accumulator::HwMeasurementAcc;\n use common::counter::hardware_counter::HardwareCounterCell;\n use common::panic;\n use itertools::Itertools;\n use log::{debug, error, info, trace, warn};\n-use parking_lot::Mutex;\n use segment::common::operation_error::OperationResult;\n use segment::index::hnsw_index::num_rayon_threads;\n use segment::types::SeqNumberType;\n@@ -19,7 +17,7 @@ use tokio::sync::{Mutex as TokioMutex, oneshot};\n use tokio::task::{self, JoinHandle};\n use tokio::time::error::Elapsed;\n-use tokio::time::{Duration, timeout};\n+use tokio::time::{timeout, Duration};\n \n use crate::collection::payload_index_schema::PayloadIndexSchema;\n use crate::collection_manager::collection_updater::CollectionUpdater;\n@@ -28,12 +26,11 @@     OptimizerThresholds, SegmentOptimizer,\n };\n use crate::collection_manager::optimizers::{Tracker, TrackerLog, TrackerStatus};\n-use crate::common::stoppable_task::{StoppableTaskHandle, spawn_stoppable};\n+use crate::common::stoppable_task::{spawn_stoppable, StoppableTaskHandle};\n use crate::config::CollectionParams;\n-use crate::operations::CollectionUpdateOperations;\n use crate::operations::shared_storage_config::SharedStorageConfig;\n use crate::operations::types::{CollectionError, CollectionResult};\n-use crate::save_on_disk::SaveOnDisk;\n+use crate::operations::CollectionUpdateOperations;\n use crate::shards::local_shard::LocalShardClocks;\n use crate::wal::WalError;\n use crate::wal_delta::LockedWal;\n@@ -46,9 +43,7 @@ pub type Optimizer = dyn SegmentOptimizer + Sync + Send;\n \n /// Information, required to perform operation and notify regarding the result\n-#[derive(Debug)]\n pub struct OperationData {\n"
    },
    "llm_calls": 1,
    "elapsed_time": 93.15695905685425,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 61348,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "qdrant_lib_collection_tests_integration_collection_test.rs",
    "exact_match": false,
    "similarity": 0.17917965939074118,
    "diff_lines": 879,
    "predicted_output": "use std::collections::{HashMap, HashSet};\nuse std::fs::File;\n\nuse api::rest::{OrderByInterface, SearchRequestInternal};\nuse collection::operations::payload_ops::{PayloadOps, SetPayloadOp};\nuse collection::operations::point_ops::{\n    BatchPersisted, BatchVectorStructPersisted, PointInsertOperationsInternal, PointOperations,\n    PointStructPersisted, WriteOrdering,\n};\nuse collection::operations::shard_selector_internal::ShardSelectorInternal;\nuse collection::operations::types::{\n    CountRequestInternal, PointRequestInternal, RecommendRequestInternal, ScrollRequestInternal,\n    UpdateStatus,\n};\nuse collection::operations::CollectionUpdateOperations;\nuse collection::recommendations::recommend_by;\nuse collection::shards::replica_set::{ReplicaSetState, ReplicaState};\nuse common::counter::hardware_accumulator::HwMeasurementAcc;\nuse itertools::Itertools;\nuse segment::data_types::order_by::{Direction, OrderBy};\nuse segment::data_types::vectors::{BatchVectorStructPersisted, VectorStructInternal};\nuse segment::types::{\n    Condition, ExtendedPointId, FieldCondition, Filter, HasIdCondition, Payload,\n    PayloadFieldSchema, PayloadSchemaType, PointIdType, WithPayloadInterface,\n};\nuse serde_json::Map;\nuse tempfile::Builder;\n\nuse crate::common::{N_SHARDS, load_local_collection, simple_collection_fixture};\n\n#[tokio::test(flavor = \"multi_thread\")]\nasync fn test_collection_updater() {\n    let collection_dir = Builder::new().prefix(\"collection\").tempdir().unwrap();\n    let collection = simple_collection_fixture(collection_dir.path(), 1).await;\n\n    let batch = BatchPersisted {\n        ids: vec![0, 1, 2, 3, 4]\n            .into_iter()\n            .map(|x| x.into())\n            .collect_vec(),\n        vectors: BatchVectorStructPersisted::Single(vec![\n            vec![1.0, 0.0, 1.0, 1.0],\n            vec![1.0, 0.0, 1.0, 0.0],\n            vec![1.0, 1.0, 1.0, 1.0],\n            vec![1.0, 1.0, 0.0, 1.0],\n            vec![1.0, 0.0, 0.0, 0.0],\n        ]),\n        payloads: None,\n    };\n\n    let insert_points = CollectionUpdateOperations::PointOperation(PointOperations::UpsertPoints(\n        PointInsertOperationsInternal::from(batch),\n    ));\n\n    let hw_counter = HwMeasurementAcc::new();\n    let insert_result = collection\n        .update_from_client_simple(insert_points, true, WriteOrdering::default(), hw_counter)\n        .await;\n\n    match insert_result {\n        Ok(res) => {\n            assert_eq!(res.status, UpdateStatus::Completed)\n        }\n        Err(err) => panic!(\"operation failed: {err:?}\"),\n    }\n\n    let search_request = SearchRequestInternal {\n        vector: vec![1.0, 1.0, 1.0, 1.0].into(),\n        with_payload: None,\n        with_vector: None,\n        filter: None,\n        params: None,\n        limit: 3,\n        offset: None,\n        score_threshold: None,\n    };\n\n    let search_res = collection.search(\n        search_request.into(),\n        None,\n        &ShardSelectorInternal::All,\n        None,\n        HwMeasurementAcc::new(),\n    ).await;\n\n    match search_res {\n        Ok(res) => {\n            assert_eq!(res.len(), 3);\n            assert_eq!(res[0].id, 2.into());\n            assert!(res[0].payload.is_none());\n        }\n        Err(err) => panic!(\"search failed: {err:?}\"),\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.17917965939074118,
      "diff_lines": 879,
      "added_lines": 11,
      "removed_lines": 824,
      "diff_preview": "--- expected+++ predicted@@ -1,25 +1,24 @@ use std::collections::{HashMap, HashSet};\n use std::fs::File;\n \n-use ahash::AHashSet;\n use api::rest::{OrderByInterface, SearchRequestInternal};\n-use collection::operations::CollectionUpdateOperations;\n use collection::operations::payload_ops::{PayloadOps, SetPayloadOp};\n use collection::operations::point_ops::{\n     BatchPersisted, BatchVectorStructPersisted, PointInsertOperationsInternal, PointOperations,\n-    PointStructPersisted, VectorStructPersisted, WriteOrdering,\n+    PointStructPersisted, WriteOrdering,\n };\n use collection::operations::shard_selector_internal::ShardSelectorInternal;\n use collection::operations::types::{\n     CountRequestInternal, PointRequestInternal, RecommendRequestInternal, ScrollRequestInternal,\n     UpdateStatus,\n };\n+use collection::operations::CollectionUpdateOperations;\n use collection::recommendations::recommend_by;\n use collection::shards::replica_set::{ReplicaSetState, ReplicaState};\n use common::counter::hardware_accumulator::HwMeasurementAcc;\n use itertools::Itertools;\n use segment::data_types::order_by::{Direction, OrderBy};\n-use segment::data_types::vectors::VectorStructInternal;\n+use segment::data_types::vectors::{BatchVectorStructPersisted, VectorStructInternal};\n use segment::types::{\n     Condition, ExtendedPointId, FieldCondition, Filter, HasIdCondition, Payload,\n     PayloadFieldSchema, PayloadSchemaType, PointIdType, WithPayloadInterface,\n@@ -31,14 +30,8 @@ \n #[tokio::test(flavor = \"multi_thread\")]\n async fn test_collection_updater() {\n-    test_collection_updater_with_shards(1).await;\n-    test_collection_updater_with_shards(N_SHARDS).await;\n-}\n-\n-async fn test_collection_updater_with_shards(shard_number: u32) {\n     let collection_dir = Builder::new().prefix(\"collection\").tempdir().unwrap();\n-\n-    let collection = simple_collection_fixture(collection_dir.path(), shard_number).await;\n+    let collection = simple_collection_fixture(collection_dir.path(), 1).await;\n \n     let batch = BatchPersisted {\n         ids: vec![0, 1, 2, 3, 4]\n@@ -82,16 +75,13 @@         score_threshold: None,\n     };\n"
    },
    "llm_calls": 1,
    "elapsed_time": 21.917728900909424,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 36828,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "qdrant_lib_segment_benches_vector_search.rs",
    "exact_match": false,
    "similarity": 0.9695181907571289,
    "diff_lines": 46,
    "predicted_output": "use std::path::Path;\nuse std::sync::atomic::AtomicBool;\nuse std::sync::Arc;\n\nuse atomic_refcell::AtomicRefCell;\nuse common::counter::hardware_counter::HardwareCounterCell;\nuse common::types::PointOffsetType;\nuse criterion::{Criterion, criterion_group, criterion_main};\nuse rand::Rng;\nuse rand::distr::StandardUniform;\nuse segment::common::rocksdb_wrapper::{DB_VECTOR_CF, open_db};\nuse segment::data_types::vectors::{DenseVector, VectorInternal, VectorRef};\nuse segment::fixtures::payload_context_fixture::FixtureIdTracker;\nuse segment::id_tracker::IdTrackerSS;\nuse segment::types::Distance;\nuse segment::vector_storage::dense::simple_dense_vector_storage::open_simple_dense_vector_storage;\nuse segment::vector_storage::{VectorStorage, VectorStorageEnum, new_raw_scorer_for_test};\nuse tempfile::Builder;\n\nconst NUM_VECTORS: usize = 100000;\nconst DIM: usize = 1024; // Larger dimensionality - greater the SIMD advantage\n\nfn random_vector(size: usize) -> DenseVector {\n    let rng = rand::rng();\n\n    rng.sample_iter(StandardUniform).take(size).collect()\n}\n\nfn init_vector_storage(\n    path: &Path,\n    dim: usize,\n    num: usize,\n    dist: Distance,\n) -> (VectorStorageEnum, Arc<AtomicRefCell<IdTrackerSS>>) {\n    let db = open_db(path, &[DB_VECTOR_CF]).unwrap();\n    let id_tracker = Arc::new(AtomicRefCell::new(FixtureIdTracker::new(num)));\n    let mut storage =\n        open_simple_dense_vector_storage(db, DB_VECTOR_CF, dim, dist, &AtomicBool::new(false))\n            .unwrap();\n\n    {\n        for i in 0..num {\n            let vector: VectorInternal = random_vector(dim).into();\n            storage\n                .insert_vector(i as PointOffsetType, VectorRef::from(&vector))\n                .unwrap();\n        }\n    }\n\n    (storage, id_tracker)\n}\n\nfn benchmark_naive(c: &mut Criterion) {\n    let dir = Builder::new().prefix(\"storage_dir\").tempdir().unwrap();\n\n    let dist = Distance::Dot;\n    let (storage, id_tracker) = init_vector_storage(dir.path(), DIM, NUM_VECTORS, dist);\n    let borrowed_id_tracker = id_tracker.borrow();\n\n    let mut group = c.benchmark_group(\"storage-score-all\");\n\n    group.bench_function(\"storage vector search\", |b| {\n        b.iter(|| {\n            let vector = random_vector(DIM);\n            let vector = vector.as_slice().into();\n            new_raw_scorer_for_test(\n                vector,\n                &storage,\n                borrowed_id_tracker.deleted_point_bitslice(),\n            )\n            .unwrap()\n            .peek_top_all(10, &DEFAULT_STOPPED)\n            .unwrap();\n        })\n    });\n}\n\nfn random_access_benchmark(c: &mut Criterion) {\n    let dir = Builder::new().prefix(\"storage_dir\").tempdir().unwrap();\n\n    let dist = Distance::Dot;\n    let (storage, id_tracker) = init_vector_storage(dir.path(), DIM, NUM_VECTORS, dist);\n    let borrowed_id_tracker = id_tracker.borrow();\n\n    let mut group = c.benchmark_group(\"storage-score-random\");\n\n    let vector = random_vector(DIM);\n    let vector = vector.as_slice().into();\n\n    let scorer = new_raw_scorer_for_test(\n        vector,\n        &storage,\n        borrowed_id_tracker.deleted_point_bitslice(),\n    )\n    .unwrap();\n\n    let mut total_score = 0.;\n    group.bench_function(\"storage vector search\", |b| {\n        b.iter(|| {\n            let random_id = rand::rng().random_range(0..NUM_VECTORS) as PointOffsetType;\n            total_score += scorer.score_point(random_id);\n        })\n    });\n}\n\ncriterion_group!(benches, benchmark_naive, random_access_benchmark);",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.9695181907571289,
      "diff_lines": 46,
      "added_lines": 4,
      "removed_lines": 10,
      "diff_preview": "--- expected+++ predicted@@ -1,6 +1,6 @@ use std::path::Path;\n+use std::sync::atomic::AtomicBool;\n use std::sync::Arc;\n-use std::sync::atomic::AtomicBool;\n \n use atomic_refcell::AtomicRefCell;\n use common::counter::hardware_counter::HardwareCounterCell;\n@@ -14,9 +14,7 @@ use segment::id_tracker::IdTrackerSS;\n use segment::types::Distance;\n use segment::vector_storage::dense::simple_dense_vector_storage::open_simple_dense_vector_storage;\n-use segment::vector_storage::{\n-    DEFAULT_STOPPED, VectorStorage, VectorStorageEnum, new_raw_scorer_for_test,\n-};\n+use segment::vector_storage::{VectorStorage, VectorStorageEnum, new_raw_scorer_for_test};\n use tempfile::Builder;\n \n const NUM_VECTORS: usize = 100000;\n@@ -40,13 +38,11 @@         open_simple_dense_vector_storage(db, DB_VECTOR_CF, dim, dist, &AtomicBool::new(false))\n             .unwrap();\n \n-    let hw_counter = HardwareCounterCell::new();\n-\n     {\n         for i in 0..num {\n             let vector: VectorInternal = random_vector(dim).into();\n             storage\n-                .insert_vector(i as PointOffsetType, VectorRef::from(&vector), &hw_counter)\n+                .insert_vector(i as PointOffsetType, VectorRef::from(&vector))\n                 .unwrap();\n         }\n     }\n@@ -105,8 +101,6 @@             total_score += scorer.score_point(random_id);\n         })\n     });\n-    eprintln!(\"total_score = {total_score:?}\");\n }\n \n-criterion_group!(benches, benchmark_naive, random_access_benchmark);\n-criterion_main!(benches);+criterion_group!(benches, benchmark_naive, random_access_benchmark);"
    },
    "llm_calls": 1,
    "elapsed_time": 20.17777991294861,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 20990,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "qdrant_lib_segment_src_entry_entry_point.rs",
    "exact_match": false,
    "similarity": 0.25704414587332053,
    "diff_lines": 325,
    "predicted_output": "use std::collections::{BTreeSet, HashMap, HashSet};\nuse std::path::PathBuf;\nuse std::sync::Arc;\nuse std::sync::atomic::AtomicBool;\n\nuse common::counter::hardware_counter::HardwareCounterCell;\nuse common::types::TelemetryDetail;\n\nuse crate::common::operation_error::{OperationResult, SegmentFailedState};\nuse crate::data_types::named_vectors::NamedVectors;\nuse crate::data_types::order_by::{OrderBy, OrderValue};\nuse crate::data_types::query_context::{FormulaContext, QueryContext, SegmentQueryContext};\nuse crate::data_types::vectors::{QueryVector, VectorInternal};\nuse crate::index::field_index::{CardinalityEstimation, FieldIndex};\nuse crate::json_path::JsonPath;\nuse crate::telemetry::SegmentTelemetry;\nuse crate::types::{\n    Filter, Payload, PayloadFieldSchema, PayloadKeyType, PayloadKeyTypeRef, PointIdType,\n    ScoredPoint, SearchParams, SegmentConfig, SegmentInfo, SegmentType, SeqNumberType,\n    SnapshotFormat, VectorName, VectorNameBuf, WithPayload, WithVector,\n};\n\n/// Define all operations which can be performed with Segment or Segment-like entity.\n/// \n/// Assume all operations are idempotent - which means that no matter how many times an operation\n/// is executed - the storage state will be the same.\npub trait SegmentEntry {\n    /// Get current update version of the segment\n    fn version(&self) -> SeqNumberType;\n\n    // The rest of the implementation...\n\n    /// Return points which satisfies filtering condition ordered by the `order_by.key` field,\n    /// starting with `order_by.start_from` value including.\n    ///\n    /// Will fail if there is no index for the order_by key.\n    /// Cancelled by `is_stopped` flag.\n    fn read_ordered_filtered<'a>(\n        &'a self,\n        limit: Option<usize>,\n        filter: Option<&'a Filter>,\n        order_by: &'a OrderBy,\n        is_stopped: &AtomicBool,\n    ) -> OperationResult<Vec<(OrderValue, PointIdType)>>;\n\n    // The rest of the methods...\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.25704414587332053,
      "diff_lines": 325,
      "added_lines": 6,
      "removed_lines": 286,
      "diff_preview": "--- expected+++ predicted@@ -7,146 +7,28 @@ use common::types::TelemetryDetail;\n \n use crate::common::operation_error::{OperationResult, SegmentFailedState};\n-use crate::data_types::facets::{FacetParams, FacetValue};\n use crate::data_types::named_vectors::NamedVectors;\n use crate::data_types::order_by::{OrderBy, OrderValue};\n use crate::data_types::query_context::{FormulaContext, QueryContext, SegmentQueryContext};\n use crate::data_types::vectors::{QueryVector, VectorInternal};\n-use crate::entry::snapshot_entry::SnapshotEntry;\n use crate::index::field_index::{CardinalityEstimation, FieldIndex};\n use crate::json_path::JsonPath;\n use crate::telemetry::SegmentTelemetry;\n use crate::types::{\n     Filter, Payload, PayloadFieldSchema, PayloadKeyType, PayloadKeyTypeRef, PointIdType,\n-    ScoredPoint, SearchParams, SegmentConfig, SegmentInfo, SegmentType, SeqNumberType, VectorName,\n-    VectorNameBuf, WithPayload, WithVector,\n+    ScoredPoint, SearchParams, SegmentConfig, SegmentInfo, SegmentType, SeqNumberType,\n+    SnapshotFormat, VectorName, VectorNameBuf, WithPayload, WithVector,\n };\n \n /// Define all operations which can be performed with Segment or Segment-like entity.\n-///\n+/// \n /// Assume all operations are idempotent - which means that no matter how many times an operation\n /// is executed - the storage state will be the same.\n-pub trait SegmentEntry: SnapshotEntry {\n+pub trait SegmentEntry {\n     /// Get current update version of the segment\n     fn version(&self) -> SeqNumberType;\n \n-    /// Get version of specified point\n-    fn point_version(&self, point_id: PointIdType) -> Option<SeqNumberType>;\n-\n-    #[allow(clippy::too_many_arguments)]\n-    fn search_batch(\n-        &self,\n-        vector_name: &VectorName,\n-        query_vectors: &[&QueryVector],\n-        with_payload: &WithPayload,\n-        with_vector: &WithVector,\n-        filter: Option<&Filter>,\n-        top: usize,\n-        params: Option<&SearchParams>,\n-        query_context: &SegmentQueryContext,\n-    ) -> OperationResult<Vec<Vec<ScoredPoint>>>;\n-\n-    /// Rescore results with a formula that can reference payload values.\n"
    },
    "llm_calls": 1,
    "elapsed_time": 14.997937202453613,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 59831,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "qdrant_lib_segment_src_id_tracker_immutable_id_tracker.rs",
    "exact_match": false,
    "similarity": 0.14429906542056076,
    "diff_lines": 1138,
    "predicted_output": "// TODO: Remove when we release the next version and integrate the immutable id tracker\n#![allow(dead_code)]\nuse std::collections::BTreeMap;\nuse std::fs::File;\nuse std::io::{BufReader, BufWriter, Read, Write};\nuse std::mem::size_of_val;\nuse std::path::{Path, PathBuf};\n\nuse bitvec::prelude::BitSlice;\nuse bitvec::vec::BitVec;\nuse byteorder::{LittleEndian, ReadBytesExt, WriteBytesExt};\nuse common::types::PointOffsetType;\nuse memory::mmap_ops::{create_and_ensure_length, open_write_mmap};\nuse uuid::Uuid;\n\nuse crate::common::mmap_bitslice_buffered_update_wrapper::MmapBitSliceBufferedUpdateWrapper;\nuse crate::common::mmap_slice_buffered_update_wrapper::MmapSliceBufferedUpdateWrapper;\nuse crate::common::mmap_type::{MmapBitSlice, MmapSlice};\nuse crate::common::operation_error::{OperationError, OperationResult};\nuse crate::common::Flusher;\nuse crate::id_tracker::IdTracker;\nuse crate::types::{ExtendedPointId, PointIdType, SeqNumberType};\n\npub const DELETED_FILE_NAME: &str = \"id_tracker.deleted\";\npub const MAPPINGS_FILE_NAME: &str = \"id_tracker.mappings\";\npub const VERSION_MAPPING_FILE_NAME: &str = \"id_tracker.versions\";\n\n#[derive(Debug)]\npub struct ImmutableIdTracker {\n    path: PathBuf,\n\n    deleted: BitVec,\n    deleted_wrapper: MmapBitSliceBufferedUpdateWrapper,\n\n    internal_to_version: Vec<SeqNumberType>,\n    internal_to_version_wrapper: MmapSliceBufferedUpdateWrapper<SeqNumberType>,\n\n    mappings: PointMappings,\n}\n\n#[derive(Clone, PartialEq, Debug)]\npub struct PointMappings {\n    pub(crate) internal_to_external: Vec<PointIdType>,\n\n    // Having two separate maps allows us iterating only over one type at a time without having to filter.\n    pub(crate) external_to_internal_num: BTreeMap<u64, PointOffsetType>,\n    pub(crate) external_to_internal_uuid: BTreeMap<Uuid, PointOffsetType>,\n}\n\ntype FileEndianess = LittleEndian;\n\nimpl PointMappings {\n    const EXTERNAL_ID_NUMBER_BYTE: u8 = 0;\n    const EXTERNAL_ID_UUID_BYTE: u8 = 1;\n\n    pub fn load<R: Read>(mut reader: R, filter: Option<&BitSlice>) -> OperationResult<Self> {\n        let len = reader.read_u64::<FileEndianess>()? as usize;\n\n        let mut internal_to_external = Vec::with_capacity(len);\n        let mut external_to_internal_num: BTreeMap<u64, PointOffsetType> = BTreeMap::new();\n        let mut external_to_internal_uuid: BTreeMap<Uuid, PointOffsetType> = BTreeMap::new();\n\n        for i in 0..len {\n            let (internal_id, external_id) = Self::read_entry(&mut reader)?;\n\n            internal_to_external.push(external_id);\n\n            let deleted = filter\n                .as_ref()\n                .and_then(|deleted| deleted.get(i).as_deref().copied())\n                .unwrap_or_default();\n\n            if deleted {\n                continue;\n            }\n\n            match external_id {\n                ExtendedPointId::NumId(num) => {\n                    external_to_internal_num.insert(num, internal_id);\n                }\n                ExtendedPointId::Uuid(uuid) => {\n                    external_to_internal_uuid.insert(uuid, internal_id);\n                }\n            }\n        }\n\n        #[cfg(debug_assertions)]\n        {\n            let mut buf = vec![];\n            let read_bytes = reader.read_to_end(&mut buf).unwrap();\n            assert_eq!(buf.len(), 0);\n            assert_eq!(read_bytes, 0);\n        }\n\n        Ok(PointMappings {\n            internal_to_external,\n            external_to_internal_num,\n            external_to_internal_uuid,\n        })\n    }\n\n    fn read_entry<R: Read>(mut reader: R) -> OperationResult<(PointOffsetType, ExtendedPointId)> {\n        let point_id_type = reader.read_u8()?;\n\n        let external_id = if point_id_type == Self::EXTERNAL_ID_NUMBER_BYTE {\n            let num = reader.read_u64::<FileEndianess>()?;\n            PointIdType::NumId(num)\n        } else if point_id_type == Self::EXTERNAL_ID_UUID_BYTE {\n            let uuid_u128 = reader.read_u128::<FileEndianess>()?;\n            PointIdType::Uuid(Uuid::from_u128_le(uuid_u128))\n        } else {\n            return Err(OperationError::InconsistentStorage {\n                description: \"Invalid byte read when deserializing Immutable id tracker\".to_string(),\n            });\n        };\n\n        let internal_id = reader.read_u32::<FileEndianess>()? as PointOffsetType;\n        Ok((internal_id, external_id))\n    }\n\n    pub fn store<W: Write>(&self, mut writer: W) -> OperationResult<()> {\n        writer.write_u64::<FileEndianess>(self.internal_to_external.len() as u64)?;\n\n        for external_id in self.internal_to_external.iter() {\n            self.write_entry(&mut writer, external_id)?;\n        }\n\n        writer.flush()?;\n        Ok(())\n    }\n\n    fn write_entry<W: Write>(\n        &self,\n        mut writer: W,\n        external_id: &PointIdType,\n    ) -> OperationResult<()> {\n        match external_id {\n            PointIdType::NumId(num) => {\n                writer.write_u8(Self::EXTERNAL_ID_NUMBER_BYTE)?;\n                writer.write_u64::<FileEndianess>(*num)?;\n            }\n            PointIdType::Uuid(uuid) => {\n                writer.write_u8(Self::EXTERNAL_ID_UUID_BYTE)?;\n                writer.write_u128::<FileEndianess>(uuid.to_u128_le())?;\n            }\n        }\n\n        let internal_id = match external_id {\n            PointIdType::NumId(n) => self.external_to_internal_num.get(n),\n            PointIdType::Uuid(u) => self.external_to_internal_uuid.get(u),\n        }\n        .ok_or(OperationError::PointIdError {\n            missed_point_id: *external_id,\n        })?;\n\n        writer.write_u32::<FileEndianess>(*internal_id)?;\n        Ok(())\n    }\n}\n\nimpl ImmutableIdTracker {\n    pub fn open(segment_path: &Path) -> OperationResult<Self> {\n        let deleted_raw = open_write_mmap(&Self::deleted_file_path(segment_path))?;\n        let deleted_mmap = MmapBitSlice::try_from(deleted_raw, 0)?;\n        let deleted_bitvec = deleted_mmap.to_bitvec();\n        let deleted_wrapper = MmapBitSliceBufferedUpdateWrapper::new(deleted_mmap);\n\n        let internal_to_version_map =\n            open_write_mmap(&Self::version_mapping_file_path(segment_path))?;\n        let internal_to_version_mapslice: MmapSlice<SeqNumberType> =\n            unsafe { MmapSlice::try_from(internal_to_version_map)? };\n        let internal_to_version = internal_to_version_mapslice.to_vec();\n        let internal_to_version_wrapper =\n            MmapSliceBufferedUpdateWrapper::new(internal_to_version_mapslice);\n\n        let reader = BufReader::new(File::open(Self::mappings_file_path(segment_path))?);\n        let mappings = PointMappings::load(reader, Some(&deleted_bitvec))?;\n\n        Ok(Self {\n            path: segment_path.to_path_buf(),\n            deleted: deleted_bitvec,\n            deleted_wrapper,\n            internal_to_version_wrapper,\n            internal_to_version,\n            mappings,\n        })\n    }\n\n    pub(super) fn new(\n        path: &Path,\n        deleted: &BitSlice,\n        internal_to_version: &[SeqNumberType],\n        mappings: PointMappings,\n    ) -> OperationResult<Self> {\n        let deleted_filepath = Self::deleted_file_path(path);\n        {\n            let deleted_size = bitmap_mmap_size(deleted);\n            create_and_ensure_length(&deleted_filepath, deleted_size)?;\n        }\n\n        let deleted_mmap = open_write_mmap(&deleted_filepath)?;\n        let mut deleted_new = MmapBitSlice::try_from(deleted_mmap, 0)?;\n        deleted_new[..deleted.len()].copy_from_bitslice(deleted);\n        let deleted_wrapper = MmapBitSliceBufferedUpdateWrapper::new(deleted_new);\n\n        let version_filepath = Self::version_mapping_file_path(path);\n        {\n            let version_size = size_of_val(internal_to_version);\n            create_and_ensure_length(&version_filepath, version_size)?;\n        }\n        let mut internal_to_version_wrapper =\n            unsafe { MmapSlice::try_from(open_write_mmap(&version_filepath)?)? };\n        internal_to_version_wrapper.copy_from_slice(internal_to_version);\n        let internal_to_version = internal_to_version_wrapper.to_vec();\n        let internal_to_version_wrapper =\n            MmapSliceBufferedUpdateWrapper::new(internal_to_version_wrapper);\n\n        let writer = BufWriter::new(File::create(Self::mappings_file_path(path))?);\n        mappings.store(writer)?;\n\n        Ok(Self {\n            path: path.to_path_buf(),\n            deleted: deleted.to_bitvec(),\n            deleted_wrapper,\n            internal_to_version_wrapper,\n            internal_to_version,\n            mappings,\n        })\n    }\n\n    fn deleted_file_path(base: &Path) -> PathBuf {\n        base.join(DELETED_FILE_NAME)\n    }\n\n    fn version_mapping_file_path(base: &Path) -> PathBuf {\n        base.join(VERSION_MAPPING_FILE_NAME)\n    }\n\n    pub(crate) fn mappings_file_path(base: &Path) -> PathBuf {\n        base.join(MAPPINGS_FILE_NAME)\n    }\n}\n\n/// Returns the required mmap filesize for a `BitSlice`.\nfn bitmap_mmap_size(deleted: &BitSlice) -> usize {\n    let usize_bytes = std::mem::size_of::<usize>();\n    let num_bytes = deleted.len().div_ceil(8); // used bytes\n    num_bytes.div_ceil(usize_bytes) * usize_bytes // Make it a multiple of usize-width.\n}\n\nimpl IdTracker for ImmutableIdTracker {\n    fn internal_version(&self, internal_id: PointOffsetType) -> Option<SeqNumberType> {\n        self.internal_to_version.get(internal_id as usize).copied()\n    }\n\n    fn set_internal_version(\n        &mut self,\n        internal_id: PointOffsetType,\n        version: SeqNumberType,\n    ) -> OperationResult<()> {\n        if self.external_id(internal_id).is_some() {\n            if let Some(old_version) = self.internal_to_version.get_mut(internal_id as usize) {\n                *old_version = version;\n                self.internal_to_version_wrapper.set(internal_id as usize, version);\n            }\n        }\n\n        Ok(())\n    }\n\n    fn internal_id(&self, external_id: PointIdType) -> Option<PointOffsetType> {\n        match external_id {\n            PointIdType::NumId(num) => self.mappings.external_to_internal_num.get(&num).copied(),\n            PointIdType::Uuid(uuid) => self.mappings.external_to_internal_uuid.get(&uuid).copied(),\n        }\n    }\n\n    fn external_id(&self, internal_id: PointOffsetType) -> Option<PointIdType> {\n        if *self.deleted.get(internal_id as usize)? {\n            return None;\n        }\n\n        self.mappings\n            .internal_to_external\n            .get(internal_id as usize)\n            .map(|i| i.into())\n    }\n\n    fn set_link(\n        &mut self,\n        _external_id: PointIdType,\n        _internal_id: PointOffsetType,\n    ) -> OperationResult<()> {\n        panic!(\"Trying to call a mutating function (`set_link`) of an immutable id tracker\");\n    }\n\n    fn drop(&mut self, external_id: PointIdType) -> OperationResult<()> {\n        let internal_id = match external_id {\n            PointIdType::NumId(num) => self.mappings.external_to_internal_num.remove(&num),\n            PointIdType::Uuid(uuid) => self.mappings.external_to_internal_uuid.remove(&uuid),\n        };\n\n        if let Some(internal_id) = internal_id {\n            self.deleted.set(internal_id as usize, true);\n            self.deleted_wrapper.set(internal_id as usize, true);\n        }\n\n        Ok(())\n    }\n\n    fn iter_external(&self) -> Box<dyn Iterator<Item = PointIdType> + '_> {\n        let iter_num = self\n            .mappings\n            .external_to_internal_num\n            .keys()\n            .map(|i| PointIdType::NumId(*i));\n\n        let iter_uuid = self\n            .mappings\n            .external_to_internal_uuid\n            .keys()\n            .map(|i| PointIdType::Uuid(*i));\n        Box::new(iter_num.chain(iter_uuid))\n    }\n\n    fn iter_internal(&self) -> Box<dyn Iterator<Item = PointOffsetType> + '_> {\n        Box::new(\n            (0..self.mappings.internal_to_external.len() as PointOffsetType)\n                .filter(move |i| !self.deleted[*i as usize]),\n        )\n    }\n\n    fn iter_from(\n        &self,\n        external_id: Option<PointIdType>,\n    ) -> Box<dyn Iterator<Item = (PointIdType, PointOffsetType)> + '_> {\n        let full_num_iter = || {\n            self.mappings\n                .external_to_internal_num\n                .iter()\n                .map(|(k, v)| (PointIdType::NumId(*k), *v))\n        };\n        let full_uuid_iter = || {\n            self.mappings\n                .external_to_internal_uuid\n                .iter()\n                .map(|(k, v)| (PointIdType::Uuid(*k), *v))\n        };\n\n        match external_id {\n            None => {\n                let iter_num = full_num_iter();\n                let iter_uuid = full_uuid_iter();\n                Box::new(iter_num.chain(iter_uuid))\n            }\n            Some(offset) => match offset {\n                PointIdType::NumId(idx) => {\n                    let iter_num = full_num_iter();\n                    let iter_uuid = full_uuid_iter();\n                    Box::new(iter_num.chain(iter_uuid))\n                }\n                PointIdType::Uuid(uuid) => {\n                    Box::new(full_uuid_iter())\n                }\n            },\n        }\n    }\n\n    fn iter_ids(&self) -> Box<dyn Iterator<Item = PointOffsetType> + '_> {\n        self.iter_internal()\n    }\n\n    fn total_point_count(&self) -> usize {\n        self.mappings.internal_to_external.len()\n    }\n\n    fn available_point_count(&self) -> usize {\n        self.mappings.external_to_internal_num.len() + self.mappings.external_to_internal_uuid.len()\n    }\n\n    fn deleted_point_count(&self) -> usize {\n        self.total_point_count() - self.available_point_count()\n    }\n\n    fn deleted_point_bitslice(&self) -> &BitSlice {\n        &self.deleted\n    }\n\n    fn is_deleted_point(&self, key: PointOffsetType) -> bool {\n        let key = key as usize;\n        if key >= self.deleted.len() {\n            return true;\n        }\n        self.deleted[key]\n    }\n\n    fn name(&self) -> &'static str {\n        \"immutable id tracker\"\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.14429906542056076,
      "diff_lines": 1138,
      "added_lines": 184,
      "removed_lines": 753,
      "diff_preview": "--- expected+++ predicted@@ -1,201 +1,128 @@+// TODO: Remove when we release the next version and integrate the immutable id tracker\n+#![allow(dead_code)]\n+use std::collections::BTreeMap;\n use std::fs::File;\n use std::io::{BufReader, BufWriter, Read, Write};\n-use std::mem::{size_of, size_of_val};\n+use std::mem::size_of_val;\n use std::path::{Path, PathBuf};\n \n use bitvec::prelude::BitSlice;\n use bitvec::vec::BitVec;\n-use byteorder::{ReadBytesExt, WriteBytesExt};\n-use common::ext::BitSliceExt as _;\n+use byteorder::{LittleEndian, ReadBytesExt, WriteBytesExt};\n use common::types::PointOffsetType;\n-use memory::madvise::AdviceSetting;\n use memory::mmap_ops::{create_and_ensure_length, open_write_mmap};\n-use memory::mmap_type::{MmapBitSlice, MmapSlice};\n use uuid::Uuid;\n \n-use crate::common::Flusher;\n use crate::common::mmap_bitslice_buffered_update_wrapper::MmapBitSliceBufferedUpdateWrapper;\n use crate::common::mmap_slice_buffered_update_wrapper::MmapSliceBufferedUpdateWrapper;\n+use crate::common::mmap_type::{MmapBitSlice, MmapSlice};\n use crate::common::operation_error::{OperationError, OperationResult};\n+use crate::common::Flusher;\n use crate::id_tracker::IdTracker;\n-use crate::id_tracker::compressed::compressed_point_mappings::CompressedPointMappings;\n-use crate::id_tracker::compressed::external_to_internal::CompressedExternalToInternal;\n-use crate::id_tracker::compressed::internal_to_external::CompressedInternalToExternal;\n-use crate::id_tracker::compressed::versions_store::CompressedVersions;\n-use crate::id_tracker::in_memory_id_tracker::InMemoryIdTracker;\n-use crate::id_tracker::point_mappings::FileEndianess;\n use crate::types::{ExtendedPointId, PointIdType, SeqNumberType};\n \n pub const DELETED_FILE_NAME: &str = \"id_tracker.deleted\";\n pub const MAPPINGS_FILE_NAME: &str = \"id_tracker.mappings\";\n pub const VERSION_MAPPING_FILE_NAME: &str = \"id_tracker.versions\";\n \n-#[derive(Copy, Clone)]\n-#[repr(u8)]\n-enum ExternalIdType {\n-    Number = 0,\n-    Uuid = 1,\n-}\n-\n-impl ExternalIdType {\n"
    },
    "llm_calls": 1,
    "elapsed_time": 94.74665117263794,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 29271,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "qdrant_lib_segment_src_index_field_index_field_index_base.rs",
    "exact_match": false,
    "similarity": 0.16637304558467297,
    "diff_lines": 594,
    "predicted_output": "use std::fmt::Formatter;\nuse std::path::PathBuf;\n\nuse common::counter::hardware_counter::HardwareCounterCell;\nuse common::types::PointOffsetType;\nuse serde_json::Value;\n\nuse super::bool_index::mmap_bool_index::MmapBoolIndexBuilder;\nuse super::bool_index::simple_bool_index::BoolIndexBuilder;\nuse super::bool_index::BoolIndex;\nuse super::facet_index::FacetIndexEnum;\nuse super::full_text_index::mmap_text_index::FullTextMmapIndexBuilder;\nuse super::full_text_index::text_index::{FullTextIndex, FullTextIndexBuilder};\nuse super::geo_index::{GeoMapIndexBuilder, GeoMapIndexMmapBuilder};\nuse super::map_index::{MapIndex, MapIndexBuilder, MapIndexMmapBuilder};\nuse super::numeric_index::{NumericIndex, NumericIndexBuilder, NumericIndexMmapBuilder, StreamRange};\nuse crate::common::operation_error::OperationResult;\nuse crate::common::Flusher;\nuse crate::data_types::order_by::OrderValue;\nuse crate::index::field_index::geo_index::GeoMapIndex;\nuse crate::index::field_index::numeric_index::NumericIndexInner;\nuse crate::index::field_index::{CardinalityEstimation, PayloadBlockCondition};\nuse crate::telemetry::PayloadIndexTelemetry;\nuse crate::types::{DateTimePayloadType, FieldCondition, FloatPayloadType, IntPayloadType, Match, MatchText, PayloadKeyType, RangeInterface, UuidIntType, UuidPayloadType};\n\npub trait PayloadFieldIndex {\n    /// Get iterator over points fitting given `condition`\n    /// Return `None` if condition does not match the index type\n    fn filter<'a>(\n        &'a self,\n        condition: &'a FieldCondition,\n        hw_counter: &'a HardwareCounterCell,\n    ) -> Option<Box<dyn Iterator<Item = PointOffsetType> + 'a>>;\n\n    /// Return estimation of amount of points which satisfy given condition.\n    /// Returns `None` if the condition does not match the index type\n    fn estimate_cardinality(\n        &self,\n        condition: &FieldCondition,\n        hw_counter: &HardwareCounterCell,\n    ) -> Option<CardinalityEstimation>;\n\n    /// Iterate conditions for payload blocks with minimum size of `threshold`\n    /// Required for building HNSW index\n    fn payload_blocks(\n        &self,\n        threshold: usize,\n        key: PayloadKeyType,\n    ) -> Box<dyn Iterator<Item = PayloadBlockCondition> + '_>;\n\n    /// Load index from disk.\n    fn load(&mut self) -> OperationResult<bool>;\n\n    /// Remove db content or files of the current payload index\n    fn cleanup(self) -> OperationResult<()>;\n\n    /// Return function that flushes all pending updates to disk.\n    fn flusher(&self) -> Flusher;\n\n    fn files(&self) -> Vec<PathBuf>;\n}\n\npub enum FieldIndex {\n    IntIndex(NumericIndex<IntPayloadType>),\n    DatetimeIndex(NumericIndex<IntPayloadType>),\n    IntMapIndex(MapIndex<IntPayloadType>),\n    KeywordIndex(MapIndex<str>),\n    FloatIndex(NumericIndex<FloatPayloadType>),\n    GeoIndex(GeoMapIndex),\n    FullTextIndex(FullTextIndex),\n    BoolIndex(BoolIndex),\n    UuidIndex(NumericIndex<UuidIntType, UuidPayloadType>),\n    UuidMapIndex(MapIndex<UuidIntType>),\n    NullIndex(MmapNullIndex),\n}\n\nimpl std::fmt::Debug for FieldIndex {\n    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {\n        match self {\n            FieldIndex::IntIndex(_index) => write!(f, \"IntIndex\"),\n            FieldIndex::DatetimeIndex(_index) => write!(f, \"DatetimeIndex\"),\n            FieldIndex::IntMapIndex(_index) => write!(f, \"IntMapIndex\"),\n            FieldIndex::KeywordIndex(_index) => write!(f, \"KeywordIndex\"),\n            FieldIndex::FloatIndex(_index) => write!(f, \"FloatIndex\"),\n            FieldIndex::GeoIndex(_index) => write!(f, \"GeoIndex\"),\n            FieldIndex::BoolIndex(_index) => write!(f, \"BoolIndex\"),\n            FieldIndex::FullTextIndex(_index) => write!(f, \"FullTextIndex\"),\n            FieldIndex::UuidIndex(_index) => write!(f, \"UuidIndex\"),\n            FieldIndex::UuidMapIndex(_index) => write!(f, \"UuidMapIndex\"),\n            FieldIndex::NullIndex(_index) => write!(f, \"NullIndex\"),\n        }\n    }\n}\n\nimpl FieldIndex {\n    pub fn load(&mut self) -> OperationResult<bool> {\n        match self {\n            FieldIndex::IntIndex(payload_field_index) => payload_field_index.load(),\n            FieldIndex::DatetimeIndex(payload_field_index) => payload_field_index.load(),\n            FieldIndex::IntMapIndex(payload_field_index) => payload_field_index.load(),\n            FieldIndex::KeywordIndex(payload_field_index) => payload_field_index.load(),\n            FieldIndex::FloatIndex(payload_field_index) => payload_field_index.load(),\n            FieldIndex::GeoIndex(payload_field_index) => payload_field_index.load(),\n            FieldIndex::BoolIndex(payload_field_index) => payload_field_index.load(),\n            FieldIndex::FullTextIndex(payload_field_index) => payload_field_index.load(),\n            FieldIndex::UuidIndex(payload_field_index) => payload_field_index.load(),\n            FieldIndex::UuidMapIndex(payload_field_index) => payload_field_index.load(),\n            FieldIndex::NullIndex(payload_field_index) => payload_field_index.load(),\n        }\n    }\n\n    pub fn add_point(\n        &mut self,\n        id: PointOffsetType,\n        payload: &[&Value],\n        hw_counter: &HardwareCounterCell,\n    ) -> OperationResult<()> {\n        match self {\n            FieldIndex::IntIndex(payload_field_index) => {\n                payload_field_index.add_point(id, payload, hw_counter)\n            }\n            FieldIndex::DatetimeIndex(payload_field_index) => {\n                payload_field_index.add_point(id, payload, hw_counter)\n            }\n            FieldIndex::IntMapIndex(payload_field_index) => {\n                payload_field_index.add_point(id, payload, hw_counter)\n            }\n            FieldIndex::KeywordIndex(payload_field_index) => {\n                payload_field_index.add_point(id, payload, hw_counter)\n            }\n            FieldIndex::FloatIndex(payload_field_index) => {\n                payload_field_index.add_point(id, payload, hw_counter)\n            }\n            FieldIndex::GeoIndex(payload_field_index) => {\n                payload_field_index.add_point(id, payload, hw_counter)\n            }\n            FieldIndex::BoolIndex(payload_field_index) => {\n                payload_field_index.add_point(id, payload, hw_counter)\n            }\n            FieldIndex::FullTextIndex(payload_field_index) => {\n                payload_field_index.add_point(id, payload, hw_counter)\n            }\n            FieldIndex::UuidIndex(payload_field_index) => {\n                payload_field_index.add_point(id, payload, hw_counter)\n            }\n            FieldIndex::UuidMapIndex(payload_field_index) => {\n                payload_field_index.add_point(id, payload, hw_counter)\n            }\n            FieldIndex::NullIndex(payload_field_index) => {\n                payload_field_index.add_point(id, payload, hw_counter)\n            }\n        }\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.16637304558467297,
      "diff_lines": 594,
      "added_lines": 18,
      "removed_lines": 521,
      "diff_preview": "--- expected+++ predicted@@ -5,45 +5,25 @@ use common::types::PointOffsetType;\n use serde_json::Value;\n \n-use super::bool_index::BoolIndex;\n use super::bool_index::mmap_bool_index::MmapBoolIndexBuilder;\n use super::bool_index::simple_bool_index::BoolIndexBuilder;\n+use super::bool_index::BoolIndex;\n use super::facet_index::FacetIndexEnum;\n use super::full_text_index::mmap_text_index::FullTextMmapIndexBuilder;\n use super::full_text_index::text_index::{FullTextIndex, FullTextIndexBuilder};\n use super::geo_index::{GeoMapIndexBuilder, GeoMapIndexMmapBuilder};\n use super::map_index::{MapIndex, MapIndexBuilder, MapIndexMmapBuilder};\n-use super::numeric_index::{\n-    NumericIndex, NumericIndexBuilder, NumericIndexMmapBuilder, StreamRange,\n-};\n+use super::numeric_index::{NumericIndex, NumericIndexBuilder, NumericIndexMmapBuilder, StreamRange};\n+use crate::common::operation_error::OperationResult;\n use crate::common::Flusher;\n-use crate::common::operation_error::OperationResult;\n use crate::data_types::order_by::OrderValue;\n use crate::index::field_index::geo_index::GeoMapIndex;\n-use crate::index::field_index::null_index::mmap_null_index::{MmapNullIndex, MmapNullIndexBuilder};\n use crate::index::field_index::numeric_index::NumericIndexInner;\n use crate::index::field_index::{CardinalityEstimation, PayloadBlockCondition};\n use crate::telemetry::PayloadIndexTelemetry;\n-use crate::types::{\n-    DateTimePayloadType, FieldCondition, FloatPayloadType, IntPayloadType, Match, MatchText,\n-    PayloadKeyType, RangeInterface, UuidIntType, UuidPayloadType,\n-};\n+use crate::types::{DateTimePayloadType, FieldCondition, FloatPayloadType, IntPayloadType, Match, MatchText, PayloadKeyType, RangeInterface, UuidIntType, UuidPayloadType};\n \n pub trait PayloadFieldIndex {\n-    /// Return number of points with at least one value indexed in here\n-    fn count_indexed_points(&self) -> usize;\n-\n-    /// Load index from disk.\n-    fn load(&mut self) -> OperationResult<bool>;\n-\n-    /// Remove db content or files of the current payload index\n-    fn cleanup(self) -> OperationResult<()>;\n-\n-    /// Return function that flushes all pending updates to disk.\n-    fn flusher(&self) -> Flusher;\n-\n-    fn files(&self) -> Vec<PathBuf>;\n-\n     /// Get iterator over points fitting given `condition`\n"
    },
    "llm_calls": 1,
    "elapsed_time": 44.48618292808533,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 54941,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "qdrant_lib_segment_src_index_field_index_full_text_index_inverted_index.rs",
    "exact_match": false,
    "similarity": 0.5329302091206853,
    "diff_lines": 403,
    "predicted_output": "use std::collections::{BTreeSet, HashMap};\n\nuse common::types::PointOffsetType;\nuse serde::{Deserialize, Serialize};\n\nuse crate::common::operation_error::{OperationError, OperationResult};\nuse crate::index::field_index::{CardinalityEstimation, PayloadBlockCondition, PrimaryCondition};\nuse crate::types::{FieldCondition, Match, PayloadKeyType};\n\npub type TokenId = u32;\n\n#[derive(Default, Serialize, Deserialize, Clone)]\npub struct Document {\n    tokens: Vec<TokenId>,\n}\n\nimpl Document {\n    pub fn new(mut tokens: Vec<TokenId>) -> Self {\n        tokens.sort_unstable();\n        Self { tokens }\n    }\n\n    pub fn len(&self) -> usize {\n        self.tokens.len()\n    }\n\n    pub fn is_empty(&self) -> bool {\n        self.tokens.is_empty()\n    }\n\n    pub fn tokens(&self) -> &[TokenId] {\n        &self.tokens\n    }\n\n    pub fn check(&self, token: TokenId) -> bool {\n        self.tokens.binary_search(&token).is_ok()\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct ParsedQuery {\n    pub tokens: Vec<Option<TokenId>>,\n}\n\nimpl ParsedQuery {\n    pub fn check_match(&self, document: &Document) -> bool {\n        if self.tokens.contains(&None) {\n            return false;\n        }\n\n        // Check that all tokens are in document\n        self.tokens\n            .iter()\n            .all(|query_token| document.check(query_token.unwrap()))\n    }\n}\n\npub trait InvertedIndex {\n    fn get_vocab_mut(&mut self) -> &mut HashMap<String, TokenId>;\n\n    fn document_from_tokens(&mut self, tokens: &BTreeSet<String>) -> Document {\n        let vocab = self.get_vocab_mut();\n        let mut document_tokens = vec![];\n        for token in tokens {\n            // check if in vocab\n            let vocab_idx = match vocab.get(token) {\n                Some(&idx) => idx,\n                None => {\n                    let next_token_id = vocab.len() as TokenId;\n                    vocab.insert(token.to_string(), next_token_id);\n                    next_token_id\n                }\n            };\n            document_tokens.push(vocab_idx);\n        }\n        Document::new(document_tokens)\n    }\n\n    fn index_document(&mut self, idx: PointOffsetType, document: Document) -> OperationResult<()>;\n\n    fn remove_document(&mut self, idx: PointOffsetType) -> bool;\n\n    fn filter(\n        &self,\n        query: &ParsedQuery,\n    ) -> Box<dyn Iterator<Item = PointOffsetType> + '_>;\n\n    fn get_posting_len(&self, token_id: TokenId) -> Option<usize>;\n\n    fn estimate_cardinality(\n        &self,\n        query: &ParsedQuery,\n        condition: &FieldCondition,\n    ) -> CardinalityEstimation {\n        let points_count = self.points_count();\n\n        let posting_lengths: Option<Vec<usize>> = query\n            .tokens\n            .iter()\n            .map(|&vocab_idx| match vocab_idx {\n                None => None,\n                Some(idx) => self.get_posting_len(idx),\n            })\n            .collect();\n        if posting_lengths.is_none() || points_count == 0 {\n            // There are unseen tokens -> no matches\n            return CardinalityEstimation {\n                primary_clauses: vec![PrimaryCondition::Condition(condition.clone())],\n                min: 0,\n                exp: 0,\n                max: 0,\n            };\n        }\n        let postings = posting_lengths.unwrap();\n        if postings.is_empty() {\n            // Empty request -> no matches\n            return CardinalityEstimation {\n                primary_clauses: vec![PrimaryCondition::Condition(condition.clone())],\n                min: 0,\n                exp: 0,\n                max: 0,\n            };\n        }\n        // Smallest posting is the largest possible cardinality\n        let smallest_posting = postings.iter().min().copied().unwrap();\n\n        if postings.len() == 1 {\n            CardinalityEstimation {\n                primary_clauses: vec![PrimaryCondition::Condition(condition.clone())],\n                min: smallest_posting,\n                exp: smallest_posting,\n                max: smallest_posting,\n            }\n        } else {\n            let expected_frac: f64 = postings\n                .iter()\n                .map(|posting| *posting as f64 / points_count as f64)\n                .product();\n            let exp = (expected_frac * points_count as f64) as usize;\n            CardinalityEstimation {\n                primary_clauses: vec![PrimaryCondition::Condition(condition.clone())],\n                min: 0, // ToDo: make better estimation\n                exp,\n                max: smallest_posting,\n            }\n        }\n    }\n\n    fn payload_blocks(\n        &self,\n        threshold: usize,\n        key: PayloadKeyType,\n    ) -> impl Iterator<Item = PayloadBlockCondition> + '_ {\n        let map_filter_condition =\n            move |(token, postings_len): (&str, usize)| {\n                if postings_len >= threshold {\n                    Some(PayloadBlockCondition {\n                        condition: FieldCondition {\n                            key: key.clone(),\n                            r#match: Some(Match::Text(MatchText {\n                                text: token.to_owned(),\n                            })),\n                            range: None,\n                            geo_bounding_box: None,\n                            geo_radius: None,\n                            geo_polygon: None,\n                            values_count: None,\n                        },\n                        cardinality: postings_len,\n                    })\n                } else {\n                    None\n                }\n            };\n\n        self.vocab_with_postings_len_iter()\n            .filter_map(map_filter_condition)\n    }\n\n    fn check_match(\n        &self,\n        parsed_query: &ParsedQuery,\n        point_id: PointOffsetType,\n    ) -> bool;\n\n    fn values_is_empty(&self, point_id: PointOffsetType) -> bool;\n\n    fn values_count(&self, point_id: PointOffsetType) -> usize;\n\n    fn points_count(&self) -> usize;\n\n    fn get_token_id(&self, token: &str) -> Option<TokenId>;\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.5329302091206853,
      "diff_lines": 403,
      "added_lines": 35,
      "removed_lines": 268,
      "diff_preview": "--- expected+++ predicted@@ -1,16 +1,15 @@ use std::collections::{BTreeSet, HashMap};\n \n-use common::counter::hardware_counter::HardwareCounterCell;\n use common::types::PointOffsetType;\n use serde::{Deserialize, Serialize};\n \n-use crate::common::operation_error::OperationResult;\n+use crate::common::operation_error::{OperationError, OperationResult};\n use crate::index::field_index::{CardinalityEstimation, PayloadBlockCondition, PrimaryCondition};\n use crate::types::{FieldCondition, Match, PayloadKeyType};\n \n pub type TokenId = u32;\n \n-#[derive(Default, Serialize, Deserialize, Debug, Clone)]\n+#[derive(Default, Serialize, Deserialize, Clone)]\n pub struct Document {\n     tokens: Vec<TokenId>,\n }\n@@ -52,7 +51,6 @@         // Check that all tokens are in document\n         self.tokens\n             .iter()\n-            // unwrap crash safety: all tokens exist in the vocabulary if it passes the above check\n             .all(|query_token| document.check(query_token.unwrap()))\n     }\n }\n@@ -75,33 +73,24 @@             };\n             document_tokens.push(vocab_idx);\n         }\n-\n         Document::new(document_tokens)\n     }\n \n-    fn index_document(\n-        &mut self,\n-        idx: PointOffsetType,\n-        document: Document,\n-        hw_counter: &HardwareCounterCell,\n-    ) -> OperationResult<()>;\n+    fn index_document(&mut self, idx: PointOffsetType, document: Document) -> OperationResult<()>;\n \n     fn remove_document(&mut self, idx: PointOffsetType) -> bool;\n \n-    fn filter<'a>(\n-        &'a self,\n-        query: ParsedQuery,\n"
    },
    "llm_calls": 1,
    "elapsed_time": 33.80569911003113,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 33649,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "qdrant_lib_segment_src_index_field_index_full_text_index_text_index.rs",
    "exact_match": false,
    "similarity": 0.2872691483818677,
    "diff_lines": 393,
    "predicted_output": "use std::collections::{BTreeSet, HashSet};\nuse std::path::PathBuf;\nuse std::sync::Arc;\n\nuse common::counter::hardware_counter::HardwareCounterCell;\nuse common::types::PointOffsetType;\nuse parking_lot::RwLock;\nuse rocksdb::DB;\nuse serde::{Deserialize, Serialize};\nuse serde_json::Value;\n\nuse super::immutable_text_index::ImmutableFullTextIndex;\nuse super::inverted_index::{Document, InvertedIndex, ParsedQuery, TokenId};\nuse super::mmap_text_index::{FullTextMmapIndexBuilder, MmapFullTextIndex};\nuse super::mutable_text_index::MutableFullTextIndex;\nuse super::tokenizers::Tokenizer;\nuse crate::common::operation_error::{OperationError, OperationResult};\nuse crate::common::rocksdb_buffered_delete_wrapper::DatabaseColumnScheduledDeleteWrapper;\nuse crate::common::rocksdb_wrapper::DatabaseColumnWrapper;\nuse crate::data_types::index::TextIndexParams;\nuse crate::index::field_index::{\n    CardinalityEstimation, FieldIndexBuilderTrait, PayloadBlockCondition, PayloadFieldIndex,\n    ValueIndexer,\n};\nuse crate::telemetry::PayloadIndexTelemetry;\nuse crate::types::{FieldCondition, Match, PayloadKeyType};\n\npub enum FullTextIndex {\n    Mutable(MutableFullTextIndex),\n    Immutable(ImmutableFullTextIndex),\n    Mmap(Box<MmapFullTextIndex>),\n}\n\nimpl FullTextIndex {\n    pub fn new_memory(\n        db: Arc<RwLock<DB>>,\n        config: TextIndexParams,\n        field: &str,\n    ) -> FullTextIndexBuilder {\n        FullTextIndexBuilder(Self::new_memory(db, config, field, true))\n    }\n\n    pub fn builder_mmap(\n        path: PathBuf,\n        config: TextIndexParams,\n        is_on_disk: bool,\n    ) -> FullTextMmapIndexBuilder {\n        FullTextMmapIndexBuilder::new(path, config, is_on_disk)\n    }\n\n    pub fn get_telemetry_data(&self) -> PayloadIndexTelemetry {\n        PayloadIndexTelemetry {\n            field_name: None,\n            index_type: match self {\n                FullTextIndex::Mutable(_) => \"mutable_full_text\",\n                FullTextIndex::Immutable(_) => \"immutable_full_text\",\n                FullTextIndex::Mmap(_) => \"mmap_full_text\",\n            },\n            points_values_count: self.points_count(),\n            points_count: self.points_count(),\n            histogram_bucket_size: None,\n        }\n    }\n\n    pub fn filter(\n        &self,\n        condition: &FieldCondition,\n        hw_counter: &HardwareCounterCell,\n    ) -> Option<Box<dyn Iterator<Item = PointOffsetType> + '_>> {\n        if let Some(Match::Text(text_match)) = &condition.r#match {\n            let parsed_query = self.parse_query(&text_match.text, hw_counter);\n            return Some(self.filter(parsed_query, hw_counter));\n        }\n        None\n    }\n\n    pub fn estimate_cardinality(\n        &self,\n        condition: &FieldCondition,\n        hw_counter: &HardwareCounterCell,\n    ) -> Option<CardinalityEstimation> {\n        if let Some(Match::Text(text_match)) = &condition.r#match {\n            let parsed_query = self.parse_query(&text_match.text, hw_counter);\n            return Some(self.estimate_cardinality(&parsed_query, condition, hw_counter));\n        }\n        None\n    }\n    \n    // Other functions omitted for brevity...\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.2872691483818677,
      "diff_lines": 393,
      "added_lines": 7,
      "removed_lines": 343,
      "diff_preview": "--- expected+++ predicted@@ -14,7 +14,6 @@ use super::mmap_text_index::{FullTextMmapIndexBuilder, MmapFullTextIndex};\n use super::mutable_text_index::MutableFullTextIndex;\n use super::tokenizers::Tokenizer;\n-use crate::common::Flusher;\n use crate::common::operation_error::{OperationError, OperationResult};\n use crate::common::rocksdb_buffered_delete_wrapper::DatabaseColumnScheduledDeleteWrapper;\n use crate::common::rocksdb_wrapper::DatabaseColumnWrapper;\n@@ -37,42 +36,6 @@         db: Arc<RwLock<DB>>,\n         config: TextIndexParams,\n         field: &str,\n-        is_appendable: bool,\n-    ) -> Self {\n-        let store_cf_name = Self::storage_cf_name(field);\n-        let db_wrapper = DatabaseColumnScheduledDeleteWrapper::new(DatabaseColumnWrapper::new(\n-            db,\n-            &store_cf_name,\n-        ));\n-        if is_appendable {\n-            Self::Mutable(MutableFullTextIndex::new(db_wrapper, config))\n-        } else {\n-            Self::Immutable(ImmutableFullTextIndex::new(db_wrapper, config))\n-        }\n-    }\n-\n-    pub fn new_mmap(\n-        path: PathBuf,\n-        config: TextIndexParams,\n-        is_on_disk: bool,\n-    ) -> OperationResult<Self> {\n-        Ok(Self::Mmap(Box::new(MmapFullTextIndex::open(\n-            path, config, is_on_disk,\n-        )?)))\n-    }\n-\n-    pub fn init(&mut self) -> OperationResult<()> {\n-        match self {\n-            Self::Mutable(index) => index.init(),\n-            Self::Immutable(index) => index.init(),\n-            Self::Mmap(_) => unreachable!(\"not applicable for mmap immutable index\"),\n-        }\n-    }\n-\n-    pub fn builder(\n-        db: Arc<RwLock<DB>>,\n-        config: TextIndexParams,\n-        field: &str,\n"
    },
    "llm_calls": 1,
    "elapsed_time": 22.09644103050232,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 34279,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "qdrant_lib_segment_src_index_field_index_map_index_mod.rs",
    "exact_match": false,
    "similarity": 0.07055568365229421,
    "diff_lines": 1429,
    "predicted_output": "pub mod immutable_map_index;\npub mod mutable_map_index;\n\nuse std::borrow::Borrow;\nuse std::fmt::{Debug, Display};\nuse std::hash::{BuildHasher, Hash};\nuse std::iter;\nuse std::str::FromStr;\nuse std::sync::Arc;\n\nuse ahash::HashMap;\nuse common::mmap_hashmap::Key;\nuse common::types::PointOffsetType;\nuse indexmap::IndexSet;\nuse itertools::Itertools;\nuse parking_lot::RwLock;\nuse rocksdb::DB;\nuse serde_json::Value;\nuse smol_str::SmolStr;\n\nuse self::immutable_map_index::ImmutableMapIndex;\nuse self::mutable_map_index::MutableMapIndex;\nuse crate::common::operation_error::{OperationError, OperationResult};\nuse crate::common::rocksdb_buffered_delete_wrapper::DatabaseColumnScheduledDeleteWrapper;\nuse crate::common::Flusher;\nuse crate::index::field_index::stat_tools::number_of_selected_points;\nuse crate::index::field_index::{\n    CardinalityEstimation, PayloadBlockCondition, PayloadFieldIndex, PrimaryCondition, ValueIndexer,\n};\nuse crate::types::{\n    AnyVariants, FieldCondition, IntPayloadType, Match, MatchAny, MatchExcept, MatchValue,\n    PayloadKeyType, ValueVariants,\n};\n\npub enum MapIndex<N: Hash + Eq + Clone + Display + FromStr + Default> {\n    Mutable(MutableMapIndex<N>),\n    Immutable(ImmutableMapIndex<N>),\n}\n\nimpl<N: Hash + Eq + Clone + Display + FromStr + Default> MapIndex<N> {\n    pub fn new(db: Arc<RwLock<DB>>, field_name: &str, is_appendable: bool) -> Self {\n        if is_appendable {\n            MapIndex::Mutable(MutableMapIndex::new(db, field_name))\n        } else {\n            MapIndex::Immutable(ImmutableMapIndex::new(db, field_name))\n        }\n    }\n    \n    fn get_db_wrapper(&self) -> &DatabaseColumnScheduledDeleteWrapper {\n        match self {\n            MapIndex::Mutable(index) => index.get_db_wrapper(),\n            MapIndex::Immutable(index) => index.get_db_wrapper(),\n        }\n    }\n\n    pub fn get_values<NRef>(\n        &self,\n        idx: PointOffsetType,\n    ) -> Option<Box<dyn Iterator<Item = &NRef> + '_>>\n    where\n        N: std::borrow::Borrow<NRef>,\n        NRef: ?Sized,\n    {\n        match self {\n            MapIndex::Mutable(index) => Some(Box::new(\n                index.get_values(idx)?.map(std::borrow::Borrow::borrow),\n            )),\n            MapIndex::Immutable(index) => Some(Box::new(\n                index.get_values(idx)?.map(std::borrow::Borrow::borrow),\n            )),\n        }\n    }\n\n    pub fn values_count(&self, idx: PointOffsetType) -> usize {\n        match self {\n            MapIndex::Mutable(index) => index.values_count(idx).unwrap_or_default(),\n            MapIndex::Immutable(index) => index.values_count(idx).unwrap_or_default(),\n        }\n    }\n\n    fn get_iterator(&self, value: &N) -> IdRefIter<'_> {\n        match self {\n            MapIndex::Mutable(index) => index.get_iterator(value),\n            MapIndex::Immutable(index) => index.get_iterator(value),\n        }\n    }\n\n    // Additional implementations omitted for brevity...\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.07055568365229421,
      "diff_lines": 1429,
      "added_lines": 28,
      "removed_lines": 1337,
      "diff_preview": "--- expected+++ predicted@@ -1,149 +1,73 @@+pub mod immutable_map_index;\n+pub mod mutable_map_index;\n+\n use std::borrow::Borrow;\n-use std::collections::hash_map::Entry;\n use std::fmt::{Debug, Display};\n use std::hash::{BuildHasher, Hash};\n use std::iter;\n-use std::path::{Path, PathBuf};\n use std::str::FromStr;\n use std::sync::Arc;\n \n use ahash::HashMap;\n-use common::counter::hardware_counter::HardwareCounterCell;\n use common::mmap_hashmap::Key;\n use common::types::PointOffsetType;\n use indexmap::IndexSet;\n use itertools::Itertools;\n-use mmap_map_index::MmapMapIndex;\n use parking_lot::RwLock;\n use rocksdb::DB;\n use serde_json::Value;\n use smol_str::SmolStr;\n-use uuid::Uuid;\n \n use self::immutable_map_index::ImmutableMapIndex;\n use self::mutable_map_index::MutableMapIndex;\n-use super::FieldIndexBuilderTrait;\n-use super::facet_index::FacetIndex;\n-use super::mmap_point_to_values::MmapValue;\n+use crate::common::operation_error::{OperationError, OperationResult};\n+use crate::common::rocksdb_buffered_delete_wrapper::DatabaseColumnScheduledDeleteWrapper;\n use crate::common::Flusher;\n-use crate::common::operation_error::{OperationError, OperationResult};\n-use crate::data_types::facets::{FacetHit, FacetValueRef};\n use crate::index::field_index::stat_tools::number_of_selected_points;\n use crate::index::field_index::{\n     CardinalityEstimation, PayloadBlockCondition, PayloadFieldIndex, PrimaryCondition, ValueIndexer,\n };\n-use crate::index::query_estimator::combine_should_estimations;\n-use crate::telemetry::PayloadIndexTelemetry;\n use crate::types::{\n     AnyVariants, FieldCondition, IntPayloadType, Match, MatchAny, MatchExcept, MatchValue,\n-    PayloadKeyType, UuidIntType, ValueVariants,\n+    PayloadKeyType, ValueVariants,\n };\n \n"
    },
    "llm_calls": 1,
    "elapsed_time": 22.591472864151,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 56553,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "qdrant_lib_segment_src_index_field_index_numeric_index_mmap_numeric_index.rs",
    "exact_match": false,
    "similarity": 0.8241260153480232,
    "diff_lines": 214,
    "predicted_output": "use std::fs::{create_dir_all, remove_dir};\nuse std::ops::Bound;\nuse std::path::{Path, PathBuf};\n\nuse common::counter::hardware_counter::HardwareCounterCell;\nuse common::counter::iterator_hw_measurement::HwMeasurementIteratorExt;\nuse common::counter::conditioned_counter::ConditionedCounter;\nuse common::types::PointOffsetType;\nuse io::file_operations::{atomic_save_json, read_json};\nuse memmap2::MmapMut;\nuse memory::madvise::{AdviceSetting, clear_disk_cache};\nuse memory::mmap_ops::{self, create_and_ensure_length};\nuse memory::mmap_type::{MmapBitSlice, MmapSlice};\nuse serde::{Deserialize, Serialize};\n\nuse super::mutable_numeric_index::InMemoryNumericIndex;\nuse super::Encodable;\nuse crate::common::mmap_bitslice_buffered_update_wrapper::MmapBitSliceBufferedUpdateWrapper;\nuse crate::common::operation_error::OperationResult;\nuse crate::index::field_index::histogram::{Histogram, Numericable, Point};\nuse crate::index::field_index::mmap_point_to_values::{MmapPointToValues, MmapValue};\n\nconst PAIRS_PATH: &str = \"data.bin\";\nconst DELETED_PATH: &str = \"deleted.bin\";\nconst CONFIG_PATH: &str = \"mmap_field_index_config.json\";\n\npub struct MmapNumericIndex<T: Encodable + Numericable + Default + MmapValue + 'static> {\n    path: PathBuf,\n    deleted: MmapBitSliceBufferedUpdateWrapper,\n    // sorted pairs (id + value), sorted by value (by id if values are equal)\n    pairs: MmapSlice<Point<T>>,\n    histogram: Histogram<T>,\n    deleted_count: usize,\n    max_values_per_point: usize,\n    point_to_values: MmapPointToValues<T>,\n    is_on_disk: bool,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\nstruct MmapNumericIndexConfig {\n    max_values_per_point: usize,\n}\n\npub(super) struct NumericIndexPairsIterator<'a, T: Encodable + Numericable> {\n    pairs: &'a [Point<T>],\n    deleted: &'a MmapBitSliceBufferedUpdateWrapper,\n    start_index: usize,\n    end_index: usize,\n}\n\nimpl<'a, T: Encodable + Numericable> Iterator for NumericIndexPairsIterator<'a, T> {\n    type Item = Point<T>;\n\n    fn next(&mut self) -> Option<Self::Item> {\n        while self.start_index < self.end_index {\n            let key = self.pairs[self.start_index].clone();\n            let deleted = self.deleted.get(key.idx as usize).unwrap_or(true);\n            self.start_index += 1;\n            if deleted {\n                continue;\n            }\n            return Some(key);\n        }\n        None\n    }\n}\n\nimpl<'a, T: Encodable + Numericable> DoubleEndedIterator for NumericIndexPairsIterator<'a, T> {\n    fn next_back(&mut self) -> Option<Self::Item> {\n        while self.start_index < self.end_index {\n            let key = self.pairs[self.end_index - 1].clone();\n            let deleted = self.deleted.get(key.idx as usize).unwrap_or(true);\n            self.end_index -= 1;\n            if deleted {\n                continue;\n            }\n            return Some(key);\n        }\n        None\n    }\n}\n\nimpl<T: Encodable + Numericable + Default + MmapValue> MmapNumericIndex<T> {\n    pub fn build(\n        in_memory_index: InMemoryNumericIndex<T>,\n        path: &Path,\n        is_on_disk: bool,\n    ) -> OperationResult<Self> {\n        create_dir_all(path)?;\n\n        let pairs_path = path.join(PAIRS_PATH);\n        let deleted_path = path.join(DELETED_PATH);\n        let config_path = path.join(CONFIG_PATH);\n\n        atomic_save_json(\n            &config_path,\n            &MmapNumericIndexConfig {\n                max_values_per_point: in_memory_index.max_values_per_point,\n            },\n        )?;\n\n        in_memory_index.histogram.save(path)?;\n\n        MmapPointToValues::<T>::from_iter(\n            path,\n            in_memory_index\n                .point_to_values\n                .iter()\n                .enumerate()\n                .map(|(idx, values)| {\n                    (\n                        idx as PointOffsetType,\n                        values.iter().map(|v| T::as_referenced(v)),\n                    )\n                }),\n        )?;\n\n        {\n            let pairs_file = create_and_ensure_length(\n                &pairs_path,\n                in_memory_index.map.len() * std::mem::size_of::<Point<T>>(),\n            )?;\n            let pairs_mmap = unsafe { MmapMut::map_mut(&pairs_file)? };\n            let mut pairs = unsafe { MmapSlice::<Point<T>>::try_from(pairs_mmap)? };\n            for (src, dst) in in_memory_index.map.iter().zip(pairs.iter_mut()) {\n                *dst = src.clone();\n            }\n        }\n\n        {\n            const BITS_IN_BYTE: usize = 8;\n            let deleted_flags_count = in_memory_index.point_to_values.len();\n            let deleted_file = create_and_ensure_length(\n                &deleted_path,\n                BITS_IN_BYTE\n                    * BITS_IN_BYTE\n                    * deleted_flags_count.div_ceil(BITS_IN_BYTE * BITS_IN_BYTE),\n            )?;\n            let mut deleted_mmap = unsafe { MmapMut::map_mut(&deleted_file)? };\n            deleted_mmap.fill(0);\n            let mut deleted_bitflags = MmapBitSlice::from(deleted_mmap, 0);\n            for (idx, values) in in_memory_index.point_to_values.iter().enumerate() {\n                if values.is_empty() {\n                    deleted_bitflags.set(idx, true);\n                }\n            }\n        }\n\n        Self::load(path, is_on_disk)\n    }\n\n    pub fn load(path: &Path, is_on_disk: bool) -> OperationResult<Self> {\n        let pairs_path = path.join(PAIRS_PATH);\n        let deleted_path = path.join(DELETED_PATH);\n        let config_path = path.join(CONFIG_PATH);\n\n        let histogram = Histogram::<T>::load(path)?;\n        let config: MmapNumericIndexConfig = read_json(&config_path)?;\n        let deleted = mmap_ops::open_write_mmap(&deleted_path, AdviceSetting::Global, false)?;\n        let deleted = MmapBitSlice::from(deleted, 0);\n        let deleted_count = deleted.count_ones();\n        let map = unsafe {\n            MmapSlice::try_from(mmap_ops::open_write_mmap(\n                &pairs_path,\n                AdviceSetting::Global,\n                false,\n            )?)?\n        };\n        let point_to_values = MmapPointToValues::open(path, false)?;\n\n        Ok(Self {\n            pairs: map,\n            deleted: MmapBitSliceBufferedUpdateWrapper::new(deleted),\n            path: path.to_path_buf(),\n            histogram,\n            deleted_count,\n            max_values_per_point: config.max_values_per_point,\n            point_to_values,\n            is_on_disk,\n        })\n    }\n\n    pub fn clear(self) -> OperationResult<()> {\n        let files = self.files();\n        let Self { path, .. } = self;\n        for file in files {\n            std::fs::remove_file(file)?;\n        }\n        let _ = remove_dir(path);\n        Ok(())\n    }\n\n    pub fn files(&self) -> Vec<PathBuf> {\n        let mut files = vec![\n            self.path.join(PAIRS_PATH),\n            self.path.join(DELETED_PATH),\n            self.path.join(CONFIG_PATH),\n        ];\n        files.extend(self.point_to_values.files());\n        files.extend(Histogram::<T>::files(&self.path));\n        files\n    }\n\n    pub fn flusher(&self) -> Flusher {\n        self.deleted.flusher()\n    }\n\n    pub fn check_values_any(\n        &self,\n        idx: PointOffsetType,\n        check_fn: impl Fn(&T) -> bool,\n        hw_counter: &HardwareCounterCell,\n    ) -> bool {\n        let hw_counter = self.make_conditioned_counter(hw_counter);\n        if self.deleted.get(idx as usize) == Some(false) {\n            self.point_to_values.check_values_any(\n                idx,\n                |v| check_fn(T::from_referenced(&v)),\n                &hw_counter,\n            )\n        } else {\n            false\n        }\n    }\n\n    pub fn get_values(&self, idx: PointOffsetType) -> Option<Box<dyn Iterator<Item = T> + '_>> {\n        if self.deleted.get(idx as usize) == Some(false) {\n            Some(Box::new(\n                self.point_to_values\n                    .get_values(idx)?\n                    .map(|v| *T::from_referenced(&v)),\n            ))\n        } else {\n            None\n        }\n    }\n\n    pub fn values_count(&self, idx: PointOffsetType) -> Option<usize> {\n        if self.deleted.get(idx as usize) == Some(false) {\n            self.point_to_values.get_values_count(idx)\n        } else {\n            None\n        }\n    }\n\n    /// Returns the number of key-value pairs in the index.\n    /// Note that is doesn't count deleted pairs.\n    pub(super) fn total_unique_values_count(&self) -> usize {\n        self.pairs.len()\n    }\n\n    pub(super) fn values_range<'a>(\n        &'a self,\n        start_bound: Bound<Point<T>>,\n        end_bound: Bound<Point<T>>,\n        hw_counter: &'a HardwareCounterCell,\n    ) -> impl Iterator<Item = PointOffsetType> + 'a {\n        self.values_range_iterator(start_bound, end_bound)\n            .map(|Point { idx, .. }| idx)\n            .inspect(move |_| {\n                hw_counter\n                    .payload_index_io_read_counter()\n                    .incr_delta(std::mem::size_of::<Point<T>>())\n            })\n    }\n\n    pub fn populate(&self) -> OperationResult<()> {\n        self.pairs.populate()?;\n        self.point_to_values.populate();\n        Ok(())\n    }\n\n    /// Drop disk cache.\n    pub fn clear_cache(&self) -> OperationResult<()> {\n        let pairs_path = self.path.join(PAIRS_PATH);\n        let deleted_path = self.path.join(DELETED_PATH);\n\n        clear_disk_cache(&pairs_path)?;\n        clear_disk_cache(&deleted_path)?;\n\n        self.point_to_values.clear_cache()?;\n\n        Ok(())\n    }\n\n    fn make_conditioned_counter<'a>(\n        &self,\n        hw_counter: &'a HardwareCounterCell,\n    ) -> ConditionedCounter<'a> {\n        ConditionedCounter::new(self.is_on_disk, hw_counter)\n    }\n\n    pub fn is_on_disk(&self) -> bool {\n        self.is_on_disk\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.8241260153480232,
      "diff_lines": 214,
      "added_lines": 28,
      "removed_lines": 116,
      "diff_preview": "--- expected+++ predicted@@ -2,21 +2,19 @@ use std::ops::Bound;\n use std::path::{Path, PathBuf};\n \n-use common::counter::conditioned_counter::ConditionedCounter;\n use common::counter::hardware_counter::HardwareCounterCell;\n use common::counter::iterator_hw_measurement::HwMeasurementIteratorExt;\n+use common::counter::conditioned_counter::ConditionedCounter;\n use common::types::PointOffsetType;\n use io::file_operations::{atomic_save_json, read_json};\n use memmap2::MmapMut;\n-use memory::fadvise::clear_disk_cache;\n-use memory::madvise::AdviceSetting;\n+use memory::madvise::{AdviceSetting, clear_disk_cache};\n use memory::mmap_ops::{self, create_and_ensure_length};\n use memory::mmap_type::{MmapBitSlice, MmapSlice};\n use serde::{Deserialize, Serialize};\n \n+use super::mutable_numeric_index::InMemoryNumericIndex;\n use super::Encodable;\n-use super::mutable_numeric_index::InMemoryNumericIndex;\n-use crate::common::Flusher;\n use crate::common::mmap_bitslice_buffered_update_wrapper::MmapBitSliceBufferedUpdateWrapper;\n use crate::common::operation_error::OperationResult;\n use crate::index::field_index::histogram::{Histogram, Numericable, Point};\n@@ -50,7 +48,7 @@     end_index: usize,\n }\n \n-impl<T: Encodable + Numericable> Iterator for NumericIndexPairsIterator<'_, T> {\n+impl<'a, T: Encodable + Numericable> Iterator for NumericIndexPairsIterator<'a, T> {\n     type Item = Point<T>;\n \n     fn next(&mut self) -> Option<Self::Item> {\n@@ -67,7 +65,7 @@     }\n }\n \n-impl<T: Encodable + Numericable> DoubleEndedIterator for NumericIndexPairsIterator<'_, T> {\n+impl<'a, T: Encodable + Numericable> DoubleEndedIterator for NumericIndexPairsIterator<'a, T> {\n     fn next_back(&mut self) -> Option<Self::Item> {\n         while self.start_index < self.end_index {\n             let key = self.pairs[self.end_index - 1].clone();\n@@ -161,15 +159,14 @@         let deleted = mmap_ops::open_write_mmap(&deleted_path, AdviceSetting::Global, false)?;\n         let deleted = MmapBitSlice::from(deleted, 0);\n         let deleted_count = deleted.count_ones();\n-        let do_populate = !is_on_disk;\n"
    },
    "llm_calls": 1,
    "elapsed_time": 63.36260461807251,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 11917,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "qdrant_lib_segment_src_index_field_index_numeric_index_mutable_numeric_index.rs",
    "exact_match": false,
    "similarity": 0.4797979797979798,
    "diff_lines": 276,
    "predicted_output": "use std::collections::BTreeSet;\nuse std::ops::Bound;\nuse std::ops::Bound::{Excluded, Unbounded};\nuse std::sync::Arc;\n\nuse common::counter::hardware_counter::HardwareCounterCell;\nuse common::types::PointOffsetType;\nuse parking_lot::RwLock;\nuse rocksdb::DB;\n\nuse super::{\n    Encodable, HISTOGRAM_MAX_BUCKET_SIZE, HISTOGRAM_PRECISION, numeric_index_storage_cf_name,\n};\nuse crate::common::operation_error::{OperationError, OperationResult};\nuse crate::common::rocksdb_buffered_delete_wrapper::DatabaseColumnScheduledDeleteWrapper;\nuse crate::index::field_index::histogram::{Histogram, Numericable, Point};\n\npub struct MutableNumericIndex<T: Encodable + Numericable> {\n    db_wrapper: DatabaseColumnScheduledDeleteWrapper,\n    in_memory_index: InMemoryNumericIndex<T>,\n}\n\n// Numeric Index with insertions and deletions without persistence\npub struct InMemoryNumericIndex<T: Encodable + Numericable> {\n    pub map: BTreeSet<Point<T>>,\n    pub histogram: Histogram<T>,\n    pub points_count: usize,\n    pub max_values_per_point: usize,\n    pub point_to_values: Vec<Vec<T>>,\n}\n\nimpl<T: Encodable + Numericable> Default for InMemoryNumericIndex<T> {\n    fn default() -> Self {\n        Self {\n            map: BTreeSet::new(),\n            histogram: Histogram::new(HISTOGRAM_MAX_BUCKET_SIZE, HISTOGRAM_PRECISION),\n            points_count: 0,\n            max_values_per_point: 0,\n            point_to_values: Default::default(),\n        }\n    }\n}\n\nimpl<T: Encodable + Numericable + Default> FromIterator<(PointOffsetType, T)>\n    for InMemoryNumericIndex<T>\n{\n    fn from_iter<I: IntoIterator<Item = (PointOffsetType, T)>>(iter: I) -> Self {\n        let mut index = InMemoryNumericIndex::default();\n        for pair in iter {\n            let (idx, value) = pair;\n\n            if index.point_to_values.len() <= idx as usize {\n                index\n                    .point_to_values\n                    .resize_with(idx as usize + 1, Vec::new)\n            }\n\n            index.point_to_values[idx as usize].push(value);\n\n            index.map.insert(Point::new(value, idx));\n            index.points_count += 1;\n            index.max_values_per_point = index.max_values_per_point.max(index.point_to_values[idx as usize].len());\n        }\n        index\n    }\n}\n\nimpl<T: Encodable + Numericable + Default> MutableNumericIndex<T> {\n    pub fn new(db: Arc<RwLock<DB>>, field: &str) -> Self {\n        let store_cf_name = numeric_index_storage_cf_name(field);\n        let db_wrapper = DatabaseColumnScheduledDeleteWrapper::new(DatabaseColumnWrapper::new(\n            db,\n            &store_cf_name,\n        ));\n        Self {\n            db_wrapper,\n            in_memory_index: InMemoryNumericIndex::default(),\n        }\n    }\n\n    pub fn load(&mut self) -> OperationResult<bool> {\n        if !self.db_wrapper.has_column_family()? {\n            return Ok(false);\n        };\n\n        self.in_memory_index = self\n            .db_wrapper\n            .lock_db()\n            .iter()?\n            .map(|(key, value)| {\n                let value_idx = u32::from_be_bytes(value.as_ref().try_into().map_err(|_| {\n                    OperationError::service_error(\"incorrect numeric index value\")\n                })?);\n                let (idx, value) = T::decode_key(&key);\n                if idx != value_idx {\n                    return Err(OperationError::service_error(\"incorrect numeric index key-value pair\"));\n                }\n                Ok((idx, value))\n            })\n            .collect::<Result<InMemoryNumericIndex<_>, OperationError>>()?;\n\n        Ok(true)\n    }\n\n    // other methods...\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.4797979797979798,
      "diff_lines": 276,
      "added_lines": 8,
      "removed_lines": 232,
      "diff_preview": "--- expected+++ predicted@@ -13,7 +13,6 @@ };\n use crate::common::operation_error::{OperationError, OperationResult};\n use crate::common::rocksdb_buffered_delete_wrapper::DatabaseColumnScheduledDeleteWrapper;\n-use crate::common::rocksdb_wrapper::DatabaseColumnWrapper;\n use crate::index::field_index::histogram::{Histogram, Numericable, Point};\n \n pub struct MutableNumericIndex<T: Encodable + Numericable> {\n@@ -58,153 +57,15 @@ \n             index.point_to_values[idx as usize].push(value);\n \n-            let key = Point::new(value, idx);\n-            InMemoryNumericIndex::add_to_map(&mut index.map, &mut index.histogram, key);\n-        }\n-        for values in &index.point_to_values {\n-            if !values.is_empty() {\n-                index.points_count += 1;\n-                index.max_values_per_point = index.max_values_per_point.max(values.len());\n-            }\n+            index.map.insert(Point::new(value, idx));\n+            index.points_count += 1;\n+            index.max_values_per_point = index.max_values_per_point.max(index.point_to_values[idx as usize].len());\n         }\n         index\n     }\n }\n \n-impl<T: Encodable + Numericable + Default> InMemoryNumericIndex<T> {\n-    pub fn check_values_any(&self, idx: PointOffsetType, check_fn: impl Fn(&T) -> bool) -> bool {\n-        self.point_to_values\n-            .get(idx as usize)\n-            .map(|values| values.iter().any(check_fn))\n-            .unwrap_or(false)\n-    }\n-\n-    pub fn get_values(&self, idx: PointOffsetType) -> Option<Box<dyn Iterator<Item = T> + '_>> {\n-        Some(Box::new(\n-            self.point_to_values\n-                .get(idx as usize)\n-                .map(|v| v.iter().cloned())?,\n-        ))\n-    }\n-\n-    pub fn values_count(&self, idx: PointOffsetType) -> Option<usize> {\n-        self.point_to_values.get(idx as usize).map(Vec::len)\n-    }\n-\n"
    },
    "llm_calls": 1,
    "elapsed_time": 19.480747938156128,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 20896,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "qdrant_lib_segment_src_index_hnsw_index_graph_layers.rs",
    "exact_match": false,
    "similarity": 0.09872121154994187,
    "diff_lines": 826,
    "predicted_output": "use serde::{Deserialize, Serialize};\nuse crate::types::{PointOffsetType, ScoreType};\nuse crate::spaces::tools::FixedLengthPriorityQueue;\nuse std::cmp::{max, min};\nuse std::path::{Path, PathBuf};\nuse crate::entry::entry_point::OperationResult;\nuse crate::common::file_operations::{read_bin, atomic_save_bin};\nuse crate::index::hnsw_index::point_scorer::FilteredScorer;\nuse crate::index::hnsw_index::entry_points::EntryPoints;\nuse crate::vector_storage::vector_storage::ScoredPointOffset;\nuse crate::index::visited_pool::{VisitedList, VisitedPool};\nuse crate::index::hnsw_index::search_context::SearchContext;\nuse crate::common::utils::rev_range;\nuse rand::distributions::Uniform;\nuse rand::prelude::ThreadRng;\nuse rand::Rng;\nuse std::collections::BinaryHeap;\nuse itertools::Itertools;\n\n\npub type LinkContainer = Vec<PointOffsetType>;\npub type LayersContainer = Vec<LinkContainer>;\n\npub const HNSW_GRAPH_FILE: &str = \"graph.bin\";\n\n#[derive(Deserialize, Serialize, Debug)]\npub struct GraphLayers {\n    max_level: usize,\n    m: usize,\n    m0: usize,\n    ef_construct: usize,\n    level_factor: f64,\n    // Exclude points according to \"not closer than base\" heuristic?\n    use_heuristic: bool,\n    // Factor of level probability\n    links_layers: Vec<LayersContainer>,\n    entry_points: EntryPoints,\n\n    // Fields used on construction phase only\n    #[serde(skip)]\n    visited_pool: VisitedPool,\n}\n\n/// Object contains links between nodes for HNSW search\n///\n/// Assume all scores are similarities. Larger score = closer points\nimpl GraphLayers {\n    pub fn new_with_params(\n        num_vectors: usize, // Initial number of points in index\n        m: usize, // Expected M for non-first layer\n        m0: usize, // Expected M for first layer\n        ef_construct: usize,\n        entry_points_num: usize, // Depends on number of points\n        use_heuristic: bool,\n        reserve: bool\n    ) -> Self {\n        let mut links_layers: Vec<LayersContainer> = vec![];\n\n        for _i in 0..num_vectors {\n            let mut links: LinkContainer = Vec::new();\n            if reserve {\n                links.reserve(m0);\n            }\n            links_layers.push(vec![links]);\n        }\n\n        GraphLayers {\n            max_level: 0,\n            m,\n            m0,\n            ef_construct,\n            level_factor: 1.0 / (m as f64).ln(),\n            use_heuristic,\n            links_layers,\n            entry_points: EntryPoints::new(entry_points_num),\n            visited_pool: VisitedPool::new(),\n        }\n    }\n\n    pub fn new(\n        num_vectors: usize, // Initial number of points in index\n        m: usize, // Expected M for non-first layer\n        m0: usize, // Expected M for first layer\n        ef_construct: usize,\n        entry_points_num: usize, // Depends on number of points\n        use_heuristic: bool,\n    ) -> Self {\n        Self::new_with_params(num_vectors, m, m0, ef_construct, entry_points_num, use_heuristic, true)\n    }\n\n    fn num_points(&self) -> usize { self.links_layers.len() }\n\n    pub fn point_level(&self, point_id: PointOffsetType) -> usize {\n        self.links_layers[point_id as usize].len() - 1\n    }\n\n    /// Get links of current point\n    fn links(&self, point_id: PointOffsetType, level: usize) -> &LinkContainer {\n        &self.links_layers[point_id as usize][level]\n    }\n\n    /// Get M based on current level\n    fn get_m(&self, level: usize) -> usize {\n        if level == 0 { self.m0 } else { self.m }\n    }\n\n    /// Generate random level for a new point, according to geometric distribution\n    pub fn get_random_layer(&self, thread_rng: &mut ThreadRng) -> usize {\n        let distribution = Uniform::new(0.0, 1.0);\n        let sample: f64 = thread_rng.sample(distribution);\n        let picked_level = -sample.ln() * self.level_factor;\n        picked_level.round() as usize\n    }\n\n    fn set_levels(&mut self, point_id: PointOffsetType, level: usize) {\n        if self.links_layers.len() <= point_id as usize {\n            self.links_layers.resize(point_id as usize, vec![]);\n        }\n        let point_layers = &mut self.links_layers[point_id as usize];\n        while point_layers.len() <= level {\n            let mut links = vec![];\n            links.reserve(self.m);\n            point_layers.push(links);\n        }\n        self.max_level = max(level, self.max_level);\n    }\n\n\n    /// Greedy search for closest points within a single graph layer\n    fn _search_on_level(&self, searcher: &mut SearchContext, level: usize, visited_list: &mut VisitedList, points_scorer: &FilteredScorer) {\n        while let Some(candidate) = searcher.candidates.pop() {\n            if candidate.score < searcher.lower_bound() {\n                break;\n            }\n            let mut links_iter = self.links(candidate.idx, level)\n                .iter()\n                .cloned()\n                .filter(|point_id| !visited_list.check_and_update_visited(*point_id));\n\n            points_scorer.score_iterable_points(\n                &mut links_iter,\n                self.get_m(level),\n                |score_point| searcher.process_candidate(score_point),\n            );\n        }\n    }\n\n    fn search_on_level(&self, level_entry: ScoredPointOffset, level: usize, ef: usize, points_scorer: &FilteredScorer) -> FixedLengthPriorityQueue<ScoredPointOffset> {\n        let mut visited_list = self.visited_pool.get(self.num_points());\n        visited_list.check_and_update_visited(level_entry.idx);\n        let mut search_context = SearchContext::new(level_entry, ef);\n\n        self._search_on_level(&mut search_context, level, &mut visited_list, points_scorer);\n        self.visited_pool.return_back(visited_list);\n        search_context.nearest\n    }\n\n    /// Greedy searches for entry point of level `target_level`.\n    /// Beam size is 1.\n    fn search_entry(&self, entry_point: PointOffsetType, top_level: usize, target_level: usize, points_scorer: &FilteredScorer) -> ScoredPointOffset {\n        let mut current_point = ScoredPointOffset {\n            idx: entry_point,\n            score: points_scorer.score_point(entry_point),\n        };\n        for level in (target_level..=top_level).rev() {\n            let mut changed = true;\n            while changed {\n                changed = false;\n                let mut links = self.links(current_point.idx, level).iter().cloned();\n                points_scorer.score_iterable_points(&mut links, self.get_m(level), |score_point| {\n                    if score_point.score > current_point.score {\n                        changed = true;\n                        current_point = score_point;\n                    }\n                });\n            }\n        }\n        current_point\n    }\n\n    pub fn link_new_point(&mut self, point_id: PointOffsetType, level: usize, points_scorer: &FilteredScorer) {\n        self.set_levels(point_id, level);\n\n        let entry_point_opt = self.entry_points.new_point(point_id, level, |point_id| points_scorer.check_point(point_id));\n        match entry_point_opt {\n            // New point is a new empty entry (for this filter, at least)\n            // We can't do much here, so just quit\n            None => {}\n\n            // Entry point found.\n            Some(entry_point) => {\n                let mut level_entry = if entry_point.level > level {\n                    // The entry point is higher than a new point\n                    // Let's find closest one on same level\n\n                    // greedy search for a single closest point\n                    self.search_entry(\n                        entry_point.point_id,\n                        entry_point.level,\n                        level,\n                        points_scorer,\n                    )\n                } else {\n                    ScoredPointOffset {\n                        idx: entry_point.point_id,\n                        score: points_scorer.score_internal(point_id, entry_point.point_id),\n                    }\n                };\n                // minimal common level for entry points\n                let linking_level = min(level, entry_point.level);\n\n                for curr_level in (0..=linking_level).rev() {\n                    let level_m = self.get_m(curr_level);\n                    let nearest_points = self.search_on_level(\n                        level_entry,\n                        curr_level,\n                        self.ef_construct,\n                        points_scorer,\n                    );\n\n                    for nearest_point in nearest_points.iter() {\n                        Self::connect_new_point(\n                            &mut self.links_layers[point_id as usize][curr_level],\n                            nearest_point.idx,\n                            point_id,\n                            level_m,\n                            |a, b| points_scorer.score_internal(a, b),\n                        );\n\n                        Self::connect_new_point(\n                            &mut self.links_layers[nearest_point.idx as usize][curr_level],\n                            point_id,\n                            nearest_point.idx,\n                            level_m,\n                            |a, b| points_scorer.score_internal(a, b),\n                        );\n                        if nearest_point.score > level_entry.score {\n                            level_entry = nearest_point.clone()\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    pub fn merge_from_other(&mut self, other: GraphLayers) {\n        let mut visited_list = self.visited_pool.get(self.num_points());\n        if other.links_layers.len() > self.links_layers.len() {\n            self.links_layers.resize(other.links_layers.len(), vec![]);\n        }\n        for (point_id, layers) in other.links_layers.into_iter().enumerate() {\n            let current_layers = &mut self.links_layers[point_id];\n            for (level, other_links) in layers.into_iter().enumerate() {\n                if current_layers.len() <= level {\n                    current_layers.push(other_links);\n                } else {\n                    visited_list.next_iteration();\n                    let current_links = &mut current_layers[level];\n                    current_links.iter().cloned().for_each(|x| { visited_list.check_and_update_visited(x); });\n                    for other_link in other_links.into_iter().filter(|x| !visited_list.check_and_update_visited(*x)) {\n                        current_links.push(other_link);\n                    }\n                }\n            }\n        }\n        self.entry_points.merge_from_other(other.entry_points);\n\n        self.visited_pool.return_back(visited_list);\n    }\n\n    pub fn search(&self, top: usize, ef: usize, mut points_scorer: FilteredScorer, custom_entry_points: Option<&[PointOffsetType]>) -> Vec<ScoredPointOffset> {\n        let Some(entry_point) = self.get_entry_point(&points_scorer, custom_entry_points) else {\n            return Vec::default();\n        };\n\n        let zero_level_entry = self.search_entry(\n            entry_point.point_id,\n            entry_point.level,\n            0,\n            &mut points_scorer,\n        );\n\n        let nearest = self.search_on_level(zero_level_entry, 0, max(top, ef), &mut points_scorer);\n        nearest.into_iter_sorted().take(top).collect_vec()\n    }\n\n    pub fn get_path(path: &Path) -> PathBuf {\n        path.join(HNSW_GRAPH_FILE)\n    }\n\n    pub fn get_links_path(path: &Path) -> PathBuf {\n        path.join(HNSW_LINKS_FILE)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::fixtures::index_fixtures::{random_vector, FakeFilterContext, TestRawScorerProducer};\n    use crate::spaces::metric::Metric;\n    use crate::spaces::simple::{CosineMetric, DotProductMetric};\n    use rand::rngs::StdRng;\n    use rand::SeedableRng;\n    use tempfile::Builder;\n\n    #[test]\n    fn search_on_level() {\n        let num_vectors = 100;\n        let dim = 8;\n        let top = 5;\n        let entry_points_num = 10;\n\n        let mut rng = StdRng::seed_from_u64(42);\n        let graph_links = vec![vec![Vec::new(); 1]; num_vectors];\n        graph_links[0][0] = vec![1, 2, 3, 4, 5, 6];\n        let graph_layers = GraphLayers {\n            m: 16,\n            m0: 32,\n            ef_construct: 64,\n            links: GraphLinksRam::from_converter(GraphLinksConverter::new(graph_links, false, 16, 32)).unwrap(),\n            entry_points: EntryPoints::new(entry_points_num),\n            visited_pool: VisitedPool::new(),\n        };\n\n        let query = random_vector(&mut rng, dim);\n        let vector_storage = TestRawScorerProducer::new(dim, num_vectors, CosineMetric {}, &mut rng);\n        let raw_scorer = vector_storage.get_raw_scorer(query.clone()).unwrap();\n\n        // The scoring context must be passed and maintained\n        let scorer = FilteredScorer::new(raw_scorer.as_ref(), Some(&FakeFilterContext));\n\n        let res = graph_layers.search(top, 128, scorer, None);\n        assert_eq!(res.len(), top);\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.09872121154994187,
      "diff_lines": 826,
      "added_lines": 260,
      "removed_lines": 488,
      "diff_preview": "--- expected+++ predicted@@ -1,151 +1,173 @@-use std::borrow::Cow;\n-use std::cmp::max;\n+use serde::{Deserialize, Serialize};\n+use crate::types::{PointOffsetType, ScoreType};\n+use crate::spaces::tools::FixedLengthPriorityQueue;\n+use std::cmp::{max, min};\n use std::path::{Path, PathBuf};\n-use std::sync::atomic::AtomicBool;\n-\n-use common::fixed_length_priority_queue::FixedLengthPriorityQueue;\n-use common::types::{PointOffsetType, ScoredPointOffset};\n-use io::file_operations::read_bin;\n+use crate::entry::entry_point::OperationResult;\n+use crate::common::file_operations::{read_bin, atomic_save_bin};\n+use crate::index::hnsw_index::point_scorer::FilteredScorer;\n+use crate::index::hnsw_index::entry_points::EntryPoints;\n+use crate::vector_storage::vector_storage::ScoredPointOffset;\n+use crate::index::visited_pool::{VisitedList, VisitedPool};\n+use crate::index::hnsw_index::search_context::SearchContext;\n+use crate::common::utils::rev_range;\n+use rand::distributions::Uniform;\n+use rand::prelude::ThreadRng;\n+use rand::Rng;\n+use std::collections::BinaryHeap;\n use itertools::Itertools;\n-use serde::{Deserialize, Serialize};\n-\n-use super::entry_points::EntryPoint;\n-use super::graph_links::{GraphLinks, GraphLinksFormat};\n-use crate::common::operation_error::{\n-    CancellableResult, OperationError, OperationResult, check_process_stopped,\n-};\n-use crate::common::utils::rev_range;\n-use crate::index::hnsw_index::entry_points::EntryPoints;\n-use crate::index::hnsw_index::graph_links::GraphLinksSerializer;\n-use crate::index::hnsw_index::point_scorer::FilteredScorer;\n-use crate::index::hnsw_index::search_context::SearchContext;\n-use crate::index::visited_pool::{VisitedListHandle, VisitedPool};\n+\n \n pub type LinkContainer = Vec<PointOffsetType>;\n pub type LayersContainer = Vec<LinkContainer>;\n \n pub const HNSW_GRAPH_FILE: &str = \"graph.bin\";\n-pub const HNSW_LINKS_FILE: &str = \"links.bin\";\n-pub const COMPRESSED_HNSW_LINKS_FILE: &str = \"links_compressed.bin\";\n-\n"
    },
    "llm_calls": 1,
    "elapsed_time": 75.95489835739136,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 69134,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "qdrant_lib_segment_src_index_hnsw_index_graph_links.rs",
    "exact_match": false,
    "similarity": 0.044024390243902436,
    "diff_lines": 468,
    "predicted_output": "use std::cmp::max;\nuse std::fs::OpenOptions;\nuse std::io::{Read as _, Write};\nuse std::ops::Range;\nuse std::path::{Path, PathBuf};\nuse std::sync::Arc;\n\nuse memmap2::{Mmap, MmapMut};\n\nuse crate::entry::entry_point::{OperationError, OperationResult};\nuse crate::types::PointOffsetType;\n\npub const MMAP_PANIC_MESSAGE: &str = \"Mmap links are not loaded\";\n\n#[derive(Debug)]\nstruct GraphLinksFileInfo {\n    point_count: usize,\n    reindex_start: usize,\n    links_start: usize,\n    offsets_start: usize,\n    offsets_end: usize,\n}\n\n#[derive(FromBytes, Immutable, IntoBytes, KnownLayout)]\n#[repr(C)]\nstruct GraphLinksFileHeader {\n    point_count: u64,\n    levels_count: u64,\n    total_links_len: u64,\n    total_offsets_len: u64,\n    offsets_padding: u64,\n}\n\nimpl GraphLinksFileInfo {\n    pub fn load(data: &[u8]) -> Option<GraphLinksFileInfo> {\n        let header = GraphLinksFileHeader::ref_from_prefix(data).ok()?;\n        \n        let reindex_start = 64 + header.levels_count as usize * std::mem::size_of::<u64>();\n        let links_start = reindex_start + header.point_count as usize * std::mem::size_of::<PointOffsetType>();\n        let offsets_start = links_start + header.total_links_len as usize * std::mem::size_of::<PointOffsetType>() + header.offsets_padding as usize;\n        let offsets_end = offsets_start + header.total_offsets_len as usize * std::mem::size_of::<u64>();\n\n        Some(GraphLinksFileInfo {\n            point_count: header.point_count as usize,\n            reindex_start,\n            links_start,\n            offsets_start,\n            offsets_end,\n        })\n    }\n}\n\n#[derive(Default)]\npub struct GraphLinksConverter {\n    edges: Vec<Vec<Vec<PointOffsetType>>>,\n    reindex: Vec<PointOffsetType>,\n    back_index: Vec<usize>,\n    total_links_len: usize,\n    total_offsets_len: usize,\n    path: Option<PathBuf>,\n    level_offsets: Vec<u64>,\n    point_count_by_level: Vec<u64>,\n}\n\nimpl GraphLinksConverter {\n    pub fn new(edges: Vec<Vec<Vec<PointOffsetType>>>) -> Self {\n        if edges.is_empty() {\n            return Self {\n                edges,\n                reindex: Vec::new(),\n                back_index: Vec::new(),\n                total_links_len: 0,\n                total_offsets_len: 1,\n                path: None,\n                level_offsets: Vec::new(),\n                point_count_by_level: Vec::new(),\n            };\n        }\n\n        let mut back_index: Vec<usize> = (0..edges.len()).collect();\n        back_index.sort_unstable_by_key(|&i| edges[i].len());\n        back_index.reverse();\n\n        let mut reindex = vec![0; back_index.len()];\n        for i in 0..back_index.len() {\n            reindex[back_index[i]] = i as PointOffsetType;\n        }\n\n        let levels_count = back_index.first().map_or(0, |&point_id| edges[point_id].len());\n        let mut point_count_by_level = vec![0; levels_count];\n\n        for point in edges.iter() {\n            point_count_by_level[point.len() - 1] += 1;\n            total_links_len += point.iter().map(Vec::len).sum::<usize>();\n        }\n\n        let mut total_offsets_len = 0;\n        let mut suffix_sum = point_count_by_level.iter().sum::<u64>();\n        for &value in point_count_by_level.iter() {\n            level_offsets.push(total_offsets_len);\n            total_offsets_len += suffix_sum;\n            suffix_sum -= value;\n        }\n        total_offsets_len += 1;\n\n        Self {\n            edges,\n            reindex,\n            back_index,\n            total_links_len,\n            total_offsets_len: total_offsets_len as usize,\n            path: None,\n            level_offsets,\n            point_count_by_level,\n        }\n    }\n\n    pub fn data_size(&self) -> usize {\n        64 + self.point_count_by_level.len() * std::mem::size_of::<u64>() + self.reindex.len() * std::mem::size_of::<PointOffsetType>() + self.total_links_len * std::mem::size_of::<PointOffsetType>()\n            + self.total_offsets_len * std::mem::size_of::<u64>()\n    }\n\n    pub fn save_as(&mut self, path: &Path) -> OperationResult<()> {\n        let temp_path = path.with_extension(\"tmp\");\n        let file = OpenOptions::new()\n            .read(true)\n            .write(true)\n            .create(true)\n            .open(temp_path.as_path())?;\n        \n        let mut bytes = vec![0; self.data_size()];\n        self.serialize_bytes_to(&mut bytes);\n\n        file.write_all(&bytes)?;\n        std::fs::rename(temp_path, path)?;\n        Ok(())\n    }\n\n    fn serialize_bytes_to(&self, raw_data: &mut [u8]) {\n        let header = GraphLinksFileHeader {\n            point_count: self.reindex.len() as u64,\n            levels_count: self.level_offsets.len() as u64,\n            total_links_len: self.total_links_len as u64,\n            total_offsets_len: self.total_offsets_len as u64,\n            offsets_padding: 0,\n        };\n\n        let byte_slice = &mut raw_data[0..std::mem::size_of::<GraphLinksFileHeader>()];\n        let arr: &mut [u64] = unsafe { std::mem::transmute(byte_slice) };\n        arr[0] = header.point_count;\n        arr[1] = header.levels_count;\n        arr[2] = header.total_links_len;\n        arr[3] = header.total_offsets_len;\n        arr[4] = header.offsets_padding;\n\n        let offsets_range = 64..64 + self.level_offsets.len() * std::mem::size_of::<u64>();\n        let offsets_slice = &mut raw_data[offsets_range];\n        let level_offsets_slice: &mut [u64] = unsafe { std::mem::transmute(offsets_slice) };\n        level_offsets_slice.copy_from_slice(&self.level_offsets);\n    }\n}",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.044024390243902436,
      "diff_lines": 468,
      "added_lines": 134,
      "removed_lines": 304,
      "diff_preview": "--- expected+++ predicted@@ -1,331 +1,161 @@-use std::path::Path;\n+use std::cmp::max;\n+use std::fs::OpenOptions;\n+use std::io::{Read as _, Write};\n+use std::ops::Range;\n+use std::path::{Path, PathBuf};\n use std::sync::Arc;\n \n-use common::types::PointOffsetType;\n-use memmap2::Mmap;\n-use memory::madvise::{Advice, AdviceSetting, Madviseable};\n-use memory::mmap_ops::open_read_mmap;\n+use memmap2::{Mmap, MmapMut};\n \n-use crate::common::operation_error::OperationResult;\n+use crate::entry::entry_point::{OperationError, OperationResult};\n+use crate::types::PointOffsetType;\n \n-mod header;\n-mod serializer;\n-mod view;\n+pub const MMAP_PANIC_MESSAGE: &str = \"Mmap links are not loaded\";\n \n-pub use serializer::GraphLinksSerializer;\n-pub use view::LinksIterator;\n-use view::{CompressionInfo, GraphLinksView};\n-\n-/*\n-Links data for whole graph layers.\n-\n-                                    sorted\n-                     points:        points:\n-points to lvl        012345         142350\n-     0 -> 0\n-     1 -> 4    lvl4:  7       lvl4: 7\n-     2 -> 2    lvl3:  Z  Y    lvl3: ZY\n-     3 -> 2    lvl2:  abcd    lvl2: adbc\n-     4 -> 3    lvl1:  ABCDE   lvl1: ADBCE\n-     5 -> 1    lvl0: 123456   lvl0: 123456  <- lvl 0 is not sorted\n-\n-\n-lvl offset:        6       11     15     17\n-                   \u2502       \u2502      \u2502      \u2502\n-                   \u2502       \u2502      \u2502      \u2502\n-                   \u25bc       \u25bc      \u25bc      \u25bc\n-indexes:  012345   6789A   BCDE   FG     H\n-\n"
    },
    "llm_calls": 1,
    "elapsed_time": 39.77058291435242,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 65935,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  },
  {
    "success": true,
    "id": "qdrant_lib_segment_src_index_struct_payload_index.rs",
    "exact_match": false,
    "similarity": 0.06100875868317729,
    "diff_lines": 767,
    "predicted_output": "use std::collections::{HashMap, HashSet};\nuse std::fs::create_dir_all;\nuse std::path::{Path, PathBuf};\nuse std::sync::Arc;\n\nuse ahash::AHashSet;\nuse atomic_refcell::AtomicRefCell;\nuse common::counter::hardware_counter::HardwareCounterCell;\nuse common::counter::iterator_hw_measurement::HwMeasurementIteratorExt;\nuse common::flags::feature_flags;\nuse common::types::PointOffsetType;\nuse itertools::Either;\nuse log::debug;\nuse parking_lot::RwLock;\nuse rocksdb::DB;\n\nuse crate::common::operation_error::{OperationError, OperationResult};\nuse crate::common::rocksdb_wrapper::open_db_with_existing_cf;\nuse crate::common::utils::IndexesMap;\nuse crate::entry::entry_point::OperationResult;\nuse crate::id_tracker::IdTrackerSS;\nuse crate::index::field_index::{\n    CardinalityEstimation, FieldIndex, PayloadBlockCondition, PrimaryCondition,\n};\nuse crate::index::field_index::index_selector::{\n    IndexSelector, IndexSelectorOnDisk, IndexSelectorRocksDb,\n};\nuse crate::index::payload_config::PayloadConfig;\nuse crate::index::query_estimator::estimate_filter;\nuse crate::index::query_optimization::payload_provider::PayloadProvider;\nuse crate::index::struct_filter_context::StructFilterContext;\nuse crate::index::visited_pool::VisitedPool;\nuse crate::payload_storage::payload_storage_enum::PayloadStorageEnum;\nuse crate::payload_storage::{FilterContext, PayloadStorage};\nuse crate::telemetry::PayloadIndexTelemetry;\nuse crate::types::{\n    infer_collection_value_type, infer_value_type, Condition, FieldCondition, Filter,\n    IsEmptyCondition, IsNullCondition, Payload, PayloadContainer, PayloadField, PayloadFieldSchema,\n    PayloadKeyType, PayloadKeyTypeRef, PayloadSchemaType, VectorNameBuf,\n};\n\npub const PAYLOAD_FIELD_INDEX_PATH: &str = \"fields\";\n\n/// `PayloadIndex` implementation, which actually uses index structures for providing faster search\n#[derive(Debug)]\npub struct StructPayloadIndex {\n    /// Payload storage\n    pub(super) payload: Arc<AtomicRefCell<PayloadStorageEnum>>,\n    /// Used for `has_id` condition and estimating cardinality\n    pub(super) id_tracker: Arc<AtomicRefCell<IdTrackerSS>>,\n    /// Vector storages for each field, used for `has_vector` condition\n    pub(super) vector_storages: HashMap<VectorNameBuf, Arc<AtomicRefCell<VectorStorageEnum>>>,\n    /// Indexes, associated with fields\n    pub field_indexes: IndexesMap,\n    config: PayloadConfig,\n    /// Root of index persistence dir\n    path: PathBuf,\n    /// Used to select unique point ids\n    visited_pool: VisitedPool,\n    db: Arc<RwLock<DB>>,\n}\n\n// Implementation details...",
    "evaluation": {
      "exact_match": false,
      "similarity": 0.06100875868317729,
      "diff_lines": 767,
      "added_lines": 11,
      "removed_lines": 718,
      "diff_preview": "--- expected+++ predicted@@ -1,4 +1,4 @@-use std::collections::HashMap;\n+use std::collections::{HashMap, HashSet};\n use std::fs::create_dir_all;\n use std::path::{Path, PathBuf};\n use std::sync::Arc;\n@@ -13,42 +13,33 @@ use log::debug;\n use parking_lot::RwLock;\n use rocksdb::DB;\n-use schemars::_serde_json::Value;\n \n-use super::field_index::FieldIndexBuilderTrait as _;\n-use super::field_index::facet_index::FacetIndexEnum;\n-use super::field_index::index_selector::{IndexSelector, IndexSelectorMmap, IndexSelectorRocksDb};\n-use crate::common::Flusher;\n use crate::common::operation_error::{OperationError, OperationResult};\n use crate::common::rocksdb_wrapper::open_db_with_existing_cf;\n use crate::common::utils::IndexesMap;\n+use crate::entry::entry_point::OperationResult;\n use crate::id_tracker::IdTrackerSS;\n-use crate::index::PayloadIndex;\n use crate::index::field_index::{\n     CardinalityEstimation, FieldIndex, PayloadBlockCondition, PrimaryCondition,\n+};\n+use crate::index::field_index::index_selector::{\n+    IndexSelector, IndexSelectorOnDisk, IndexSelectorRocksDb,\n };\n use crate::index::payload_config::PayloadConfig;\n use crate::index::query_estimator::estimate_filter;\n use crate::index::query_optimization::payload_provider::PayloadProvider;\n use crate::index::struct_filter_context::StructFilterContext;\n use crate::index::visited_pool::VisitedPool;\n-use crate::json_path::JsonPath;\n use crate::payload_storage::payload_storage_enum::PayloadStorageEnum;\n use crate::payload_storage::{FilterContext, PayloadStorage};\n use crate::telemetry::PayloadIndexTelemetry;\n use crate::types::{\n-    Condition, FieldCondition, Filter, IsEmptyCondition, IsNullCondition, Payload,\n-    PayloadContainer, PayloadFieldSchema, PayloadKeyType, PayloadKeyTypeRef, PayloadSchemaType,\n-    VectorNameBuf, infer_collection_value_type, infer_value_type,\n+    infer_collection_value_type, infer_value_type, Condition, FieldCondition, Filter,\n+    IsEmptyCondition, IsNullCondition, Payload, PayloadContainer, PayloadField, PayloadFieldSchema,\n+    PayloadKeyType, PayloadKeyTypeRef, PayloadSchemaType, VectorNameBuf,\n };\n-use crate::vector_storage::{VectorStorage, VectorStorageEnum};\n \n-#[derive(Debug)]\n"
    },
    "llm_calls": 1,
    "elapsed_time": 21.350111961364746,
    "repo": "qdrant",
    "language": "Rust",
    "prompt_tokens": 82321,
    "partition_strategy": "none",
    "retrieval_method": "none",
    "parallel_subqueries": false,
    "method": "baseline"
  }
]