# LoCoDiff Benchmark Integration - Implementation Summary

## ðŸŽ¯ What Was Built

Successfully integrated the **LoCoDiff benchmark** for evaluating RLM on git history reconstruction tasks.

### Files Created

```
locodiff_eval/
â”œâ”€â”€ __init__.py                    # Package initialization
â”œâ”€â”€ locodiff_evaluation.py         # Main evaluator (~690 lines)
â”œâ”€â”€ analyze_locodiff_results.py    # Results analyzer (~550 lines)
â”œâ”€â”€ test_setup.py                  # Setup verification (~220 lines)
â”œâ”€â”€ baseline_test.py               # Quick sanity check (~165 lines)
â””â”€â”€ README.md                      # Comprehensive documentation

locodiff_data/
â””â”€â”€ locodiff-250425/               # Dataset (198 examples)
    â”œâ”€â”€ prompts/                   # 200 prompt files
    â”‚   â”œâ”€â”€ *_prompt.txt
    â”‚   â””â”€â”€ *_expectedoutput.txt
    â””â”€â”€ results/                   # Reference results from paper

locodiff_results/                  # Output directory (auto-created)
â””â”€â”€ plots/                         # Visualization outputs
```

## ðŸ“Š Dataset Details

- **Total Examples**: 198 (out of 200 - 2 missing expected outputs)
- **Repositories**: 5 real-world codebases
  - ghostty (Zig): 40 examples
  - qdrant (Rust): 40 examples
  - react (JavaScript): 40 examples
  - tldraw (TypeScript): 40 examples
  - aider (Python): 38 examples

- **Languages**: Python, JavaScript, TypeScript, Rust, Zig

- **Context Lengths**:
  - Min: 1,994 tokens
  - Max: 97,521 tokens
  - Average: 40,765 tokens
  - Distribution:
    - 0-10k tokens: 17 examples (8.6%)
    - 10-25k tokens: 48 examples (24.2%)
    - 25-50k tokens: 58 examples (29.3%)
    - 50k+ tokens: 75 examples (37.9%)

## ðŸ”§ Key Features

### 1. Dual Evaluation Modes

**Baseline Mode**: Direct LLM calls for comparison
- Single API call with full git history
- No RLM processing
- Baseline for measuring RLM improvement

**RLM Mode**: Configurable partition/retrieval strategies
- Partition strategies: None, token, structural, semantic
- Retrieval methods: none, unfiltered, regex, embedding
- Parallel processing options

### 2. Comprehensive Metrics

**Primary Metric**: Exact Match Accuracy
- LoCoDiff uses strict exact matching (no partial credit)
- Character-for-character comparison after normalization

**Secondary Metrics**:
- Sequence similarity score (0-1)
- Processing time per example
- Number of LLM API calls
- Number of REPL iterations
- Recursion depth

**Debugging Info**:
- Unified diff between predicted and expected
- Count of added/removed lines
- Diff preview (first 50 lines)

### 3. Multi-Dimensional Analysis

**By Context Length** (Key LoCoDiff Insight):
```
Token Range  | Count | Accuracy
0-10k        | 17    | ??%
10-25k       | 48    | ??%
25-50k       | 58    | ??%
50k+         | 75    | ??%
```

**By Programming Language**:
- Python vs JavaScript vs TypeScript vs Rust vs Zig
- Which languages benefit most from RLM?

**By Repository**:
- Performance differences across codebases
- Repo-specific patterns

**RLM vs Baseline**:
- Side-by-side accuracy comparison
- Time/cost tradeoffs

### 4. Visualizations

- **Accuracy vs Context Length**: Line plot showing performance degradation
- **Configuration Comparison**: Bar charts of exact match accuracy
- **CSV Export**: Full results for custom analysis

## ðŸš€ How to Use

### Quick Test (2 examples)
```bash
source venv/bin/activate
python locodiff_eval/baseline_test.py
```

### Setup Verification
```bash
python locodiff_eval/test_setup.py
```

### Small Evaluation (5-10 examples)
```bash
# Edit locodiff_evaluation.py:
# - Set max_examples=10
# - Choose your model (gpt-4o, claude-sonnet-4, etc.)
python locodiff_eval/locodiff_evaluation.py
```

### Full Evaluation (all 198 examples)
```bash
# Edit locodiff_evaluation.py:
# - Set max_examples=None
# Warning: Takes time and costs API credits!
python locodiff_eval/locodiff_evaluation.py
```

### Analyze Results
```bash
python locodiff_eval/analyze_locodiff_results.py
```

## ðŸ“ˆ Research Questions

This benchmark helps answer:

1. **Does RLM improve git history understanding?**
   - Hypothesis: RLM's partitioning helps with long commit histories

2. **Which partition strategy works best for code diffs?**
   - Test: token vs structural vs semantic
   - Hypothesis: Structural (commit boundaries) may work best

3. **Does retrieval help with long git histories?**
   - Test: regex vs embedding vs unfiltered
   - Hypothesis: Embedding retrieval finds relevant commits

4. **How does performance degrade with context length?**
   - Key LoCoDiff insight: All models drop to <50% at 25k tokens
   - Question: Can RLM maintain higher accuracy at longer contexts?

5. **Language-specific insights**
   - Which languages benefit most from RLM partitioning?
   - Do verbose diffs (JavaScript/TypeScript) benefit more than terse ones (Rust)?

## ðŸ”„ Integration with Existing Infrastructure

### Follows OOLONGBench Pattern

The implementation mirrors the existing `oolongbench_eval/` structure:
- Similar evaluator class design
- Same metrics tracking approach
- Consistent analysis pipeline
- Parallel evaluation infrastructure

### Shared Components

Uses existing RLM infrastructure:
- `rlm.RLM` class with partition/retrieval config
- `rlm.partitions` for text partitioning
- `rlm.retrieval` for relevance scoring
- LiteLLM for model API calls

## âœ… Implementation Status

### Completed âœ“
- [x] Dataset downloaded (198/200 examples)
- [x] Main evaluator implemented
- [x] Results analyzer implemented
- [x] Setup verification script
- [x] Quick baseline test
- [x] Comprehensive documentation
- [x] Import errors fixed
- [x] API key handling fixed
- [x] Division by zero errors fixed
- [x] Multi-model support (OpenAI, Anthropic, Gemini)

### Ready for Testing
- [ ] Run baseline test (in progress)
- [ ] Validate exact match evaluation
- [ ] Run small evaluation (5-10 examples)
- [ ] Generate example visualizations
- [ ] Document initial findings

### Future Enhancements (Optional)
- [ ] Per-commit accuracy (track which commits are reconstructed correctly)
- [ ] Branch-aware partitioning (partition by git branch)
- [ ] Merge conflict handling analysis
- [ ] Integration with LoCoDiff leaderboard

## ðŸ“ Configuration Examples

### Using OpenAI
```python
evaluator = LoCoDiffEvaluator(
    model="gpt-4o",
    recursive_model="gpt-4o-mini"
)
```

### Using Anthropic
```python
evaluator = LoCoDiffEvaluator(
    model="claude-sonnet-4",
    recursive_model="claude-haiku-4"
)
```

### Using Google Gemini
```python
evaluator = LoCoDiffEvaluator(
    model="gemini/gemini-2.5-pro",
    recursive_model="gemini/gemini-2.5-flash"
)
```

### Custom Partition Config
```python
rlm = RLM(
    model="gpt-4o",
    partition_strategy="structural",
    retrieval_method="embedding",
    max_partition_tokens=4000,
    partition_overlap_tokens=200,
    parallel_subqueries=False
)
```

## ðŸŽ“ References

- **LoCoDiff Paper**: "Long-Context Code Diff Understanding" (AbanteAI)
- **LoCoDiff Website**: https://abanteai.github.io/LoCoDiff-bench/
- **GitHub**: https://github.com/AbanteAI/LoCoDiff-bench
- **RLM Paper**: "Recursive Language Models" (Zhang & Khattab, MIT 2025)

## ðŸ“Š Expected Results Format

### Summary Report
```
Configuration                                 Exact Match  Avg Time  Avg Calls
baseline                                      35.0%        15.2s     1.0
rlm_none_none_parallel=False                  42.0%        22.3s     3.5
rlm_token_unfiltered_parallel=False           48.0%        19.8s     3.2
rlm_structural_embedding_parallel=False       55.0%        21.1s     3.8
```

### By Context Length
```
Token Range  | Baseline | RLM (best) | Improvement
0-10k        | 85%      | 88%        | +3%
10-25k       | 60%      | 72%        | +12%
25-50k       | 35%      | 50%        | +15%
50k+         | 18%      | 35%        | +17%
```

*(Actual results TBD - run evaluation to get real numbers)*


## ðŸ“¦ Next Steps

1. **Run baseline test** to verify everything works
2. **Small evaluation** (5-10 examples) to test all configurations
3. **Analyze results** and generate visualizations
4. **Document findings** in results directory
5. **Full evaluation** on all 198 examples
6. **Compare with paper results** from LoCoDiff leaderboard

---

**Status**: âœ… Implementation Complete, Testing In Progress

**Last Updated**: November 19, 2025
